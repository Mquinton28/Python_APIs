 1/1:
# Running the basic "Hello World" code
hello = "Hello World"
print(hello)
 1/2:
# Doing simple math
4 + 4
 1/3:
# Storing results in variables
a = 5
 1/4:
# Using those variables elsewhere in the code
a
 1/5:
# Running the basic "Hello World" code
hello = "Hello World"
print(hello)
 2/1:
# Running the basic "Hello World" code
hello = "Hello World"
print(hello)
 2/2:
# Doing simple math
4 + 4
 2/3:
# Storing results in variables
a = 5
 2/4:
# Using those variables elsewhere in the code
a
 2/5:
# Variables will hold the value most recently run
# This means that, if we run the code above, it will now print 2
a = 2
 3/1:
# Running the basic "Hello World" code
hello = "Hello World"
print(hello)
 3/2:
# Doing simple math
4 + 4
 3/3:
# Storing results in variables
a = 5
 3/4:
# Using those variables elsewhere in the code
a
 3/5:
# Variables will hold the value most recently run
# This means that, if we run the code above, it will now print 2
a = 2
 5/1:
# Modules
import os
import csv
 5/2: video = input("What show or movie are you looking for? ")
 5/3: csvpath = os.path.join("Resources", "netflix_ratings.csv")
 5/4: found = False
 5/5:
for row in csvreader:
        if row[0] == video:
            print(row[0] + " is rated " + row[1] + " with a rating of " + row[6])
 5/6:
for row in csvreader:
        if row[0] == video:
            print(row[0] + " is rated " + row[1] + " with a rating of " + row[6])
 5/7:
for row in csvreader:
        if row[0] == video:
            print(row[0] + " is rated " + row[1] + " with a rating of " + row[6])
 6/1:
# Modules
import os
import csv
 6/2: video = input("What show or movie are you looking for? ")
 6/3: csvpath = os.path.join("Resources", "netflix_ratings.csv")
 6/4: found = False
 6/5:
with open(csvpath) as csvfile:
    csvreader = csv.reader(csvfile, delimiter=",")
 6/6:
for row in csvreader:
        if row[0] == video:
            print(row[0] + " is rated " + row[1] + " with a rating of " + row[6])
11/1:
# Modules
import os
import csv
11/2: video = input("What show or movie are you looking for? ")
11/3: csvpath = os.path.join("Resources", "netflix_ratings.csv")
11/4: found = False
11/5:
with open(csvpath) as csvfile:
    csvreader = csv.reader(csvfile, delimiter=",")
11/6:
for row in csvreader:
        if row[0] == video:
            print(row[0] + " is rated " + row[1] + " with a rating of " + row[6])
11/7:
# Modules
import os
import csv
11/8: video = input("What show or movie are you looking for? ")
13/1:
# Modules
import os
import csv
13/2: video = input("What show or movie are you looking for? ")
13/3: csvpath = os.path.join("Resources", "netflix_ratings.csv")
13/4: found = False
13/5:
with open(csvpath) as csvfile:
    csvreader = csv.reader(csvfile, delimiter=",")
13/6:
for row in csvreader:
        if row[0] == video:
            print(row[0] + " is rated " + row[1] + " with a rating of " + row[6])
13/7:
# Modules
import os
import csv
13/8: video = input("What show or movie are you looking for? ")
13/9: csvpath = os.path.join("Resources", "netflix_ratings.csv")
13/10: found = False
13/11:
with open(csvpath) as csvfile:
    csvreader = csv.reader(csvfile, delimiter=",")
13/12:
for row in csvreader:
        if row[0] == video:
            print(row[0] + " is rated " + row[1] + " with a rating of " + row[6])
14/1:
# Modules
import os
import csv
14/2: video = input("What show or movie are you looking for? ")
14/3: csvpath = os.path.join("Resources", "netflix_ratings.csv")
14/4: found = False
14/5:
with open(csvpath) as csvfile:
    csvreader = csv.reader(csvfile, delimiter=",")
14/6:
for row in csvreader:
        if row[0] == video:
            print(row[0] + " is rated " + row[1] + " with a rating of " + row[6])
15/1:
# Modules
import os
import csv
15/2: video = input("What show or movie are you looking for? ")
15/3: csvpath = os.path.join("Resources", "netflix_ratings.csv")
15/4: found = False
15/5:
with open(csvpath) as csvfile:
    csvreader = csv.reader(csvfile, delimiter=",")
15/6:
for row in csvreader:
        if row[0] == video:
            print(row[0] + " is rated " + row[1] + " with a rating of " + row[6])
16/1:
# Modules
import os
import csv
16/2: video = input("What show or movie are you looking for? ")
16/3: csvpath = os.path.join("Resources", "netflix_ratings.csv")
16/4: found = False
16/5:
with open(csvpath) as csvfile:
    csvreader = csv.reader(csvfile, delimiter=",")
16/6:
for row in csvreader:
        if row[0] == video:
            print(row[0] + " is rated " + row[1] + " with a rating of " + row[6])
            
            found = True
            
        if found is False:
             print("Sorry about this, we don't seem to have what you are looking for!")
17/1:
# Modules
import os
import csv
17/2: video = input("What show or movie are you looking for? ")
17/3: csvpath = os.path.join("Resources", "netflix_ratings.csv")
17/4: found = False
17/5:
with open(csvpath) as csvfile:
    csvreader = csv.reader(csvfile, delimiter=",")
17/6:
# Open the CSV
with open(csvpath) as csvfile:
    csvreader = csv.reader(csvfile, delimiter=",")

    # Loop through looking for the video
    for row in csvreader:
        if row[0] == video:
            print(row[0] + " is rated " + row[1] + " with a rating of " + row[6])

            # Set variable to confirm we have found the video
            found = True

    # If the video is never found, alert the user
    if found is False:
        print("Sorry about this, we don't seem to have what you are looking for!")
20/1:
# DataFrame should have 3 columns: Frame, Price, and Sales AND 5 rows of data
frame_dicts = [{"Frame": "Ornate", "Price": "15.0", "Sales": "100"},
              {"Frame": "Classical", "Price": "12.5", "Sales": "200"},
             {"Frame": "Modern", "Price": "10.0", "Sales": "150"},
             {"Frame": "Wood", "Price": "5.0", "Sales": "300"},
             {"Frame": "Cardboard", "Price": "1.0", "Sales": "N/A"}]

frame_df = pd.DataFrame(frame_dicts)
frame_df
20/2:
# Use a different method of creating DataFrames to
# Create a DataFrame for an art gallery that contains three columns - "Painting", "Price", and "Popularity"
# and has 4 rows of data
frametypes_df = pd.DataFrame({"Painting": ["Mona Lisa(Knockoff)", "Van Gough(Knockoff)", "Starving Artist", "Toddler Drawing"],
                            "Popularity": ["Very Popular", "Popular", "Average", "Not Popular"],
                             "Price": ["25", "20", "10", "1"]})
frametypes_df
20/3:
# Create a DataFrame of frames using a dictionary of lists
frame_df = pd.DataFrame({
    "Frame": ["Ornate", "Classical", "Modern", "Wood", "Cardboard"],
    "Price": [15.00, 12.50, 10.00, 5.00, 1.00],
    "Sales": [100, 200, 150, 300, "N/A"]
})
frame_df
20/4:
# Create a DataFrame of frames using a dictionary of lists
frame_df = pd.DataFrame({
    "Frame": ["Ornate", "Classical", "Modern", "Wood", "Cardboard"],
    "Price": [15.00, 12.50, 10.00, 5.00, 1.00],
    "Sales": [100, 200, 150, 300, "N/A"]
})
frame_df
22/1:
# Import Dependencies
import pandas as pd
22/2:
# Create a DataFrame of frames using a dictionary of lists
frame_df = pd.DataFrame({
    "Frame": ["Ornate", "Classical", "Modern", "Wood", "Cardboard"],
    "Price": [15.00, 12.50, 10.00, 5.00, 1.00],
    "Sales": [100, 200, 150, 300, "N/A"]
})
frame_df
22/3:
# Create a DataFrame of paintings using a list of dictionaries
painting_df = pd.DataFrame([
    {"Painting": "Mona Lisa (Knockoff)", "Price": 25,
     "Popularity": "Very Popular"},
    {"Painting": "Van Gogh (Knockoff)", "Price": 20, "Popularity": "Popular"},
    {"Painting": "Starving Artist", "Price": 10, "Popularity": "Average"},
    {"Painting": "Toddler Drawing", "Price": 1, "Popularity": "Not Popular"}
])
painting_df
23/1:
# Collecting a summary of all numeric data
training_df.describe()
23/2:
# Collecting a summary of all numeric data
training_df.describe()
25/1:
# Import Dependencies
import pandas as pd
25/2:
# A gigantic DataFrame of individuals' names, their trainers, their weight, and their days as gym members
training_df = pd.DataFrame({
    "Name":["Gino Walker","Hiedi Wasser","Kerrie Wetzel","Elizabeth Sackett","Jack Mitten","Madalene Wayman","Jamee Horvath","Arlena Reddin","Tula Levan","Teisha Dreier","Leslie Carrier","Arlette Hartson","Romana Merkle","Heath Viviani","Andres Zimmer","Allyson Osman","Yadira Caggiano","Jeanmarie Friedrichs","Leann Ussery","Bee Mom","Pandora Charland","Karena Wooten","Elizabet Albanese","Augusta Borjas","Erma Yadon","Belia Lenser","Karmen Sancho","Edison Mannion","Sonja Hornsby","Morgan Frei","Florencio Murphy","Christoper Hertel","Thalia Stepney","Tarah Argento","Nicol Canfield","Pok Moretti","Barbera Stallings","Muoi Kelso","Cicely Ritz","Sid Demelo","Eura Langan","Vanita An","Frieda Fuhr","Ernest Fitzhenry","Ashlyn Tash","Melodi Mclendon","Rochell Leblanc","Jacqui Reasons","Freeda Mccroy","Vanna Runk","Florinda Milot","Cierra Lecompte","Nancey Kysar","Latasha Dalton","Charlyn Rinaldi","Erline Averett","Mariko Hillary","Rosalyn Trigg","Sherwood Brauer","Hortencia Olesen","Delana Kohut","Geoffrey Mcdade","Iona Delancey","Donnie Read","Cesar Bhatia","Evia Slate","Kaye Hugo","Denise Vento","Lang Kittle","Sherry Whittenberg","Jodi Bracero","Tamera Linneman","Katheryn Koelling","Tonia Shorty","Misha Baxley","Lisbeth Goering","Merle Ladwig","Tammie Omar","Jesusa Avilla","Alda Zabala","Junita Dogan","Jessia Anglin","Peggie Scranton","Dania Clodfelter","Janis Mccarthy","Edmund Galusha","Tonisha Posey","Arvilla Medley","Briana Barbour","Delfina Kiger","Nia Lenig","Ricarda Bulow","Odell Carson","Nydia Clonts","Andree Resendez","Daniela Puma","Sherill Paavola","Gilbert Bloomquist","Shanon Mach","Justin Bangert","Arden Hokanson","Evelyne Bridge","Hee Simek","Ward Deangelis","Jodie Childs","Janis Boehme","Beaulah Glowacki","Denver Stoneham","Tarra Vinton","Deborah Hummell","Ulysses Neil","Kathryn Marques","Rosanna Dake","Gavin Wheat","Tameka Stoke","Janella Clear","Kaye Ciriaco","Suk Bloxham","Gracia Whaley","Philomena Hemingway","Claudette Vaillancourt","Olevia Piche","Trey Chiles","Idalia Scardina","Jenine Tremble","Herbert Krider","Alycia Schrock","Miss Weibel","Pearlene Neidert","Kina Callender","Charlotte Skelley","Theodora Harrigan","Sydney Shreffler","Annamae Trinidad","Tobi Mumme","Rosia Elliot","Debbra Putt","Rena Delosantos","Genna Grennan","Nieves Huf","Berry Lugo","Ayana Verdugo","Joaquin Mazzei","Doris Harmon","Patience Poss","Magaret Zabel","Marylynn Hinojos","Earlene Marcantel","Yuki Evensen","Rema Gay","Delana Haak","Patricia Fetters","Vinnie Elrod","Octavia Bellew","Burma Revard","Lakenya Kato","Vinita Buchner","Sierra Margulies","Shae Funderburg","Jenae Groleau","Louetta Howie","Astrid Duffer","Caron Altizer","Kymberly Amavisca","Mohammad Diedrich","Thora Wrinkle","Bethel Wiemann","Patria Millet","Eldridge Burbach","Alyson Eddie","Zula Hanna","Devin Goodwin","Felipa Kirkwood","Kurtis Kempf","Kasey Lenart","Deena Blankenship","Kandra Wargo","Sherrie Cieslak","Ron Atha","Reggie Barreiro","Daria Saulter","Tandra Eastman","Donnell Lucious","Talisha Rosner","Emiko Bergh","Terresa Launius","Margy Hoobler","Marylou Stelling","Lavonne Justice","Kala Langstaff","China Truett","Louanne Dussault","Thomasena Samaniego","Charlesetta Tarbell","Fatimah Lade","Malisa Cantero","Florencia Litten","Francina Fraise","Patsy London","Deloris Mclaughlin"],
    "Trainer":['Bettyann Savory','Mariah Barberio','Gordon Perrine','Pa Dargan','Blanch Victoria','Aldo Byler','Aldo Byler','Williams Camire','Junie Ritenour','Gordon Perrine','Bettyann Savory','Mariah Barberio','Aldo Byler','Barton Stecklein','Bettyann Savory','Barton Stecklein','Gordon Perrine','Pa Dargan','Aldo Byler','Brittani Brin','Bettyann Savory','Phyliss Houk','Bettyann Savory','Junie Ritenour','Aldo Byler','Calvin North','Brittani Brin','Junie Ritenour','Blanch Victoria','Brittani Brin','Bettyann Savory','Blanch Victoria','Mariah Barberio','Bettyann Savory','Blanch Victoria','Brittani Brin','Junie Ritenour','Pa Dargan','Gordon Perrine','Phyliss Houk','Pa Dargan','Mariah Barberio','Phyliss Houk','Phyliss Houk','Calvin North','Williams Camire','Brittani Brin','Gordon Perrine','Bettyann Savory','Bettyann Savory','Pa Dargan','Phyliss Houk','Barton Stecklein','Blanch Victoria','Coleman Dunmire','Phyliss Houk','Blanch Victoria','Pa Dargan','Harland Coolidge','Calvin North','Bettyann Savory','Phyliss Houk','Bettyann Savory','Harland Coolidge','Gordon Perrine','Junie Ritenour','Harland Coolidge','Blanch Victoria','Mariah Barberio','Coleman Dunmire','Aldo Byler','Bettyann Savory','Gordon Perrine','Bettyann Savory','Barton Stecklein','Harland Coolidge','Aldo Byler','Aldo Byler','Pa Dargan','Junie Ritenour','Brittani Brin','Junie Ritenour','Gordon Perrine','Mariah Barberio','Mariah Barberio','Mariah Barberio','Bettyann Savory','Brittani Brin','Aldo Byler','Phyliss Houk','Blanch Victoria','Pa Dargan','Phyliss Houk','Brittani Brin','Barton Stecklein','Coleman Dunmire','Bettyann Savory','Bettyann Savory','Gordon Perrine','Blanch Victoria','Junie Ritenour','Phyliss Houk','Coleman Dunmire','Williams Camire','Harland Coolidge','Williams Camire','Aldo Byler','Harland Coolidge','Gordon Perrine','Brittani Brin','Coleman Dunmire','Calvin North','Phyliss Houk','Brittani Brin','Aldo Byler','Bettyann Savory','Brittani Brin','Gordon Perrine','Calvin North','Harland Coolidge','Coleman Dunmire','Harland Coolidge','Aldo Byler','Junie Ritenour','Blanch Victoria','Harland Coolidge','Blanch Victoria','Junie Ritenour','Harland Coolidge','Junie Ritenour','Gordon Perrine','Brittani Brin','Coleman Dunmire','Williams Camire','Junie Ritenour','Brittani Brin','Calvin North','Barton Stecklein','Barton Stecklein','Mariah Barberio','Coleman Dunmire','Bettyann Savory','Mariah Barberio','Pa Dargan','Barton Stecklein','Coleman Dunmire','Brittani Brin','Barton Stecklein','Pa Dargan','Barton Stecklein','Junie Ritenour','Bettyann Savory','Williams Camire','Pa Dargan','Calvin North','Williams Camire','Coleman Dunmire','Aldo Byler','Barton Stecklein','Coleman Dunmire','Blanch Victoria','Mariah Barberio','Mariah Barberio','Harland Coolidge','Barton Stecklein','Phyliss Houk','Pa Dargan','Bettyann Savory','Barton Stecklein','Harland Coolidge','Junie Ritenour','Pa Dargan','Mariah Barberio','Blanch Victoria','Williams Camire','Phyliss Houk','Phyliss Houk','Coleman Dunmire','Mariah Barberio','Gordon Perrine','Coleman Dunmire','Brittani Brin','Pa Dargan','Coleman Dunmire','Brittani Brin','Blanch Victoria','Coleman Dunmire','Gordon Perrine','Coleman Dunmire','Aldo Byler','Aldo Byler','Mariah Barberio','Williams Camire','Phyliss Houk','Aldo Byler','Williams Camire','Aldo Byler','Williams Camire','Coleman Dunmire','Phyliss Houk'],
    "Weight":[128,180,193,177,237,166,224,208,177,241,114,161,162,151,220,142,193,193,124,130,132,141,190,239,213,131,172,127,184,157,215,122,181,240,218,205,239,217,234,158,180,131,194,171,177,110,117,114,217,123,248,189,198,127,182,121,224,111,151,170,188,150,137,231,222,186,139,175,178,246,150,154,129,216,144,198,228,183,173,129,157,199,186,232,172,157,246,239,214,161,132,208,187,224,164,177,175,224,219,235,112,241,243,179,208,196,131,207,182,233,191,162,173,197,190,182,231,196,196,143,250,174,138,135,164,204,235,192,114,179,215,127,185,213,250,213,153,217,176,190,119,167,118,208,113,206,200,236,159,218,168,159,156,183,121,203,215,209,179,219,174,220,129,188,217,250,166,157,112,236,182,144,189,243,238,147,165,115,160,134,245,174,238,157,150,184,174,134,134,248,199,165,117,119,162,112,170,224,247,217],
    "Membership(Days)":[52,70,148,124,186,157,127,155,37,185,158,129,93,69,124,13,76,153,164,161,48,121,167,69,39,163,7,34,176,169,108,162,195,86,155,77,197,200,80,142,179,67,58,145,188,147,125,15,13,173,125,4,61,29,132,110,62,137,197,135,162,174,32,151,149,65,18,42,63,62,104,200,189,40,38,199,1,12,8,2,195,30,7,72,130,144,2,34,200,143,43,196,22,115,171,54,143,59,14,52,109,115,187,185,26,19,178,18,120,169,45,52,130,69,168,178,96,22,78,152,39,51,118,130,60,156,108,69,103,158,165,142,86,91,117,77,57,169,86,188,97,111,22,83,81,177,163,35,12,164,21,181,171,138,22,107,58,51,38,128,19,193,157,13,104,89,13,10,26,190,179,101,7,159,100,49,120,109,56,199,51,108,47,171,69,162,74,119,148,88,32,159,65,146,140,171,88,18,59,13]
})
training_df.head(10)
25/3:
# Collecting a summary of all numeric data
training_df.describe()
25/4: # Finding the names of the trainers
25/5: # Finding how many students each trainer has
25/6: # Finding the average weight of all students
25/7: # Finding the combined weight of all students
25/8: # Converting the membership days into weeks and then adding a column to the DataFrame
26/1:
# Import Dependencies
import pandas as pd
26/2:
# A gigantic DataFrame of individuals' names, their trainers, their weight, and their days as gym members
training_df = pd.DataFrame({
    "Name":["Gino Walker","Hiedi Wasser","Kerrie Wetzel","Elizabeth Sackett","Jack Mitten","Madalene Wayman","Jamee Horvath","Arlena Reddin","Tula Levan","Teisha Dreier","Leslie Carrier","Arlette Hartson","Romana Merkle","Heath Viviani","Andres Zimmer","Allyson Osman","Yadira Caggiano","Jeanmarie Friedrichs","Leann Ussery","Bee Mom","Pandora Charland","Karena Wooten","Elizabet Albanese","Augusta Borjas","Erma Yadon","Belia Lenser","Karmen Sancho","Edison Mannion","Sonja Hornsby","Morgan Frei","Florencio Murphy","Christoper Hertel","Thalia Stepney","Tarah Argento","Nicol Canfield","Pok Moretti","Barbera Stallings","Muoi Kelso","Cicely Ritz","Sid Demelo","Eura Langan","Vanita An","Frieda Fuhr","Ernest Fitzhenry","Ashlyn Tash","Melodi Mclendon","Rochell Leblanc","Jacqui Reasons","Freeda Mccroy","Vanna Runk","Florinda Milot","Cierra Lecompte","Nancey Kysar","Latasha Dalton","Charlyn Rinaldi","Erline Averett","Mariko Hillary","Rosalyn Trigg","Sherwood Brauer","Hortencia Olesen","Delana Kohut","Geoffrey Mcdade","Iona Delancey","Donnie Read","Cesar Bhatia","Evia Slate","Kaye Hugo","Denise Vento","Lang Kittle","Sherry Whittenberg","Jodi Bracero","Tamera Linneman","Katheryn Koelling","Tonia Shorty","Misha Baxley","Lisbeth Goering","Merle Ladwig","Tammie Omar","Jesusa Avilla","Alda Zabala","Junita Dogan","Jessia Anglin","Peggie Scranton","Dania Clodfelter","Janis Mccarthy","Edmund Galusha","Tonisha Posey","Arvilla Medley","Briana Barbour","Delfina Kiger","Nia Lenig","Ricarda Bulow","Odell Carson","Nydia Clonts","Andree Resendez","Daniela Puma","Sherill Paavola","Gilbert Bloomquist","Shanon Mach","Justin Bangert","Arden Hokanson","Evelyne Bridge","Hee Simek","Ward Deangelis","Jodie Childs","Janis Boehme","Beaulah Glowacki","Denver Stoneham","Tarra Vinton","Deborah Hummell","Ulysses Neil","Kathryn Marques","Rosanna Dake","Gavin Wheat","Tameka Stoke","Janella Clear","Kaye Ciriaco","Suk Bloxham","Gracia Whaley","Philomena Hemingway","Claudette Vaillancourt","Olevia Piche","Trey Chiles","Idalia Scardina","Jenine Tremble","Herbert Krider","Alycia Schrock","Miss Weibel","Pearlene Neidert","Kina Callender","Charlotte Skelley","Theodora Harrigan","Sydney Shreffler","Annamae Trinidad","Tobi Mumme","Rosia Elliot","Debbra Putt","Rena Delosantos","Genna Grennan","Nieves Huf","Berry Lugo","Ayana Verdugo","Joaquin Mazzei","Doris Harmon","Patience Poss","Magaret Zabel","Marylynn Hinojos","Earlene Marcantel","Yuki Evensen","Rema Gay","Delana Haak","Patricia Fetters","Vinnie Elrod","Octavia Bellew","Burma Revard","Lakenya Kato","Vinita Buchner","Sierra Margulies","Shae Funderburg","Jenae Groleau","Louetta Howie","Astrid Duffer","Caron Altizer","Kymberly Amavisca","Mohammad Diedrich","Thora Wrinkle","Bethel Wiemann","Patria Millet","Eldridge Burbach","Alyson Eddie","Zula Hanna","Devin Goodwin","Felipa Kirkwood","Kurtis Kempf","Kasey Lenart","Deena Blankenship","Kandra Wargo","Sherrie Cieslak","Ron Atha","Reggie Barreiro","Daria Saulter","Tandra Eastman","Donnell Lucious","Talisha Rosner","Emiko Bergh","Terresa Launius","Margy Hoobler","Marylou Stelling","Lavonne Justice","Kala Langstaff","China Truett","Louanne Dussault","Thomasena Samaniego","Charlesetta Tarbell","Fatimah Lade","Malisa Cantero","Florencia Litten","Francina Fraise","Patsy London","Deloris Mclaughlin"],
    "Trainer":['Bettyann Savory','Mariah Barberio','Gordon Perrine','Pa Dargan','Blanch Victoria','Aldo Byler','Aldo Byler','Williams Camire','Junie Ritenour','Gordon Perrine','Bettyann Savory','Mariah Barberio','Aldo Byler','Barton Stecklein','Bettyann Savory','Barton Stecklein','Gordon Perrine','Pa Dargan','Aldo Byler','Brittani Brin','Bettyann Savory','Phyliss Houk','Bettyann Savory','Junie Ritenour','Aldo Byler','Calvin North','Brittani Brin','Junie Ritenour','Blanch Victoria','Brittani Brin','Bettyann Savory','Blanch Victoria','Mariah Barberio','Bettyann Savory','Blanch Victoria','Brittani Brin','Junie Ritenour','Pa Dargan','Gordon Perrine','Phyliss Houk','Pa Dargan','Mariah Barberio','Phyliss Houk','Phyliss Houk','Calvin North','Williams Camire','Brittani Brin','Gordon Perrine','Bettyann Savory','Bettyann Savory','Pa Dargan','Phyliss Houk','Barton Stecklein','Blanch Victoria','Coleman Dunmire','Phyliss Houk','Blanch Victoria','Pa Dargan','Harland Coolidge','Calvin North','Bettyann Savory','Phyliss Houk','Bettyann Savory','Harland Coolidge','Gordon Perrine','Junie Ritenour','Harland Coolidge','Blanch Victoria','Mariah Barberio','Coleman Dunmire','Aldo Byler','Bettyann Savory','Gordon Perrine','Bettyann Savory','Barton Stecklein','Harland Coolidge','Aldo Byler','Aldo Byler','Pa Dargan','Junie Ritenour','Brittani Brin','Junie Ritenour','Gordon Perrine','Mariah Barberio','Mariah Barberio','Mariah Barberio','Bettyann Savory','Brittani Brin','Aldo Byler','Phyliss Houk','Blanch Victoria','Pa Dargan','Phyliss Houk','Brittani Brin','Barton Stecklein','Coleman Dunmire','Bettyann Savory','Bettyann Savory','Gordon Perrine','Blanch Victoria','Junie Ritenour','Phyliss Houk','Coleman Dunmire','Williams Camire','Harland Coolidge','Williams Camire','Aldo Byler','Harland Coolidge','Gordon Perrine','Brittani Brin','Coleman Dunmire','Calvin North','Phyliss Houk','Brittani Brin','Aldo Byler','Bettyann Savory','Brittani Brin','Gordon Perrine','Calvin North','Harland Coolidge','Coleman Dunmire','Harland Coolidge','Aldo Byler','Junie Ritenour','Blanch Victoria','Harland Coolidge','Blanch Victoria','Junie Ritenour','Harland Coolidge','Junie Ritenour','Gordon Perrine','Brittani Brin','Coleman Dunmire','Williams Camire','Junie Ritenour','Brittani Brin','Calvin North','Barton Stecklein','Barton Stecklein','Mariah Barberio','Coleman Dunmire','Bettyann Savory','Mariah Barberio','Pa Dargan','Barton Stecklein','Coleman Dunmire','Brittani Brin','Barton Stecklein','Pa Dargan','Barton Stecklein','Junie Ritenour','Bettyann Savory','Williams Camire','Pa Dargan','Calvin North','Williams Camire','Coleman Dunmire','Aldo Byler','Barton Stecklein','Coleman Dunmire','Blanch Victoria','Mariah Barberio','Mariah Barberio','Harland Coolidge','Barton Stecklein','Phyliss Houk','Pa Dargan','Bettyann Savory','Barton Stecklein','Harland Coolidge','Junie Ritenour','Pa Dargan','Mariah Barberio','Blanch Victoria','Williams Camire','Phyliss Houk','Phyliss Houk','Coleman Dunmire','Mariah Barberio','Gordon Perrine','Coleman Dunmire','Brittani Brin','Pa Dargan','Coleman Dunmire','Brittani Brin','Blanch Victoria','Coleman Dunmire','Gordon Perrine','Coleman Dunmire','Aldo Byler','Aldo Byler','Mariah Barberio','Williams Camire','Phyliss Houk','Aldo Byler','Williams Camire','Aldo Byler','Williams Camire','Coleman Dunmire','Phyliss Houk'],
    "Weight":[128,180,193,177,237,166,224,208,177,241,114,161,162,151,220,142,193,193,124,130,132,141,190,239,213,131,172,127,184,157,215,122,181,240,218,205,239,217,234,158,180,131,194,171,177,110,117,114,217,123,248,189,198,127,182,121,224,111,151,170,188,150,137,231,222,186,139,175,178,246,150,154,129,216,144,198,228,183,173,129,157,199,186,232,172,157,246,239,214,161,132,208,187,224,164,177,175,224,219,235,112,241,243,179,208,196,131,207,182,233,191,162,173,197,190,182,231,196,196,143,250,174,138,135,164,204,235,192,114,179,215,127,185,213,250,213,153,217,176,190,119,167,118,208,113,206,200,236,159,218,168,159,156,183,121,203,215,209,179,219,174,220,129,188,217,250,166,157,112,236,182,144,189,243,238,147,165,115,160,134,245,174,238,157,150,184,174,134,134,248,199,165,117,119,162,112,170,224,247,217],
    "Membership(Days)":[52,70,148,124,186,157,127,155,37,185,158,129,93,69,124,13,76,153,164,161,48,121,167,69,39,163,7,34,176,169,108,162,195,86,155,77,197,200,80,142,179,67,58,145,188,147,125,15,13,173,125,4,61,29,132,110,62,137,197,135,162,174,32,151,149,65,18,42,63,62,104,200,189,40,38,199,1,12,8,2,195,30,7,72,130,144,2,34,200,143,43,196,22,115,171,54,143,59,14,52,109,115,187,185,26,19,178,18,120,169,45,52,130,69,168,178,96,22,78,152,39,51,118,130,60,156,108,69,103,158,165,142,86,91,117,77,57,169,86,188,97,111,22,83,81,177,163,35,12,164,21,181,171,138,22,107,58,51,38,128,19,193,157,13,104,89,13,10,26,190,179,101,7,159,100,49,120,109,56,199,51,108,47,171,69,162,74,119,148,88,32,159,65,146,140,171,88,18,59,13]
})
training_df.head(10)
26/3:
# Collecting a summary of all numeric data
training_df.describe()
26/4:
# Finding the names of the trainers
unique = training_df["Name"].unique()
unique
26/5:
# Finding how many students each trainer has
count = training_df["Trainer"].value_counts()
count
26/6:
# Finding the average weight of all students
average = training_df["Weight"].mean()
average
26/7:
# Finding the combined weight of all students
total = training_df["Weight"].sum()
26/8: # Converting the membership days into weeks and then adding a column to the DataFrame
26/9:
# Finding the combined weight of all students
total = training_df["Weight"].sum()
total
28/1:
# Import Dependencies
import pandas as pd
28/2:
# A gigantic DataFrame of individuals' names, their trainers, their weight, and their days as gym members
training_df = pd.DataFrame({
    "Name":["Gino Walker","Hiedi Wasser","Kerrie Wetzel","Elizabeth Sackett","Jack Mitten","Madalene Wayman","Jamee Horvath","Arlena Reddin","Tula Levan","Teisha Dreier","Leslie Carrier","Arlette Hartson","Romana Merkle","Heath Viviani","Andres Zimmer","Allyson Osman","Yadira Caggiano","Jeanmarie Friedrichs","Leann Ussery","Bee Mom","Pandora Charland","Karena Wooten","Elizabet Albanese","Augusta Borjas","Erma Yadon","Belia Lenser","Karmen Sancho","Edison Mannion","Sonja Hornsby","Morgan Frei","Florencio Murphy","Christoper Hertel","Thalia Stepney","Tarah Argento","Nicol Canfield","Pok Moretti","Barbera Stallings","Muoi Kelso","Cicely Ritz","Sid Demelo","Eura Langan","Vanita An","Frieda Fuhr","Ernest Fitzhenry","Ashlyn Tash","Melodi Mclendon","Rochell Leblanc","Jacqui Reasons","Freeda Mccroy","Vanna Runk","Florinda Milot","Cierra Lecompte","Nancey Kysar","Latasha Dalton","Charlyn Rinaldi","Erline Averett","Mariko Hillary","Rosalyn Trigg","Sherwood Brauer","Hortencia Olesen","Delana Kohut","Geoffrey Mcdade","Iona Delancey","Donnie Read","Cesar Bhatia","Evia Slate","Kaye Hugo","Denise Vento","Lang Kittle","Sherry Whittenberg","Jodi Bracero","Tamera Linneman","Katheryn Koelling","Tonia Shorty","Misha Baxley","Lisbeth Goering","Merle Ladwig","Tammie Omar","Jesusa Avilla","Alda Zabala","Junita Dogan","Jessia Anglin","Peggie Scranton","Dania Clodfelter","Janis Mccarthy","Edmund Galusha","Tonisha Posey","Arvilla Medley","Briana Barbour","Delfina Kiger","Nia Lenig","Ricarda Bulow","Odell Carson","Nydia Clonts","Andree Resendez","Daniela Puma","Sherill Paavola","Gilbert Bloomquist","Shanon Mach","Justin Bangert","Arden Hokanson","Evelyne Bridge","Hee Simek","Ward Deangelis","Jodie Childs","Janis Boehme","Beaulah Glowacki","Denver Stoneham","Tarra Vinton","Deborah Hummell","Ulysses Neil","Kathryn Marques","Rosanna Dake","Gavin Wheat","Tameka Stoke","Janella Clear","Kaye Ciriaco","Suk Bloxham","Gracia Whaley","Philomena Hemingway","Claudette Vaillancourt","Olevia Piche","Trey Chiles","Idalia Scardina","Jenine Tremble","Herbert Krider","Alycia Schrock","Miss Weibel","Pearlene Neidert","Kina Callender","Charlotte Skelley","Theodora Harrigan","Sydney Shreffler","Annamae Trinidad","Tobi Mumme","Rosia Elliot","Debbra Putt","Rena Delosantos","Genna Grennan","Nieves Huf","Berry Lugo","Ayana Verdugo","Joaquin Mazzei","Doris Harmon","Patience Poss","Magaret Zabel","Marylynn Hinojos","Earlene Marcantel","Yuki Evensen","Rema Gay","Delana Haak","Patricia Fetters","Vinnie Elrod","Octavia Bellew","Burma Revard","Lakenya Kato","Vinita Buchner","Sierra Margulies","Shae Funderburg","Jenae Groleau","Louetta Howie","Astrid Duffer","Caron Altizer","Kymberly Amavisca","Mohammad Diedrich","Thora Wrinkle","Bethel Wiemann","Patria Millet","Eldridge Burbach","Alyson Eddie","Zula Hanna","Devin Goodwin","Felipa Kirkwood","Kurtis Kempf","Kasey Lenart","Deena Blankenship","Kandra Wargo","Sherrie Cieslak","Ron Atha","Reggie Barreiro","Daria Saulter","Tandra Eastman","Donnell Lucious","Talisha Rosner","Emiko Bergh","Terresa Launius","Margy Hoobler","Marylou Stelling","Lavonne Justice","Kala Langstaff","China Truett","Louanne Dussault","Thomasena Samaniego","Charlesetta Tarbell","Fatimah Lade","Malisa Cantero","Florencia Litten","Francina Fraise","Patsy London","Deloris Mclaughlin"],
    "Trainer":['Bettyann Savory','Mariah Barberio','Gordon Perrine','Pa Dargan','Blanch Victoria','Aldo Byler','Aldo Byler','Williams Camire','Junie Ritenour','Gordon Perrine','Bettyann Savory','Mariah Barberio','Aldo Byler','Barton Stecklein','Bettyann Savory','Barton Stecklein','Gordon Perrine','Pa Dargan','Aldo Byler','Brittani Brin','Bettyann Savory','Phyliss Houk','Bettyann Savory','Junie Ritenour','Aldo Byler','Calvin North','Brittani Brin','Junie Ritenour','Blanch Victoria','Brittani Brin','Bettyann Savory','Blanch Victoria','Mariah Barberio','Bettyann Savory','Blanch Victoria','Brittani Brin','Junie Ritenour','Pa Dargan','Gordon Perrine','Phyliss Houk','Pa Dargan','Mariah Barberio','Phyliss Houk','Phyliss Houk','Calvin North','Williams Camire','Brittani Brin','Gordon Perrine','Bettyann Savory','Bettyann Savory','Pa Dargan','Phyliss Houk','Barton Stecklein','Blanch Victoria','Coleman Dunmire','Phyliss Houk','Blanch Victoria','Pa Dargan','Harland Coolidge','Calvin North','Bettyann Savory','Phyliss Houk','Bettyann Savory','Harland Coolidge','Gordon Perrine','Junie Ritenour','Harland Coolidge','Blanch Victoria','Mariah Barberio','Coleman Dunmire','Aldo Byler','Bettyann Savory','Gordon Perrine','Bettyann Savory','Barton Stecklein','Harland Coolidge','Aldo Byler','Aldo Byler','Pa Dargan','Junie Ritenour','Brittani Brin','Junie Ritenour','Gordon Perrine','Mariah Barberio','Mariah Barberio','Mariah Barberio','Bettyann Savory','Brittani Brin','Aldo Byler','Phyliss Houk','Blanch Victoria','Pa Dargan','Phyliss Houk','Brittani Brin','Barton Stecklein','Coleman Dunmire','Bettyann Savory','Bettyann Savory','Gordon Perrine','Blanch Victoria','Junie Ritenour','Phyliss Houk','Coleman Dunmire','Williams Camire','Harland Coolidge','Williams Camire','Aldo Byler','Harland Coolidge','Gordon Perrine','Brittani Brin','Coleman Dunmire','Calvin North','Phyliss Houk','Brittani Brin','Aldo Byler','Bettyann Savory','Brittani Brin','Gordon Perrine','Calvin North','Harland Coolidge','Coleman Dunmire','Harland Coolidge','Aldo Byler','Junie Ritenour','Blanch Victoria','Harland Coolidge','Blanch Victoria','Junie Ritenour','Harland Coolidge','Junie Ritenour','Gordon Perrine','Brittani Brin','Coleman Dunmire','Williams Camire','Junie Ritenour','Brittani Brin','Calvin North','Barton Stecklein','Barton Stecklein','Mariah Barberio','Coleman Dunmire','Bettyann Savory','Mariah Barberio','Pa Dargan','Barton Stecklein','Coleman Dunmire','Brittani Brin','Barton Stecklein','Pa Dargan','Barton Stecklein','Junie Ritenour','Bettyann Savory','Williams Camire','Pa Dargan','Calvin North','Williams Camire','Coleman Dunmire','Aldo Byler','Barton Stecklein','Coleman Dunmire','Blanch Victoria','Mariah Barberio','Mariah Barberio','Harland Coolidge','Barton Stecklein','Phyliss Houk','Pa Dargan','Bettyann Savory','Barton Stecklein','Harland Coolidge','Junie Ritenour','Pa Dargan','Mariah Barberio','Blanch Victoria','Williams Camire','Phyliss Houk','Phyliss Houk','Coleman Dunmire','Mariah Barberio','Gordon Perrine','Coleman Dunmire','Brittani Brin','Pa Dargan','Coleman Dunmire','Brittani Brin','Blanch Victoria','Coleman Dunmire','Gordon Perrine','Coleman Dunmire','Aldo Byler','Aldo Byler','Mariah Barberio','Williams Camire','Phyliss Houk','Aldo Byler','Williams Camire','Aldo Byler','Williams Camire','Coleman Dunmire','Phyliss Houk'],
    "Weight":[128,180,193,177,237,166,224,208,177,241,114,161,162,151,220,142,193,193,124,130,132,141,190,239,213,131,172,127,184,157,215,122,181,240,218,205,239,217,234,158,180,131,194,171,177,110,117,114,217,123,248,189,198,127,182,121,224,111,151,170,188,150,137,231,222,186,139,175,178,246,150,154,129,216,144,198,228,183,173,129,157,199,186,232,172,157,246,239,214,161,132,208,187,224,164,177,175,224,219,235,112,241,243,179,208,196,131,207,182,233,191,162,173,197,190,182,231,196,196,143,250,174,138,135,164,204,235,192,114,179,215,127,185,213,250,213,153,217,176,190,119,167,118,208,113,206,200,236,159,218,168,159,156,183,121,203,215,209,179,219,174,220,129,188,217,250,166,157,112,236,182,144,189,243,238,147,165,115,160,134,245,174,238,157,150,184,174,134,134,248,199,165,117,119,162,112,170,224,247,217],
    "Membership(Days)":[52,70,148,124,186,157,127,155,37,185,158,129,93,69,124,13,76,153,164,161,48,121,167,69,39,163,7,34,176,169,108,162,195,86,155,77,197,200,80,142,179,67,58,145,188,147,125,15,13,173,125,4,61,29,132,110,62,137,197,135,162,174,32,151,149,65,18,42,63,62,104,200,189,40,38,199,1,12,8,2,195,30,7,72,130,144,2,34,200,143,43,196,22,115,171,54,143,59,14,52,109,115,187,185,26,19,178,18,120,169,45,52,130,69,168,178,96,22,78,152,39,51,118,130,60,156,108,69,103,158,165,142,86,91,117,77,57,169,86,188,97,111,22,83,81,177,163,35,12,164,21,181,171,138,22,107,58,51,38,128,19,193,157,13,104,89,13,10,26,190,179,101,7,159,100,49,120,109,56,199,51,108,47,171,69,162,74,119,148,88,32,159,65,146,140,171,88,18,59,13]
})
training_df.head(10)
28/3:
# Collecting a summary of all numeric data
training_df.describe()
28/4:
# Finding the names of the trainers
unique = training_df["Name"].unique()
unique
28/5:
# Finding how many students each trainer has
count = training_df["Trainer"].value_counts()
count
28/6:
# Finding the average weight of all students
average = training_df["Weight"].mean()
average
28/7:
# Finding the combined weight of all students
total = training_df["Weight"].sum()
total
28/8:
# Converting the membership days into weeks and then adding a column to the DataFrame
weeks = training_df["Membership (Days)"]/7
training_df["Membership (Weeks)"] = weeks

training_df.head()
28/9:
# Converting the membership days into weeks and then adding a column to the DataFrame
weeks = training_df["Membership (Days)"]/7
training_df["Membership (Weeks)"] = weeks

training_df.head()
28/10:
# Converting the membership days into weeks and then adding a column to the DataFrame
weeks = training_df["Membership(Days)"]/7
training_df["Membership(Weeks)"] = weeks

training_df.head()
29/1:
# Import Dependencies
import pandas as pd
29/2:
# A gigantic DataFrame of individuals' names, their trainers, their weight, and their days as gym members
training_df = pd.DataFrame({
    "Name":["Gino Walker","Hiedi Wasser","Kerrie Wetzel","Elizabeth Sackett","Jack Mitten","Madalene Wayman","Jamee Horvath","Arlena Reddin","Tula Levan","Teisha Dreier","Leslie Carrier","Arlette Hartson","Romana Merkle","Heath Viviani","Andres Zimmer","Allyson Osman","Yadira Caggiano","Jeanmarie Friedrichs","Leann Ussery","Bee Mom","Pandora Charland","Karena Wooten","Elizabet Albanese","Augusta Borjas","Erma Yadon","Belia Lenser","Karmen Sancho","Edison Mannion","Sonja Hornsby","Morgan Frei","Florencio Murphy","Christoper Hertel","Thalia Stepney","Tarah Argento","Nicol Canfield","Pok Moretti","Barbera Stallings","Muoi Kelso","Cicely Ritz","Sid Demelo","Eura Langan","Vanita An","Frieda Fuhr","Ernest Fitzhenry","Ashlyn Tash","Melodi Mclendon","Rochell Leblanc","Jacqui Reasons","Freeda Mccroy","Vanna Runk","Florinda Milot","Cierra Lecompte","Nancey Kysar","Latasha Dalton","Charlyn Rinaldi","Erline Averett","Mariko Hillary","Rosalyn Trigg","Sherwood Brauer","Hortencia Olesen","Delana Kohut","Geoffrey Mcdade","Iona Delancey","Donnie Read","Cesar Bhatia","Evia Slate","Kaye Hugo","Denise Vento","Lang Kittle","Sherry Whittenberg","Jodi Bracero","Tamera Linneman","Katheryn Koelling","Tonia Shorty","Misha Baxley","Lisbeth Goering","Merle Ladwig","Tammie Omar","Jesusa Avilla","Alda Zabala","Junita Dogan","Jessia Anglin","Peggie Scranton","Dania Clodfelter","Janis Mccarthy","Edmund Galusha","Tonisha Posey","Arvilla Medley","Briana Barbour","Delfina Kiger","Nia Lenig","Ricarda Bulow","Odell Carson","Nydia Clonts","Andree Resendez","Daniela Puma","Sherill Paavola","Gilbert Bloomquist","Shanon Mach","Justin Bangert","Arden Hokanson","Evelyne Bridge","Hee Simek","Ward Deangelis","Jodie Childs","Janis Boehme","Beaulah Glowacki","Denver Stoneham","Tarra Vinton","Deborah Hummell","Ulysses Neil","Kathryn Marques","Rosanna Dake","Gavin Wheat","Tameka Stoke","Janella Clear","Kaye Ciriaco","Suk Bloxham","Gracia Whaley","Philomena Hemingway","Claudette Vaillancourt","Olevia Piche","Trey Chiles","Idalia Scardina","Jenine Tremble","Herbert Krider","Alycia Schrock","Miss Weibel","Pearlene Neidert","Kina Callender","Charlotte Skelley","Theodora Harrigan","Sydney Shreffler","Annamae Trinidad","Tobi Mumme","Rosia Elliot","Debbra Putt","Rena Delosantos","Genna Grennan","Nieves Huf","Berry Lugo","Ayana Verdugo","Joaquin Mazzei","Doris Harmon","Patience Poss","Magaret Zabel","Marylynn Hinojos","Earlene Marcantel","Yuki Evensen","Rema Gay","Delana Haak","Patricia Fetters","Vinnie Elrod","Octavia Bellew","Burma Revard","Lakenya Kato","Vinita Buchner","Sierra Margulies","Shae Funderburg","Jenae Groleau","Louetta Howie","Astrid Duffer","Caron Altizer","Kymberly Amavisca","Mohammad Diedrich","Thora Wrinkle","Bethel Wiemann","Patria Millet","Eldridge Burbach","Alyson Eddie","Zula Hanna","Devin Goodwin","Felipa Kirkwood","Kurtis Kempf","Kasey Lenart","Deena Blankenship","Kandra Wargo","Sherrie Cieslak","Ron Atha","Reggie Barreiro","Daria Saulter","Tandra Eastman","Donnell Lucious","Talisha Rosner","Emiko Bergh","Terresa Launius","Margy Hoobler","Marylou Stelling","Lavonne Justice","Kala Langstaff","China Truett","Louanne Dussault","Thomasena Samaniego","Charlesetta Tarbell","Fatimah Lade","Malisa Cantero","Florencia Litten","Francina Fraise","Patsy London","Deloris Mclaughlin"],
    "Trainer":['Bettyann Savory','Mariah Barberio','Gordon Perrine','Pa Dargan','Blanch Victoria','Aldo Byler','Aldo Byler','Williams Camire','Junie Ritenour','Gordon Perrine','Bettyann Savory','Mariah Barberio','Aldo Byler','Barton Stecklein','Bettyann Savory','Barton Stecklein','Gordon Perrine','Pa Dargan','Aldo Byler','Brittani Brin','Bettyann Savory','Phyliss Houk','Bettyann Savory','Junie Ritenour','Aldo Byler','Calvin North','Brittani Brin','Junie Ritenour','Blanch Victoria','Brittani Brin','Bettyann Savory','Blanch Victoria','Mariah Barberio','Bettyann Savory','Blanch Victoria','Brittani Brin','Junie Ritenour','Pa Dargan','Gordon Perrine','Phyliss Houk','Pa Dargan','Mariah Barberio','Phyliss Houk','Phyliss Houk','Calvin North','Williams Camire','Brittani Brin','Gordon Perrine','Bettyann Savory','Bettyann Savory','Pa Dargan','Phyliss Houk','Barton Stecklein','Blanch Victoria','Coleman Dunmire','Phyliss Houk','Blanch Victoria','Pa Dargan','Harland Coolidge','Calvin North','Bettyann Savory','Phyliss Houk','Bettyann Savory','Harland Coolidge','Gordon Perrine','Junie Ritenour','Harland Coolidge','Blanch Victoria','Mariah Barberio','Coleman Dunmire','Aldo Byler','Bettyann Savory','Gordon Perrine','Bettyann Savory','Barton Stecklein','Harland Coolidge','Aldo Byler','Aldo Byler','Pa Dargan','Junie Ritenour','Brittani Brin','Junie Ritenour','Gordon Perrine','Mariah Barberio','Mariah Barberio','Mariah Barberio','Bettyann Savory','Brittani Brin','Aldo Byler','Phyliss Houk','Blanch Victoria','Pa Dargan','Phyliss Houk','Brittani Brin','Barton Stecklein','Coleman Dunmire','Bettyann Savory','Bettyann Savory','Gordon Perrine','Blanch Victoria','Junie Ritenour','Phyliss Houk','Coleman Dunmire','Williams Camire','Harland Coolidge','Williams Camire','Aldo Byler','Harland Coolidge','Gordon Perrine','Brittani Brin','Coleman Dunmire','Calvin North','Phyliss Houk','Brittani Brin','Aldo Byler','Bettyann Savory','Brittani Brin','Gordon Perrine','Calvin North','Harland Coolidge','Coleman Dunmire','Harland Coolidge','Aldo Byler','Junie Ritenour','Blanch Victoria','Harland Coolidge','Blanch Victoria','Junie Ritenour','Harland Coolidge','Junie Ritenour','Gordon Perrine','Brittani Brin','Coleman Dunmire','Williams Camire','Junie Ritenour','Brittani Brin','Calvin North','Barton Stecklein','Barton Stecklein','Mariah Barberio','Coleman Dunmire','Bettyann Savory','Mariah Barberio','Pa Dargan','Barton Stecklein','Coleman Dunmire','Brittani Brin','Barton Stecklein','Pa Dargan','Barton Stecklein','Junie Ritenour','Bettyann Savory','Williams Camire','Pa Dargan','Calvin North','Williams Camire','Coleman Dunmire','Aldo Byler','Barton Stecklein','Coleman Dunmire','Blanch Victoria','Mariah Barberio','Mariah Barberio','Harland Coolidge','Barton Stecklein','Phyliss Houk','Pa Dargan','Bettyann Savory','Barton Stecklein','Harland Coolidge','Junie Ritenour','Pa Dargan','Mariah Barberio','Blanch Victoria','Williams Camire','Phyliss Houk','Phyliss Houk','Coleman Dunmire','Mariah Barberio','Gordon Perrine','Coleman Dunmire','Brittani Brin','Pa Dargan','Coleman Dunmire','Brittani Brin','Blanch Victoria','Coleman Dunmire','Gordon Perrine','Coleman Dunmire','Aldo Byler','Aldo Byler','Mariah Barberio','Williams Camire','Phyliss Houk','Aldo Byler','Williams Camire','Aldo Byler','Williams Camire','Coleman Dunmire','Phyliss Houk'],
    "Weight":[128,180,193,177,237,166,224,208,177,241,114,161,162,151,220,142,193,193,124,130,132,141,190,239,213,131,172,127,184,157,215,122,181,240,218,205,239,217,234,158,180,131,194,171,177,110,117,114,217,123,248,189,198,127,182,121,224,111,151,170,188,150,137,231,222,186,139,175,178,246,150,154,129,216,144,198,228,183,173,129,157,199,186,232,172,157,246,239,214,161,132,208,187,224,164,177,175,224,219,235,112,241,243,179,208,196,131,207,182,233,191,162,173,197,190,182,231,196,196,143,250,174,138,135,164,204,235,192,114,179,215,127,185,213,250,213,153,217,176,190,119,167,118,208,113,206,200,236,159,218,168,159,156,183,121,203,215,209,179,219,174,220,129,188,217,250,166,157,112,236,182,144,189,243,238,147,165,115,160,134,245,174,238,157,150,184,174,134,134,248,199,165,117,119,162,112,170,224,247,217],
    "Membership(Days)":[52,70,148,124,186,157,127,155,37,185,158,129,93,69,124,13,76,153,164,161,48,121,167,69,39,163,7,34,176,169,108,162,195,86,155,77,197,200,80,142,179,67,58,145,188,147,125,15,13,173,125,4,61,29,132,110,62,137,197,135,162,174,32,151,149,65,18,42,63,62,104,200,189,40,38,199,1,12,8,2,195,30,7,72,130,144,2,34,200,143,43,196,22,115,171,54,143,59,14,52,109,115,187,185,26,19,178,18,120,169,45,52,130,69,168,178,96,22,78,152,39,51,118,130,60,156,108,69,103,158,165,142,86,91,117,77,57,169,86,188,97,111,22,83,81,177,163,35,12,164,21,181,171,138,22,107,58,51,38,128,19,193,157,13,104,89,13,10,26,190,179,101,7,159,100,49,120,109,56,199,51,108,47,171,69,162,74,119,148,88,32,159,65,146,140,171,88,18,59,13]
})
training_df.head(10)
29/3:
# Collecting a summary of all numeric data
training_df.describe()
29/4:
# Finding the names of the trainers
unique = training_df["Name"].unique()
unique
29/5:
# Finding how many students each trainer has
count = training_df["Trainer"].value_counts()
count
29/6:
# Finding the average weight of all students
average = training_df["Weight"].mean()
average
29/7:
# Finding the combined weight of all students
total = training_df["Weight"].sum()
total
29/8:
# Converting the membership days into weeks and then adding a column to the DataFrame
weeks = training_df["Membership(Days)"]/7
training_df["Membership(Weeks)"] = weeks.round(2)

training_df.head()
32/1:
# import dependencies
import pandas as pd
32/2:
# Create a data frame with given columns and value
hey_arnold_df = pd.DataFrame(
    {"Character_in_show": ["Arnold", "Gerald", "Helga", "Phoebe", "Harold", "Eugene"],
     "color_of_hair": ["blonde", "black", "blonde", "black", "unknown", "red"],
     "Height": ["average", "tallish", "tallish", "short", "tall", "short"],
     "Football_Shaped_Head": [True, False, False, False, False, False]
     })

hey_arnold_df
32/3:
# Rename columns to clean up the look
hey_arnold_renamed_df = hey_arnold_renamed_df.rename(columns={"Character_in_show": "Character",
                                                "color_of_hair": "Hair Color",
                                                "Height": "Height",
                                                "Football_Shaped_Head": "Football Head"})
hey_arnold_renamed_df
34/1:
# import dependencies
import pandas as pd
34/2:
# Create a data frame with given columns and value
hey_arnold_df = pd.DataFrame(
    {"Character_in_show": ["Arnold", "Gerald", "Helga", "Phoebe", "Harold", "Eugene"],
     "color_of_hair": ["blonde", "black", "blonde", "black", "unknown", "red"],
     "Height": ["average", "tallish", "tallish", "short", "tall", "short"],
     "Football_Shaped_Head": [True, False, False, False, False, False]
     })

hey_arnold_df
34/3:
# Rename columns to clean up the look
hey_arnold_renamed_df = hey_arnold_renamed_df.rename(columns={"Character_in_show": "Character",
                                                "color_of_hair": "Hair Color",
                                                "Height": "Height",
                                                "Football_Shaped_Head": "Football Head"})
hey_arnold_renamed_df
34/4:
# Rename columns to clean up the look
hey_arnold_renamed_df = hey_arnold_renamed_df.rename(columns={"Character_in_show":"Character",
                                                "color_of_hair": "Hair Color",
                                                "Height": "Height",
                                                "Football_Shaped_Head":"Football Head"})
hey_arnold_renamed_df
34/5:
# Organize columns into a more logical order
hey_arnold_alphabetical_df = hey_arnold_renamed_df[[
    "Character", "Football Head", "Hair Color", "Height"]]

hey_arnold_alphabetical_df
35/1:
# import dependencies
import pandas as pd
35/2:
# Create a data frame with given columns and value
hey_arnold_df = pd.DataFrame(
    {"Character_in_show": ["Arnold", "Gerald", "Helga", "Phoebe", "Harold", "Eugene"],
     "color_of_hair": ["blonde", "black", "blonde", "black", "unknown", "red"],
     "Height": ["average", "tallish", "tallish", "short", "tall", "short"],
     "Football_Shaped_Head": [True, False, False, False, False, False]
     })

hey_arnold_df
35/3:
# Rename columns to clean up the look
hey_arnold_renamed_df = hey_arnold_renamed_df.rename(columns={"Character_in_show":"Character",
                                                "color_of_hair": "Hair Color",
                                                "Height": "Height",
                                                "Football_Shaped_Head":"Football Head"})
hey_arnold_renamed_df
35/4:
# Organize columns into a more logical order
hey_arnold_alphabetical_df = hey_arnold_renamed_df[["Character", "Football Head", "Hair Color", "Height"]]

hey_arnold_alphabetical_df
36/1:
# Dependencies
import pandas as pd
36/2:
# Create a DataFrame with given columns and value
hey_arnold_df = pd.DataFrame(
    {"Character_in_show": ["Arnold", "Gerald", "Helga", "Phoebe", "Harold", "Eugene"],
     "color_of_hair": ["blonde", "black", "blonde", "black", "unknown", "red"],
     "Height": ["average", "tallish", "tallish", "short", "tall", "short"],
     "Football_Shaped_Head": [True, False, False, False, False, False]
     })

hey_arnold_df
36/3:
# Rename columns for readability
hey_arnold_renamed_df = hey_arnold_df.rename(columns={"Character_in_show": "Character",
                                                "color_of_hair": "Hair Color",
                                                "Height": "Height",
                                                "Football_Shaped_Head": "Football Head"
                                                })
hey_arnold_renamed_df
36/4:
# Organize the columns so they are in a more logical order
hey_arnold_alphabetical_df = hey_arnold_renamed_df[[
    "Character", "Football Head", "Hair Color", "Height"]]

hey_arnold_alphabetical_df
38/1:
# Dependencies
import pandas as pd
38/2:
# Create a DataFrame with given columns and value
hey_arnold_df = pd.DataFrame(
    {"Character_in_show": ["Arnold", "Gerald", "Helga", "Phoebe", "Harold", "Eugene"],
     "color_of_hair": ["blonde", "black", "blonde", "black", "unknown", "red"],
     "Height": ["average", "tallish", "tallish", "short", "tall", "short"],
     "Football_Shaped_Head": [True, False, False, False, False, False]
     })

hey_arnold_df
38/3:
# Rename columns for readability
hey_arnold_renamed_df = hey_arnold_df.rename(columns={"Character_in_show": "Character",
                                                "color_of_hair": "Hair Color",
                                                "Height": "Height",
                                                "Football_Shaped_Head": "Football Head"
                                                })
hey_arnold_renamed_df
38/4:
# Organize the columns so they are in a more logical order
hey_arnold_alphabetical_df = hey_arnold_renamed_df[[
    "Character", "Football Head", "Hair Color", "Height"]]

hey_arnold_alphabetical_df
38/5:
# Rename columns for readability
hey_arnold_renamed_df = hey_arnold_df.rename(columns={"Character_in_show": "Character",
                                                "color_of_hair": "Hair Color",
                                                "Height": "Height",
                                                "Football_Shaped_Head": "Football Head?"
                                                })
hey_arnold_renamed_df
38/6:
# Organize the columns so they are in a more logical order
hey_arnold_alphabetical_df = hey_arnold_renamed_df[[
    "Character", "Football Head?", "Hair Color", "Height"]]

hey_arnold_alphabetical_df
42/1:
# Import Dependencies
import pandas as pd
42/2:
# Make a reference to the books.csv file path
csv_path = "Resources/books.csv"
# Import the books.csv file as a DataFrame
books_df = pd.read_csv(csv_path, encoding="utf-8")
books_df.head()
42/3:
# Remove unecessary columns from the DataFrame and save the new DataFrame
# Only keep: "isbn", "original_publication_year", "original_title", "authors",
# "ratings_1", "ratings_2", "ratings_3", "ratings_4", "ratings_5"
reduced_df = books_df[["isbn", "original_publication_year", "original_title", "authors",
                       "ratings_1", "ratings_2", "ratings_3", "ratings_4", "ratings_5"]]
reduced_df
42/4:
# Rename the headers to be more explanatory
renamed_df = reduced_df.rename(columns={"isbn": "ISBN",
                                        "original_title": "Original Title",
                                        "original_publication_year": "Publication Year",
                                        "authors": "Authors",
                                        "ratings_1": "One Star Reviews",
                                        "ratings_2": "Two Star Reviews",
                                        "ratings_3": "Three Star Reviews",
                                        "ratings_4": "Four Star Reviews",
                                        "ratings_5": "Five Star Reviews",})
renamed_df.head()
42/5:
# Push the remade DataFrame to a new CSV file
renamed_df.to_csv("Output/books_clean.csv",
                 encoding="utf-8", index=False, header=True)
46/1:
# Import Dependencies
import pandas as pd
46/2:
# File to Load
goodreads_path = "Resources/books_clean.csv"

# Read the modified GoodReads csv and store into Pandas DataFrame
goodreads_df = pd.read_csv(goodreads_path, encoding="utf-8")
goodreads_df.head()
46/3:
# Calculate the number of unique authors in the DataFrame
author_count = len(goodreads_df["Authors"].unique())

# Calculate the earliest/latest year a book was published
earliest_year = goodreads_df["Publication Year"].min()
latest_year = goodreads_df["Publication Year"].max()


# Calculate the total reviews for the entire dataset
# Hint: use the pandas' sum() method to get the sum for each row
goodreads_df['Total Reviews'] = goodreads_df.iloc[:, 4:].sum(axis=1)
total_reviews = sum(goodreads_df['Total Rreviewes'])
46/4:
# Calculate the number of unique authors in the DataFrame
author_count = len(goodreads_df["Authors"].unique())

# Calculate the earliest/latest year a book was published
earliest_year = goodreads_df["Publication Year"].min()
latest_year = goodreads_df["Publication Year"].max()


# Calculate the total reviews for the entire dataset
# Hint: use the pandas' sum() method to get the sum for each row
goodreads_df['Total Reviews'] = goodreads_df.iloc[:, 4:].sum(axis=1)
total_reviews = sum(goodreads_df['Total Rreviewes'])
46/5:
# Place all of the data found into a summary DataFrame
summary_df = pd.DataFrame({"Total Unique Authors": [author_count],
                          "Earliest Year": earlriest_year,
                          "Latest Year": latest_year,
                          "Total Reviews": total_reviewes})
summary_df
48/1:
# Import Dependencies
import pandas as pd
48/2:
# File to Load
goodreads_path = "Resources/books_clean.csv"

# Read the modified GoodReads csv and store into Pandas DataFrame
goodreads_df = pd.read_csv(goodreads_path, encoding="utf-8")
goodreads_df.head()
48/3:
# Calculate the number of unique authors in the DataFrame
author_count = len(goodreads_df["Authors"].unique())

# Calculate the earliest/latest year a book was published
earliest_year = goodreads_df["Publication Year"].min()
latest_year = goodreads_df["Publication Year"].max()


# Calculate the total reviews for the entire dataset
# Hint: use the pandas' sum() method to get the sum for each row
goodreads_df['Total Reviews'] = goodreads_df.iloc[:, 4:].sum(axis=1)
total_reviews = sum(goodreads_df['Total Rreviewes'])
49/1:
# Import Dependencies
import pandas as pd
49/2:
# File to Load
goodreads_path = "Resources/books_clean.csv"

# Read the modified GoodReads csv and store into Pandas DataFrame
goodreads_df = pd.read_csv(goodreads_path, encoding="utf-8")
goodreads_df.head()
49/3:
# Calculate the number of unique authors in the DataFrame
author_count = len(goodreads_df["Authors"].unique())

# Calculate the earliest/latest year a book was published
earliest_year = goodreads_df["Publication Year"].min()
latest_year = goodreads_df["Publication Year"].max()

# Calculate the total reviews for the entire dataset
# Hint: use the pandas' sum() method to get the sum for each row
goodreads_df['Total Reviews'] = goodreads_df.iloc[:, 4:].sum(axis=1)
total_reviews = sum(goodreads_df['Total Reviews'])
49/4:
# Place all of the data found into a summary DataFrame
summary_df = pd.DataFrame({"Total Unique Authors": [author_count],
                              "Earliest Year": earliest_year,
                              "Latest Year": latest_year,
                              "Total Reviews": total_reviews})
summary_df
48/4:
# Calculate the number of unique authors in the DataFrame
author_count = len(goodreads_df["Authors"].unique())

# Calculate the earliest/latest year a book was published
earliest_year = goodreads_df["Publication Year"].min()
latest_year = goodreads_df["Publication Year"].max()


# Calculate the total reviews for the entire dataset
# Hint: use the pandas' sum() method to get the sum for each row
goodreads_df['Total Reviews'] = goodreads_df.iloc[:, 4:].sum(axis=1)
total_reviews = sum(goodreads_df['Total Reviews'])
48/5:
# Place all of the data found into a summary DataFrame
summary_df = pd.DataFrame({"Total Unique Authors": [author_count],
                          "Earliest Year": earlriest_year,
                          "Latest Year": latest_year,
                          "Total Reviews": total_reviewes})
summary_df
48/6:
# Place all of the data found into a summary DataFrame
summary_df = pd.DataFrame({"Total Unique Authors": [author_count],
                          "Earliest Year": earliest_year,
                          "Latest Year": latest_year,
                          "Total Reviews": total_reviews})
summary_df
51/1:
# Read and display with pandas
original_df = pd.read_csv(file)
original_df.head()
53/1:
# Dependencies
import pandas as pd
53/2:
# Load in File from resources
# 'movie_scores.csv'
file = "Resources/movie_scores.csv"
53/3:
# Read and display with pandas
original_df = pd.read_csv(file)
original_df.head()
53/4: # List all the columns the table provides
53/5: # only care about Imdb, so create a new table that takes the Film and all the columns relating to IMDB
53/6: # List only movies whose ratings are over 7 (out of 10) in IMDB
53/7: # Find lesser-known movies to watch, with fewer than 20K votes
53/8: # Finally, export this file to an Excel spreadsheet -- without the DataFrame index.
53/9:
# List all the columns the table provides
goodMovies_columns = df.iloc[0:1, 0:23]
53/10:
# List all the columns the table provides
movie_file_df.columns
55/1:
# Dependencies
import pandas as pd
55/2:
# Load in File from resources
# 'movie_scores.csv'
movie_file = "Resources/movie_scores.csv"
55/3:
# Read and display with pandas
movie_file_df = pd.read_csv(movie_file)
movie_file_df.head()
55/4:
# List all the columns the table provides
movie_file_df.columns
55/5: # only care about Imdb, so create a new table that takes the Film and all the columns relating to IMDB
55/6: # List only movies whose ratings are over 7 (out of 10) in IMDB
55/7: # Find lesser-known movies to watch, with fewer than 20K votes
55/8: # Finally, export this file to an Excel spreadsheet -- without the DataFrame index.
55/9:
# List only movies whose ratings are over 7 (out of 10) in IMDB
rating_df = movie_file_df.loc[movie_file_df["IMDB"] > 7, ["FILM", "IMDB", "IMDB_user_vote_count"]]
rating_df.head()
55/10:
# only care about Imdb, so create a new table that takes the Film and all the columns relating to IMDB
imbd_df = movie_file_df[["FILM", 
                         "IMDB",
                         "IMDB_norm",
                        "IMDB_norm_round",
                         "IMDB_user_vote_count"]]
imbd_df.head()
56/1:
# Dependencies
import pandas as pd
56/2:
# Load in File from resources
# 'movie_scores.csv'
movie_file = "Resources/movie_scores.csv"
56/3:
# Read and display with pandas
movie_file_df = pd.read_csv(movie_file)
movie_file_df.head()
56/4:
# List all the columns the table provides
movie_file_df.columns
56/5:
# only care about Imdb, so create a new table that takes the Film and all the columns relating to IMDB
imbd_df = movie_file_df[["FILM", 
                         "IMDB",
                         "IMDB_norm",
                        "IMDB_norm_round",
                         "IMDB_user_vote_count"]]
imbd_df.head()
56/6:
# List only movies whose ratings are over 7 (out of 10) in IMDB
rating_df = movie_file_df.loc[movie_file_df["IMDB"] > 7, ["FILM", "IMDB", "IMDB_user_vote_count"]]
rating_df.head()
56/7:
# Find lesser-known movies to watch, with fewer than 20K votes
unknown_movies_df = rating_df.loc[rating_df]["IMDB_user_vote_count"] < 20000, ["FILM", "IMDB", "IMDB_user_vote_count"]]
unknown_movies_df.head()
56/8:
# Finally, export this file to an Excel spreadsheet -- without the DataFrame index.
unknown_movies_df.to_excel("output/movieWatchlist.xlsx", index=False)
56/9:
# Find lesser-known movies to watch, with fewer than 20K votes
unknown_movies_df = rating_df.loc[rating_df["IMDB_user_vote_count"] < 20000, ["FILM", "IMDB", "IMDB_user_vote_count"]]
unknown_movies_df.head()
56/10:
# Finally, export this file to an Excel spreadsheet -- without the DataFrame index.
unknown_movies_df.to_excel("output/movieWatchlist.xlsx", index=False)
56/11:
# Finally, export this file to an Excel spreadsheet -- without the DataFrame index.
unknown_movies_df.to_excel("output/movieWatchlist.xlsx", index=False)
57/1:
# Dependencies
import pandas as pd
57/2:
# Load in File from resources
# 'movie_scores.csv'
movie_file = "Resources/movie_scores.csv"
57/3:
# Read and display with pandas
movie_file_df = pd.read_csv(movie_file, encoding = "utf8")
movie_file_df.head()
57/4:
# List all the columns the table provides
movie_file_df.columns
57/5:
# only care about Imdb, so create a new table that takes the Film and all the columns relating to IMDB
imbd_df = movie_file_df[["FILM", 
                         "IMDB",
                         "IMDB_norm",
                        "IMDB_norm_round",
                         "IMDB_user_vote_count"]]
imbd_df.head()
57/6:
# List only movies whose ratings are over 7 (out of 10) in IMDB
rating_df = movie_file_df.loc[movie_file_df["IMDB"] > 7, ["FILM", "IMDB", "IMDB_user_vote_count"]]
rating_df.head()
57/7:
# Find lesser-known movies to watch, with fewer than 20K votes
unknown_movies_df = rating_df.loc[rating_df["IMDB_user_vote_count"] < 20000, ["FILM", "IMDB", "IMDB_user_vote_count"]]
unknown_movies_df.head()
57/8:
# Finally, export this file to an Excel spreadsheet -- without the DataFrame index.
unknown_movies_df.to_excel("output/movieWatchlist.xlsx", index=False)
58/1:
# Dependencies
import pandas as pd
58/2:
# Load in File from resources
# 'movie_scores.csv'
movie_file = "Resources/movie_scores.csv"
58/3:
# Read and display with pandas
movie_file_df = pd.read_csv(movie_file, encoding = "utf-8")
movie_file_df.head()
58/4:
# List all the columns the table provides
movie_file_df.columns
58/5:
# only care about Imdb, so create a new table that takes the Film and all the columns relating to IMDB
imbd_df = movie_file_df[["FILM", 
                         "IMDB",
                         "IMDB_norm",
                        "IMDB_norm_round",
                         "IMDB_user_vote_count"]]
imbd_df.head()
58/6:
# List only movies whose ratings are over 7 (out of 10) in IMDB
rating_df = movie_file_df.loc[movie_file_df["IMDB"] > 7, ["FILM", "IMDB", "IMDB_user_vote_count"]]
rating_df.head()
58/7:
# Find lesser-known movies to watch, with fewer than 20K votes
unknown_movies_df = rating_df.loc[rating_df["IMDB_user_vote_count"] < 20000, ["FILM", "IMDB", "IMDB_user_vote_count"]]
unknown_movies_df.head()
58/8:
# Finally, export this file to an Excel spreadsheet -- without the DataFrame index.
unknown_movies_df.to_excel("output/movieWatchlist.xlsx", index=False)
59/1:
# Reference the file where the CSV is located
portland_crime = "Resources/crime_incident_data2017.csv"

# Import the data into a Pandas DataFrame
portland_crime_df = pd.read_csv(portland_crime, encoding = "utf-8")
portland_crime.df()
61/1:
# Import Dependencies
import pandas as pd
61/2:
# Reference the file where the CSV is located
portland_crime = "Resources/crime_incident_data2017.csv"

# Import the data into a Pandas DataFrame
portland_crime_df = pd.read_csv(portland_crime, encoding = "utf-8")
portland_crime.df()
62/1:
# Import Dependencies
import pandas as pd
62/2:
# Reference the file where the CSV is located
portland_crime = "Resources/crime_incident_data2017.csv"

# Import the data into a Pandas DataFrame
portland_crime_df = pd.read_csv(portland_crime, encoding = "utf-8")
portland_crime.df()
62/3:
# Reference the file where the CSV is located
portland_crime = "Resources/crime_incident_data2017.csv"

# Import the data into a Pandas DataFrame
portland_crime_df = pd.read_csv(portland_crime, encoding="utf-8")
portland_crime.df()
62/4:
# Reference the file where the CSV is located
portland_crime = "Resources/crime_incident_data2017.csv"

# Import the data into a Pandas DataFrame
portland_crime_df = pd.read_csv(portland_crime, encoding="utf-8")
portland_crime_df.head()
62/5:
# look for missing values
df.count()
62/6:
# look for missing values
portland_crime_df.count()
62/7:
# drop null rows
portland_crime_df = portland_crime_df.dropna(how='any')
62/8:
# verify counts
portland_crime_df.count()
62/9:
# Check to see if there are any values with mispelled or similar values in "Offense Type"
portland_crime_df["Offense Type"].value_counts()
62/10:
# Check to see if you comnbined similar offenses correctly in "Offense Type".
portland_crimes_df["Offense Type"].value_counts()
62/11:
# Combine similar offenses
portland_crime_df["Offense Type"] = portland_crime_df["Offense Type"].replace(
    {'Theft From Motor Vehicle': 'Motor Vehicle Theft', 'Commercial Sex Acts': 'Prostitution', 'Impersonation': 'Identity Theft', 'Robbery': 'Burglary'})
62/12:
# Check to see if you comnbined similar offenses correctly in "Offense Type".
portland_crime_df["Offense Type"].value_counts()
62/13:
# Combine similar offenses
portland_crime_df["Offense Type"] = portland_crime_df["Offense Type"].replace(
    {'Theft From Motor Vehicle': 'Motor Vehicle Theft', 'Commercial Sex Acts': 'Prostitution', 'Impersonation': 'Identity Theft', 'Robbery': 'Burglary', 'Pocket-Picking':'Purse-Snatching'})
62/14:
# Check to see if you comnbined similar offenses correctly in "Offense Type".
portland_crime_df["Offense Type"].value_counts()
62/15:
# Create a new DataFrame that looks into a specific neighborhood
only_centennial = portland_crime_df.loc[portland_crime_df["Neighborhood"] == "Centennial", :]
print(only_centennial)
62/16:
# Create a new DataFrame that looks into a specific neighborhood
only_centennial = portland_crime_df.loc[portland_crime_df["Neighborhood"] == "Centennial", :]
only_centennial.head()
67/1:
# Read with Pandas
pokemon_df = pd.read.csv(pokemon, encoding="utf-8")
pokemon_df.head()
69/1:
# Dependencies
import pandas as pd
69/2:
# Save file path to variable
pokemon = "Resources/Pokemon.csv"
69/3:
# Read with Pandas
pokemon_df = pd.read.csv(pokemon, encoding="utf-8")
pokemon_df.head()
69/4:
# Read with Pandas
pokemon_df = pd.read_csv(pokemon,encoding="utf-8")
pokemon_df.head()
69/5:
# Extract the following columns: "Type 1", "HP", "Attack", "Sp. Atk", "Sp. Def", and "Speed"
pokemon_type_df = pokemon_df[["Type 1", "HP", "Attack", "Sp. Atk", "Sp. Def", "Speed"]]
pokemon_type_df.head()
69/6:
# Create a dataframe of the average stats for each type of pokemon.
pokemon_groupedtype_df = pokemon_type_df.groupby("Type 1")
pokemon_groupedtype_df.head()
69/7:
# Create a dataframe of the average stats for each type of pokemon.
pokemon_groupedtype_df = pokemon_type_df.groupby("Type 1")
pokemon_groupedtype_df.head(10)
69/8:
# Create a dataframe of the average stats for each type of pokemon.
pokemon_groupedtype_df = pokemon_type_df.groupby(["Type 1"])
pokemon_groupedtype_df.head(10)
69/9:
# Calculate the total power level of each type of pokemon by summing all of the stats together.
# Place the results into a new column.
# Hint: Research the sum method from pandas.
69/10:
# Create a dataframe of the average stats for each type of pokemon.
pokemon_groupedtype_df = pokemon_type_df.groupby(["Type 1"])
pokemon_groupedtype_df.head(10)
69/11:
# Create a dataframe of the average stats for each type of pokemon.
pokemon_groupedtype_df. = pokemon_type_df.groupby(["Type 1"])
pokemon_groupedtype_df.count().head(10)
69/12:
# Create a dataframe of the average stats for each type of pokemon.
pokemon_groupedtype_df = pokemon_type_df.groupby(["Type 1"])
pokemon_groupedtype_df.count().head(10)
69/13:
# Create a dataframe of the average stats for each type of pokemon.
pokemon_groupedtype_df = pokemon_type_df.groupby(["Type 1"])
pokemon_groupedtype_df.mean().head(10)
69/14:
# Create a dataframe of the average stats for each type of pokemon.
pokemon_groupedtype_df = pokemon_type_df.groupby(["Type 1"])
pokemon_groupedtype_df.mean().head(10)
69/15:
# Create a dataframe of the average stats for each type of pokemon.
pokemon_groupedtype_df = pokemon_type_df.groupby(["Type 1"])
pokemon_groupedtype_df.mean().head(10).round()
69/16:
# Create a dataframe of the average stats for each type of pokemon.
pokemon_groupedtype_df = pokemon_type_df.groupby(["Type 1"])
pokemon_groupedtype_df.mean().head(10).round(2)
69/17:
# Calculate the total power level of each type of pokemon by summing all of the stats together.
# Place the results into a new column.
# Hint: Research the sum method from pandas.
pokemon_comparison_df["Total"] = pokemon_comparison_df.sum(axis=1)
pokemon_comparison_df["Total"]
69/18:
# Calculate the total power level of each type of pokemon by summing all of the stats together.
# Place the results into a new column.
# Hint: Research the sum method from pandas.
pokemon_groupedtype_df["Total"] = pokemon_groupedtype_df.sum(axis=1)
pokemon_groupedtype_df["Total"]
69/19:
# Calculate the total power level of each type of pokemon by summing all of the stats together.
# Place the results into a new column.
# Hint: Research the sum method from pandas.
pokemon_summary_df = pokemon_groupedtype_df.sum(axis=1)
69/20:
# Calculate the total power level of each type of pokemon by summing all of the stats together.
# Place the results into a new column.
# Hint: Research the sum method from pandas.
pokemon_summary_df["Total"] = pokemon_groupedtype_df.sum(axis=1)
pokemon_summary_df["Total"]
69/21:
# Calculate the total power level of each type of pokemon by summing all of the stats together.
# Place the results into a new column.
# Hint: Research the sum method from pandas.
pokemon_summary_df["Total"] = pokemon_groupedtype_df.sum(axis=1)
pokemon_summary_df["Total"]
71/1:
# Collect a list of all the unique values in "Preferred Position"
soccer_2018_df["Preferred Position"].unique()
73/1:
# Import Dependencies
import pandas as pd
73/2:
# Create reference to CSV file
csv_path = "Resources/Soccer2018Data.csv"

# Import the CSV into a pandas DataFrame
soccer_2018_df = pd.read_csv(csv_path, low_memory=False)
soccer_2018_df
73/3:
# Collect a list of all the unique values in "Preferred Position"
soccer_2018_df["Preferred Position"].unique()
73/4: # Looking only at strikers (ST) to start
73/5:
# Sort the DataFrame by the values in the "ST" column to find the worst

# Reset the index so that the index is now based on the sorting locations
73/6: # Save all of the information collected on the worst striker
73/7:
# Looking only at strikers (ST) to start
soccer_2018ST_df = soccer_2018_df.sort_values("ST", ascending=false)
soccer_2018ST_df.head()
73/8:
# Looking only at strikers (ST) to start
soccer_2018ST_df = soccer_2018_df.sort_values("ST", ascending=False)
soccer_2018ST_df.head()
73/9:
# Looking only at strikers (ST) to start
ST_2018_df = soccer_2018_df.loc[soccer_2018_df["Preferred Position"] == "ST", :]
ST_2018_df.head()
73/10:
# Sort the DataFrame by the values in the "ST" column to find the worst
STR_2018_df = soccer_2018_df.sort_values("Preferred Position", ascending=False)
STR_2018_df.head()

# Reset the index so that the index is now based on the sorting locations
73/11:
# Sort the DataFrame by the values in the "ST" column to find the worst
STR_2018_df = soccer_2018_df.sort_values("Preferred Position", ascending=False)
STR_2018_df.head()

# Reset the index so that the index is now based on the sorting locations
new_index_df = STR_2018_df.reset_index(drop=True)
73/12:
# Sort the DataFrame by the values in the "ST" column to find the worst
STR_2018_df = soccer_2018_df.sort_values("Preferred Position", ascending=False)
STR_2018_df.head()

# Reset the index so that the index is now based on the sorting locations
new_index_df = STR_2018_df.reset_index(drop=True)
73/13:
# Sort the DataFrame by the values in the "ST" column to find the worst
STR_2018_df = soccer_2018_df.sort_values("ST")

# Reset the index so that the index is now based on the sorting locations
STR_2018_df = STR_2018_df.reset_index(drop=True)
STR_2018_df.head()
73/14:
# Sort the DataFrame by the values in the "ST" column to find the worst
STR_2018_df = soccer_2018_df.sort_values("ST", ascending=False)

# Reset the index so that the index is now based on the sorting locations
STR_2018_df = STR_2018_df.reset_index(drop=True)
STR_2018_df.head()
73/15:
# Save all of the information collected on the worst striker
worst_ST = ST_2018_df.loc[0, :]
worst_ST
73/16:
# Sort the DataFrame by the values in the "ST" column to find the worst
STR_2018_df = soccer_2018_df.sort_values("ST", ascending=True)

# Reset the index so that the index is now based on the sorting locations
STR_2018_df = STR_2018_df.reset_index(drop=False)
STR_2018_df.head()
73/17:
# Save all of the information collected on the worst striker
worst_ST = ST_2018_df.loc[0, :]
worst_ST
73/18:
# Sort the DataFrame by the values in the "ST" column to find the worst
STR_2018_df = soccer_2018_df.sort_values("ST")

# Reset the index so that the index is now based on the sorting locations
STR_2018_df = STR_2018_df.reset_index(drop=True)
STR_2018_df.head()
73/19:
# Sort the DataFrame by the values in the "ST" column to find the worst
STR_2018_df = soccer_2018_df.sort_values("ST")

# Reset the index so that the index is now based on the sorting locations
STR_2018_df = STR_2018_df.reset_index(drop=True)
STR_2018_df.head()
73/20:
# Save all of the information collected on the worst striker
worst_ST = ST_2018_df.loc[0, :]
worst_ST
74/1:
# Import Dependencies
import pandas as pd
74/2:
# Create reference to CSV file
csv_path = "Resources/Soccer2018Data.csv"

# Import the CSV into a pandas DataFrame
soccer_2018_df = pd.read_csv(csv_path, low_memory=False)
soccer_2018_df
74/3:
# Collect a list of all the unique values in "Preferred Position"
soccer_2018_df["Preferred Position"].unique()
74/4:
# Looking only at strikers (ST) to start
ST_2018_df = soccer_2018_df.loc[soccer_2018_df["Preferred Position"] == "ST", :]
ST_2018_df.head()
74/5:
# Sort the DataFrame by the values in the "ST" column to find the worst
STR_2018_df = soccer_2018_df.sort_values("ST")

# Reset the index so that the index is now based on the sorting locations
STR_2018_df = STR_2018_df.reset_index(drop=True)
STR_2018_df.head()
74/6:
# Save all of the information collected on the worst striker
worst_ST = ST_2018_df.loc[0, :]
worst_ST
74/7:
# Looking only at strikers (ST) to start
ST_2018_df = soccer_2018_df.loc[soccer_2018_df["Preferred Position"] == "ST", :]
ST_2018_df.head
74/8:
# Looking only at strikers (ST) to start
ST_2018_df = soccer_2018_df.loc[soccer_2018_df["Preferred Position"] == "ST", :]
ST_2018_df
77/1:
# Read and display the CSV with Pandas
pycityschools_file_df = pd.read_csv(pycityschools_file)
pycityschools_file_df.head()
80/1:
# Dependencies
import pandas as pd
80/2:
# load in file
pycityschools_file = "Resources/schools_complete.csv"
80/3:
# Read and display the CSV with Pandas
pycityschools_file_df = pd.read_csv(pycityschools_file)
pycityschools_file_df.head()
80/4:
# Count total schools
total_schools = pycityschools_file_df["school_name"].value_counts()
80/5:
# Count total schools
total_schools = pycityschools_file_df["school_name"].value_counts()
total_schools
86/1:
# Import Dependencies
import pandas as pd
86/2:
bitcoin_csv = "Resources/bitcoin_cash_price.csv"
dash_csv = "Resources/dash_price.csv"
86/3:
bitcoin_df = pd.read_csv(bitcoin_csv)
dash_df = pd.read_csv(dash_csv)
86/4: bitcoin_df.head()
86/5: dash_df.head()
86/6:
# Merge the two DataFrames together based on the Dates they share
merge_df = pd.meger(bitcoin_df, dash_df, on="Date", how="inner")
86/7:
# Merge the two DataFrames together based on the Dates they share
merge_df = pd.meger(bitcoin_df, dash_df, on="Date", how="inner")
merge_df
86/8:
# Merge the two DataFrames together based on the Dates they share
merge_df = pd.merge(bitcoin_df, dash_df, on="Date", how="inner")
merge_df
86/9:
# Merge the two DataFrames together based on the Dates they share
merge_df = pd.merge(bitcoin_df, dash_df, on="Date", how="inner")
merge_df.head()
86/10:
# Rename columns so that they are differentiated
merge_df.rename(columns = {'Open_x': 'Open_bitcoin', 
                           'High_x': 'High_bitcoin', 
                           'Low_x': 'Low_bitcoin',
                           'Close_x': 'Close_bitcoin'})
86/11:
# Merge the two DataFrames together based on the Dates they share
merge_df = pd.merge(bitcoin_df, dash_df, on="Date")
merge_df.head()
88/1:
# Figure out the minimum and maximum views for a TED Talk
print(ted_df["views"].max())
print(ted_df["views"].min())
90/1:
# Import Dependencies
import pandas as pd
90/2:
# Create a path to the csv and read it into a Pandas DataFrame
csv_path = "Resources/ted_talks.csv"
ted_df = pd.read_csv(csv_path)

ted_df.head()
90/3:
# Figure out the minimum and maximum views for a TED Talk
print(ted_df["views"].max())
print(ted_df["views"].min())
90/4:
# Create bins in which to place values based upon TED Talk views

# Create labels for these bins
90/5: # Slice the data and place it into bins
90/6: # Place the data series into a new column inside of the DataFrame
90/7:
# Create a GroupBy object based upon "View Group"


# Find how many rows fall into each bin


# Get the average of each column within the GroupBy object
90/8:
# Slice the data and place it into bins
pd.cut(ted_df["views"], bins, label=bin_label).head()
90/9:
# Slice the data and place it into bins
pd.cut(ted_df["views"], bins, labels=bin_label).head()
91/1:
# Import Dependencies
import pandas as pd
91/2:
# Create a path to the csv and read it into a Pandas DataFrame
csv_path = "Resources/ted_talks.csv"
ted_df = pd.read_csv(csv_path)

ted_df.head()
91/3:
# Figure out the minimum and maximum views for a TED Talk
print(ted_df["views"].max())
print(ted_df["views"].min())
91/4:
# Create bins in which to place values based upon TED Talk views
bins = [0, 199999, 399999, 599999, 799999, 999999,
        1999999, 2999999, 3999999, 4999999, 50000000]

# Create labels for these bins
bin_label = ["0-199k", "200k-399k", "400k-599k", "600k-799k", "800k-999k", "1mil-2mil",
                "2mil-3mil", "3mil-4mil", "4mil-5mil", "5mil-50mil"]
91/5:
# Slice the data and place it into bins
pd.cut(ted_df["views"], bins, labels=bin_label).head()
91/6: # Place the data series into a new column inside of the DataFrame
91/7:
# Create a GroupBy object based upon "View Group"


# Find how many rows fall into each bin


# Get the average of each column within the GroupBy object
92/1:
# Import Dependencies
import pandas as pd
92/2:
# Create a path to the csv and read it into a Pandas DataFrame
csv_path = "Resources/ted_talks.csv"
ted_df = pd.read_csv(csv_path)

ted_df.head()
92/3:
# Figure out the minimum and maximum views for a TED Talk
print(ted_df["views"].max())
print(ted_df["views"].min())
92/4:
# Create bins in which to place values based upon TED Talk views
bins = [0, 199999, 399999, 599999, 799999, 999999,
        1999999, 2999999, 3999999, 4999999, 50000000]

# Create labels for these bins
bin_label = ["0-199k", "200k-399k", "400k-599k", "600k-799k", "800k-999k", "1mil-2mil",
                "2mil-3mil", "3mil-4mil", "4mil-5mil", "5mil-50mil"]
92/5:
# Slice the data and place it into bins
pd.cut(ted_df["views"], bins, labels=bin_label).head()
92/6:
# Place the data series into a new column inside of the DataFrame
ted_df["view group"] = pd.cut(ted_df["views"], bins, labels=bin_label)
ted_df.head()
92/7:
# Create a GroupBy object based upon "View Group"


# Find how many rows fall into each bin


# Get the average of each column within the GroupBy object
93/1:
# Import Dependencies
import pandas as pd
93/2:
# Create a path to the csv and read it into a Pandas DataFrame
csv_path = "Resources/ted_talks.csv"
ted_df = pd.read_csv(csv_path)

ted_df.head()
93/3:
# Figure out the minimum and maximum views for a TED Talk
print(ted_df["views"].max())
print(ted_df["views"].min())
93/4:
# Create bins in which to place values based upon TED Talk views
bins = [0, 199999, 399999, 599999, 799999, 999999,
        1999999, 2999999, 3999999, 4999999, 50000000]

# Create labels for these bins
bin_label = ["0-199k", "200k-399k", "400k-599k", "600k-799k", "800k-999k", "1mil-2mil",
                "2mil-3mil", "3mil-4mil", "4mil-5mil", "5mil-50mil"]
93/5:
# Slice the data and place it into bins
pd.cut(ted_df["views"], bins, labels=bin_label).head()
93/6:
# Place the data series into a new column inside of the DataFrame
ted_df["view group"] = pd.cut(ted_df["views"], bins, labels=bin_label)
ted_df.head()
93/7:
# Create a GroupBy object based upon "View Group"
ted_group = ted_df.groupby("View Group")

# Find how many rows fall into each bin
print(ted_group["comments"].count())


# Get the average of each column within the GroupBy object
ted_group[["comments", "duration", "languages"]].mean()
94/1:
# Import Dependencies
import pandas as pd
94/2:
# Create a path to the csv and read it into a Pandas DataFrame
csv_path = "Resources/ted_talks.csv"
ted_df = pd.read_csv(csv_path)

ted_df.head()
94/3:
# Figure out the minimum and maximum views for a TED Talk
print(ted_df["views"].max())
print(ted_df["views"].min())
94/4:
# Create bins in which to place values based upon TED Talk views
bins = [0, 199999, 399999, 599999, 799999, 999999,
        1999999, 2999999, 3999999, 4999999, 50000000]

# Create labels for these bins
bin_label = ["0-199k", "200k-399k", "400k-599k", "600k-799k", "800k-999k", "1mil-2mil",
                "2mil-3mil", "3mil-4mil", "4mil-5mil", "5mil-50mil"]
94/5:
# Slice the data and place it into bins
pd.cut(ted_df["views"], bins, labels=bin_label).head()
94/6:
# Place the data series into a new column inside of the DataFrame
ted_df["view group"] = pd.cut(ted_df["views"], bins, labels=bin_label)
ted_df.head()
94/7:
# Create a GroupBy object based upon "View Group"
ted_group = ted_df.groupby("view group")

# Find how many rows fall into each bin
print(ted_group["comments"].count())


# Get the average of each column within the GroupBy object
ted_group[["comments", "duration", "languages"]].mean()
96/1: import pandas as pd
96/2:
# The path to our CSV file
csv_path = "Resources/KickstarterData.csv"

# Read our Kickstarter data into pandas
kickstarter_df = pd.read_csv(csv_path)
kickstarter_df.head()
96/3:
# Get a list of all of our columns for easy reference
kickstarter_df = df.columns.values.tolist()
96/4:
# Get a list of all of our columns for easy reference
df.columns
98/1: import pandas as pd
98/2:
# The path to our CSV file
csv_path = "Resources/KickstarterData.csv"

# Read our Kickstarter data into pandas
kickstarter_df = pd.read_csv(csv_path)
kickstarter_df.head()
98/3:
# Get a list of all of our columns for easy reference
df.columns
98/4:
# Get a list of all of our columns for easy reference
kickstarterdata_df.columns
98/5:
# Get a list of all of our columns for easy reference
kickstarter_df.columns
99/1: import pandas as pd
99/2:
# The path to our CSV file
csv_path = "Resources/KickstarterData.csv"

# Read our Kickstarter data into pandas
kickstarter_df = pd.read_csv(csv_path)
kickstarter_df.head()
99/3:
# Get a list of all of our columns for easy reference
kickstarter_df.columns
99/4:
# Extract "name", "goal", "pledged", "state", "country", "staff_pick",
# "backers_count", and "spotlight"
extracted_kickstarter_df = df.loc["name", "goal", "pledged", "state", "country", "staff_pick", "bakers_count", "spotlight"]
extracted_kickstarter_df
99/5:
# Extract "name", "goal", "pledged", "state", "country", "staff_pick",
# "backers_count", and "spotlight"
extracted_kickstarter_df = df.loc["name", "goal", "pledged", "state", "country", "staff_pick", "bakers_count", "spotlight"]
extracted_kickstarter_df.head()
100/1: import pandas as pd
100/2:
# The path to our CSV file
csv_path = "Resources/KickstarterData.csv"

# Read our Kickstarter data into pandas
kickstarter_df = pd.read_csv(csv_path)
kickstarter_df.head()
100/3:
# Get a list of all of our columns for easy reference
kickstarter_df.columns
100/4:
# Extract "name", "goal", "pledged", "state", "country", "staff_pick",
# "backers_count", and "spotlight"
extracted_kickstarter_df = kickstarter_df.loc["name", "goal", "pledged", "state", "country", "staff_pick", "bakers_count", "spotlight"]
extracted_kickstarter_df.head()
100/5:
# Extract "name", "goal", "pledged", "state", "country", "staff_pick",
# "backers_count", and "spotlight"
extracted_kickstarter_df = kickstarter_df.loc[:, ["name", "goal", "pledged", "state", "country", "staff_pick", "bakers_count", "spotlight"]]
extracted_kickstarter_df.head()
100/6:
# Extract "name", "goal", "pledged", "state", "country", "staff_pick",
# "backers_count", and "spotlight"
extracted_kickstarter_df = kickstarter_df.loc[:, ['name', 'goal', 'pledged', 'state', 'country', 'staff_pick', 'bakers_count', 'spotlight']]
extracted_kickstarter_df.head()
101/1: import pandas as pd
101/2:
# The path to our CSV file
csv_path = "Resources/KickstarterData.csv"

# Read our Kickstarter data into pandas
kickstarter_df = pd.read_csv(csv_path)
kickstarter_df.head()
101/3:
# Get a list of all of our columns for easy reference
kickstarter_df.columns
101/4:
# Extract "name", "goal", "pledged", "state", "country", "staff_pick",
# "backers_count", and "spotlight"
extracted_kickstarter_df = kickstarter_df.loc[:, ['name', 'goal', 'pledged', 'state', 'country', 'staff_pick', 'bakers_count', 'spotlight']]
extracted_kickstarter_df.head()
102/1: import pandas as pd
102/2:
# The path to our CSV file
file = "Resources/KickstarterData.csv"

# Read our Kickstarter data into pandas
df = pd.read_csv(file)
df.head()
102/3:
# Get a list of all of our columns for easy reference
df.columns
102/4:
# Extract "name", "goal", "pledged", "state", "country", "staff_pick",
# "backers_count", and "spotlight"
reduced_kickstarter_df = df.loc[:, ["name", "goal", "pledged",
                                    "state", "country", "staff_pick", "backers_count", "spotlight"]]
reduced_kickstarter_df
102/5:
# Remove projects that made no money at all
reduced_kickstarter_df = reduced_kickstarter_df.loc[(
    reduced_kickstarter_df["pledged"] > 0)]
reduced_kickstarter_df.head()
102/6:
# Collect only those projects that were hosted in the US.

# Create a list of the columns
columns = [
    "name", "goal", "pledged", "state", 
    "country", "staff_pick", "backers_count", "spotlight"]

#  Create a new df for "US" with the columns. 
hosted_in_us_df = reduced_kickstarter_df.loc[reduced_kickstarter_df["country"] == "US",  columns]
hosted_in_us_df.head()
102/7:
# Create a new column that finds the average amount pledged to a project
average_donation = hosted_in_us_df['pledged'] / hosted_in_us_df['backers_count']
average_donation
102/8:
# Create a new column that finds the average amount pledged to a project
hosted_in_us_df["average_donation"] = hosted_in_us_df['pledged'] / \
    hosted_in_us_df['backers_count']
102/9:
# First convert "average_donation", "goal", and "pledged" columns to float
# Then Format to go to two decimal places, include a dollar sign, and use comma notation

hosted_in_us_df["average_donation"] = hosted_in_us_df["average_donation"].astype(float).map(
    "${:,.2f}".format)
hosted_in_us_df["goal"] = hosted_in_us_df["goal"].astype(float).map("${:,.2f}".format)
hosted_in_us_df["pledged"] = hosted_in_us_df["pledged"].astype(float).map("${:,.2f}".format)
102/10: hosted_in_us_df.head()
102/11:
# Calculate the total number of backers for all US projects
hosted_in_us_df["backers_count"].sum()
102/12:
# Calculate the average number of backers for all US projects
hosted_in_us_df["backers_count"].mean()
102/13:
# Collect only those US campaigns that have been picked as a "Staff Pick"
picked_by_staff_df = hosted_in_us_df.loc[hosted_in_us_df["staff_pick"] == True]
picked_by_staff_df
102/14:
# Group by the state of the campaigns and see if staff picks matter (Seems to matter quite a bit)
state_groups = picked_by_staff_df.groupby("state")
state_groups["name"].count()
103/1:
# Create a reference to the CSV and import it into a Pandas DataFrame
csv_path = "../Resources/EclipseBugs.csv"
pd.read_csv(csv_path)
103/2:
# Create a reference to the CSV and import it into a Pandas DataFrame
csv_path = "../Resources/EclipseBugs.csv"
debug_df = pd.read_csv(csv_path)
debug_df.head()
103/3:
import pandas as pd

# Create a reference to the CSV and import it into a Pandas DataFrame
csv_path = "../Resources/EclipseBugs.csv"
debug_df = pd.read_csv(csv_path)
debug_df.head()
103/4:
import pandas as pd

# Create a reference to the CSV and import it into a Pandas DataFrame
csv_path = "../Resources/EclipseBugs.csv"
debug_df = pd.read_csv(csv_path)
debug_df.head()
103/5:
import pandas as pd

# Create a reference to the CSV and import it into a Pandas DataFrame
csv_path = "Resources/EclipseBugs.csv"
debug_df = pd.read_csv(csv_path)
debug_df.head()
103/6:
# Finding the average number of comments per bug
average_comments = eclipse_df["Number of Comments"].mean()
average_comments
103/7:
import pandas as pd

# Create a reference to the CSV and import it into a Pandas DataFrame
csv_path = "Resources/EclipseBugs.csv"
eclipse_df = pd.read_csv(csv_path)
eclipse_df.head()
104/1:
import pandas as pd

# Create a reference to the CSV and import it into a Pandas DataFrame
csv_path = "Resources/EclipseBugs.csv"
eclipse_df = pd.read_csv(csv_path)
eclipse_df.head()
104/2:
# Finding the average number of comments per bug
average_comments = eclipse_df["Number of Comments"].mean()
average_comments
104/3:
# Finding the average number of comments per bug
average_comments = eclipse_df["Number of/nComments"].mean()
average_comments
104/4:
# Finding the average number of comments per bug
average_comments = eclipse_df["Number of\nComments"].mean()
average_comments
104/5:
# Grouping the DataFrame by "Assignee"
assignee_group = eclipse_df.groupby("Assignee")

# Count how many of each component Assignees worked on and create DataFrame
assignee_work_df = pd.DataFrame(assignee_group["Component"].value_counts())
assignee_work_df.head()
104/6:
# Rename the "Component" column to "Component Bug Count"
assignee_work_df = assignee_work_df.rename(
    columns={"Component": "Component Bug Count"})
assignee_work_df.head()
104/7:
# Find the percentage of bugs overall fixed by each Assignee
total_bugs = assignee_group["Assignee"].count()
bugs_per_user = assignee_group["Assignee"].value_counts()

user_bug_percent_df = pd.DataFrame((bugs_per_user/total_bugs)*100)
user_bug_percent_df.head()
104/8:
# Rename the "Assignee" column to "Percent of Total Bugs Assigned"
user_bug_percent_df = user_bug_percent_df.rename(
    columns={"Assignee": "Percent of Total Bugs Assigned"})

# Reset the index for this DataFrame so "Assignee" is a column
user_bug_percent_df = user_bug_percent_df.reset_index()
user_bug_percent_df.head()
104/9:
# Find the percentage of bugs overall fixed by each Assignee
total_bugs = len(eclipse_df)
bugs_per_user = assignee_group["Assignee"].counts()

user_bug_percent_df = pd.DataFrame((bugs_per_user/total_bugs)*100)
user_bug_percent_df.head()
104/10:
# Find the percentage of bugs overall fixed by each Assignee
total_bugs = len(eclipse_df)
bugs_per_user = assignee_group["Assignee"].count()

user_bug_percent_df = pd.DataFrame((bugs_per_user/total_bugs)*100)
user_bug_percent_df.head()
104/11:
# Rename the "Assignee" column to "Percent of Total Bugs Assigned"
user_bug_percent_df = user_bug_percent_df.rename(
    columns={"Assignee": "Percent of Total Bugs Assigned"})

# Reset the index for this DataFrame so "Assignee" is a column
user_bug_percent_df = user_bug_percent_df.reset_index()
user_bug_percent_df.head()
104/12:
# Reset the index of "assignee_group" so that "Assignee" and "Component" are columns
assignee_work_df = assignee_work_df.reset_index()
asignee_work_df.head()
104/13:
# Reset the index of "assignee_group" so that "Assignee" and "Component" are columns
assignee_work_df = assignee_work_df.reset_index()
assignee_work_df.head()
104/14:
# Merge the "Percent of Total Bugs Assigned" into the DataFrame
assignee_work_df = assignee_work_df.merge(user_bug_percent_df, on="Component")
assignee_work_df.head()
104/15:
# Merge the "Percent of Total Bugs Assigned" into the DataFrame
assignee_work_df = assignee_work_df.merge(user_bug_percent_df, on="Assignee")
assignee_work_df.head()
106/1:
# Count total schools
total_schools = pycityschools_file_df["school_name"].count()
total_schools
107/1:
# Dependencies
import pandas as pd
107/2:
# load in file
pycityschools_file = "Resources/schools_complete.csv"
107/3:
# Read and display the CSV with Pandas
pycityschools_file_df = pd.read_csv(pycityschools_file)
pycityschools_file_df.head()
107/4:
# Count total schools
total_schools = pycityschools_file_df["school_name"].count()
total_schools
107/5:
# Read and display the CSV with Pandas
pycityschools_file_df = pd.read_csv(pycityschools_file)
pycityschools_file_df.head(10)
107/6:
# Read and display the CSV with Pandas
pycityschools_file_df = pd.read_csv(pycityschools_file)
pycityschools_file_df.head(16)
107/7:
# Read and display the CSV with Pandas
pycityschools_file_df = pd.read_csv(pycityschools_file)
pycityschools_file_df.head(17)
107/8:
# Read and display the CSV with Pandas
pycityschools_file_df = pd.read_csv(pycityschools_file)
pycityschools_file_df.head()
107/9:
# Read and display the CSV with Pandas
pycityschools_data = pd.read_csv(pycityschools_file)
pycityschools_data()
107/10:
# Read School and Student Data File and store into Pandas DataFrames
pycityschools_data_df = pd.read_csv(pycityschools_file)
student_data_df = pd.read_csv(student_file)
pycityschools_data_df.head()
student_data_df.head()
108/1:
# Dependencies
import pandas as pd
108/2:
# load in file
pycityschools_file = "Resources/schools_complete.csv"
student_file = "Resources/students_complete.csv"
108/3:
# Read School and Student Data File and store into Pandas DataFrames
pycityschools_data_df = pd.read_csv(pycityschools_file)
student_data_df = pd.read_csv(student_file)
pycityschools_data_df.head()
student_data_df.head()
109/1:
# Dependencies
import pandas as pd
109/2:
# load in file
pycityschools_file = "Resources/schools_complete.csv"
student_file = "Resources/students_complete.csv"
109/3:
# Read School and Student Data File and store into Pandas DataFrames
pycityschools_data_df = pd.read_csv(pycityschools_file)
student_data_df = pd.read_csv(student_file)
pycityschools_data_df.head()
student_data_df.head()
109/4:
# Count total schools
total_schools = pycityschools_file_df["school_name"].count()
total_schools
110/1:
# Dependencies
import pandas as pd
110/2:
# load in file
pycityschools_file = "Resources/schools_complete.csv"
student_file = "Resources/students_complete.csv"
110/3:
# Read School and Student Data File and store into Pandas DataFrames
pycityschools_data_df = pd.read_csv(pycityschools_file)
student_data_df = pd.read_csv(student_file)

pycityschools_data_df.head()
student_data_df.head()
110/4:
pycityschools_data_complete = pd.merge(student_data_df, pycityschools_data_df, 
                                       on=["school_name", "school_name"], how="left")

pycityschools_data_complete.head()
110/5:
# Count total schools
total_schools = pycityschools_file_df["school_name"].count()
total_schools
110/6:
# Count total schools
total_schools = pycityschools_data_complete["school_name"].count()
total_schools
110/7:
# Calculate the total number of students
total_students = pycityschools_data_complete["student_name"].count()
total_students
110/8:
# Count total schools
total_schools = pycityschools_data_complete["school_name"].value_counts()
total_schools
110/9:
# Count total schools
total_schools = pycityschools_data_complete["school_name"].count()
total_schools
110/10:
# Calculate the total number of students
total_students = pycityschools_data_complete["student_name"].value_counts()
total_students
110/11:
# Calculate the total number of students
total_students = pycityschools_data_complete["student_name"].count()
total_students
110/12:
# Calculate Total Budget
total_budget = pycityschools_data_complete["budget"].sum()
total_budget
110/13:
# Calculate the average math score
average_mathscore = pycityschools_data_complete["math_score"].mean()
average_mathscore
110/14:
# Calculate the average reading score
average_readingscore = pycityschools_data_complete["reading_score"].mean()
average_reradingscore
110/15:
# Calculate the average reading score
average_readingscore = pycityschools_data_complete["reading_score"].mean()
average_readingscore
110/16:
# Calculate the average math score
average_mathscore = pycityschools_data_complete["math_score"].mean().round(2)
average_mathscore
110/17:
# Calculate the average math score
average_mathscore = pycityschools_data_complete["math_score"].mean().round(2)
average_mathscore
110/18:
# Calculate the average math score
average_mathscore = pycityschools_data_complete["math_score"].mean()
average_mathscore.round(2)
114/1:
# Dependencies
import pandas as pd
114/2:
# load in file
pycityschools_file = "Resources/schools_complete.csv"
student_file = "Resources/students_complete.csv"
114/3:
# Read School and Student Data File and store into Pandas DataFrames
pycityschools_data_df = pd.read_csv(pycityschools_file)
student_data_df = pd.read_csv(student_file)

pycityschools_data_df.head()
student_data_df.head()
114/4:
pycityschools_data_complete = pd.merge(student_data_df, pycityschools_data_df, 
                                       on=["school_name", "school_name"], how="left")

pycityschools_data_complete.head()
114/5:
# Count total schools
total_schools = pycityschools_data_complete["school_name"].count()
total_schools
114/6:
# Calculate the total number of students
total_students = pycityschools_data_complete["student_name"].count()
total_students
114/7:
# Calculate Total Budget
total_budget = pycityschools_data_complete["budget"].sum()
total_budget
114/8:
# Calculate the average math score
average_mathscore = pycityschools_data_complete["math_score"].mean()
average_mathscore.round(2)
114/9:
# Calculate the average math score
average_mathscore = pycityschools_data_complete["math_score"].mean()
average_mathscore
114/10:
# Calculate the average math score
average_mathscore = pycityschools_data_complete["math_score"].mean()
average_mathscore.round()
114/11:
# Calculate the average math score
average_mathscore = pycityschools_data_complete(round["math_score"].mean())
average_mathscore
114/12:
# Calculate the average math score
average_mathscore = round(pycityschools_data_complete["math_score"].mean())
average_mathscore
114/13:
# Calculate the average math score
average_mathscore = round(pycityschools_data_complete["math_score"].mean(), 2)
average_mathscore
114/14:
# Calculate the average reading score
average_readingscore = round(pycityschools_data_complete["reading_score"].mean(), 2)
average_readingscore
115/1:
# Calculate the total number of students
total_students = len[pycityschools_data_complete["student_name"]]
total_students
115/2:
# Calculate the total number of students
total_students = pycityschools_data_complete["student_name"].count()
total_students
115/3:
# Calculate the total number of students
total_students = pycityschools_data_complete["student_name"].count()
total_students
119/1:
# Dependencies
import pandas as pd
121/1:
# Dependencies
import pandas as pd
124/1:
# Dependencies
import pandas as pd
127/1:
# Dependencies
import pandas as pd
129/1:
# Dependencies
import pandas as pd
131/1:
# Dependencies
import pandas as pd
134/1:
# Dependencies
import pandas as pd
132/1:
# Dependencies
import pandas as pd
136/1:
# load in file
pycityschools_file = "Resources/schools_complete.csv"
student_file = "Resources/students_complete.csv"
136/2:
# Read School and Student Data File and store into Pandas DataFrames
pycityschools_data_df = pd.read_csv(pycityschools_file)
student_data_df = pd.read_csv(student_file)

pycityschools_data_df.head()
student_data_df.head()
137/1:
# Dependencies
import pandas as pd
139/1:
# Dependencies
import pandas as pd
142/1:
# Dependencies
import pandas as pd
142/2:
# load in file
pycityschools_file = "Resources/schools_complete.csv"
student_file = "Resources/students_complete.csv"
142/3:
# Read School and Student Data File and store into Pandas DataFrames
pycityschools_data_df = pd.read_csv(pycityschools_file)
student_data_df = pd.read_csv(student_file)

pycityschools_data_df.head()
student_data_df.head()
143/1:
# Dependencies
import pandas as pd
145/1:
# Dependencies
import pandas as pd
145/2:
# load in file
pycityschools_file = "Resources/schools_complete.csv"
student_file = "Resources/students_complete.csv"
145/3:
# Read School and Student Data File and store into Pandas DataFrames
pycityschools_data_df = pd.read_csv(pycityschools_file)
student_data_df = pd.read_csv(student_file)

pycityschools_data_df.head()
student_data_df.head()
145/4:
pycityschools_data_complete = pd.merge(student_data_df, pycityschools_data_df, 
                                       on=["school_name", "school_name"], how="left")

pycityschools_data_complete.head()
145/5:
# Count total schools
total_schools = pycityschools_data_complete["school_name"].count()
total_schools
145/6:
# Calculate the total number of students
total_students = pycityschools_data_complete["student_name"].count()
total_students
145/7:
# Calculate Total Budget
total_budget = pycityschools_data_complete["budget"].sum()
total_budget
145/8:
# Calculate the average math score
average_mathscore = round(pycityschools_data_complete["math_score"].mean(), 2)
average_mathscore
145/9:
# Calculate the average reading score
average_readingscore = round(pycityschools_data_complete["reading_score"].mean(), 2)
average_readingscore
145/10:
# Calculate the percentage of students with a passing math score (70 or greater)
math_percentage = pycityschools_data_complete.loc[pycityschools_data_complete["math_score"] >= 70] / 
                  len(pycityschools_data_complete
145/11:
# Calculate the total number of students
total_students = pycityschools_data_complete["student_name"].count().unique()
total_students
145/12:
# Calculate the total number of students
total_students = pycityschools_data_complete["student_name"].sum()
total_students
145/13:
# Calculate the total number of students
total_students = pycityschools_data_complete["student_name"].sum()
total_students.head()
145/14:
# Calculate the total number of students
total_students = pycityschools_data_complete["student_name"].count()
total_students
145/15:
# Calculate the percentage of students with a passing math score (70 or greater)
math_passing = pycityschools_data_complete.loc[pycityschools_data_complete["math_score"] >= 70]
math_passing
145/16:
# Calculate the percentage of students with a passing math score (70 or greater)
math_passing = pycityschools_data_complete.loc[pycityschools_data_complete["math_score"] >= 70].count()
math_passing
145/17:
# Calculate the percentage of students with a passing math score (70 or greater)
math_passing = pycityschools_data_complete.query("math_score >=70")["School ID"].count() / total_students*100
math_passing
145/18:
# Calculate the percentage of students with a passing math score (70 or greater)
math_passing = pycityschools_data_complete.query("math_score >=70")["School ID"].count() / round(total_students*100, 2)
math_passing
145/19:
# Calculate the percentage of students with a passing math score (70 or greater)
math_passing = pycityschools_data_complete.query("math_score >=70")["School ID"].count() / total_students*100
math_passing
145/20:
# Calculate the percentage of students with a passing math score (70 or greater)
math_passing = round(pycityschools_data_complete.query("math_score >=70")["School ID"].count() / total_students*100, 2)
math_passing
145/21:
# Calculate the percentage of students with a passing reading score (70 or greater)
reading_passing = round(pycityschools_data_complete.query("reading_score >=70")["School ID"].count() / total_students*100, 2)
reading_passing
145/22:
# Calculate the percentage of students who passed math and reading (% Overall Passing)
overall_passing = round(pycityschools_data_complete.query("reading_score >=70" & "math_score >=70")["School ID"].count() / total_students*100)
overall_passing
145/23:
# Calculate the percentage of students who passed math and reading (% Overall Passing)
overall_passing = (reading_passing + math_passing) / 2
overall_passing
145/24:
# Calculate the percentage of students who passed math and reading (% Overall Passing)
overall_passing = round((reading_passing + math_passing) / 2, 2)
overall_passing
145/25:
# Create a dataframe to hold the above results
summary_df = pd.DataFrame({"Total Schools": [total_schools],
                          "Total Students": [total_students],
                          "Total Budget": [total_budget],
                          "Average Math Score": [average_mathscore],
                          "Average Reading Score": [average_readingscore],
                          "% of Students with Passing Math Score": [math_passing],
                          "% of Students with Passing Reading Score": [reading_passing],
                          "% of students with Passing Reading and Math Score": [overall_passing]
                          })
summary_df
145/26:
# Calculate the total number of students
total_students = pycityschools_data_complete["student_name"].sum()
total_students
145/27:
# Calculate the total number of students
total_students = pycityschools_data_complete["student_name"].count()
total_students
145/28:
# Count total schools
total_schools = pycityschools_data_complete["School ID"].count()
total_schools
145/29:
# Count total schools
total_schools = pycityschools_data_complete["School ID"].count()
total_schools
145/30:
# Calculate the total number of students
total_students = pycityschools_data_complete["size"].sum()
total_students
149/1:
# Dependencies
import pandas as pd
import numpy as np
149/2:
# load in file
pycityschools_file = "Resources/schools_complete.csv"
student_file = "Resources/students_complete.csv"
149/3:
# Read School and Student Data File and store into Pandas DataFrames
pycityschools_data_df = pd.read_csv(pycityschools_file)
student_data_df = pd.read_csv(student_file)

pycityschools_data_df.head()
student_data_df.head()
149/4:
pycityschools_data_complete = pd.merge(student_data_df, pycityschools_data_df, 
                                       on=["school_name", "school_name"], how="left")

pycityschools_data_complete.head()
149/5:
# Count total schools
total_schools = pycityschools_data_complete["School ID"].count()
total_schools
149/6:
# Calculate the total number of students
total_students = pycityschools_data_complete["size"].sum()
total_students
149/7:
# Calculate Total Budget
total_budget = pycityschools_data_complete["budget"].sum()
total_budget
149/8:
# Calculate the average math score
average_mathscore = round(pycityschools_data_complete["math_score"].mean(), 2)
average_mathscore
149/9:
# Calculate the average reading score
average_readingscore = round(pycityschools_data_complete["reading_score"].mean(), 2)
average_readingscore
149/10:
# Calculate the percentage of students with a passing math score (70 or greater)
math_passing = round(pycityschools_data_complete.query("math_score >=70")["School ID"].count() / total_students*100, 2)
math_passing
149/11:
# Calculate the percentage of students with a passing reading score (70 or greater)
reading_passing = round(pycityschools_data_complete.query("reading_score >=70")["School ID"].count() / total_students*100, 2)
reading_passing
149/12:
# Calculate the percentage of students who passed math and reading (% Overall Passing)
overall_passing = round((reading_passing + math_passing) / 2, 2)
overall_passing
149/13:
# Create a dataframe to hold the above results
summary_df = pd.DataFrame({"Total Schools": [total_schools],
                          "Total Students": [total_students],
                          "Total Budget": [total_budget],
                          "Average Math Score": [average_mathscore],
                          "Average Reading Score": [average_readingscore],
                          "% of Students with Passing Math Score": [math_passing],
                          "% of Students with Passing Reading Score": [reading_passing],
                          "% of students with Passing Reading and Math Score": [overall_passing]
                          })
summary_df
149/14: # Give the displayed data cleaner formatting
149/15:
# Create an overview table that summarizes key metrics about each school
    # School Name
    # School Type
    # Total Students
    # Total School Budget
    # Per Student Budget
    # Average Math Score
    # Average Reading Score
    # % Passing Math
    # % Passing Reading
    # % Overall Passing (The percentage of students that passed math and reading.)
    
# Create a dataframe to hold the above results
149/16:
# Give the displayed data cleaner formatting
summary_df["Total Schools"] = summary_df["Total Schools"].astype(float).map("{:,.2f}".format)
149/17:
# Give the displayed data cleaner formatting
summary_df["Total Schools"] = summary_df["Total Schools"].astype(float).map("{:,.2f}".format)
summary_df
149/18:
# Give the displayed data cleaner formatting
summary_df["Total Schools"] = summary_df["Total Schools"].astype(str).map("{:,.2f}".format)
summary_df
149/19:
# Give the displayed data cleaner formatting
summary_df["Total Schools"] = summary_df["Total Schools"].map("{:,.2f}".format)
summary_df
154/1:
import numpy as np
import matplotlib.pyplot as plt
154/2:
x_axis = np.arange(1,13,1)
x_axis
154/3: points = [39, 42, 51, 62, 72, 82, 86, 84, 77, 65, 55, 44]
154/4:
plt.plot(x_axis, point)
plt.show()
154/5:
plt.plot(x_axis, points)
plt.show()
154/6:
# Use a list comprehension to convert the temperature to degrees Celsius.
points_celsius = [(x-32) * 0.56 for x in points]
points_celsius
154/7:
# Plot using Celsius
plt.plot(x_axis, points_celsius)
plt.show()
154/8:
# Plot both
plt.plot(x_axis, points)
plt.plot(x_axis, points_celsius)
plt.show()
154/9:
# Use a list comprehension to convert the temperature to degrees Celsius.
points_celsius = [(temp-32) * 0.56 for temp in points]
points_celsius
155/1:
import numpy as np
import matplotlib.pyplot as plt
155/2:
x_axis = np.arange(1,13,1)
x_axis
155/3: points_f = [39, 42, 51, 62, 72, 82, 86, 84, 77, 65, 55, 44]
155/4:
plt.plot(x_axis, points_f)
plt.show()
155/5:
# Use a list comprehension to convert the temperature to degrees Celsius.
points_c = [(temp-32) * 0.56 for temp in points]
points_c
155/6:
# Use a list comprehension to convert the temperature to degrees Celsius.
points_c = [(temp-32) * 0.56 for temp in points_f]
points_c
155/7:
# Plot using Celsius
plt.plot(x_axis, points_c)
plt.show()
155/8:
# Plot both
plt.plot(x_axis, points_f)
plt.plot(x_axis, points_c)
plt.show()
159/1:
# Include this line to make plots interactive
%matplotlib notebook
159/2:
#Dependencies
import matplotlib.pyplot as plt
import numpy as np
159/3:
# Set x-axis 
x_axis = np.arange(1,31)
x_axis
159/4:
#average weather
points_f = [39, 42, 51, 62, 72, 82, 86, 84, 77, 65, 55, 44]
159/5:
# Convert to celcius 
points_c = [(temp-32) * 0.56 for temp in points_f]
points_c
159/6:
farenheit, = plt.plot(x_axis, points_f, marker="+", color="blue", linewidth=1, label="Fahreneit")
celsius, = plt.plot(X_axis, points_c, marke="s", color="Red", linewidth=1, label="Celsius")
160/1:
farenheit, = plt.plot(x_axis, points_f, marker="+", color="blue", linewidth=1, label="Fahreneit")
celsius, = plt.plot(X_axis, points_c, marke="s", color="Red", linewidth=1, label="Celsius")
160/2:
fahrenheit, = plt.plot(x_axis, points_F, marker="+",color="blue", linewidth=1, label="Fahreneit")
celsius, = plt.plot(x_axis, points_C, marker="s", color="Red", linewidth=1, label="Celsius")
161/1:
# Include this line to make plots interactive
%matplotlib notebook
161/2:
# Dependencies
import matplotlib.pyplot as plt
import numpy as np
161/3:
# Set x axis to numerical value for month
x_axis = np.arange(1,13,1)
x_axis
161/4:
# Avearge weather temp
points_F = [39, 42, 51, 62, 72, 82, 86, 84, 77, 65, 55, 44]
161/5:
# Convert to Celsius C = (F-32) * 0.56
points_C = [(x-32) * 0.56 for x in points_F]
points_C
161/6:
# Create a handle for each plot
fahrenheit, = plt.plot(x_axis, points_F, marker="+",color="blue", linewidth=1, label="Fahreneit")
celsius, = plt.plot(x_axis, points_C, marker="s", color="Red", linewidth=1, label="Celsius")
161/7:
# Set our legend to where the chart thinks is best
plt.legend(handles=[fahrenheit, celsius], loc="best")
161/8:
# Create labels for the X and Y axis
plt.xlabel("Months")
plt.ylabel("Degrees")
161/9:
# Save and display the chart
plt.savefig("../Images/avg_temp.png")
plt.show()
162/1:
# Include this line to make plots interactive
%matplotlib notebook
162/2:
#Dependencies
import matplotlib.pyplot as plt
import numpy as np
162/3:
# Set x-axis 
x_axis = np.arange(1,31)
x_axis
162/4:
#average weather
points_f = [39, 42, 51, 62, 72, 82, 86, 84, 77, 65, 55, 44]
162/5:
# Convert to celcius 
points_c = [(temp-32) * 0.56 for temp in points_f]
points_c
162/6:
fahrenheit, = plt.plot(x_axis, points_F, marker="+",color="blue", linewidth=1, label="Fahreneit")
celsius, = plt.plot(x_axis, points_C, marker="s", color="Red", linewidth=1, label="Celsius")
163/1:
# Include this line to make plots interactive
%matplotlib notebook
163/2:
#Dependencies
import matplotlib.pyplot as plt
import numpy as np
163/3:
# Set x-axis 
x_axis = np.arange(1,31)
x_axis
163/4:
#average weather
points_f = [39, 42, 51, 62, 72, 82, 86, 84, 77, 65, 55, 44]
163/5:
# Convert to celcius 
points_c = [(temp-32) * 0.56 for temp in points_f]
points_c
163/6:
fahrenheit, = plt.plot(x_axis, points_f, marker="+",color="blue", linewidth=1, label="Fahreneit")
celsius, = plt.plot(x_axis, points_c, marker="s", color="Red", linewidth=1, label="Celsius")
163/7:
# Set x-axis 
x_axis = np.arange(1,31,1)
x_axis
163/8:
fahrenheit, = plt.plot(x_axis, points_f, marker="+",color="blue", linewidth=1, label="Fahreneit")
celsius, = plt.plot(x_axis, points_c, marker="s", color="Red", linewidth=1, label="Celsius")
165/1:
# Include this line to make plots interactive
%matplotlib notebook
165/2:
#Dependencies
import matplotlib.pyplot as plt
import numpy as np
165/3:
# Set x-axis 
x_axis = np.arange(1,31,1)
x_axis
165/4:
#average weather
points_f = [39, 42, 51, 62, 72, 82, 86, 84, 77, 65, 55, 44]
165/5:
# Convert to celcius 
points_c = [(temp-32) * 0.56 for temp in points_f]
points_c
165/6:
fahrenheit, = plt.plot(x_axis, points_f, marker="+",color="blue", linewidth=1, label="Fahreneit")
celsius, = plt.plot(x_axis, points_c, marker="s", color="Red", linewidth=1, label="Celsius")
166/1:
fahrenheit_handle, = plt.plot(x_axis, points_f, marker="+",color="blue", linewidth=1, label="Fahreneit")
celsius_handle, = plt.plot(x_axis, points_c, marker="s", color="Red", linewidth=1, label="Celsius")
166/2: plt.legend(handles=[fahrenheit, celsius], loc="best")
167/1:
# Include this line to make plots interactive
%matplotlib notebook
167/2:
#Dependencies
import matplotlib.pyplot as plt
import numpy as np
167/3:
# Set x-axis 
x_axis = np.arange(1,31,1)
x_axis
167/4:
#average weather
points_f = [39, 42, 51, 62, 72, 82, 86, 84, 77, 65, 55, 44]
167/5:
# Convert to celcius 
points_c = [(temp-32) * 0.56 for temp in points_f]
points_c
167/6:
fahrenheit_handle, = plt.plot(x_axis, points_f, marker="+",color="blue", linewidth=1, label="Fahreneit")
celsius_handle, = plt.plot(x_axis, points_c, marker="s", color="Red", linewidth=1, label="Celsius")
168/1:
# Include this line to make plots interactive
%matplotlib notebook
168/2:
#Dependencies
import matplotlib.pyplot as plt
import numpy as np
168/3:
# Set x-axis 
x_axis = np.arange(1,31,1)
x_axis
168/4:
#average weather
points_f = [39, 42, 51, 62, 72, 82, 86, 84, 77, 65, 55, 44]
168/5:
# Convert to celcius 
points_c = [(temp-32) * 0.56 for temp in points_f]
points_c
168/6:
fahrenheit_handle, = plt.plot(x_axis, points_f, marker="+",color="blue", linewidth=1, label="Fahreneit")
celsius_handle, = plt.plot(x_axis, points_c, marker="s", color="Red", linewidth=1, label="Celsius")
169/1:
# Include this line to make plots interactive
%matplotlib notebook
169/2:
#Dependencies
import matplotlib.pyplot as plt
import numpy as np
169/3:
# Set x-axis 
x_axis = np.arange(1, 31, 1)
x_axis
169/4:
#average weather
points_f = [39, 42, 51, 62, 72, 82, 86, 84, 77, 65, 55, 44]
169/5:
# Convert to celcius 
points_c = [(temp-32) * 0.56 for temp in points_f]
points_c
169/6:
fahrenheit_handle, = plt.plot(x_axis, points_f, marker="+",color="blue", linewidth=1, label="Fahreneit")
celsius_handle, = plt.plot(x_axis, points_c, marker="s", color="Red", linewidth=1, label="Celsius")
172/1: %matplotlib notebook
172/2:
import matplotlib.pyplot as plt
import numpy as np
172/3:
# Create x and y axis
time = np.arange(1,130,10)
danger_drop_speeds = [9,8, 90, 85, 80, 70, 70, 65, 55, 60, 70, 65, 50]
railgun_speeds = [75, 70, 60, 65, 60, 45, 55, 50, 40, 40, 35, 35, 30]
172/4:
# plot the chart
danger_drop, = plt.plot(time, danger_drop_speeds, color="red", label="Danger Drop")
railgun, = plt.plot(time, railgun_speeds, color="blue", label="RailGun")
173/1: %matplotlib notebook
173/2:
import matplotlib.pyplot as plt
import numpy as np
173/3:
# Create x and y axis
time = np.arange(1,130,10)
danger_drop_speeds = [9,8, 90, 85, 80, 70, 70, 65, 55, 60, 70, 65, 50]
railgun_speeds = [75, 70, 60, 65, 60, 45, 55, 50, 40, 40, 35, 35, 30]
173/4:
# plot the chart
danger_drop, = plt.plot(time, danger_drop_speeds, color="red", label="Danger Drop")
railgun, = plt.plot(time, railgun_speeds, color="blue", label="RailGun")
173/5:
# Add labels and titles
plt.title("Coaster Speed over Time")
plt.xlabel("Coaster Runtime")
plt.ylabel("Speed (MPH)")
173/6:
# Set limits
plt.xlim(0,120)
plt.ylim(5, 95)
173/7:
# Create legend for the Chart
plt.legend(handles=[danger_drop, railgun], loc="best")
173/8:
# add grid lines
plt.grid()
173/9: plt.show()
176/1: %matplotlib notebook
176/2:
import matplotlib.pyplot as plt
import numpy as np
176/3:
cities = ["New Orleans", "Milwaukee", "Omaha", "Pittsburgh", "Toledo"]
bars_in_cities = [8.6, 8.5, 8.3, 7.9, 7.2]
x_axis = np.arange(len(bars_in_cities))
176/4:
# Create a bar chart based upon the above data
plt.bar(x_axis, bars_in_cities, color="g", alpha=0.5, align="center")
176/5:
# Create the ticks for our bar chart's x axis
tick_locations = [value for value in x_axis]
plt.xticks(tick_locations, cities)
176/6:
# Set the limits of the x axis
plt.xlim(-0.75, len(x_axis)-0.25)
176/7:
# Set the limits of the y axis
plt.ylim(0, max(bars_in_cities)+0.4)
176/8:
# Give the chart a title, x label, and y label
plt.title("Density of Bars in Cities")
plt.xlabel("Cities")
plt.ylabel("Bars Per 10,000 Households")
176/9:
# Save an image of the chart and print it to the screen
plt.savefig("../Images/BarDensity.png")
plt.show()
177/1: %matplotlib notebook
177/2:
import matplotlib.pyplot as plt
import numpy as np
177/3:
cities = ["New Orleans", "Milwaukee", "Omaha", "Pittsburgh", "Toledo"]
bars_in_cities = [8.6, 8.5, 8.3, 7.9, 7.2]
x_axis = np.arange(len(bars_in_cities))
177/4:
# Create a bar chart based upon the above data
plt.bar(x_axis, bars_in_cities, color="g", alpha=0.5, align="center")
177/5:
# Create the ticks for our bar chart's x axis
tick_locations = [value for value in x_axis]
plt.xticks(tick_locations, cities)
177/6:
# Set the limits of the x axis
plt.xlim(-0.75, len(x_axis)-0.25)
177/7:
# Set the limits of the y axis
plt.ylim(0, max(bars_in_cities)+5)
177/8:
# Give the chart a title, x label, and y label
plt.title("Density of Bars in Cities")
plt.xlabel("Cities")
plt.ylabel("Bars Per 10,000 Households")
177/9:
# Save an image of the chart and print it to the screen
plt.savefig("../Images/BarDensity.png")
plt.show()
178/1: %matplotlib notebook
178/2:
import matplotlib.pyplot as plt
import numpy as np
178/3:
cities = ["New Orleans", "Milwaukee", "Omaha", "Pittsburgh", "Toledo"]
bars_in_cities = [8.6, 8.5, 8.3, 7.9, 7.2]
x_axis = np.arange(len(bars_in_cities))
178/4:
# Create a bar chart based upon the above data
plt.bar(x_axis, bars_in_cities, color="g", alpha=0.5, align="center")
178/5:
# Create the ticks for our bar chart's x axis
tick_locations = [value for value in x_axis]
plt.xticks(tick_locations, cities)
178/6:
# Set the limits of the x axis
plt.xlim(-0.75, len(x_axis)-0.25)
178/7:
# Set the limits of the y axis
plt.ylim(0, max(bars_in_cities)+1)
178/8:
# Give the chart a title, x label, and y label
plt.title("Density of Bars in Cities")
plt.xlabel("Cities")
plt.ylabel("Bars Per 10,000 Households")
178/9:
# Save an image of the chart and print it to the screen
plt.savefig("../Images/BarDensity.png")
plt.show()
181/1: %matplotlib notebook
181/2:
import matplotlib.pyplot as plt
import numpy as np
181/3:
pies = ["Apple", "Pumpkin", "Chocolate Creme", "Cherry", "Apple Crumb",
        "Pecan", "Lemon Meringue", "Blueberry", "Key Lime", "Peach"]
pie_votes = [47, 37, 32, 27, 25, 24, 24, 21, 18, 16]
colors = ["yellow", "green", "lightblue", "orange", "red",
          "purple", "pink", "yellowgreen", "lightskyblue", "lightcoral"]
explode = (0.1, 0, 0, 0, 0, 0, 0, 0, 0, 0)
181/4:
# Tell matplotlib to create a pie chart based upon the above data
plt.pie(pie_votes, explode=explode, labels=pies, colors=colors,
       autopct="%1.2f%%", shadow=True, startangle=140)

# Create axes which are equal so we have a perfect circle
plt.axis("equal")

# Save an image of our chart and print the final product to the screen
plt.savefig("../Images/PyPies.png")
184/1: %matplotlib notebook
184/2:
import matplotlib.pyplot as plt
import numpy as np
184/3:
temp = [14.2, 16.4, 11.9, 15.2, 18.5, 22.1, 19.4, 25.1, 23.4, 18.1, 22.6, 17.2]
sales = [215, 325, 185, 332, 406, 522, 412, 614, 544, 421, 445, 408]
184/4:
# Tell matplotlib to create a scatter plot based upon the above data
plt.scatter(temp, sales, marker="o", facecolors="purple", edgecolors="blue")
184/5:
# Set the upper and lower limits of our y axis
plt.ylim(180,620)
184/6:
# Set the upper and lower limits of our x axis
plt.xlim(11,26)
184/7:
# Create a title, x label, and y label for our chart
plt.title("Ice Cream Sales v Temperature")
plt.xlabel("Temperature")
plt.ylabel("Sales (Dollars)")
184/8:
# Save an image of the chart and print to screen
# NOTE: If your plot shrinks after saving an image,
# update matplotlib to 2.2 or higher,
# or simply run the above cells again.
plt.savefig("../Images/IceCreamSales.png")
plt.show()
185/1: %matplotlib notebook
185/2:
import matplotlib.pyplot as plt
import numpy as np
185/3:
temp = [14.2, 16.4, 11.9, 15.2, 18.5, 22.1, 19.4, 25.1, 23.4, 18.1, 22.6, 17.2]
sales = [215, 325, 185, 332, 406, 522, 412, 614, 544, 421, 445, 408]
185/4:
# Tell matplotlib to create a scatter plot based upon the above data
plt.scatter(temp, sales, marker="o", facecolors="purple", edgecolors="blue")
185/5:
# Set the upper and lower limits of our y axis
plt.ylim(150,620)
185/6:
# Set the upper and lower limits of our x axis
plt.xlim(11,26)
185/7:
# Create a title, x label, and y label for our chart
plt.title("Ice Cream Sales vs. Temperature")
plt.xlabel("Temperature")
plt.ylabel("Sales (Dollars)")
185/8:
# Save an image of the chart and print to screen
# NOTE: If your plot shrinks after saving an image,
# update matplotlib to 2.2 or higher,
# or simply run the above cells again.
plt.savefig("../Images/IceCreamSales.png")
plt.show()
186/1: %matplotlib notebook
186/2:
import matplotlib.pyplot as plt
import numpy as np
186/3:
temp = [14.2, 16.4, 11.9, 15.2, 18.5, 22.1, 19.4, 25.1, 23.4, 18.1, 22.6, 17.2]
sales = [215, 325, 185, 332, 406, 522, 412, 614, 544, 421, 445, 408]
186/4:
# Tell matplotlib to create a scatter plot based upon the above data
plt.scatter(temp, sales, marker="o", facecolors="purple", edgecolors="blue")
186/5:
# Set the upper and lower limits of our y axis
plt.ylim(150,640)
186/6:
# Set the upper and lower limits of our x axis
plt.xlim(11,26)
186/7:
# Create a title, x label, and y label for our chart
plt.title("Ice Cream Sales vs. Temperature")
plt.xlabel("Temperature")
plt.ylabel("Sales (Dollars)")
186/8:
# Save an image of the chart and print to screen
# NOTE: If your plot shrinks after saving an image,
# update matplotlib to 2.2 or higher,
# or simply run the above cells again.
plt.savefig("../Images/IceCreamSales.png")
plt.show()
189/1: %matplotlib notebook
189/2:
# Dependencies
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
189/3:
# Load in csv
rain_df = pd.read_csv("../Resources/avg_rain_state.csv")
rain_df.head()
189/4:
# Create a list indicating where to write x labels & set figure size
plt.figure(figsize=(20,3))
plt.bar(x_axis, rain_df["Inches"], color='b', alpha=0.5, align="edge")
plt.xticks(tick_locations, rain_df["State"], rotation="vertical")
190/1: %matplotlib notebook
190/2:
# Dependencies
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
190/3:
# Load in csv
rain_df = pd.read_csv("../Resources/avg_rain_state.csv")
rain_df.head()
190/4:
# Set x axis and tick locations
x_axis = np.arange(len(rain_df))
tick_locations = [value+0.4 for value in x_axis]
190/5:
# Create a list indicating where to write x labels & set figure size
plt.figure(figsize=(20,3))
plt.bar(x_axis, rain_df["Inches"], color='b', alpha=0.5, align="edge")
plt.xticks(tick_locations, rain_df["State"], rotation="vertical")
190/6:
# set x and y limits
plt.xlim(-0.25, len(x_axis))
plt.ylim(0, max(rain_df["Inches"])+10)
191/1: %matplotlib notebook
191/2:
# Dependencies
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
191/3:
# Load in csv
rain_df = pd.read_csv("../Resources/avg_rain_state.csv")
rain_df.head()
191/4:
# Set x axis and tick locations
x_axis = np.arange(len(rain_df))
tick_locations = [value+0.4 for value in x_axis]
191/5:
# Create a list indicating where to write x labels & set figure size
plt.figure(figsize=(20,3))
plt.bar(x_axis, rain_df["Inches"], color='b', alpha=0.5, align="edge")
plt.xticks(tick_locations, rain_df["State"], rotation="vertical")
191/6:
# set x and y limits
plt.xlim(-0.25, len(x_axis))
plt.ylim(0, max(rain_df["Inches"])+10)
191/7:
# Save our graph and display it
plt.tight_layout()
plt.savefig("../Images/avg_state_rain.png")
plt.show()
156/1:
# Calculate the percentage of students with a passing math score (70 or greater)
math_passing = round(pycityschools_data_complete.query("math_score >=70")["School ID"].count() / total_students, 2)*100
math_passing
156/2:
# Calculate the percentage of students with a passing math score (70 or greater)
math_passing = round(pycityschools_data_complete.query("math_score >=70")["School ID"].count() / total_students, 2)
math_passing
156/3:
# Calculate the percentage of students with a passing math score (70 or greater)
math_passing = round(pycityschools_data_complete.query("math_score >=70")["School ID"].count() / total_students*100, 2)
math_passing
192/1:
# Dependencies
import pandas as pd
import numpy as np
192/2:
# load in file
pycityschools_file = "Resources/schools_complete.csv"
student_file = "Resources/students_complete.csv"
192/3:
# Read School and Student Data File and store into Pandas DataFrames
pycityschools_data_df = pd.read_csv(pycityschools_file)
student_data_df = pd.read_csv(student_file)

pycityschools_data_df.head()
student_data_df.head()
192/4:
pycityschools_data_complete = pd.merge(student_data_df, pycityschools_data_df, 
                                       on=["school_name", "school_name"], how="left")

pycityschools_data_complete.head()
192/5:
# Count total schools
total_schools = pycityschools_data_complete["School ID"].count()
total_schools
192/6:
# Calculate the total number of students
total_students = pycityschools_data_complete["size"].sum()
total_students
192/7:
# Calculate Total Budget
total_budget = pycityschools_data_complete["budget"].sum()
total_budget
192/8:
# Calculate the average math score
average_mathscore = round(pycityschools_data_complete["math_score"].mean(), 2)
average_mathscore
192/9:
# Calculate the average reading score
average_readingscore = round(pycityschools_data_complete["reading_score"].mean(), 2)
average_readingscore
192/10:
# Calculate the percentage of students with a passing math score (70 or greater)
math_passing = round(pycityschools_data_complete.query("math_score >=70")["School ID"].count() / total_students*100, 2)
math_passing
192/11:
# Calculate the percentage of students with a passing reading score (70 or greater)
reading_passing = round(pycityschools_data_complete.query("reading_score >=70")["School ID"].count() / total_students*100, 2)
reading_passing
192/12:
# Calculate the percentage of students who passed math and reading (% Overall Passing)
overall_passing = round((reading_passing + math_passing) / 2, 2)
overall_passing
192/13:
# Create a dataframe to hold the above results
summary_df = pd.DataFrame({"Total Schools": [total_schools],
                          "Total Students": [total_students],
                          "Total Budget": [total_budget],
                          "Average Math Score": [average_mathscore],
                          "Average Reading Score": [average_readingscore],
                          "% of Students with Passing Math Score": [math_passing],
                          "% of Students with Passing Reading Score": [reading_passing],
                          "% of students with Passing Reading and Math Score": [overall_passing]
                          })
summary_df
192/14:
# Give the displayed data cleaner formatting
summary_df["Total Schools"] = summary_df["Total Schools"].map("{:,.2f}".format)
summary_df
192/15:
# Create an overview table that summarizes key metrics about each school
    # School Name
    # School Type
    # Total Students
    # Total School Budget
    # Per Student Budget
    # Average Math Score
    # Average Reading Score
    # % Passing Math
    # % Passing Reading
    # % Overall Passing (The percentage of students that passed math and reading.)
    
# Create a dataframe to hold the above results
192/16:
# Give the displayed data cleaner formatting
summary_df["Total Schools"] = summary_df["Total Schools"].map("{:,.1f}".format)
summary_df
193/1:
# Dependencies
import pandas as pd
import numpy as np
193/2:
# load in file
pycityschools_file = "Resources/schools_complete.csv"
student_file = "Resources/students_complete.csv"
193/3:
# Read School and Student Data File and store into Pandas DataFrames
pycityschools_data_df = pd.read_csv(pycityschools_file)
student_data_df = pd.read_csv(student_file)

pycityschools_data_df.head()
student_data_df.head()
193/4:
pycityschools_data_complete = pd.merge(student_data_df, pycityschools_data_df, 
                                       on=["school_name", "school_name"], how="left")

pycityschools_data_complete.head()
193/5:
# Count total schools
total_schools = pycityschools_data_complete["School ID"].count()
total_schools
193/6:
# Calculate the total number of students
total_students = pycityschools_data_complete["size"].sum()
total_students
193/7:
# Calculate Total Budget
total_budget = pycityschools_data_complete["budget"].sum()
total_budget
193/8:
# Calculate the average math score
average_mathscore = round(pycityschools_data_complete["math_score"].mean(), 2)
average_mathscore
193/9:
# Calculate the average reading score
average_readingscore = round(pycityschools_data_complete["reading_score"].mean(), 2)
average_readingscore
193/10:
# Calculate the percentage of students with a passing math score (70 or greater)
math_passing = round(pycityschools_data_complete.query("math_score >=70")["School ID"].count() / total_students*100, 2)
math_passing
193/11:
# Calculate the percentage of students with a passing reading score (70 or greater)
reading_passing = round(pycityschools_data_complete.query("reading_score >=70")["School ID"].count() / total_students*100, 2)
reading_passing
193/12:
# Calculate the percentage of students who passed math and reading (% Overall Passing)
overall_passing = round((reading_passing + math_passing) / 2, 2)
overall_passing
193/13:
# Create a dataframe to hold the above results
summary_df = pd.DataFrame({"Total Schools": [total_schools],
                          "Total Students": [total_students],
                          "Total Budget": [total_budget],
                          "Average Math Score": [average_mathscore],
                          "Average Reading Score": [average_readingscore],
                          "% of Students with Passing Math Score": [math_passing],
                          "% of Students with Passing Reading Score": [reading_passing],
                          "% of students with Passing Reading and Math Score": [overall_passing]
                          })
summary_df
193/14:
# Give the displayed data cleaner formatting
summary_df["Total Schools"] = summary_df["Total Schools"].map("{:,.2f}".format)
summary_df
193/15:
# Create an overview table that summarizes key metrics about each school
    # School Name
    # School Type
    # Total Students
    # Total School Budget
    # Per Student Budget
    # Average Math Score
    # Average Reading Score
    # % Passing Math
    # % Passing Reading
    # % Overall Passing (The percentage of students that passed math and reading.)
    
# Create a dataframe to hold the above results
193/16:
# Give the displayed data cleaner formatting
summary_df["Total Schools"] = summary_df["Total Schools"].map("{:,.2f}".format)
summary_df["Total Students"] = summary_df["Total Students"].map("{:,.2f}".format)
summary_df
194/1:
# Dependencies
import pandas as pd
import numpy as np
194/2:
# load in file
pycityschools_file = "Resources/schools_complete.csv"
student_file = "Resources/students_complete.csv"
194/3:
# Read School and Student Data File and store into Pandas DataFrames
pycityschools_data_df = pd.read_csv(pycityschools_file)
student_data_df = pd.read_csv(student_file)

pycityschools_data_df.head()
student_data_df.head()
194/4:
pycityschools_data_complete = pd.merge(student_data_df, pycityschools_data_df, 
                                       on=["school_name", "school_name"], how="left")

pycityschools_data_complete.head()
194/5:
# Count total schools
total_schools = pycityschools_data_complete["School ID"].count()
total_schools
194/6:
# Calculate the total number of students
total_students = pycityschools_data_complete["size"].sum()
total_students
194/7:
# Calculate Total Budget
total_budget = pycityschools_data_complete["budget"].sum()
total_budget
194/8:
# Calculate the average math score
average_mathscore = round(pycityschools_data_complete["math_score"].mean(), 2)
average_mathscore
194/9:
# Calculate the average reading score
average_readingscore = round(pycityschools_data_complete["reading_score"].mean(), 2)
average_readingscore
194/10:
# Calculate the percentage of students with a passing math score (70 or greater)
math_passing = round(pycityschools_data_complete.query("math_score >=70")["School ID"].count() / total_students*100, 2)
math_passing
194/11:
# Calculate the percentage of students with a passing reading score (70 or greater)
reading_passing = round(pycityschools_data_complete.query("reading_score >=70")["School ID"].count() / total_students*100, 2)
reading_passing
194/12:
# Calculate the percentage of students who passed math and reading (% Overall Passing)
overall_passing = round((reading_passing + math_passing) / 2, 2)
overall_passing
194/13:
# Create a dataframe to hold the above results
summary_df = pd.DataFrame({"Total Schools": [total_schools],
                          "Total Students": [total_students],
                          "Total Budget": [total_budget],
                          "Average Math Score": [average_mathscore],
                          "Average Reading Score": [average_readingscore],
                          "% of Students with Passing Math Score": [math_passing],
                          "% of Students with Passing Reading Score": [reading_passing],
                          "% of students with Passing Reading and Math Score": [overall_passing]
                          })
summary_df
194/14:
# Give the displayed data cleaner formatting
summary_df["Total Schools"] = summary_df["Total Schools"].map("{:,.2f}".format)
summary_df["Total Students"] = summary_df["Total Students"].map("{:,.2f}".format)
summary_df
194/15:
# Create an overview table that summarizes key metrics about each school
    # School Name
    # School Type
    # Total Students
    # Total School Budget
    # Per Student Budget
    # Average Math Score
    # Average Reading Score
    # % Passing Math
    # % Passing Reading
    # % Overall Passing (The percentage of students that passed math and reading.)
    
# Create a dataframe to hold the above results
195/1:
# Dependencies
import pandas as pd
import numpy as np
195/2:
# load in file
pycityschools_file = "Resources/schools_complete.csv"
student_file = "Resources/students_complete.csv"
195/3:
# Read School and Student Data File and store into Pandas DataFrames
pycityschools_data_df = pd.read_csv(pycityschools_file)
student_data_df = pd.read_csv(student_file)

pycityschools_data_df.head()
student_data_df.head()
195/4:
pycityschools_data_complete = pd.merge(student_data_df, pycityschools_data_df, 
                                       on=["school_name", "school_name"], how="left")

pycityschools_data_complete.head()
195/5:
# Count total schools
total_schools = pycityschools_data_complete["School ID"].count()
total_schools
195/6:
# Calculate the total number of students
total_students = pycityschools_data_complete["size"].sum()
total_students
195/7:
# Calculate Total Budget
total_budget = pycityschools_data_complete["budget"].sum()
total_budget
195/8:
# Calculate the average math score
average_mathscore = round(pycityschools_data_complete["math_score"].mean(), 2)
average_mathscore
195/9:
# Calculate the average reading score
average_readingscore = round(pycityschools_data_complete["reading_score"].mean(), 2)
average_readingscore
195/10:
# Calculate the percentage of students with a passing math score (70 or greater)
math_passing = round(pycityschools_data_complete.query("math_score >=70")["School ID"].count() / total_students*100, 2)
math_passing
195/11:
# Calculate the percentage of students with a passing reading score (70 or greater)
reading_passing = round(pycityschools_data_complete.query("reading_score >=70")["School ID"].count() / total_students*100, 2)
reading_passing
195/12:
# Calculate the percentage of students who passed math and reading (% Overall Passing)
overall_passing = round((reading_passing + math_passing) / 2, 2)
overall_passing
195/13:
# Create a dataframe to hold the above results
summary_df = pd.DataFrame({"Total Schools": [total_schools],
                          "Total Students": [total_students],
                          "Total Budget": [total_budget],
                          "Average Math Score": [average_mathscore],
                          "Average Reading Score": [average_readingscore],
                          "% of Students with Passing Math Score": [math_passing],
                          "% of Students with Passing Reading Score": [reading_passing],
                          "% of students with Passing Reading and Math Score": [overall_passing]
                          })
summary_df
195/14:
# Give the displayed data cleaner formatting
summary_df["Total Schools"] = summary_df["Total Schools"].map("{:,.2f}".format)
summary_df["Total Students"] = summary_df["Total Students"].map("{:,.2f}".format)
summary_df["Total Budget"] = summary_df["Total Budget"].map("{:,.2f}".format)
summary_df
195/15:
# Create an overview table that summarizes key metrics about each school
    # School Name
    # School Type
    # Total Students
    # Total School Budget
    # Per Student Budget
    # Average Math Score
    # Average Reading Score
    # % Passing Math
    # % Passing Reading
    # % Overall Passing (The percentage of students that passed math and reading.)
    
# Create a dataframe to hold the above results
196/1:
# Dependencies
import pandas as pd
import numpy as np
196/2:
# load in file
pycityschools_file = "Resources/schools_complete.csv"
student_file = "Resources/students_complete.csv"
196/3:
# Read School and Student Data File and store into Pandas DataFrames
pycityschools_data_df = pd.read_csv(pycityschools_file)
student_data_df = pd.read_csv(student_file)

pycityschools_data_df.head()
student_data_df.head()
196/4:
pycityschools_data_complete = pd.merge(student_data_df, pycityschools_data_df, 
                                       on=["school_name", "school_name"], how="left")

pycityschools_data_complete.head()
196/5:
# Count total schools
total_schools = pycityschools_data_complete["School ID"].count()
total_schools
196/6:
# Calculate the total number of students
total_students = pycityschools_data_complete["size"].sum()
total_students
196/7:
# Calculate Total Budget
total_budget = pycityschools_data_complete["budget"].sum()
total_budget
196/8:
# Calculate the average math score
average_mathscore = round(pycityschools_data_complete["math_score"].mean(), 2)
average_mathscore
196/9:
# Calculate the average reading score
average_readingscore = round(pycityschools_data_complete["reading_score"].mean(), 2)
average_readingscore
196/10:
# Calculate the percentage of students with a passing math score (70 or greater)
math_passing = round(pycityschools_data_complete("math_score >=70")["School ID"].count() / total_students*100, 2)
math_passing
197/1:
# Dependencies
import pandas as pd
import numpy as np
197/2:
# load in file
pycityschools_file = "Resources/schools_complete.csv"
student_file = "Resources/students_complete.csv"
197/3:
# Read School and Student Data File and store into Pandas DataFrames
pycityschools_data_df = pd.read_csv(pycityschools_file)
student_data_df = pd.read_csv(student_file)

pycityschools_data_df.head()
student_data_df.head()
197/4:
pycityschools_data_complete = pd.merge(student_data_df, pycityschools_data_df, 
                                       on=["school_name", "school_name"], how="left")

pycityschools_data_complete.head()
197/5:
# Count total schools
total_schools = pycityschools_data_complete["School ID"].count()
total_schools
197/6:
# Calculate the total number of students
total_students = pycityschools_data_complete["size"].sum()
total_students
197/7:
# Calculate Total Budget
total_budget = pycityschools_data_complete["budget"].sum()
total_budget
197/8:
# Calculate the average math score
average_mathscore = round(pycityschools_data_complete["math_score"].mean(), 2)
average_mathscore
197/9:
# Calculate the average reading score
average_readingscore = round(pycityschools_data_complete["reading_score"].mean(), 2)
average_readingscore
197/10:
# Calculate the percentage of students with a passing math score (70 or greater)
math_passing = round(pycityschools_data_complete("math_score >=70")["School ID"].count() / total_students*100, 2)
math_passing
198/1:
# Dependencies
import pandas as pd
import numpy as np
198/2:
# load in file
pycityschools_file = "Resources/schools_complete.csv"
student_file = "Resources/students_complete.csv"
198/3:
# Read School and Student Data File and store into Pandas DataFrames
pycityschools_data_df = pd.read_csv(pycityschools_file)
student_data_df = pd.read_csv(student_file)

pycityschools_data_df.head()
student_data_df.head()
198/4:
pycityschools_data_complete = pd.merge(student_data_df, pycityschools_data_df, 
                                       on=["school_name", "school_name"], how="left")

pycityschools_data_complete.head()
198/5:
# Count total schools
total_schools = pycityschools_data_complete["School ID"].count()
total_schools
198/6:
# Calculate the total number of students
total_students = pycityschools_data_complete["size"].sum()
total_students
198/7:
# Calculate Total Budget
total_budget = pycityschools_data_complete["budget"].sum()
total_budget
198/8:
# Calculate the average math score
average_mathscore = round(pycityschools_data_complete["math_score"].mean(), 2)
average_mathscore
198/9:
# Calculate the average reading score
average_readingscore = round(pycityschools_data_complete["reading_score"].mean(), 2)
average_readingscore
198/10:
# Calculate the percentage of students with a passing math score (70 or greater)
# math_passing = round(pycityschools_data_complete("math_score >=70")["School ID"].count() / total_students*100, 2)
# math_passing
198/11:
# Calculate the percentage of students with a passing reading score (70 or greater)
reading_passing = round(pycityschools_data_complete("reading_score >=70")["School ID"].count() / total_students*100, 2)
reading_passing
199/1:
# Dependencies
import pandas as pd
import numpy as np
199/2:
# load in file
pycityschools_file = "Resources/schools_complete.csv"
student_file = "Resources/students_complete.csv"
199/3:
# Read School and Student Data File and store into Pandas DataFrames
pycityschools_data_df = pd.read_csv(pycityschools_file)
student_data_df = pd.read_csv(student_file)

pycityschools_data_df.head()
student_data_df.head()
199/4:
pycityschools_data_complete = pd.merge(student_data_df, pycityschools_data_df, 
                                       on=["school_name", "school_name"], how="left")

pycityschools_data_complete.head()
199/5:
# Count total schools
total_schools = pycityschools_data_complete["School ID"].count()
total_schools
199/6:
# Calculate the total number of students
total_students = pycityschools_data_complete["size"].sum()
total_students
199/7:
# Calculate Total Budget
total_budget = pycityschools_data_complete["budget"].sum()
total_budget
199/8:
# Calculate the average math score
average_mathscore = round(pycityschools_data_complete["math_score"].mean(), 2)
average_mathscore
199/9:
# Calculate the average reading score
average_readingscore = round(pycityschools_data_complete["reading_score"].mean(), 2)
average_readingscore
199/10:
# Calculate the percentage of students with a passing math score (70 or greater)
# math_passing = round(pycityschools_data_complete("math_score >=70")["School ID"].count() / total_students*100, 2)
# math_passing
199/11:
# Calculate the percentage of students with a passing reading score (70 or greater)
# reading_passing = round(pycityschools_data_complete("reading_score >=70")["School ID"].count() / total_students*100, 2)
# reading_passing
199/12:
# Calculate the percentage of students who passed math and reading (% Overall Passing)
# overall_passing = round((reading_passing + math_passing) / 2, 2)
# overall_passing
199/13:
# Create a dataframe to hold the above results
summary_df = pd.DataFrame({"Total Schools": [total_schools],
                          "Total Students": [total_students],
                          "Total Budget": [total_budget],
                          "Average Math Score": [average_mathscore],
                          "Average Reading Score": [average_readingscore],
                          "% of Students with Passing Math Score": [math_passing],
                          "% of Students with Passing Reading Score": [reading_passing],
                          "% of students with Passing Reading and Math Score": [overall_passing]
                          })
summary_df
199/14:
# Calculate the percentage of students with a passing math score (70 or greater)
math_passing = round(pycityschools_data_complete("math_score">=70)["School ID"].count() / total_students*100, 2)
math_passing
199/15:
# Calculate the percentage of students with a passing math score (70 or greater)
math_passing = round(pycityschools_data_complete("math_score")["School ID"]>=70 / total_students*100, 2)
math_passing
199/16:
# Calculate the percentage of students with a passing math score (70 or greater)
math_passing = pycityschools_data_complete.loc[pycityschools_data_complete["math score"] >=70]["math score"].count()
percent_math_passing = (math_passing/total_students) * 100
199/17:
# Calculate the percentage of students with a passing math score (70 or greater)
math_passing = pycityschools_data_complete.loc[pycityschools_data_complete["math_score"] >=70]["math_score"].count()
percent_math_passing = (math_passing/total_students) * 100
199/18:
# Calculate the percentage of students with a passing math score (70 or greater)
math_passing = pycityschools_data_complete.loc[pycityschools_data_complete["math_score"] >=70]["math_score"].count()
percent_math_passing = (math_passing/total_students) * 100
percent_math_passing
199/19:
# Calculate the percentage of students with a passing math score (70 or greater)
math_passing = pycityschools_data_complete.loc[pycityschools_data_complete["math_score"] >=70]["math_score"].count()
percent_math_passing = round(math_passing/total_students) * 100, 2)
percent_math_passing
199/20:
# Calculate the percentage of students with a passing math score (70 or greater)
math_passing = pycityschools_data_complete.loc[pycityschools_data_complete["math_score"] >=70]["math_score"].count()
percent_math_passing = round((math_passing/total_students) * 100, 2))
percent_math_passing
199/21:
# Calculate the percentage of students with a passing math score (70 or greater)
math_passing = pycityschools_data_complete.loc[pycityschools_data_complete["math_score"] >=70]["math_score"].count()
percent_math_passing = (math_passing/total_students) * 100
percent_math_passing
199/22:
# Calculate the percentage of students with a passing math score (70 or greater)
math_passing = pycityschools_data_complete.loc[pycityschools_data_complete["math_score"] >=70]["math_score"].count().round(2)
percent_math_passing = (math_passing/total_students) * 100
percent_math_passing
199/23:
# Calculate the percentage of students with a passing math score (70 or greater)
math_passing = pycityschools_data_complete.loc[pycityschools_data_complete["math_score"] >=70]["math_score"].count().round(2)
percent_math_passing = (math_passing/total_students) * 100
percent_math_passing
199/24:
# Calculate the percentage of students with a passing math score (70 or greater)
math_passing = pycityschools_data_complete.loc[pycityschools_data_complete["math_score"] >=70]["math_score"].count()
percent_math_passing = round([math_passing/total_students] * 100, 2)
percent_math_passing
199/25:
# Calculate the percentage of students with a passing math score (70 or greater)
math_passing = pycityschools_data_complete.loc[pycityschools_data_complete["math_score"] >=70]["math_score"].count()
percent_math_passing = (math_passing/total_students) * 100
percent_math_passing
199/26:
# Calculate the percentage of students with a passing reading score (70 or greater)
reading_passing = pycityschools_data_complete.loc[pycityschools_data_complete["reading_score"] >=70]["reading_score"].count()
percent_reading_passing = (reading_passing/total_students) * 100
perecent_reading_passing
199/27:
# Calculate the percentage of students with a passing reading score (70 or greater)
reading_passing = pycityschools_data_complete.loc[pycityschools_data_complete["reading_score"] >=70]["reading_score"].count()
percent_reading_passing = (reading_passing/total_students) * 100
percent_reading_passing
199/28:
# Count total schools
total_schools = pycityschools_data_complete["School ID"].unique().count()
total_schools
199/29:
# Count total schools
unique_schools = pycityschools_data_complete["School ID"].unique()
total_schools = unique_schools.count()
total_schools
199/30:
# Count total schools
unique_schools = pycityschools_data_complete["School ID"].unique()
total_schools = len(unique_schools)
total_schools
199/31:
# Calculate the total number of students
total_students = pycityschools_data_complete["size"].sum()
total_students
200/1:
# Dependencies
import pandas as pd
import numpy as np
200/2:
# load in file
pycityschools_file = "Resources/schools_complete.csv"
student_file = "Resources/students_complete.csv"
200/3:
# Read School and Student Data File and store into Pandas DataFrames
pycityschools_data_df = pd.read_csv(pycityschools_file)
student_data_df = pd.read_csv(student_file)

pycityschools_data_df.head()
student_data_df.head()
200/4:
pycityschools_data_complete = pd.merge(student_data_df, pycityschools_data_df, 
                                       on=["school_name", "school_name"], how="left")

pycityschools_data_complete.head()
200/5:
# Count total schools
unique_schools = pycityschools_data_complete["School ID"].unique()
total_schools = len(unique_schools)
total_schools
200/6:
# Calculate the total number of students
total_students = pycityschools_data_complete["size"].sum()
total_students
200/7:
# Calculate Total Budget
total_budget = pycityschools_data_complete["budget"].sum()
total_budget
200/8:
# Calculate the average math score
average_mathscore = round(pycityschools_data_complete["math_score"].mean(), 2)
average_mathscore
200/9:
# Calculate the average reading score
average_readingscore = round(pycityschools_data_complete["reading_score"].mean(), 2)
average_readingscore
200/10:
# Calculate the percentage of students with a passing math score (70 or greater)
math_passing = pycityschools_data_complete.loc[pycityschools_data_complete["math_score"] >=70]["math_score"].count()
percent_math_passing = (math_passing/total_students) * 100
percent_math_passing
200/11:
# Calculate the percentage of students with a passing reading score (70 or greater)
reading_passing = pycityschools_data_complete.loc[pycityschools_data_complete["reading_score"] >=70]["reading_score"].count()
percent_reading_passing = (reading_passing/total_students) * 100
percent_reading_passing
200/12:
# Calculate the percentage of students who passed math and reading (% Overall Passing)
# overall_passing = round((reading_passing + math_passing) / 2, 2)
# overall_passing
200/13:
# Create a dataframe to hold the above results
summary_df = pd.DataFrame({"Total Schools": [total_schools],
                          "Total Students": [total_students],
                          "Total Budget": [total_budget],
                          "Average Math Score": [average_mathscore],
                          "Average Reading Score": [average_readingscore],
                          "% of Students with Passing Math Score": [math_passing],
                          "% of Students with Passing Reading Score": [reading_passing],
                          "% of students with Passing Reading and Math Score": [overall_passing]
                          })
summary_df
200/14:
# Create a dataframe to hold the above results
summary_df = pd.DataFrame({"Total Schools": [total_schools],
                          "Total Students": [total_students],
                          "Total Budget": [total_budget],
                          "Average Math Score": [average_mathscore],
                          "Average Reading Score": [average_readingscore],
                          "% of Students with Passing Math Score": [percent_math_passing],
                          "% of Students with Passing Reading Score": [percent_reading_passing],
                          "% of students with Passing Reading and Math Score": [overall_passing]
                          })
summary_df
200/15:
# Calculate the percentage of students who passed math and reading (% Overall Passing)
overall_passing = round((percent_reading_passing + percent_math_passing) / 2, 2)
overall_passing
200/16:
# Create a dataframe to hold the above results
summary_df = pd.DataFrame({"Total Schools": [total_schools],
                          "Total Students": [total_students],
                          "Total Budget": [total_budget],
                          "Average Math Score": [average_mathscore],
                          "Average Reading Score": [average_readingscore],
                          "% of Students with Passing Math Score": [percent_math_passing],
                          "% of Students with Passing Reading Score": [percent_reading_passing],
                          "% of students with Passing Reading and Math Score": [overall_passing]
                          })
summary_df
201/1:
# Dependencies
import pandas as pd
import numpy as np
201/2:
# load in file
pycityschools_file = "Resources/schools_complete.csv"
student_file = "Resources/students_complete.csv"
201/3:
# Read School and Student Data File and store into Pandas DataFrames
pycityschools_data_df = pd.read_csv(pycityschools_file)
student_data_df = pd.read_csv(student_file)

pycityschools_data_df.head()
student_data_df.head()
201/4:
pycityschools_data_complete = pd.merge(student_data_df, pycityschools_data_df, 
                                       on=["school_name", "school_name"], how="left")

pycityschools_data_complete.head()
201/5:
# Count total schools
unique_schools = pycityschools_data_complete["School ID"].unique()
total_schools = len(unique_schools)
total_schools
201/6:
# Calculate the total number of students
total_students = pycityschools_data_complete["size"].sum()
total_students
201/7:
# Calculate Total Budget
total_budget = pycityschools_data_complete["budget"].sum()
total_budget
201/8:
# Calculate the average math score
average_mathscore = round(pycityschools_data_complete["math_score"].mean(), 2)
average_mathscore
201/9:
# Calculate the average reading score
average_readingscore = round(pycityschools_data_complete["reading_score"].mean(), 2)
average_readingscore
201/10:
# Calculate the percentage of students with a passing math score (70 or greater)
math_passing = pycityschools_data_complete.loc[pycityschools_data_complete["math_score"] <=70]["math_score"].count()
percent_math_passing = (math_passing/total_students) * 100
percent_math_passing
201/11:
# Calculate the percentage of students with a passing reading score (70 or greater)
reading_passing = pycityschools_data_complete.loc[pycityschools_data_complete["reading_score"] <=70]["reading_score"].count()
percent_reading_passing = (reading_passing/total_students) * 100
percent_reading_passing
201/12:
# Calculate the percentage of students who passed math and reading (% Overall Passing)
overall_passing = round((percent_reading_passing + percent_math_passing) / 2, 2)
overall_passing
201/13:
# Create a dataframe to hold the above results
summary_df = pd.DataFrame({"Total Schools": [total_schools],
                          "Total Students": [total_students],
                          "Total Budget": [total_budget],
                          "Average Math Score": [average_mathscore],
                          "Average Reading Score": [average_readingscore],
                          "% of Students with Passing Math Score": [percent_math_passing],
                          "% of Students with Passing Reading Score": [percent_reading_passing],
                          "% of students with Passing Reading and Math Score": [overall_passing]
                          })
summary_df
201/14:
# Give the displayed data cleaner formatting
summary_df["Total Schools"] = summary_df["Total Schools"].map("{:,.2f}".format)
summary_df["Total Students"] = summary_df["Total Students"].map("{:,.2f}".format)
summary_df["Total Budget"] = summary_df["Total Budget"].map("{:,.2f}".format)
summary_df
201/15:
# Create an overview table that summarizes key metrics about each school
    # School Name
    # School Type
    # Total Students
    # Total School Budget
    # Per Student Budget
    # Average Math Score
    # Average Reading Score
    # % Passing Math
    # % Passing Reading
    # % Overall Passing (The percentage of students that passed math and reading.)
    
# Create a dataframe to hold the above results
202/1:
# Dependencies
import pandas as pd
import numpy as np
202/2:
# load in file
pycityschools_file = "Resources/schools_complete.csv"
student_file = "Resources/students_complete.csv"
202/3:
# Read School and Student Data File and store into Pandas DataFrames
pycityschools_data_df = pd.read_csv(pycityschools_file)
student_data_df = pd.read_csv(student_file)

pycityschools_data_df.head()
student_data_df.head()
202/4:
pycityschools_data_complete = pd.merge(student_data_df, pycityschools_data_df, 
                                       on=["school_name", "school_name"], how="left")

pycityschools_data_complete.head()
202/5:
# Count total schools
unique_schools = pycityschools_data_complete["School ID"].unique()
total_schools = len(unique_schools)
total_schools
202/6:
# Calculate the total number of students
total_students = pycityschools_data_complete["size"].sum()
total_students
202/7:
# Calculate Total Budget
total_budget = pycityschools_data_complete["budget"].sum()
total_budget
202/8:
# Calculate the average math score
average_mathscore = round(pycityschools_data_complete["math_score"].mean(), 2)
average_mathscore
202/9:
# Calculate the average reading score
average_readingscore = round(pycityschools_data_complete["reading_score"].mean(), 2)
average_readingscore
202/10:
# Calculate the percentage of students with a passing math score (70 or greater)
math_passing = pycityschools_data_complete.loc[pycityschools_data_complete["math_score"] >=70]["math_score"].count()
percent_math_passing = (math_passing/total_students) * 100
percent_math_passing
202/11:
# Calculate the percentage of students with a passing reading score (70 or greater)
reading_passing = pycityschools_data_complete.loc[pycityschools_data_complete["reading_score"] >=70]["reading_score"].count()
percent_reading_passing = (reading_passing/total_students) * 100
percent_reading_passing
202/12:
# Calculate the percentage of students who passed math and reading (% Overall Passing)
overall_passing = round((percent_reading_passing + percent_math_passing) / 2, 2)
overall_passing
202/13:
# Create a dataframe to hold the above results
summary_df = pd.DataFrame({"Total Schools": [total_schools],
                          "Total Students": [total_students],
                          "Total Budget": [total_budget],
                          "Average Math Score": [average_mathscore],
                          "Average Reading Score": [average_readingscore],
                          "% of Students with Passing Math Score": [percent_math_passing],
                          "% of Students with Passing Reading Score": [percent_reading_passing],
                          "% of students with Passing Reading and Math Score": [overall_passing]
                          })
summary_df
202/14:
# Give the displayed data cleaner formatting
summary_df["Total Schools"] = summary_df["Total Schools"].map("{:,.2f}".format)
summary_df["Total Students"] = summary_df["Total Students"].map("{:,.2f}".format)
summary_df["Total Budget"] = summary_df["Total Budget"].map("{:,.2f}".format)
summary_df
202/15:
# Create an overview table that summarizes key metrics about each school
    # School Name
    # School Type
    # Total Students
    # Total School Budget
    # Per Student Budget
    # Average Math Score
    # Average Reading Score
    # % Passing Math
    # % Passing Reading
    # % Overall Passing (The percentage of students that passed math and reading.)
    
# Create a dataframe to hold the above results
202/16:
# Calculate the total number of students
unique_students = pycityschools_data_complete["student_name"].unique()
total_students = len(unique_students)
total_students
203/1:
# Dependencies
import pandas as pd
import numpy as np
203/2:
# load in file
pycityschools_file = "Resources/schools_complete.csv"
student_file = "Resources/students_complete.csv"
203/3:
# Read School and Student Data File and store into Pandas DataFrames
pycityschools_data_df = pd.read_csv(pycityschools_file)
student_data_df = pd.read_csv(student_file)

pycityschools_data_df.head()
student_data_df.head()
203/4:
pycityschools_data_complete = pd.merge(student_data_df, pycityschools_data_df, 
                                       on=["school_name", "school_name"], how="left")

pycityschools_data_complete.head()
203/5:
# Count total schools
unique_schools = pycityschools_data_complete["School ID"].unique()
total_schools = len(unique_schools)
total_schools
203/6:
# Calculate the total number of students
unique_students = pycityschools_data_complete["student_name"].unique()
total_students = len(unique_students)
total_students
203/7:
# Calculate Total Budget
total_budget = pycityschools_data_complete["budget"].sum()
total_budget
203/8:
# Calculate the average math score
average_mathscore = round(pycityschools_data_complete["math_score"].mean(), 2)
average_mathscore
203/9:
# Calculate the average reading score
average_readingscore = round(pycityschools_data_complete["reading_score"].mean(), 2)
average_readingscore
203/10:
# Calculate the percentage of students with a passing math score (70 or greater)
math_passing = pycityschools_data_complete.loc[pycityschools_data_complete["math_score"] >=70]["math_score"].count()
percent_math_passing = (math_passing/total_students) * 100
percent_math_passing
203/11:
# Calculate the percentage of students with a passing reading score (70 or greater)
reading_passing = pycityschools_data_complete.loc[pycityschools_data_complete["reading_score"] >=70]["reading_score"].count()
percent_reading_passing = (reading_passing/total_students) * 100
percent_reading_passing
203/12:
# Calculate the percentage of students who passed math and reading (% Overall Passing)
overall_passing = round((percent_reading_passing + percent_math_passing) / 2, 2)
overall_passing
203/13:
# Create a dataframe to hold the above results
summary_df = pd.DataFrame({"Total Schools": [total_schools],
                          "Total Students": [total_students],
                          "Total Budget": [total_budget],
                          "Average Math Score": [average_mathscore],
                          "Average Reading Score": [average_readingscore],
                          "% of Students with Passing Math Score": [percent_math_passing],
                          "% of Students with Passing Reading Score": [percent_reading_passing],
                          "% of students with Passing Reading and Math Score": [overall_passing]
                          })
summary_df
203/14:
# Give the displayed data cleaner formatting
summary_df["Total Schools"] = summary_df["Total Schools"].map("{:,.2f}".format)
summary_df["Total Students"] = summary_df["Total Students"].map("{:,.2f}".format)
summary_df["Total Budget"] = summary_df["Total Budget"].map("{:,.2f}".format)
summary_df
203/15:
# Create an overview table that summarizes key metrics about each school
    # School Name
    # School Type
    # Total Students
    # Total School Budget
    # Per Student Budget
    # Average Math Score
    # Average Reading Score
    # % Passing Math
    # % Passing Reading
    # % Overall Passing (The percentage of students that passed math and reading.)
    
# Create a dataframe to hold the above results
203/16:
# Calculate the percentage of students with a passing reading score (70 or greater)
reading_passing = pycityschools_data_complete.loc[pycityschools_data_complete["reading_score"] >=70].count()
percent_reading_passing = (reading_passing/total_students) * 100
percent_reading_passing
203/17:
# Calculate the percentage of students with a passing math score (70 or greater)
math_passing = pycityschools_data_complete.loc[pycityschools_data_complete["math_score"] >=70]["math_score"].count()
percent_math_passing = (math_passing/total_students) * 100
percent_math_passing
203/18:
# Calculate the percentage of students with a passing reading score (70 or greater)
reading_passing = pycityschools_data_complete.loc[pycityschools_data_complete["reading_score"] >=70]["reading_score"].count()
percent_reading_passing = (reading_passing/total_students) * 100
percent_reading_passing
203/19:
# Calculate the percentage of students with a passing reading score (70 or greater)
reading_passing = pycityschools_data_complete.loc[pycityschools_data_complete["reading_score"] >=70]["reading_score"].count()
reading_passing
percent_reading_passing = (reading_passing/total_students) * 100
percent_reading_passing
203/20:
# Calculate the percentage of students with a passing reading score (70 or greater)
reading_passing = pycityschools_data_complete.loc[pycityschools_data_complete["reading_score"] >=70]["reading_score"].count()
reading_passing
# percent_reading_passing = (reading_passing/total_students) * 100
# percent_reading_passing
204/1:
# Dependencies
import pandas as pd
import numpy as np
204/2:
# load in file
pycityschools_file = "Resources/schools_complete.csv"
student_file = "Resources/students_complete.csv"
204/3:
# Read School and Student Data File and store into Pandas DataFrames
pycityschools_data_df = pd.read_csv(pycityschools_file)
student_data_df = pd.read_csv(student_file)

pycityschools_data_df.head()
student_data_df.head()
204/4:
pycityschools_data_complete = pd.merge(student_data_df, pycityschools_data_df, 
                                       on=["school_name", "school_name"], how="left")

pycityschools_data_complete.head()
204/5:
# Count total schools
unique_schools = pycityschools_data_complete["School ID"].unique()
total_schools = len(unique_schools)
total_schools
204/6:
# Calculate the total number of students
unique_students = pycityschools_data_complete["student_name"].unique()
total_students = len(unique_students)
total_students
204/7:
# Calculate Total Budget
total_budget = pycityschools_data_complete["budget"].sum()
total_budget
204/8:
# Calculate the average math score
average_mathscore = round(pycityschools_data_complete["math_score"].mean(), 2)
average_mathscore
204/9:
# Calculate the average reading score
average_readingscore = round(pycityschools_data_complete["reading_score"].mean(), 2)
average_readingscore
204/10:
# Calculate the percentage of students with a passing math score (70 or greater)
math_passing = pycityschools_data_complete.loc[pycityschools_data_complete["math_score"] >=70]["math_score"].count()
percent_math_passing = (math_passing/total_students) * 100
percent_math_passing
204/11:
# Calculate the percentage of students with a passing reading score (70 or greater)
reading_passing = pycityschools_data_complete.loc[pycityschools_data_complete["reading_score"] >=70]["reading_score"].count()
reading_passing
# percent_reading_passing = (reading_passing/total_students) * 100
# percent_reading_passing
204/12:
# Calculate the percentage of students who passed math and reading (% Overall Passing)
overall_passing = round((percent_reading_passing + percent_math_passing) / 2, 2)
overall_passing
204/13:
# Calculate the percentage of students with a passing reading score (70 or greater)
reading_passing = pycityschools_data_complete.loc[pycityschools_data_complete["reading_score"] >=70]["reading_score"].count()
percent_reading_passing = (reading_passing/total_students) * 100
percent_reading_passing
204/14:
# Calculate the percentage of students who passed math and reading (% Overall Passing)
overall_passing = round((percent_reading_passing + percent_math_passing) / 2, 2)
overall_passing
204/15:
# Calculate the percentage of students with a passing reading score (70 or greater)
reading_passing = pycityschools_data_complete.loc[pycityschools_data_complete["reading_score"] >=70]["reading_score"].count()
percent_reading_passing = (reading_passing/total_students)
percent_reading_passing
204/16:
# Calculate the percentage of students with a passing reading score (70 or greater)
reading_passing = pycityschools_data_complete.loc[pycityschools_data_complete["reading_score"] >=70]["reading_score"].count()
percent_reading_passing = (total_students/reading_passing)*100
percent_reading_passing
204/17:
# Calculate the percentage of students who passed math and reading (% Overall Passing)
overall_passing = round((percent_reading_passing + percent_math_passing) / 2, 2)
overall_passing
204/18:
# Calculate the percentage of students with a passing math score (70 or greater)
math_passing = pycityschools_data_complete.loc[pycityschools_data_complete["math_score"] >=70]["math_score"].count()
percent_math_passing = (total_students/math_passing) * 100
percent_math_passing
204/19:
# Calculate the percentage of students with a passing math score (70 or greater)
math_passing = pycityschools_data_complete.loc[pycityschools_data_complete["math_score"] >=70]["math_score"].count()
percent_math_passing = (math_passing/total_students) * 100
percent_math_passing
205/1:
# Dependencies
import pandas as pd
import numpy as np
205/2:
# load in file
pycityschools_file = "Resources/schools_complete.csv"
student_file = "Resources/students_complete.csv"
205/3:
# Read School and Student Data File and store into Pandas DataFrames
pycityschools_data_df = pd.read_csv(pycityschools_file)
student_data_df = pd.read_csv(student_file)

pycityschools_data_df.head()
student_data_df.head()
205/4:
pycityschools_data_complete = pd.merge(student_data_df, pycityschools_data_df, 
                                       on=["school_name", "school_name"], how="left")

pycityschools_data_complete.head()
205/5:
# Count total schools
unique_schools = pycityschools_data_complete["School ID"].unique()
total_schools = len(unique_schools)
total_schools
205/6:
# Calculate the total number of students
unique_students = pycityschools_data_complete["student_name"].unique()
total_students = len(unique_students)
total_students
205/7:
# Calculate Total Budget
total_budget = pycityschools_data_complete["budget"].sum()
total_budget
205/8:
# Calculate the average math score
average_mathscore = round(pycityschools_data_complete["math_score"].mean(), 2)
average_mathscore
205/9:
# Calculate the average reading score
average_readingscore = round(pycityschools_data_complete["reading_score"].mean(), 2)
average_readingscore
205/10:
# Calculate the percentage of students with a passing math score (70 or greater)
math_passing = pycityschools_data_complete.loc[pycityschools_data_complete["math_score"] >=70]["math_score"].count()
percent_math_passing = (math_passing/total_students) * 100
percent_math_passing
205/11:
# Calculate the percentage of students with a passing reading score (70 or greater)
reading_passing = pycityschools_data_complete.loc[pycityschools_data_complete["reading_score"] >=70]["reading_score"].count()
percent_reading_passing = (total_students/reading_passing)*100
percent_reading_passing
205/12:
# Calculate the percentage of students who passed math and reading (% Overall Passing)
overall_passing = round((percent_reading_passing + percent_math_passing) / 2, 2)
overall_passing
205/13:
# Create a dataframe to hold the above results
summary_df = pd.DataFrame({"Total Schools": [total_schools],
                          "Total Students": [total_students],
                          "Total Budget": [total_budget],
                          "Average Math Score": [average_mathscore],
                          "Average Reading Score": [average_readingscore],
                          "% of Students with Passing Math Score": [percent_math_passing],
                          "% of Students with Passing Reading Score": [percent_reading_passing],
                          "% of students with Passing Reading and Math Score": [overall_passing]
                          })
summary_df
205/14:
# Give the displayed data cleaner formatting
summary_df["Total Schools"] = summary_df["Total Schools"].map("{:,.2f}".format)
summary_df["Total Students"] = summary_df["Total Students"].map("{:,.2f}".format)
summary_df["Total Budget"] = summary_df["Total Budget"].map("{:,.2f}".format)
summary_df
205/15:
# Create an overview table that summarizes key metrics about each school
    # School Name
    # School Type
    # Total Students
    # Total School Budget
    # Per Student Budget
    # Average Math Score
    # Average Reading Score
    # % Passing Math
    # % Passing Reading
    # % Overall Passing (The percentage of students that passed math and reading.)
    
# Create a dataframe to hold the above results
205/16:
# Give the displayed data cleaner formatting
summary_df["Total Students"] = summary_df["Total Students"].map("{:,2f}".format)
summary_df["Total Budget"] = summary_df["Total Budget"].map("{:,2f}".format)
summary_df
205/17:
# Give the displayed data cleaner formatting
summary_df["Total Students"] = summary_df["Total Students"].map("{:.,2f}".format)
summary_df["Total Budget"] = summary_df["Total Budget"].map("{:.,2f}".format)
summary_df
205/18:
# Give the displayed data cleaner formatting
summary_df["Total Students"] = summary_df["Total Students"].map("{:,.2f}".format)
summary_df["Total Budget"] = summary_df["Total Budget"].map("{:,.2f}".format)
summary_df
206/1:
# Count total schools
total_schools = len(pycityschools_data_complete["school_name"].unique())
total_schools
206/2:
# Count total schools
total_schools = len(pycityschools_data_complete["School ID"].unique())
total_schools
210/1:
# Dependencies
import pandas as pd
import numpy as np
210/2:
# load in file
pycityschools_file = "Resources/schools_complete.csv"
student_file = "Resources/students_complete.csv"
210/3:
# Read School and Student Data File and store into Pandas DataFrames
pycityschools_data_df = pd.read_csv(pycityschools_file)
student_data_df = pd.read_csv(student_file)

pycityschools_data_df.head()
student_data_df.head()
210/4:
pycityschools_data_complete = pd.merge(student_data_df, pycityschools_data_df, 
                                       on=["school_name", "school_name"], how="left")

pycityschools_data_complete.head()
210/5:
# Count total schools
total_schools = len(pycityschools_data_complete["School ID"].unique())
total_schools
210/6:
# Calculate the total number of students
total_students = len(pycityschools_data_complete["student_name"].unique())
total_students
210/7:
# Calculate Total Budget
total_budget = pycityschools_data_complete["budget"].sum()
total_budget
210/8:
# Calculate the average math score
average_mathscore = round(pycityschools_data_complete["math_score"].mean(), 2)
average_mathscore
210/9:
# Calculate the average reading score
average_readingscore = round(pycityschools_data_complete["reading_score"].mean(), 2)
average_readingscore
210/10:
# Calculate the percentage of students with a passing math score (70 or greater)
math_passing = pycityschools_data_complete.loc[pycityschools_data_complete["math_score"] =70]["math_score"].count()
percent_math_passing = (math_passing/total_students) * 100
percent_math_passing
212/1:
x_axis = np.arange(0, len(gyms))
tick_locations = []
for x in x_axis:
    tick_locations.append(x)
    
plt.title("NYC Gym Popularity")
plt.xlabel("Gym Name")
plt.ylabel("Number of Members")

plt.xlim(-0.75, len(gyms)-.25)
plt.ylim(0, max(members) + 5)

plt.bar(x_axis, members, facecolor="pink", alpha=0.5, align="center")
plt.xticks(tick_locations, gyms)
plt.show()
214/1:
# Import Dependencies
import numpy as np
import matplotlib.pyplot as plt
214/2:
# DATA SET 1
gyms = ["Crunch", "Planet Fitness", "NY Sports Club", "Rickie's Gym"]
members = [49, 92, 84, 53]
214/3:
x_axis = np.arange(0, len(gyms))
tick_locations = []
for x in x_axis:
    tick_locations.append(x)
    
plt.title("NYC Gym Popularity")
plt.xlabel("Gym Name")
plt.ylabel("Number of Members")

plt.xlim(-0.75, len(gyms)-.25)
plt.ylim(0, max(members) + 5)

plt.bar(x_axis, members, facecolor="pink", alpha=0.5, align="center")
plt.xticks(tick_locations, gyms)
plt.show()
214/4:
# DATA SET 2
x_lim = 2 * np.pi
x_axis = np.arange(0, x_lim, 0.1)
sin = np.sin(x_axis)
214/5:
# DATA SET 3
gyms = ["Crunch", "Planet Fitness", "NY Sports Club", "Rickie's Gym"]
members = [49, 92, 84, 53]
x_axis = np.arange(0, len(gyms))
colors = ["yellowgreen", "red", "lightcoral", "lightskyblue"]
explode = (0, 0.05, 0, 0)
214/6:
# DATA SET 4
x_axis = np.arange(0, 10, 0.1)
times = []
for x in x_axis:
    times.append(x * x + np.random.randint(0, np.ceil(max(x_axis))))
215/1:
# Import Dependencies
import numpy as np
import matplotlib.pyplot as plt
215/2:
# DATA SET 1
gyms = ["Crunch", "Planet Fitness", "NY Sports Club", "Rickie's Gym"]
members = [49, 92, 84, 53]
215/3:
x_axis = np.arange(0, len(gyms))
tick_locations = []
for x in x_axis:
    tick_locations.append(x)
    
plt.title("NYC Gym Popularity")
plt.xlabel("Gym Name")
plt.ylabel("Number of Members")

plt.xlim(-0.75, len(gyms)-.25)
plt.ylim(0, max(members) + 5)

plt.bar(x_axis, members, facecolor="pink", alpha=1, align="center")
plt.xticks(tick_locations, gyms)
plt.show()
215/4:
# DATA SET 2
x_lim = 2 * np.pi
x_axis = np.arange(0, x_lim, 0.1)
sin = np.sin(x_axis)
215/5:
# DATA SET 3
gyms = ["Crunch", "Planet Fitness", "NY Sports Club", "Rickie's Gym"]
members = [49, 92, 84, 53]
x_axis = np.arange(0, len(gyms))
colors = ["yellowgreen", "red", "lightcoral", "lightskyblue"]
explode = (0, 0.05, 0, 0)
215/6:
# DATA SET 4
x_axis = np.arange(0, 10, 0.1)
times = []
for x in x_axis:
    times.append(x * x + np.random.randint(0, np.ceil(max(x_axis))))
215/7:
plt.title("Sin from 0 to 2$\pi$")
plt.xlabel("Real Number frim 0 to 2$\pi$")
plt.ylabel("sin(x)")

plt.hlines(0, 0, x_lim, alpha=0.2)
plt.xlim(0, x_lim)
plt.ylim(-1.25, 1.25)

plt.plot(x_axis, sin, marker="o", color="red", linewidth=1)
plt.show()
216/1:
# Import Dependencies
import numpy as np
import matplotlib.pyplot as plt
216/2:
# DATA SET 1
gyms = ["Crunch", "Planet Fitness", "NY Sports Club", "Rickie's Gym"]
members = [49, 92, 84, 53]
216/3:
x_axis = np.arange(0, len(gyms))
tick_locations = []
for x in x_axis:
    tick_locations.append(x)
    
plt.title("NYC Gym Popularity")
plt.xlabel("Gym Name")
plt.ylabel("Number of Members")

plt.xlim(-0.75, len(gyms)-.25)
plt.ylim(0, max(members) + 5)

plt.bar(x_axis, members, facecolor="pink", alpha=1, align="center")
plt.xticks(tick_locations, gyms)
plt.show()
216/4:
# DATA SET 2
x_lim = 2 * np.pi
x_axis = np.arange(0, x_lim, 0.1)
sin = np.sin(x_axis)
216/5:
plt.title("Sin from 0 to 2$\pi$")
plt.xlabel("Real Number frim 0 to 2$\pi$")
plt.ylabel("sin(x)")

plt.hlines(0, 0, x_lim, alpha=0.2)
plt.xlim(0, x_lim)
plt.ylim(-1.25, 1.25)

plt.plot(x_axis, sin, marker="o", color="red", linewidth=1)
plt.show()
216/6:
# DATA SET 3
gyms = ["Crunch", "Planet Fitness", "NY Sports Club", "Rickie's Gym"]
members = [49, 92, 84, 53]
x_axis = np.arange(0, len(gyms))
colors = ["yellowgreen", "red", "lightcoral", "lightskyblue"]
explode = (0, 0.05, 0, 0)
216/7:
# DATA SET 4
x_axis = np.arange(0, 10, 0.1)
times = []
for x in x_axis:
    times.append(x * x + np.random.randint(0, np.ceil(max(x_axis))))
216/8:
plt.title("NYC Gym Popularity")
plt.pie(members, explode=explode, labels=gyms, colors=colors,
       autopct="%1.1f%%", shadow=True, startangle=90)
plt.axis("equal")
plt.show()
216/9:
plt.title("Running Time of FakeSort for Sample Input Sizes")
plt.xlabel("Length of Input Array")
plt.ylabel("Time to Sort (s)")

plt.scatter(x_axis, times, marker="o", color="blue")
plt.show()
218/1:
# Dependencies
import pandas as pd
import numpy as np
218/2:
# load in file
pycityschools_file = "Resources/schools_complete.csv"
student_file = "Resources/students_complete.csv"
218/3:
# Read School and Student Data File and store into Pandas DataFrames
pycityschools_data_df = pd.read_csv(pycityschools_file)
student_data_df = pd.read_csv(student_file)

pycityschools_data_df.head()
student_data_df.head()
218/4:
pycityschools_data_complete = pd.merge(student_data_df, pycityschools_data_df, 
                                       on=["school_name", "school_name"], how="left")

pycityschools_data_complete.head()
218/5:
# Count total schools
total_schools = len(pycityschools_data_complete["School ID"].unique())
total_schools
218/6:
# Calculate the total number of students
total_students = len(pycityschools_data_complete["student_name"].unique())
total_students
218/7:
# Calculate Total Budget
total_budget = pycityschools_data_complete["budget"].sum()
total_budget
218/8:
# Calculate the average math score
average_mathscore = round(pycityschools_data_complete["math_score"].mean(), 2)
average_mathscore
218/9:
# Calculate the average reading score
average_readingscore = round(pycityschools_data_complete["reading_score"].mean(), 2)
average_readingscore
218/10:
# Calculate the percentage of students with a passing math score (70 or greater)
math_passing = pycityschools_data_complete.loc[pycityschools_data_complete["math_score"] =70]["math_score"].count()
percent_math_passing = (math_passing/total_students) * 100
percent_math_passing
219/1:
# Dependencies
import pandas as pd
import numpy as np
219/2:
# load in file
pycityschools_file = "Resources/schools_complete.csv"
student_file = "Resources/students_complete.csv"
219/3:
# Read School and Student Data File and store into Pandas DataFrames
pycityschools_data_df = pd.read_csv(pycityschools_file)
student_data_df = pd.read_csv(student_file)

pycityschools_data_df.head()
student_data_df.head()
219/4:
pycityschools_data_complete = pd.merge(student_data_df, pycityschools_data_df, 
                                       on=["school_name", "school_name"], how="left")

pycityschools_data_complete.head()
219/5:
# Count total schools
total_schools = len(pycityschools_data_complete["School ID"].unique())
total_schools
219/6:
# Calculate the total number of students
total_students = len(pycityschools_data_complete["student_name"].unique())
total_students
219/7:
# Calculate Total Budget
total_budget = pycityschools_data_complete["budget"].sum()
total_budget
219/8:
# Calculate the average math score
average_mathscore = round(pycityschools_data_complete["math_score"].mean(), 2)
average_mathscore
219/9:
# Calculate the average reading score
average_readingscore = round(pycityschools_data_complete["reading_score"].mean(), 2)
average_readingscore
219/10:
# Calculate the percentage of students with a passing math score (70 or greater)
math_passing = pycityschools_data_complete.loc[pycityschools_data_complete["math_score"] =70]["math_score"].count()
percent_math_passing = (math_passing/total_students)*100
percent_math_passing
220/1:
# Dependencies
import pandas as pd
import numpy as np
220/2:
# load in file
pycityschools_file = "Resources/schools_complete.csv"
student_file = "Resources/students_complete.csv"
220/3:
# Read School and Student Data File and store into Pandas DataFrames
pycityschools_data_df = pd.read_csv(pycityschools_file)
student_data_df = pd.read_csv(student_file)

pycityschools_data_df.head()
student_data_df.head()
220/4:
pycityschools_data_complete = pd.merge(student_data_df, pycityschools_data_df, 
                                       on=["school_name", "school_name"], how="left")

pycityschools_data_complete.head()
220/5:
# Count total schools
total_schools = len(pycityschools_data_complete["School ID"].unique())
total_schools
220/6:
# Calculate the total number of students
total_students = len(pycityschools_data_complete["student_name"].unique())
total_students
220/7:
# Calculate Total Budget
total_budget = pycityschools_data_complete["budget"].sum()
total_budget
220/8:
# Calculate the average math score
average_mathscore = round(pycityschools_data_complete["math_score"].mean(), 2)
average_mathscore
220/9:
# Calculate the average reading score
average_readingscore = round(pycityschools_data_complete["reading_score"].mean(), 2)
average_readingscore
220/10:
# Calculate the percentage of students with a passing math score (70 or greater)
math_passing = pycityschools_data_complete.loc[pycityschools_data_complete["math_score"] >=70]["math_score"].count()
percent_math_passing = (math_passing/total_students)*100
percent_math_passing
220/11:
# Calculate the percentage of students with a passing reading score (70 or greater)
reading_passing = pycityschools_data_complete.loc[pycityschools_data_complete["reading_score"] >=70]["reading_score"].count()
percent_reading_passing = (total_students/reading_passing)*100
percent_reading_passing
220/12:
# Calculate the percentage of students who passed math and reading (% Overall Passing)
overall_passing = round((percent_reading_passing + percent_math_passing) / 2, 2)
overall_passing
220/13:
# Create a dataframe to hold the above results
summary_df = pd.DataFrame({"Total Schools": [total_schools],
                          "Total Students": [total_students],
                          "Total Budget": [total_budget],
                          "Average Math Score": [average_mathscore],
                          "Average Reading Score": [average_readingscore],
                          "% of Students with Passing Math Score": [percent_math_passing],
                          "% of Students with Passing Reading Score": [percent_reading_passing],
                          "% of students with Passing Reading and Math Score": [overall_passing]
                          })
summary_df
220/14:
# Give the displayed data cleaner formatting
summary_df["Total Students"] = summary_df["Total Students"].map("{:,.2f}".format)
summary_df["Total Budget"] = summary_df["Total Budget"].map("{:,.2f}".format)
summary_df
220/15:
# Create an overview table that summarizes key metrics about each school
    # School Name
    # School Type
    # Total Students
    # Total School Budget
    # Per Student Budget
    # Average Math Score
    # Average Reading Score
    # % Passing Math
    # % Passing Reading
    # % Overall Passing (The percentage of students that passed math and reading.)
    
# Create a dataframe to hold the above results
220/16:
schools_overview_df = pd.DataFrames({"School Name": school_name,
                                    "School Type": type,
                                    "Total Students": size,
                                    "Total School Budget": }
                                    
                                    "Total Unique Authors": [author_count],
                              "Earliest Year": earliest_year,
                              "Latest Year": latest_year,
                              "Total Reviews": total_reviews})
220/17:
# Calculate the percentage of students with a passing reading score (70 or greater)
reading_passing = pycityschools_data_complete.loc[pycityschools_data_complete["reading_score"] >=70]["reading_score"].count()
percent_reading_passing = (reading_passing/total_students)*100
percent_reading_passing
220/18:
# Calculate the percentage of students with a passing reading score (70 or greater)
reading_passing = pycityschools_data_complete.loc[pycityschools_data_complete["reading_score"] >=70]["reading_score"].count()
reading_passing
#percent_reading_passing = (reading_passing/total_students)*100
#percent_reading_passing
220/19:
# Calculate the percentage of students with a passing reading score (70 or greater)
reading_passing = pycityschools_data_complete.loc[pycityschools_data_complete["reading_score"] >=70]["reading_score"].count()
total_students
#reading_passing
#percent_reading_passing = (reading_passing/total_students)*100
#percent_reading_passing
220/20:
# Calculate the percentage of students with a passing math score (70 or greater)
unique_math_passing = len(pycityschools_data_complete.loc[pycityschools_data_complete["math_score"] >=70].unique())
percent_math_passing = (unique_math_passing/total_students)*100
percent_math_passing
221/1:
# Dependencies
import pandas as pd
import numpy as np
221/2:
# load in file
pycityschools_file = "Resources/schools_complete.csv"
student_file = "Resources/students_complete.csv"
221/3:
# Read School and Student Data File and store into Pandas DataFrames
pycityschools_data_df = pd.read_csv(pycityschools_file)
student_data_df = pd.read_csv(student_file)

pycityschools_data_df.head()
student_data_df.head()
221/4:
pycityschools_data_complete = pd.merge(student_data_df, pycityschools_data_df, 
                                       on=["school_name", "school_name"], how="left")

pycityschools_data_complete.head()
221/5:
# Count total schools
total_schools = len(pycityschools_data_complete["School ID"].unique())
total_schools
221/6:
# Calculate the total number of students
total_students = len(pycityschools_data_complete["student_name"].unique())
total_students
221/7:
# Calculate Total Budget
total_budget = pycityschools_data_complete["budget"].sum()
total_budget
221/8:
# Calculate the average math score
average_mathscore = round(pycityschools_data_complete["math_score"].mean(), 2)
average_mathscore
221/9:
# Calculate the average reading score
average_readingscore = round(pycityschools_data_complete["reading_score"].mean(), 2)
average_readingscore
221/10:
# Calculate the percentage of students with a passing math score (70 or greater)
unique_math_passing = len(pycityschools_data_complete.loc[pycityschools_data_complete["math_score"] >=70].unique())
percent_math_passing = (unique_math_passing/total_students)*100
percent_math_passing
221/11:
# Calculate the percentage of students with a passing math score (70 or greater)
unique_math_passing = len(pycityschools_data_complete.loc[pycityschools_data_complete["math_score"] >=70]["student_name"].unique())
percent_math_passing = (unique_math_passing/total_students)*100
percent_math_passing
221/12:
# Calculate the percentage of students with a passing reading score (70 or greater)
unique_reading_passing = len(pycityschools_data_complete.loc[pycityschools_data_complete["reading_score"] >=70]["student_name"].unique())
percent_reading_passing = (unique_reading_passing/total_students)*100
percent_reading_passing
221/13:
# Calculate the percentage of students with a passing reading score (70 or greater)
unique_reading_passing = len(pycityschools_data_complete.loc[pycityschools_data_complete["reading_score"] >=70]["student_name"].unique())
percent_reading_passing = round((unique_reading_passing/total_students)*100), 2)
percent_reading_passing
221/14:
# Calculate the percentage of students with a passing reading score (70 or greater)
unique_reading_passing = len(pycityschools_data_complete.loc[pycityschools_data_complete["reading_score"] >=70]["student_name"].unique())
percent_reading_passing = (unique_reading_passing/total_students)*100
percent_reading_passing
221/15:
# Calculate the percentage of students who passed math and reading (% Overall Passing)
overall_passing = round((percent_reading_passing + percent_math_passing) / 2, 2)
overall_passing
221/16:
# Create a dataframe to hold the above results
summary_df = pd.DataFrame({"Total Schools": [total_schools],
                          "Total Students": [total_students],
                          "Total Budget": [total_budget],
                          "Average Math Score": [average_mathscore],
                          "Average Reading Score": [average_readingscore],
                          "% of Students with Passing Math Score": [percent_math_passing],
                          "% of Students with Passing Reading Score": [percent_reading_passing],
                          "% of students with Passing Reading and Math Score": [overall_passing]
                          })
summary_df
221/17:
# Give the displayed data cleaner formatting
summary_df["Total Students"] = summary_df["Total Students"].map("{:,.2f}".format)
summary_df["Total Budget"] = summary_df["Total Budget"].map("{:,.2f}".format)
summary_df
221/18:
# Give the displayed data cleaner formatting
summary_df["Total Students"] = summary_df["Total Students"].map("{:,.2f}".format)
summary_df["Total Budget"] = summary_df["Total Budget"].map("{:,.2f}".format)
summary_df["% of Students with Passing Math Score"] = summary_df.round(2)
221/19:
# Give the displayed data cleaner formatting
summary_df["Total Students"] = summary_df["Total Students"].map("{:,.2f}".format)
summary_df["Total Budget"] = summary_df["Total Budget"].map("{:,.2f}".format)
summary_df["% of Students with Passing Math Score"] = summary_df["% of Students with Passing Math Score"].map("{:,.2df}".format)
222/1:
# Dependencies
import pandas as pd
import numpy as np
222/2:
# load in file
pycityschools_file = "Resources/schools_complete.csv"
student_file = "Resources/students_complete.csv"
222/3:
# Read School and Student Data File and store into Pandas DataFrames
pycityschools_data_df = pd.read_csv(pycityschools_file)
student_data_df = pd.read_csv(student_file)

pycityschools_data_df.head()
student_data_df.head()
222/4:
pycityschools_data_complete = pd.merge(student_data_df, pycityschools_data_df, 
                                       on=["school_name", "school_name"], how="left")

pycityschools_data_complete.head()
222/5:
# Count total schools
total_schools = len(pycityschools_data_complete["School ID"].unique())
total_schools
222/6:
# Calculate the total number of students
total_students = len(pycityschools_data_complete["student_name"].unique())
total_students
222/7:
# Calculate Total Budget
total_budget = pycityschools_data_complete["budget"].sum()
total_budget
222/8:
# Calculate the average math score
average_mathscore = round(pycityschools_data_complete["math_score"].mean(), 2)
average_mathscore
222/9:
# Calculate the average reading score
average_readingscore = round(pycityschools_data_complete["reading_score"].mean(), 2)
average_readingscore
222/10:
# Calculate the percentage of students with a passing math score (70 or greater)
unique_math_passing = len(pycityschools_data_complete.loc[pycityschools_data_complete["math_score"] >=70]["student_name"].unique())
percent_math_passing = (unique_math_passing/total_students)*100
percent_math_passing
222/11:
# Calculate the percentage of students with a passing reading score (70 or greater)
unique_reading_passing = len(pycityschools_data_complete.loc[pycityschools_data_complete["reading_score"] >=70]["student_name"].unique())
percent_reading_passing = (unique_reading_passing/total_students)*100
percent_reading_passing
222/12:
# Calculate the percentage of students who passed math and reading (% Overall Passing)
overall_passing = round((percent_reading_passing + percent_math_passing) / 2, 2)
overall_passing
222/13:
# Create a dataframe to hold the above results
summary_df = pd.DataFrame({"Total Schools": [total_schools],
                          "Total Students": [total_students],
                          "Total Budget": [total_budget],
                          "Average Math Score": [average_mathscore],
                          "Average Reading Score": [average_readingscore],
                          "% of Students with Passing Math Score": [percent_math_passing],
                          "% of Students with Passing Reading Score": [percent_reading_passing],
                          "% of students with Passing Reading and Math Score": [overall_passing]
                          })
summary_df
222/14:
# Give the displayed data cleaner formatting
summary_df["Total Students"] = summary_df["Total Students"].map("{:,.2f}".format)
summary_df["Total Budget"] = summary_df["Total Budget"].map("{:,.2f}".format)
summary_df["% of Students with Passing Math Score"] = summary_df["% of Students with Passing Math Score"].map("{:,.2df}".format)
225/1:
# Dependencies
import pandas as pd
import numpy as np
225/2:
# load in file
pycityschools_file = "Resources/schools_complete.csv"
student_file = "Resources/students_complete.csv"
225/3:
# Read School and Student Data File and store into Pandas DataFrames
pycityschools_data_df = pd.read_csv(pycityschools_file)
student_data_df = pd.read_csv(student_file)

pycityschools_data_df.head()
student_data_df.head()
225/4:
pycityschools_data_complete = pd.merge(student_data_df, pycityschools_data_df, 
                                       on=["school_name", "school_name"], how="left")

pycityschools_data_complete.head()
225/5:
# Count total schools
total_schools = len(pycityschools_data_complete["School ID"].unique())
total_schools
225/6:
# Calculate the total number of students
total_students = len(pycityschools_data_complete["student_name"].unique())
total_students
225/7:
# Calculate Total Budget
total_budget = pycityschools_data_complete["budget"].sum()
total_budget
225/8:
# Calculate the average math score
average_mathscore = round(pycityschools_data_complete["math_score"].mean(), 2)
average_mathscore
225/9:
# Calculate the average reading score
average_readingscore = round(pycityschools_data_complete["reading_score"].mean(), 2)
average_readingscore
225/10:
# Calculate the percentage of students with a passing math score (70 or greater)
unique_math_passing = len(pycityschools_data_complete.loc[pycityschools_data_complete["math_score"] >=70]["student_name"].unique())
percent_math_passing = (unique_math_passing/total_students)*100
percent_math_passing
225/11:
# Calculate the percentage of students with a passing reading score (70 or greater)
unique_reading_passing = len(pycityschools_data_complete.loc[pycityschools_data_complete["reading_score"] >=70]["student_name"].unique())
percent_reading_passing = (unique_reading_passing/total_students)*100
percent_reading_passing
225/12:
# Calculate the percentage of students who passed math and reading (% Overall Passing)
overall_passing = round((percent_reading_passing + percent_math_passing) / 2, 2)
overall_passing
225/13:
# Create a dataframe to hold the above results
summary_df = pd.DataFrame({"Total Schools": [total_schools],
                          "Total Students": [total_students],
                          "Total Budget": [total_budget],
                          "Average Math Score": [average_mathscore],
                          "Average Reading Score": [average_readingscore],
                          "% of Students with Passing Math Score": [percent_math_passing],
                          "% of Students with Passing Reading Score": [percent_reading_passing],
                          "% of students with Passing Reading and Math Score": [overall_passing]
                          })
summary_df
225/14:
# Give the displayed data cleaner formatting
summary_df["Total Students"] = summary_df["Total Students"].map("{:,.2f}".format)
summary_df["Total Budget"] = summary_df["Total Budget"].map("{:,.2f}".format)
# summary_df["% of Students with Passing Math Score"] = summary_df["% of Students with Passing Math Score"].map("{:,.2df}".format)
225/15:
# Create an overview table that summarizes key metrics about each school
    # School Name
    # School Type
    # Total Students
    # Total School Budget
    # Per Student Budget
    # Average Math Score
    # Average Reading Score
    # % Passing Math
    # % Passing Reading
    # % Overall Passing (The percentage of students that passed math and reading.)
    
# Create a dataframe to hold the above results
225/16:
schools_overview_df = pd.DataFrames({"School Name": school_name,
                                    "School Type": type,
                                    "Total Students": size,
                                    "Total School Budget": }
                                    
                                    "Total Unique Authors": [author_count],
                              "Earliest Year": earliest_year,
                              "Latest Year": latest_year,
                              "Total Reviews": total_reviews})
225/17:
# Calculate the percentage of students with a passing reading score (70 or greater)
unique_reading_passing = len(pycityschools_data_complete.loc[pycityschools_data_complete["reading_score"] >=70]["student_name"].unique())
percent_reading_passing = round[(unique_reading_passing/total_students)*100, 2]
percent_reading_passing
225/18:
# Calculate the percentage of students with a passing reading score (70 or greater)
unique_reading_passing = len(pycityschools_data_complete.loc[pycityschools_data_complete["reading_score"] >=70]["student_name"].unique())
percent_reading_passing = [(unique_reading_passing/total_students)*100].round(2)
percent_reading_passing
225/19:
# Calculate the percentage of students with a passing reading score (70 or greater)
unique_reading_passing = len(pycityschools_data_complete.loc[pycityschools_data_complete["reading_score"] >=70]["student_name"].unique())
percent_reading_passing = ((unique_reading_passing/total_students)*100).round(2)
percent_reading_passing
225/20:
# Calculate the percentage of students with a passing reading score (70 or greater)
unique_reading_passing = len(pycityschools_data_complete.loc[pycityschools_data_complete["reading_score"] >=70]["student_name"].unique())
percent_reading_passing = round((unique_reading_passing/total_students)*100, 2)
percent_reading_passing
225/21:
# Calculate the percentage of students with a passing math score (70 or greater)
unique_math_passing = len(pycityschools_data_complete.loc[pycityschools_data_complete["math_score"] >=70]["student_name"].unique())
percent_math_passing = round((unique_math_passing/total_students)*100, 2)
percent_math_passing
225/22:
# Give the displayed data cleaner formatting
summary_df["Total Students"] = summary_df["Total Students"].map("{:,.2f}".format)
summary_df["Total Budget"] = summary_df["Total Budget"].map("{:,.2f}".format)
summary_df
226/1:
# Dependencies
import pandas as pd
import numpy as np
226/2:
# load in file
pycityschools_file = "Resources/schools_complete.csv"
student_file = "Resources/students_complete.csv"
226/3:
# Read School and Student Data File and store into Pandas DataFrames
pycityschools_data_df = pd.read_csv(pycityschools_file)
student_data_df = pd.read_csv(student_file)

pycityschools_data_df.head()
student_data_df.head()
226/4:
pycityschools_data_complete = pd.merge(student_data_df, pycityschools_data_df, 
                                       on=["school_name", "school_name"], how="left")

pycityschools_data_complete.head()
226/5:
# Count total schools
total_schools = len(pycityschools_data_complete["School ID"].unique())
total_schools
226/6:
# Calculate the total number of students
total_students = len(pycityschools_data_complete["student_name"].unique())
total_students
226/7:
# Calculate Total Budget
total_budget = pycityschools_data_complete["budget"].sum()
total_budget
226/8:
# Calculate the average math score
average_mathscore = round(pycityschools_data_complete["math_score"].mean(), 2)
average_mathscore
226/9:
# Calculate the average reading score
average_readingscore = round(pycityschools_data_complete["reading_score"].mean(), 2)
average_readingscore
226/10:
# Calculate the percentage of students with a passing math score (70 or greater)
unique_math_passing = len(pycityschools_data_complete.loc[pycityschools_data_complete["math_score"] >=70]["student_name"].unique())
percent_math_passing = round((unique_math_passing/total_students)*100, 2)
percent_math_passing
226/11:
# Calculate the percentage of students with a passing reading score (70 or greater)
unique_reading_passing = len(pycityschools_data_complete.loc[pycityschools_data_complete["reading_score"] >=70]["student_name"].unique())
percent_reading_passing = round((unique_reading_passing/total_students)*100, 2)
percent_reading_passing
226/12:
# Calculate the percentage of students who passed math and reading (% Overall Passing)
overall_passing = round((percent_reading_passing + percent_math_passing) / 2, 2)
overall_passing
226/13:
# Create a dataframe to hold the above results
summary_df = pd.DataFrame({"Total Schools": [total_schools],
                          "Total Students": [total_students],
                          "Total Budget": [total_budget],
                          "Average Math Score": [average_mathscore],
                          "Average Reading Score": [average_readingscore],
                          "% of Students with Passing Math Score": [percent_math_passing],
                          "% of Students with Passing Reading Score": [percent_reading_passing],
                          "% of students with Passing Reading and Math Score": [overall_passing]
                          })
summary_df
226/14:
# Give the displayed data cleaner formatting
summary_df["Total Students"] = summary_df["Total Students"].map("{:,.2f}".format)
summary_df["Total Budget"] = summary_df["Total Budget"].map("{:,.2f}".format)
summary_df
226/15:
# Create an overview table that summarizes key metrics about each school
    # School Name
    # School Type
    # Total Students
    # Total School Budget
    # Per Student Budget
    # Average Math Score
    # Average Reading Score
    # % Passing Math
    # % Passing Reading
    # % Overall Passing (The percentage of students that passed math and reading.)
    
# Create a dataframe to hold the above results
226/16:
schools_overview_df = pd.DataFrames({"School Name": school_name,
                                    "School Type": type,
                                    "Total Students": size,
                                    "Total School Budget": }
                                    
                                    "Total Unique Authors": [author_count],
                              "Earliest Year": earliest_year,
                              "Latest Year": latest_year,
                              "Total Reviews": total_reviews})
226/17:
# Create a dataframe to hold the above results
summary_df = pd.DataFrame({"Total Schools": [total_schools],
                          "Total Students": [total_students],
                          "Total Budget": [total_budget],
                          "Average Math Score": [average_mathscore],
                          "Average Reading Score": [average_readingscore],
                          "% of Students with Passing Math Score": [percent_math_passing],
                          "% of Students with Passing Reading Score": [percent_reading_passing],
                          "% of students with Passing Reading and Math Score": [overall_passing]
                          })
summary_df.object()
226/18:
# Create a dataframe to hold the above results
summary_df = pd.DataFrame({"Total Schools": [total_schools],
                          "Total Students": [total_students],
                          "Total Budget": [total_budget],
                          "Average Math Score": [average_mathscore],
                          "Average Reading Score": [average_readingscore],
                          "% of Students with Passing Math Score": [percent_math_passing],
                          "% of Students with Passing Reading Score": [percent_reading_passing],
                          "% of students with Passing Reading and Math Score": [overall_passing]
                          })
summary_df
227/1:
# Import Dependencies
import numpy as np
import matplotlib.pyplot as plt
227/2:
# DATA SET 1
gyms = ["Crunch", "Planet Fitness", "NY Sports Club", "Rickie's Gym"]
members = [49, 92, 84, 53]
227/3:
x_axis = np.arange(0, len(gyms))
tick_locations = []
for x in x_axis:
    tick_locations.append(x)
    
plt.title("NYC Gym Popularity")
plt.xlabel("Gym Name")
plt.ylabel("Number of Members")

plt.xlim(-0.75, len(gyms)-.25)
plt.ylim(0, max(members) + 5)

plt.bar(x_axis, members, facecolor="pink", alpha=1, align="center")
plt.xticks(tick_locations, gyms)
plt.show()
227/4:
# DATA SET 2
x_lim = 2 * np.pi
x_axis = np.arange(0, x_lim, 0.1)
sin = np.sin(x_axis)
227/5:
plt.title("Sin from 0 to 2$\pi$")
plt.xlabel("Real Number frim 0 to 2$\pi$")
plt.ylabel("sin(x)")

plt.hlines(0, 0, x_lim, alpha=0.2)
plt.xlim(0, x_lim)
plt.ylim(-1.25, 1.25)

plt.plot(x_axis, sin, marker="o", color="red", linewidth=1)
plt.show()
227/6:
# DATA SET 3
gyms = ["Crunch", "Planet Fitness", "NY Sports Club", "Rickie's Gym"]
members = [49, 92, 84, 53]
x_axis = np.arange(0, len(gyms))
colors = ["yellowgreen", "red", "lightcoral", "lightskyblue"]
explode = (0, 0.05, 0, 0)
227/7:
plt.title("NYC Gym Popularity")
plt.pie(members, explode=explode, labels=gyms, colors=colors,
       autopct="%1.1f%%", shadow=True, startangle=90)
plt.axis("equal")
plt.show()
227/8:
# DATA SET 4
x_axis = np.arange(0, 10, 0.1)
times = []
for x in x_axis:
    times.append(x * x + np.random.randint(0, np.ceil(max(x_axis))))
227/9:
# Import Dependencies
import numpy as np
import matplotlib.pyplot as plt
227/10:
# DATA SET 1
gyms = ["Crunch", "Planet Fitness", "NY Sports Club", "Rickie's Gym"]
members = [49, 92, 84, 53]
227/11:
x_axis = np.arange(0, len(gyms))
tick_locations = []
for x in x_axis:
    tick_locations.append(x)
    
plt.title("NYC Gym Popularity")
plt.xlabel("Gym Name")
plt.ylabel("Number of Members")

plt.xlim(-0.75, len(gyms)-.25)
plt.ylim(0, max(members) + 5)

plt.bar(x_axis, members, facecolor="pink", alpha=1, align="center")
plt.xticks(tick_locations, gyms)
plt.show()
227/12:
# DATA SET 2
x_lim = 2 * np.pi
x_axis = np.arange(0, x_lim, 0.1)
sin = np.sin(x_axis)
227/13:
plt.title("Sin from 0 to 2$\pi$")
plt.xlabel("Real Number frim 0 to 2$\pi$")
plt.ylabel("sin(x)")

plt.hlines(0, 0, x_lim, alpha=0.2)
plt.xlim(0, x_lim)
plt.ylim(-1.25, 1.25)

plt.plot(x_axis, sin, marker="o", color="red", linewidth=1)
plt.show()
227/14:
# DATA SET 3
gyms = ["Crunch", "Planet Fitness", "NY Sports Club", "Rickie's Gym"]
members = [49, 92, 84, 53]
x_axis = np.arange(0, len(gyms))
colors = ["yellowgreen", "red", "lightcoral", "lightskyblue"]
explode = (0, 0.05, 0, 0)
227/15:
plt.title("NYC Gym Popularity")
plt.pie(members, explode=explode, labels=gyms, colors=colors,
       autopct="%1.1f%%", shadow=True, startangle=90)
plt.axis("equal")
plt.show()
227/16:
# DATA SET 4
x_axis = np.arange(0, 10, 0.1)
times = []
for x in x_axis:
    times.append(x * x + np.random.randint(0, np.ceil(max(x_axis))))
227/17:
# Import Dependencies
import numpy as np
import matplotlib.pyplot as plt
227/18:
# DATA SET 1
gyms = ["Crunch", "Planet Fitness", "NY Sports Club", "Rickie's Gym"]
members = [49, 92, 84, 53]
227/19:
x_axis = np.arange(0, len(gyms))
tick_locations = []
for x in x_axis:
    tick_locations.append(x)
    
plt.title("NYC Gym Popularity")
plt.xlabel("Gym Name")
plt.ylabel("Number of Members")

plt.xlim(-0.75, len(gyms)-.25)
plt.ylim(0, max(members) + 5)

plt.bar(x_axis, members, facecolor="pink", alpha=1, align="center")
plt.xticks(tick_locations, gyms)
plt.show()
227/20:
# DATA SET 2
x_lim = 2 * np.pi
x_axis = np.arange(0, x_lim, 0.1)
sin = np.sin(x_axis)
227/21:
plt.title("Sin from 0 to 2$\pi$")
plt.xlabel("Real Number frim 0 to 2$\pi$")
plt.ylabel("sin(x)")

plt.hlines(0, 0, x_lim, alpha=0.2)
plt.xlim(0, x_lim)
plt.ylim(-1.25, 1.25)

plt.plot(x_axis, sin, marker="o", color="red", linewidth=1)
plt.show()
227/22:
# DATA SET 3
gyms = ["Crunch", "Planet Fitness", "NY Sports Club", "Rickie's Gym"]
members = [49, 92, 84, 53]
x_axis = np.arange(0, len(gyms))
colors = ["yellowgreen", "red", "lightcoral", "lightskyblue"]
explode = (0, 0.05, 0, 0)
227/23:
plt.title("NYC Gym Popularity")
plt.pie(members, explode=explode, labels=gyms, colors=colors,
       autopct="%1.1f%%", shadow=True, startangle=90)
plt.axis("equal")
plt.show()
227/24:
# DATA SET 4
x_axis = np.arange(0, 10, 0.1)
times = []
for x in x_axis:
    times.append(x * x + np.random.randint(0, np.ceil(max(x_axis))))
227/25:
plt.title("Running Time of FakeSort for Sample Input Sizes")
plt.xlabel("Length of Input Array")
plt.ylabel("Time to Sort (s)")

plt.scatter(x_axis, times, marker="o", color="blue")
plt.show()
231/1:
# Dependencies
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
231/2:
# Read CSV
GameOfThrones_data = pd.read_csv("Resources/got.csv")
GameOfThrones_data
231/3: # Get attacker and defender data
231/4: # Get total battle data
231/5: # Configure plot and ticks
231/6: # Set textual properties
231/7: # Show plot
240/1:
# Dependencies
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
240/2:
# Read CSV
GameOfThrones_data = pd.read_csv("Resources/got.csv")
GameOfThrones_data
240/3:
# Get attacker and defender data
attacker_king_data = GameOfThrones_data["attacker_king"].value_counts()
defender_king_data = GameOfThrones_data["defende_king"].value_counts()
240/4:
# Get attacker and defender data
attacker_king_data = GameOfThrones_data["attacker_king"].value_counts()
defender_king_data = GameOfThrones_data["defender_king"].value_counts()
240/5:
# Get total battle data
total_battle_data = attacker_kind_data.add(defender_king_data, fill_value=0)
240/6:
# Get total battle data
total_battle_data = attacker_king_data.add(defender_king_data, fill_value=0)
240/7:
# Configure plot and ticks
total_battle_data.plot(kind="bar", facecolor="blue")
240/8:
# Configure plot and ticks
total_battle_data.plot(kind="bar", facecolor="blue", alpha=0.5)
241/1:
# Dependencies
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
241/2:
# Read CSV
GameOfThrones_data = pd.read_csv("Resources/got.csv")
GameOfThrones_data
241/3:
# Get attacker and defender data
attacker_king_data = GameOfThrones_data["attacker_king"].value_counts()
defender_king_data = GameOfThrones_data["defender_king"].value_counts()
241/4:
# Get total battle data
total_battle_data = attacker_king_data.add(defender_king_data, fill_value=0)
241/5:
# Configure plot and ticks
total_battle_data.plot(kind="bar", facecolor="blue", alpha=0.5)
241/6:
# Set textual properties
plt.title("Total Battle Data")
plt.ylable("Number of Battles")
plt.xlable("Kings")
241/7:
# Set textual properties
plt.title("Total Battle Data")
plt.ylabel("Number of Battles")
plt.xlabel("Kings")
241/8:
# Show plot
plt.show()
plt.tight_layout()
242/1: %matplotlib notebook
242/2:
# Dependencies
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
242/3:
# Read CSV
got_data = pd.read_csv("Resources/got.csv")
got_data
242/4:
# Get attacker and defender king data
attacker_data = got_data["attacker_king"].value_counts()
defender_data = got_data["defender_king"].value_counts()
242/5:
# Get total battle data
battle_data = attacker_data.add(defender_data, fill_value=0)
242/6:
# Configure plot and ticks
battle_data.plot(kind="bar", facecolor="red")
242/7:
# Set textual properties
plt.title("The Bloodthirst of Kings")
plt.ylabel("Number of Battles Participated In")
plt.xlabel("King")
242/8:
# Show plot
plt.show()
242/9:
# Resize plot to display labels
plt.tight_layout()
243/1: %matplotlib notebook
243/2:
# Dependencies
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
243/3:
# Read CSV
GameOfThrones_data = pd.read_csv("Resources/got.csv")
GameOfThrones_data
243/4:
# Get attacker and defender data
attacker_king_data = GameOfThrones_data["attacker_king"].value_counts()
defender_king_data = GameOfThrones_data["defender_king"].value_counts()
243/5:
# Get total battle data
total_battle_data = attacker_king_data.add(defender_king_data, fill_value=0)
243/6:
# Configure plot and ticks
total_battle_data.plot(kind="bar", facecolor="blue", alpha=0.5)
243/7:
# Set textual properties
plt.title("Total Battle Data")
plt.ylabel("Number of Battles")
plt.xlabel("Kings")
243/8:
# Show plot
plt.show()
plt.tight_layout()
246/1: %matplotlib notebook
246/2:
# Import Dependencies
import matplotlib.pyplot as plt
import pandas as pd
import numpy as np
246/3:
# Read CSV
trip_file = "../Resources/trip.csv"
bike_trips_df = pd.read_csv(trip_file, low_memory=False)

bike_trips_df.head()
247/1: %matplotlib notebook
247/2:
# Import Dependencies
import matplotlib.pyplot as plt
import pandas as pd
import numpy as np
247/3:
# Read CSV
trip_file = "../Resources/trip.csv"
bike_trips_df = pd.read_csv(trip_file, low_memory=False)

bike_trips_df.head()
247/4:
# Group by gender
gender_group = bike_trips_df.groupby('gender')

# number of trips per gender
gender_trips = gender_group['tripduration'].count()

# remove stoptime row that is within the group
gender_trips = gender_trips.drop(gender_trips.index[3])

# Chart the data
gender_chart = gender_trips.plot(kind="bar", title="Bike Trips by Gender")
gender_chart.set_xlabel("Gender")
gender_chart.set_ylabel("Number of Trips Taken")

plt.show()
plt.tight_layout()
248/1: %matplotlib notebook
248/2:
# Import Dependencies
import matplotlib.pyplot as plt
import pandas as pd
import numpy as np
248/3:
# Read CSV
trip_file = "../Resources/trip.csv"
bike_trips_df = pd.read_csv(trip_file, low_memory=False)

bike_trips_df.head()
248/4:
# Group by gender
gender_group = bike_trips_df.groupby('gender')

# number of trips per gender
gender_trips = gender_group['tripduration'].count()

# remove stoptime row that is within the group
gender_trips = gender_trips.drop(gender_trips.index[3])

# Chart the data
gender_chart = gender_trips.plot(kind="bar", title="Bike Trips by Gender")
gender_chart.set_xlabel("Gender")
gender_chart.set_ylabel("Number of Trips Taken")

plt.show()
plt.tight_layout()
248/5:
bike_groups = bike_trips_df.groupby(['bikeid','gender'])

sum_biker_groups = bike_groups.sum()
sum_biker_groups.head(15)
248/6:
bike_id = "SEA00001"

just_one_bike = sum_biker_group.loc[bike_id]

biker_pie = just_one_bike.plot(kind="pie", y='tripduration', title=("Trips of " + bike_id))
biker_pie.set_ylabel("Trip Duration")

plt.show()
plt.axis("equal")
249/1: %matplotlib notebook
249/2:
# Import Dependencies
import matplotlib.pyplot as plt
import pandas as pd
import numpy as np
249/3:
# Read CSV
trip_file = "../Resources/trip.csv"
bike_trips_df = pd.read_csv(trip_file, low_memory=False)

bike_trips_df.head()
249/4:
# Group by gender
gender_group = bike_trips_df.groupby('gender')

# number of trips per gender
gender_trips = gender_group['tripduration'].count()

# remove stoptime row that is within the group
gender_trips = gender_trips.drop(gender_trips.index[3])

# Chart the data
gender_chart = gender_trips.plot(kind="bar", title="Bike Trips by Gender")
gender_chart.set_xlabel("Gender")
gender_chart.set_ylabel("Number of Trips Taken")

plt.show()
plt.tight_layout()
249/5:
bike_groups = bike_trips_df.groupby(['bikeid','gender'])

sum_biker_groups = bike_groups.sum()
sum_biker_groups.head(15)
249/6:
bike_id = "SEA00001"

just_one_bike = sum_biker_group.loc[bike_id]

biker_pie = just_one_bike.plot(kind="pie", y='tripduration', title=("Trips of " + bike_id))
biker_pie.set_ylabel("Trip Duration")

plt.show()
plt.axis("equal")
250/1: %matplotlib notebook
250/2:
# Import Dependencies
import matplotlib.pyplot as plt
import pandas as pd
import numpy as np
250/3:
# Read CSV
trip_file = "../Resources/trip.csv"
bike_trips_df = pd.read_csv(trip_file, low_memory=False)

bike_trips_df.head()
250/4:
# Group by gender
gender_group = bike_trips_df.groupby('gender')

# number of trips per gender
gender_trips = gender_group['tripduration'].count()

# remove stoptime row that is within the group
gender_trips = gender_trips.drop(gender_trips.index[3])

# Chart the data
gender_chart = gender_trips.plot(kind="bar", title="Bike Trips by Gender")
gender_chart.set_xlabel("Gender")
gender_chart.set_ylabel("Number of Trips Taken")

plt.show()
plt.tight_layout()
250/5:
bike_groups = bike_trips_df.groupby(['bikeid','gender'])

sum_biker_groups = bike_groups.sum()
sum_biker_groups.head(15)
250/6:
bike_id = "SEA00001"

just_one_bike = sum_biker_groups.loc[bike_id]

biker_pie = just_one_bike.plot(kind="pie", y='tripduration', title=("Trips of " + bike_id))
biker_pie.set_ylabel("Trip Duration")

plt.show()
plt.axis("equal")
251/1:
mpg_data = pd.read_csv("../Resources/mpg.csv")
mpg_data.head()
253/1: %matplotlib notebook
253/2:
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
253/3:
mpg_data = pd.read_csv("../Resources/mpg.csv")
mpg_data.head()
253/4:
mpg_data = mpg_data.loc[mpg_data['horsepower']!= "?"]
mpg_data.head()
253/5:
# Set the index
mpg_data = mpg_data.set_index('car name')

# Remove the origin column
del car_data['origin']

mpg_data.head()
253/6:
# Set the index
mpg_data = mpg_data.set_index('car name')

# Remove the origin column
del mpg_data['origin']

mpg_data.head()
254/1: %matplotlib notebook
254/2:
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
254/3:
mpg_data = pd.read_csv("../Resources/mpg.csv")
mpg_data.head()
254/4:
mpg_data = mpg_data.loc[mpg_data['horsepower']!= "?"]
mpg_data.head()
254/5:
# Set the index
mpg_data = mpg_data.set_index('car name')

# Remove the origin column
del mpg_data['origin']

mpg_data.head()
254/6:
mpg_data.plot(kind="scatter", x="horsepower", y="mpg", grid=True, figsize=(8,8), title="MPG vs. Horsepower")
plt.show()
254/7:
mpg_data.plot(kind="scatter", x="horsepower", y="mpg", grid=True, figsize=(8,8), title="MPG vs. Horsepower")
plt.show()
plt.tight_layout()
255/1: %matplotlib notebook
255/2:
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
255/3:
mpg_data = pd.read_csv("../Resources/mpg.csv")
mpg_data.head()
255/4:
mpg_data = mpg_data.loc[mpg_data['horsepower']!= "?"]
mpg_data.head()
255/5:
# Set the index
mpg_data = mpg_data.set_index('car name')

# Remove the origin column
del mpg_data['origin']

mpg_data.head()
255/6: mpg_data['horsepower'] = pd.to_numeric(mpg_data['horsepower'])
255/7:
mpg_data.plot(kind="scatter", x="horsepower", y="mpg", grid=True, figsize=(8,8), title="MPG vs. Horsepower")
plt.show()
plt.tight_layout()
258/1:
# Import the necessary modules
%matplotlib notebook
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
258/2:
# Bring each CSV into a separate data frame
wrestling_2013 = "../Resources/WWE-Data-2013.csv"
wrestling_2014 = "../Resources/WWE-Data-2014.csv"
wrestling_2015 = "../Resources/WWE-Data-2015.csv"
wrestling_2016 = "../Resources/WWE-Data-2015.csv"

wrestlers_2013_df = pd.read_csv(wrestling_2013)
wrestlers_2014_df = pd.read_csv(wrestling_2014)
wrestlers_2015_df = pd.read_csv(wrestling_2015)
wrestlers_2016_df = pd.read_csv(wrestling_2016)
258/3:
# Merge the first two datasets (2013 and 2014) on "Wrestler" so that no data is lost (should be 182 rows)
merged_wrestlers_df = pd.merge(wrestlers_2013_df, wrestlers_2014_df, how='outer', on='Wrestler')
merged_wrestlers_df.head()
258/4:
# Rename our _x columns to "2013 Wins", "2013 Losses", and "2013 Draws"

# Rename our _y columns to "2014 Wins", "2014 Losses", and "2014 Draws"
258/5: # Merge our newly combined dataframe with the 2015 dataframe
258/6: # Rename "wins", "losses", and "draws" to "2015 Wins", "2015 Losses", and "2015 Draws"
258/7: # Merge our newly combined dataframe with the 2016 dataframe
258/8: # Rename "wins", "losses", and "draws" to "2016 Wins", "2016 Losses", and "2016 Draws"
258/9:
# Rename our _x columns to "2013 Wins", "2013 Losses", and "2013 Draws"
merged_wrestlers_df = merged_wrestlers_df.rename(columns={"Wins_x":"2013 Wins",
                                                          "Losses_x": "2013 Losses",
                                                          "Draws_x": "2013 Draws"})

# Rename our _y columns to "2014 Wins", "2014 Losses", and "2014 Draws"
merged_wrestlers_df = merged_wrestlers_df.rename(columns={"Wins_y":"2014 Wins",
                                                          "Losses_y": "2014 Losses",
                                                          "Draws_y": "2014 Draws"})
merged_wrestlers_df.head()
258/10:
# Merge our newly combined dataframe with the 2015 dataframe
merged_wrestlers_df = pd.merge(merged_wrestlers_df, wrestlers_2015_df, how="outer", on="Wrestler")
merged_wrestlers_df
258/11:
# Rename "wins", "losses", and "draws" to "2015 Wins", "2015 Losses", and "2015 Draws"
merged_wrestlers_df = merged_wrestlers_df.rename(columns={"Wins":"2015 Wins","Losses":"2015 Losses","Draws":"2015 Draws"})

merged_wrestlers_df.head()
258/12:
# Merge our newly combined dataframe with the 2016 dataframe
merged_wrestlers_df = pd.merge(merged_wrestlers_df, wrestlers_2016_df, how="outer", on="Wrestler")
merged_wrestlers_df
258/13:
# Rename "wins", "losses", and "draws" to "2016 Wins", "2016 Losses", and "2016 Draws"
merged_wrestlers_df = merged_wrestlers_df.rename(columns={"Wins":"2016 Wins","Losses":"2016 Losses","Draws":"2016 Draws"})

merged_wrestlers_df.head(10)
259/1:
# Replace all NaN values with 0
combined_wrestlers_df = combined_wrestlers_df.fillna(0)

# Create a new column called "Total Wins" and add up each wrestler's wins per year to fill in the values
combined_wrestlers_df["Total Wins"] = combined_wrestlers_df["2013 Wins"] + combined_wrestlers_df["2014 Wins"] + combined_wrestlers_df["2015 Wins"] + combined_wrestlers_df["2016 Wins"]

# Create a new column called "Total Losses" and add up each wrestler's losses per year to fill in the values
combined_wrestlers_df["Total Losses"] = combined_wrestlers_df["2013 Losses"] + combined_wrestlers_df["2014 Losses"] + combined_wrestlers_df["2015 Losses"] + combined_wrestlers_df["2016 Losses"]


# Create a new column called "Total Draws" and add up each wrestler's draws per year to fill in the values
combined_wrestlers_df["Total Draws"] = combined_wrestlers_df["2013 Draws"] + combined_wrestlers_df["2014 Draws"] + combined_wrestlers_df["2015 Draws"] + combined_wrestlers_df["2016 Draws"]

# Create a new column called "Total Matches" and add up the total wins, losses, and draws for each wrestler to fill in the values
combined_wrestlers_df["Total Matches"] = combined_wrestlers_df["Total Wins"] + combined_wrestlers_df["Total Losses"] + combined_wrestlers_df["Total Draws"]
combined_wrestlers_df.head()
261/1:
import matplotlib.pyplot as plt
import pandas as pd
import numpy as np
261/2:
# Take in all of our wrestling data and read it into pandas
wrestling_2013 = "../Resources/WWE-Data-2013.csv"
wrestling_2014 = "../Resources/WWE-Data-2014.csv"
wrestling_2015 = "../Resources/WWE-Data-2015.csv"
wrestling_2016 = "../Resources/WWE-Data-2016.csv"

wrestlers_2013_df = pd.read_csv(wrestling_2013)
wrestlers_2014_df = pd.read_csv(wrestling_2014)
wrestlers_2015_df = pd.read_csv(wrestling_2015)
wrestlers_2016_df = pd.read_csv(wrestling_2016)
261/3:
# Merge the first two datasets on "Wrestler" so that no data is lost (should be 182 rows)
combined_wrestlers_df = pd.merge(wrestlers_2013_df, wrestlers_2014_df,
                                 how='outer', on='Wrestler')
combined_wrestlers_df.head()
261/4:
# Rename our _x columns to "2013 Wins", "2013 Losses", and "2013 Draws"
combined_wrestlers_df = combined_wrestlers_df.rename(columns={"Wins_x":"2013 Wins",
                                                              "Losses_x":"2013 Losses",
                                                              "Draws_x":"2013 Draws"})

# Rename our _y columns to "2014 Wins", "2014 Losses", and "2014 Draws"
combined_wrestlers_df = combined_wrestlers_df.rename(columns={"Wins_y":"2014 Wins",
                                                              "Losses_y":"2014 Losses",
                                                              "Draws_y":"2014 Draws"})

combined_wrestlers_df.head()
261/5:
# Merge our newly combined dataframe with the 2015 dataframe
combined_wrestlers_df = pd.merge(combined_wrestlers_df, wrestlers_2015_df, how="outer", on="Wrestler")
combined_wrestlers_df
261/6:
# Rename "wins", "losses", and "draws" to "2015 Wins", "2015 Losses", and "2015 Draws"
combined_wrestlers_df = combined_wrestlers_df.rename(columns={"Wins":"2015 Wins","Losses":"2015 Losses","Draws":"2015 Draws"})

combined_wrestlers_df.head()
261/7:
# Merge our newly combined dataframe with the 2016 dataframe
combined_wrestlers_df = pd.merge(combined_wrestlers_df, wrestlers_2016_df, how="outer", on="Wrestler")
combined_wrestlers_df
261/8:
# Rename "wins", "losses", and "draws" to "2016 Wins", "2016 Losses", and "2016 Draws"
combined_wrestlers_df = combined_wrestlers_df.rename(columns={"Wins":"2016 Wins","Losses":"2016 Losses","Draws":"2016 Draws"})

combined_wrestlers_df.head(10)
261/9:
# Replace all NaN values with 0
combined_wrestlers_df = combined_wrestlers_df.fillna(0)

# Create a new column called "Total Wins" and add up each wrestler's wins per year to fill in the values
combined_wrestlers_df["Total Wins"] = combined_wrestlers_df["2013 Wins"] + combined_wrestlers_df["2014 Wins"] + combined_wrestlers_df["2015 Wins"] + combined_wrestlers_df["2016 Wins"]

# Create a new column called "Total Losses" and add up each wrestler's losses per year to fill in the values
combined_wrestlers_df["Total Losses"] = combined_wrestlers_df["2013 Losses"] + combined_wrestlers_df["2014 Losses"] + combined_wrestlers_df["2015 Losses"] + combined_wrestlers_df["2016 Losses"]


# Create a new column called "Total Draws" and add up each wrestler's draws per year to fill in the values
combined_wrestlers_df["Total Draws"] = combined_wrestlers_df["2013 Draws"] + combined_wrestlers_df["2014 Draws"] + combined_wrestlers_df["2015 Draws"] + combined_wrestlers_df["2016 Draws"]

# Create a new column called "Total Matches" and add up the total wins, losses, and draws for each wrestler to fill in the values
combined_wrestlers_df["Total Matches"] = combined_wrestlers_df["Total Wins"] + combined_wrestlers_df["Total Losses"] + combined_wrestlers_df["Total Draws"]
combined_wrestlers_df.head()
261/10:
# Create a new dataframe for those wrestlers who have wrestled at least 100 matches,
# have at least one win in 2013,
# and have at least one win in 2016

# Set the index of this new dataframe to be the wrestlers names
261/11:
# Create a new dataframe for those wrestlers who have wrestled at least 100 matches,
# have at least one win in 2013,
# and have at least one win in 2016
wrestled_100times = combined_wrestlers_df.loc[(combined_wrestlers_df["Total Matches"] >=100) & 
                                              (combined_wrestlers_df["2013 Wins"] >0) &
                                             (combined_wrestlers_df["2016 Wins"] >0)]

# Set the index of this new dataframe to be the wrestlers names
wrestled_100times = wrestled_100times.set_index("Wrestler")
wrestled_100time.head()
262/1:
import matplotlib.pyplot as plt
import pandas as pd
import numpy as np
262/2:
# Take in all of our wrestling data and read it into pandas
wrestling_2013 = "../Resources/WWE-Data-2013.csv"
wrestling_2014 = "../Resources/WWE-Data-2014.csv"
wrestling_2015 = "../Resources/WWE-Data-2015.csv"
wrestling_2016 = "../Resources/WWE-Data-2016.csv"

wrestlers_2013_df = pd.read_csv(wrestling_2013)
wrestlers_2014_df = pd.read_csv(wrestling_2014)
wrestlers_2015_df = pd.read_csv(wrestling_2015)
wrestlers_2016_df = pd.read_csv(wrestling_2016)
262/3:
# Merge the first two datasets on "Wrestler" so that no data is lost (should be 182 rows)
combined_wrestlers_df = pd.merge(wrestlers_2013_df, wrestlers_2014_df,
                                 how='outer', on='Wrestler')
combined_wrestlers_df.head()
262/4:
# Rename our _x columns to "2013 Wins", "2013 Losses", and "2013 Draws"
combined_wrestlers_df = combined_wrestlers_df.rename(columns={"Wins_x":"2013 Wins",
                                                              "Losses_x":"2013 Losses",
                                                              "Draws_x":"2013 Draws"})

# Rename our _y columns to "2014 Wins", "2014 Losses", and "2014 Draws"
combined_wrestlers_df = combined_wrestlers_df.rename(columns={"Wins_y":"2014 Wins",
                                                              "Losses_y":"2014 Losses",
                                                              "Draws_y":"2014 Draws"})

combined_wrestlers_df.head()
262/5:
# Merge our newly combined dataframe with the 2015 dataframe
combined_wrestlers_df = pd.merge(combined_wrestlers_df, wrestlers_2015_df, how="outer", on="Wrestler")
combined_wrestlers_df
262/6:
# Rename "wins", "losses", and "draws" to "2015 Wins", "2015 Losses", and "2015 Draws"
combined_wrestlers_df = combined_wrestlers_df.rename(columns={"Wins":"2015 Wins","Losses":"2015 Losses","Draws":"2015 Draws"})

combined_wrestlers_df.head()
262/7:
# Merge our newly combined dataframe with the 2016 dataframe
combined_wrestlers_df = pd.merge(combined_wrestlers_df, wrestlers_2016_df, how="outer", on="Wrestler")
combined_wrestlers_df
262/8:
# Rename "wins", "losses", and "draws" to "2016 Wins", "2016 Losses", and "2016 Draws"
combined_wrestlers_df = combined_wrestlers_df.rename(columns={"Wins":"2016 Wins","Losses":"2016 Losses","Draws":"2016 Draws"})

combined_wrestlers_df.head(10)
262/9:
# Replace all NaN values with 0
combined_wrestlers_df = combined_wrestlers_df.fillna(0)

# Create a new column called "Total Wins" and add up each wrestler's wins per year to fill in the values
combined_wrestlers_df["Total Wins"] = combined_wrestlers_df["2013 Wins"] + combined_wrestlers_df["2014 Wins"] + combined_wrestlers_df["2015 Wins"] + combined_wrestlers_df["2016 Wins"]

# Create a new column called "Total Losses" and add up each wrestler's losses per year to fill in the values
combined_wrestlers_df["Total Losses"] = combined_wrestlers_df["2013 Losses"] + combined_wrestlers_df["2014 Losses"] + combined_wrestlers_df["2015 Losses"] + combined_wrestlers_df["2016 Losses"]


# Create a new column called "Total Draws" and add up each wrestler's draws per year to fill in the values
combined_wrestlers_df["Total Draws"] = combined_wrestlers_df["2013 Draws"] + combined_wrestlers_df["2014 Draws"] + combined_wrestlers_df["2015 Draws"] + combined_wrestlers_df["2016 Draws"]

# Create a new column called "Total Matches" and add up the total wins, losses, and draws for each wrestler to fill in the values
combined_wrestlers_df["Total Matches"] = combined_wrestlers_df["Total Wins"] + combined_wrestlers_df["Total Losses"] + combined_wrestlers_df["Total Draws"]
combined_wrestlers_df.head()
262/10:
# Create a new dataframe for those wrestlers who have wrestled at least 100 matches,
# have at least one win in 2013,
# and have at least one win in 2016
wrestled_100times = combined_wrestlers_df.loc[(combined_wrestlers_df["Total Matches"] >=100) & 
                                              (combined_wrestlers_df["2013 Wins"] >0) &
                                             (combined_wrestlers_df["2016 Wins"] >0)]

# Set the index of this new dataframe to be the wrestlers names
wrestled_100times = wrestled_100times.set_index("Wrestler")
wrestled_100times.head()
262/11:
# Create a new dataframe for those wrestlers who have wrestled at least 100 matches,
# have at least one win in 2013,
# and have at least one win in 2016
wrestled_100times = combined_wrestlers_df.loc[
    (combined_wrestlers_df["Total Matches"] >=100) &
    (combined_wrestlers_df["2013 Wins"] >0) &
    (combined_wrestlers_df["2016 Wins"] >0)
]

# Set the index of this new dataframe to be the wrestlers names
wrestled_100times = wrestled_100times.set_index("Wrestler")
wrestled_100times.head()
265/1: %matplotlib notebook
265/2:
import matplotlib.pyplot as plt
import pandas as pd
import numpy as np
265/3:
# Take in all of our wrestling data and read it into pandas
wrestling_2013 = "../Resources/WWE-Data-2013.csv"
wrestling_2014 = "../Resources/WWE-Data-2014.csv"
wrestling_2015 = "../Resources/WWE-Data-2015.csv"
wrestling_2016 = "../Resources/WWE-Data-2016.csv"

wrestlers_2013_df = pd.read_csv(wrestling_2013)
wrestlers_2014_df = pd.read_csv(wrestling_2014)
wrestlers_2015_df = pd.read_csv(wrestling_2015)
wrestlers_2016_df = pd.read_csv(wrestling_2016)
265/4:
# Merge the first two datasets on "Wrestler" so that no data is lost (should be 182 rows)
combined_wrestlers_df = pd.merge(wrestlers_2013_df, wrestlers_2014_df, how='outer', on='Wrestler')
combined_wrestlers_df
265/5:
# Rename our _x columns to "2013 Wins", "2013 Losses", and "2013 Draws"
combined_wrestlers_df = combined_wrestlers_df.rename(columns={"Wins_x":"2013 Wins", "Losses_x":"2013 Losses", "Draws_x":"2013 Draws"})

# Rename our _y columns to "2014 Wins", "2014 Losses", and "2014 Draws"
combined_wrestlers_df = combined_wrestlers_df.rename(columns={"Wins_y":"2014 Wins","Losses_y":"2014 Losses","Draws_y":"2014 Draws"})

combined_wrestlers_df.head()
265/6:
# Merge our newly combined dataframe with the 2015 dataframe
combined_wrestlers_df = pd.merge(combined_wrestlers_df, wrestlers_2015_df, how="outer", on="Wrestler")
combined_wrestlers_df
265/7:
# Rename "wins", "losses", and "draws" to "2015 Wins", "2015 Losses", and "2015 Draws"
combined_wrestlers_df = combined_wrestlers_df.rename(columns={"Wins":"2015 Wins","Losses":"2015 Losses","Draws":"2015 Draws"})

combined_wrestlers_df.head()
265/8:
# Merge our newly combined dataframe with the 2016 dataframe
combined_wrestlers_df = pd.merge(combined_wrestlers_df, wrestlers_2016_df, how="outer", on="Wrestler")
combined_wrestlers_df
265/9:
# Rename "wins", "losses", and "draws" to "2016 Wins", "2016 Losses", and "2016 Draws"
combined_wrestlers_df = combined_wrestlers_df.rename(columns={"Wins":"2016 Wins","Losses":"2016 Losses","Draws":"2016 Draws"})

combined_wrestlers_df.head()
265/10:
# Replace all NaN values with 0 
combined_wrestlers_df = combined_wrestlers_df.fillna(0)

# Create a new column called "Total Wins" and add up each wrestler's wins per year to fill in the values
combined_wrestlers_df["Total Wins"] = combined_wrestlers_df["2013 Wins"] + combined_wrestlers_df["2014 Wins"] + combined_wrestlers_df["2015 Wins"] + combined_wrestlers_df["2016 Wins"]

# Create a new column called "Total Losses" and add up each wrestler's losses per year to fill in the values
combined_wrestlers_df["Total Losses"] = combined_wrestlers_df["2013 Losses"] + combined_wrestlers_df["2014 Losses"] + combined_wrestlers_df["2015 Losses"] + combined_wrestlers_df["2016 Losses"]

# Create a new column called "Total Draws" and add up each wrestler's draws per year to fill in the values
combined_wrestlers_df["Total Draws"] = combined_wrestlers_df["2013 Draws"] + combined_wrestlers_df["2014 Draws"] + combined_wrestlers_df["2015 Draws"] + combined_wrestlers_df["2016 Draws"]

# Create a new column called "Total Matches" and add up the total wins, losses, and draws for each wrestler to fill in the values
combined_wrestlers_df["Total Matches"] = combined_wrestlers_df["Total Wins"] + combined_wrestlers_df["Total Losses"] + combined_wrestlers_df["Total Draws"]

combined_wrestlers_df
265/11:
# Create a new dataframe for those wrestlers who have wrestled at least 100 matches,
# have at least one win in 2013,
# and have at least one win in 2016
wrestled_over_hundred = combined_wrestlers_df.loc[(combined_wrestlers_df["Total Matches"] >= 100) &
                                                 (combined_wrestlers_df["2013 Wins"] > 0) &
                                                 (combined_wrestlers_df["2016 Wins"] > 0)]

# Set the index of this new dataframe to be the wrestlers names
wrestled_over_hundred = wrestled_over_hundred.set_index("Wrestler")

wrestled_over_hundred.head()
265/12:
# Collect the user's input to search through our data frame
wrestler_name = input("What wrestler's career would you like to look at?")
265/13:
# Create a series that looks for a wrestler by name and then traces their wins from 2013 to 2016
wins_over_time = wrestled_over_hundred.loc[wrestler_name,["2013 Wins","2014 Wins", "2015 Wins", "2016 Wins"]]

# Create a series that looks for a wrestler by name and then traces their losses from 2013 to 2016
losses_over_time = wrestled_over_hundred.loc[wrestler_name,["2013 Losses","2014 Losses",
                                                            "2015 Losses", "2016 Losses"]]
265/14:
# Create a list of the years that we will use as our x axis
years = [2013,2014,2015,2016]

# Plot our line that will be used to track a wrestler's wins over the years
plt.plot(years, wins_over_time, color="green", label="Wins")

# Plot our line that will be used to track a wrestler's losses over the years
plt.plot(years, losses_over_time, color="blue", label="Losses")

# Place a legend on the chart in what matplotlib believes to be the "best" location
plt.legend(loc="best")

plt.title(wrestler_name + "'s Recent Career")
plt.xlabel("Years")
plt.xticks(np.arange(min(years), max(years)+1, 1.0))
plt.ylabel("Number of Wins/Losses")

# Print our chart to the screen
plt.show()
276/1:
# Give the displayed data cleaner formatting
summary_df["Total Students"] = summary_df["Total Students"].map("{:,.2f}".format)
summary_df["Total Budget"] = summary_df["Total Budget"].map("{:,.2f}".format)
summary_df["Average Math Score"] = summary_df["Average Math Score"].map("%{:,.2f}".format)
summary_df
276/2:
# Dependencies
import pandas as pd
import numpy as np
276/3:
# load in file
pycityschools_file = "Resources/schools_complete.csv"
student_file = "Resources/students_complete.csv"
276/4:
# Read School and Student Data File and store into Pandas DataFrames
pycityschools_data_df = pd.read_csv(pycityschools_file)
student_data_df = pd.read_csv(student_file)

pycityschools_data_df.head()
student_data_df.head()
276/5:
pycityschools_data_complete = pd.merge(student_data_df, pycityschools_data_df, 
                                       on=["school_name", "school_name"], how="left")

pycityschools_data_complete.head()
276/6:
# Count total schools
total_schools = len(pycityschools_data_complete["School ID"].unique())
total_schools
276/7:
# Calculate the total number of students
total_students = len(pycityschools_data_complete["student_name"].unique())
total_students
276/8:
# Calculate Total Budget
total_budget = pycityschools_data_complete["budget"].sum()
total_budget
276/9:
# Calculate the average math score
average_mathscore = round(pycityschools_data_complete["math_score"].mean(), 2)
average_mathscore
276/10:
# Calculate the average reading score
average_readingscore = round(pycityschools_data_complete["reading_score"].mean(), 2)
average_readingscore
276/11:
# Calculate the percentage of students with a passing math score (70 or greater)
unique_math_passing = len(pycityschools_data_complete.loc[pycityschools_data_complete["math_score"] >=70]["student_name"].unique())
percent_math_passing = round((unique_math_passing/total_students)*100, 2)
percent_math_passing
276/12:
# Calculate the percentage of students with a passing reading score (70 or greater)
unique_reading_passing = len(pycityschools_data_complete.loc[pycityschools_data_complete["reading_score"] >=70]["student_name"].unique())
percent_reading_passing = round((unique_reading_passing/total_students)*100, 2)
percent_reading_passing
276/13:
# Calculate the percentage of students who passed math and reading (% Overall Passing)
overall_passing = round((percent_reading_passing + percent_math_passing) / 2, 2)
overall_passing
276/14:
# Create a dataframe to hold the above results
summary_df = pd.DataFrame({"Total Schools": [total_schools],
                          "Total Students": [total_students],
                          "Total Budget": [total_budget],
                          "Average Math Score": [average_mathscore],
                          "Average Reading Score": [average_readingscore],
                          "% of Students with Passing Math Score": [percent_math_passing],
                          "% of Students with Passing Reading Score": [percent_reading_passing],
                          "% of students with Passing Reading and Math Score": [overall_passing]
                          })
summary_df
276/15:
# Give the displayed data cleaner formatting
summary_df["Total Students"] = summary_df["Total Students"].map("{:,.2f}".format)
summary_df["Total Budget"] = summary_df["Total Budget"].map("{:,.2f}".format)
summary_df["Average Math Score"] = summary_df["Average Math Score"].map("%{:,.2f}".format)
summary_df["Total Budget"] = summary_df["Total Budget"].map("{:,.2f}".format)
summary_df
276/16:
# Give the displayed data cleaner formatting
summary_df["Total Students"] = summary_df["Total Students"].map("{:,.2f}".format)
summary_df["Total Budget"] = summary_df["Total Budget"].map("{:,.2f}".format)
summary_df["Average Math Score"] = summary_df["Average Math Score"].map("%{:,.2f}".format)
summary_df["Average Reading Score"] = summary_df["Average Reading Score"].map("%{:,.2f}".format)
summary_df
277/1:
# Dependencies
import pandas as pd
import numpy as np
281/1:
# Dependencies
import pandas as pd
import numpy as np
281/2:
# load in file
pycityschools_file = "Resources/schools_complete.csv"
student_file = "Resources/students_complete.csv"
281/3:
# Read School and Student Data File and store into Pandas DataFrames
pycityschools_data_df = pd.read_csv(pycityschools_file)
student_data_df = pd.read_csv(student_file)

pycityschools_data_df.head()
student_data_df.head()
281/4:
pycityschools_data_complete = pd.merge(student_data_df, pycityschools_data_df, 
                                       on=["school_name", "school_name"], how="left")

pycityschools_data_complete.head()
281/5:
# Count total schools
total_schools = len(pycityschools_data_complete["School ID"].unique())
total_schools
281/6:
# Calculate the total number of students
total_students = len(pycityschools_data_complete["student_name"].unique())
total_students
281/7:
# Calculate Total Budget
total_budget = pycityschools_data_complete["budget"].sum()
total_budget
281/8:
# Calculate the average math score
average_mathscore = round(pycityschools_data_complete["math_score"].mean(), 2)
average_mathscore
281/9:
# Calculate the average reading score
average_readingscore = round(pycityschools_data_complete["reading_score"].mean(), 2)
average_readingscore
281/10:
# Calculate the percentage of students with a passing math score (70 or greater)
unique_math_passing = len(pycityschools_data_complete.loc[pycityschools_data_complete["math_score"] >=70]["student_name"].unique())
percent_math_passing = round((unique_math_passing/total_students)*100, 2)
percent_math_passing
281/11:
# Calculate the percentage of students with a passing reading score (70 or greater)
unique_reading_passing = len(pycityschools_data_complete.loc[pycityschools_data_complete["reading_score"] >=70]["student_name"].unique())
percent_reading_passing = round((unique_reading_passing/total_students)*100, 2)
percent_reading_passing
281/12:
# Calculate the percentage of students who passed math and reading (% Overall Passing)
overall_passing = round((percent_reading_passing + percent_math_passing) / 2, 2)
overall_passing
281/13:
# Create a dataframe to hold the above results
summary_df = pd.DataFrame({"Total Schools": [total_schools],
                          "Total Students": [total_students],
                          "Total Budget": [total_budget],
                          "Average Math Score": [average_mathscore],
                          "Average Reading Score": [average_readingscore],
                          "% of Students with Passing Math Score": [percent_math_passing],
                          "% of Students with Passing Reading Score": [percent_reading_passing],
                          "% of students with Passing Reading and Math Score": [overall_passing]
                          })
summary_df
281/14:
# Give the displayed data cleaner formatting
summary_df["Total Students"] = summary_df["Total Students"].map("{:,.2f}".format)
summary_df["Total Budget"] = summary_df["Total Budget"].map("{:,.2f}".format)
summary_df["Average Math Score"] = summary_df["Average Math Score"].map("%{:,.2f}".format)
summary_df["Average Reading Score"] = summary_df["Average Reading Score"].map("%{:,.2f}".format)
summary_df
281/15:
# Create an overview table that summarizes key metrics about each school
    # School Name
    # School Type
    # Total Students
    # Total School Budget
    # Per Student Budget
    # Average Math Score
    # Average Reading Score
    # % Passing Math
    # % Passing Reading
    # % Overall Passing (The percentage of students that passed math and reading.)
    
# Create a dataframe to hold the above results
281/16:
# schools_overview_df = pd.DataFrames({"School Name": school_name,
                                   # "School Type": type,
                                    # "Total Students": size,
                                    # "Total School Budget": }
                                    
                                    # "Total Unique Authors": [author_count],
                              # "Earliest Year": earliest_year,
                              # "Latest Year": latest_year,
                              # "Total Reviews": total_reviews})
282/1:
# Dependencies
import pandas as pd
import numpy as np
282/2:
# load in file
pycityschools_file = "Resources/schools_complete.csv"
student_file = "Resources/students_complete.csv"
282/3:
# Read School and Student Data File and store into Pandas DataFrames
pycityschools_data_df = pd.read_csv(pycityschools_file)
student_data_df = pd.read_csv(student_file)

pycityschools_data_df.head()
student_data_df.head()
282/4:
pycityschools_data_complete = pd.merge(student_data_df, pycityschools_data_df, 
                                       on=["school_name", "school_name"], how="left")

pycityschools_data_complete.head()
282/5:
# Count total schools
total_schools = len(pycityschools_data_complete["School ID"].unique())
total_schools
282/6:
# Calculate the total number of students
total_students = len(pycityschools_data_complete["student_name"].unique())
total_students
282/7:
# Calculate Total Budget
total_budget = pycityschools_data_complete["budget"].sum()
total_budget
282/8:
# Calculate the average math score
average_mathscore = round(pycityschools_data_complete["math_score"].mean(), 2)
average_mathscore
282/9:
# Calculate the average reading score
average_readingscore = round(pycityschools_data_complete["reading_score"].mean(), 2)
average_readingscore
282/10:
# Calculate the percentage of students with a passing math score (70 or greater)
unique_math_passing = len(pycityschools_data_complete.loc[pycityschools_data_complete["math_score"] >=70]["student_name"].unique())
percent_math_passing = round((unique_math_passing/total_students)*100, 2)
percent_math_passing
282/11:
# Calculate the percentage of students with a passing reading score (70 or greater)
unique_reading_passing = len(pycityschools_data_complete.loc[pycityschools_data_complete["reading_score"] >=70]["student_name"].unique())
percent_reading_passing = round((unique_reading_passing/total_students)*100, 2)
percent_reading_passing
282/12:
# Calculate the percentage of students who passed math and reading (% Overall Passing)
overall_passing = round((percent_reading_passing + percent_math_passing) / 2, 2)
overall_passing
282/13:
# Create a dataframe to hold the above results
summary_df = pd.DataFrame({"Total Schools": [total_schools],
                          "Total Students": [total_students],
                          "Total Budget": [total_budget],
                          "Average Math Score": [average_mathscore],
                          "Average Reading Score": [average_readingscore],
                          "% of Students with Passing Math Score": [percent_math_passing],
                          "% of Students with Passing Reading Score": [percent_reading_passing],
                          "% of students with Passing Reading and Math Score": [overall_passing]
                          })
summary_df
282/14:
# Give the displayed data cleaner formatting
summary_df["Total Students"] = summary_df["Total Students"].map("{:,.2f}".format)
summary_df["Total Budget"] = summary_df["Total Budget"].map("{:,.2f}".format)
summary_df["Average Math Score"] = summary_df["Average Math Score"].map("{:,.2f}%".format)
summary_df["Average Reading Score"] = summary_df["Average Reading Score"].map("{:,.2f}%".format)
summary_df["% of Students with Passing Math Score"] = summary_df["% of Students with Passing Math Score"].map("{:,.2f}%".format)
summary_df["% of Students with Passing Reading Score"] = summary_df["% of Students with Passing Reading Score"].map("{:,.2f}%".format)
summary_df
282/15:
# Create an overview table that summarizes key metrics about each school
    # School Name
    # School Type
    # Total Students
    # Total School Budget
    # Per Student Budget
    # Average Math Score
    # Average Reading Score
    # % Passing Math
    # % Passing Reading
    # % Overall Passing (The percentage of students that passed math and reading.)
    
# Create a dataframe to hold the above results
282/16:
# schools_overview_df = pd.DataFrames({"School Name": school_name,
                                   # "School Type": type,
                                    # "Total Students": size,
                                    # "Total School Budget": }
                                    
                                    # "Total Unique Authors": [author_count],
                              # "Earliest Year": earliest_year,
                              # "Latest Year": latest_year,
                              # "Total Reviews": total_reviews})
283/1:
# Dependencies
import pandas as pd
import numpy as np
283/2:
# load in file
pycityschools_file = "Resources/schools_complete.csv"
student_file = "Resources/students_complete.csv"
283/3:
# Read School and Student Data File and store into Pandas DataFrames
pycityschools_data_df = pd.read_csv(pycityschools_file)
student_data_df = pd.read_csv(student_file)

pycityschools_data_df.head()
student_data_df.head()
283/4:
pycityschools_data_complete = pd.merge(student_data_df, pycityschools_data_df, 
                                       on=["school_name", "school_name"], how="left")

pycityschools_data_complete.head()
283/5:
# Count total schools
total_schools = len(pycityschools_data_complete["School ID"].unique())
total_schools
283/6:
# Calculate the total number of students
total_students = len(pycityschools_data_complete["student_name"].unique())
total_students
283/7:
# Calculate Total Budget
total_budget = pycityschools_data_complete["budget"].sum()
total_budget
283/8:
# Calculate the average math score
average_mathscore = round(pycityschools_data_complete["math_score"].mean(), 2)
average_mathscore
283/9:
# Calculate the average reading score
average_readingscore = round(pycityschools_data_complete["reading_score"].mean(), 2)
average_readingscore
283/10:
# Calculate the percentage of students with a passing math score (70 or greater)
unique_math_passing = len(pycityschools_data_complete.loc[pycityschools_data_complete["math_score"] >=70]["student_name"].unique())
percent_math_passing = round((unique_math_passing/total_students)*100, 2)
percent_math_passing
283/11:
# Calculate the percentage of students with a passing reading score (70 or greater)
unique_reading_passing = len(pycityschools_data_complete.loc[pycityschools_data_complete["reading_score"] >=70]["student_name"].unique())
percent_reading_passing = round((unique_reading_passing/total_students)*100, 2)
percent_reading_passing
283/12:
# Calculate the percentage of students who passed math and reading (% Overall Passing)
overall_passing = round((percent_reading_passing + percent_math_passing) / 2, 2)
overall_passing
283/13:
# Create a dataframe to hold the above results
summary_df = pd.DataFrame({"Total Schools": [total_schools],
                          "Total Students": [total_students],
                          "Total Budget": [total_budget],
                          "Average Math Score": [average_mathscore],
                          "Average Reading Score": [average_readingscore],
                          "% of Students with Passing Math Score": [percent_math_passing],
                          "% of Students with Passing Reading Score": [percent_reading_passing],
                          "% of students with Passing Reading and Math Score": [overall_passing]
                          })
summary_df
283/14:
# Give the displayed data cleaner formatting
summary_df["Total Students"] = summary_df["Total Students"].map("{:,.2f}".format)
summary_df["Total Budget"] = summary_df["Total Budget"].map("{:,.2f}".format)
summary_df["Average Math Score"] = summary_df["Average Math Score"].map("{:,.2f}%".format)
summary_df["Average Reading Score"] = summary_df["Average Reading Score"].map("{:,.2f}%".format)
summary_df["% of Students with Passing Math Score"] = summary_df["% of Students with Passing Math Score"].map("{:,.2f}%".format)
summary_df["% of Students with Passing Reading Score"] = summary_df["% of Students with Passing Reading Score"].map("{:,.2f}%".format)
summary_df["% of Students with Passing Reading and Math Score"] = summary_df["% of Students with Passing Reading and Math Score"].map("{:,.2f}%".format)
summary_df
283/15:
# Create a dataframe to hold the above results
summary_df = pd.DataFrame({"Total Schools": [total_schools],
                          "Total Students": [total_students],
                          "Total Budget": [total_budget],
                          "Average Math Score": [average_mathscore],
                          "Average Reading Score": [average_readingscore],
                          "% of Students with Passing Math Score": [percent_math_passing],
                          "% of Students with Passing Reading Score": [percent_reading_passing],
                          "% of students with Passing Reading and Math Score": [overall_passing]
                          })
summary_df
284/1:
# Dependencies
import pandas as pd
import numpy as np
284/2:
# load in file
pycityschools_file = "Resources/schools_complete.csv"
student_file = "Resources/students_complete.csv"
284/3:
# Read School and Student Data File and store into Pandas DataFrames
pycityschools_data_df = pd.read_csv(pycityschools_file)
student_data_df = pd.read_csv(student_file)

pycityschools_data_df.head()
student_data_df.head()
284/4:
pycityschools_data_complete = pd.merge(student_data_df, pycityschools_data_df, 
                                       on=["school_name", "school_name"], how="left")

pycityschools_data_complete.head()
284/5:
# Count total schools
total_schools = len(pycityschools_data_complete["School ID"].unique())
total_schools
284/6:
# Calculate the total number of students
total_students = len(pycityschools_data_complete["student_name"].unique())
total_students
284/7:
# Calculate Total Budget
total_budget = pycityschools_data_complete["budget"].sum()
total_budget
284/8:
# Calculate the average math score
average_mathscore = round(pycityschools_data_complete["math_score"].mean(), 2)
average_mathscore
284/9:
# Calculate the average reading score
average_readingscore = round(pycityschools_data_complete["reading_score"].mean(), 2)
average_readingscore
284/10:
# Calculate the percentage of students with a passing math score (70 or greater)
unique_math_passing = len(pycityschools_data_complete.loc[pycityschools_data_complete["math_score"] >=70]["student_name"].unique())
percent_math_passing = round((unique_math_passing/total_students)*100, 2)
percent_math_passing
284/11:
# Calculate the percentage of students with a passing reading score (70 or greater)
unique_reading_passing = len(pycityschools_data_complete.loc[pycityschools_data_complete["reading_score"] >=70]["student_name"].unique())
percent_reading_passing = round((unique_reading_passing/total_students)*100, 2)
percent_reading_passing
284/12:
# Calculate the percentage of students who passed math and reading (% Overall Passing)
overall_passing = round((percent_reading_passing + percent_math_passing) / 2, 2)
overall_passing
284/13:
# Create a dataframe to hold the above results
summary_df = pd.DataFrame({"Total Schools": [total_schools],
                          "Total Students": [total_students],
                          "Total Budget": [total_budget],
                          "Average Math Score": [average_mathscore],
                          "Average Reading Score": [average_readingscore],
                          "% of Students with Passing Math Score": [percent_math_passing],
                          "% of Students with Passing Reading Score": [percent_reading_passing],
                          "% of students with Passing Reading and Math Score": [overall_passing]
                          })
summary_df
284/14:
# Give the displayed data cleaner formatting
summary_df["Total Students"] = summary_df["Total Students"].map("{:,.2f}".format)
summary_df["Total Budget"] = summary_df["Total Budget"].map("{:,.2f}".format)
summary_df["Average Math Score"] = summary_df["Average Math Score"].map("{:,.2f}%".format)
summary_df["Average Reading Score"] = summary_df["Average Reading Score"].map("{:,.2f}%".format)
summary_df["% of Students with Passing Math Score"] = summary_df["% of Students with Passing Math Score"].map("{:,.2f}%".format)
summary_df["% of Students with Passing Reading Score"] = summary_df["% of Students with Passing Reading Score"].map("{:,.2f}%".format)
summary_df["% of Students with Passing Reading and Math Score"] = summary_df["% of Students with Passing Reading and Math Score"].map("{:,.2f}%".format)
summary_df
285/1:
# Dependencies
import pandas as pd
import numpy as np
285/2:
# load in file
pycityschools_file = "Resources/schools_complete.csv"
student_file = "Resources/students_complete.csv"
285/3:
# Read School and Student Data File and store into Pandas DataFrames
pycityschools_data_df = pd.read_csv(pycityschools_file)
student_data_df = pd.read_csv(student_file)

pycityschools_data_df.head()
student_data_df.head()
285/4:
pycityschools_data_complete = pd.merge(student_data_df, pycityschools_data_df, 
                                       on=["school_name", "school_name"], how="left")

pycityschools_data_complete.head()
285/5:
# Count total schools
total_schools = len(pycityschools_data_complete["School ID"].unique())
total_schools
285/6:
# Calculate the total number of students
total_students = len(pycityschools_data_complete["student_name"].unique())
total_students
285/7:
# Calculate Total Budget
total_budget = pycityschools_data_complete["budget"].sum()
total_budget
285/8:
# Calculate the average math score
average_mathscore = round(pycityschools_data_complete["math_score"].mean(), 2)
average_mathscore
285/9:
# Calculate the average reading score
average_readingscore = round(pycityschools_data_complete["reading_score"].mean(), 2)
average_readingscore
285/10:
# Calculate the percentage of students with a passing math score (70 or greater)
unique_math_passing = len(pycityschools_data_complete.loc[pycityschools_data_complete["math_score"] >=70]["student_name"].unique())
percent_math_passing = round((unique_math_passing/total_students)*100, 2)
percent_math_passing
285/11:
# Calculate the percentage of students with a passing reading score (70 or greater)
unique_reading_passing = len(pycityschools_data_complete.loc[pycityschools_data_complete["reading_score"] >=70]["student_name"].unique())
percent_reading_passing = round((unique_reading_passing/total_students)*100, 2)
percent_reading_passing
285/12:
# Calculate the percentage of students who passed math and reading (% Overall Passing)
overall_passing = round((percent_reading_passing + percent_math_passing) / 2, 2)
overall_passing
285/13:
# Create a dataframe to hold the above results
summary_df = pd.DataFrame({"Total Schools": [total_schools],
                          "Total Students": [total_students],
                          "Total Budget": [total_budget],
                          "Average Math Score": [average_mathscore],
                          "Average Reading Score": [average_readingscore],
                          "% of Students with Passing Math Score": [percent_math_passing],
                          "% of Students with Passing Reading Score": [percent_reading_passing],
                          "% of Students with Passing Reading and Math Score": [overall_passing]
                          })
summary_df
285/14:
# Give the displayed data cleaner formatting
summary_df["Total Students"] = summary_df["Total Students"].map("{:,.2f}".format)
summary_df["Total Budget"] = summary_df["Total Budget"].map("{:,.2f}".format)
summary_df["Average Math Score"] = summary_df["Average Math Score"].map("{:,.2f}%".format)
summary_df["Average Reading Score"] = summary_df["Average Reading Score"].map("{:,.2f}%".format)
summary_df["% of Students with Passing Math Score"] = summary_df["% of Students with Passing Math Score"].map("{:,.2f}%".format)
summary_df["% of Students with Passing Reading Score"] = summary_df["% of Students with Passing Reading Score"].map("{:,.2f}%".format)
summary_df["% of Students with Passing Reading and Math Score"] = summary_df["% of Students with Passing Reading and Math Score"].map("{:,.2f}%".format)
summary_df
285/15:
# Create an overview table that summarizes key metrics about each school
    # School Name
    # School Type
    # Total Students
    # Total School Budget
    # Per Student Budget
    # Average Math Score
    # Average Reading Score
    # % Passing Math
    # % Passing Reading
    # % Overall Passing (The percentage of students that passed math and reading.)
    
# Create a dataframe to hold the above results
285/16:
# schools_overview_df = pd.DataFrames({"School Name": school_name,
                                   # "School Type": type,
                                    # "Total Students": size,
                                    # "Total School Budget": }
                                    
                                    # "Total Unique Authors": [author_count],
                              # "Earliest Year": earliest_year,
                              # "Latest Year": latest_year,
                              # "Total Reviews": total_reviews})
285/17:
# Calculate Total Budget
total_budget = pycityschools_data_df ["budget"].unique()
total_budget
285/18:
# Calculate Total Budget
total_budget = pycityschools_data_df ["budget"].sum()
total_budget
287/1:
# Dependencies
import pandas as pd
import numpy as np
287/2:
# load in file
pycityschools_file = "Resources/schools_complete.csv"
student_file = "Resources/students_complete.csv"
287/3:
# Read School and Student Data File and store into Pandas DataFrames
pycityschools_data_df = pd.read_csv(pycityschools_file)
student_data_df = pd.read_csv(student_file)

pycityschools_data_df.head()
student_data_df.head()
287/4:
pycityschools_data_complete = pd.merge(student_data_df, pycityschools_data_df, 
                                       on=["school_name", "school_name"], how="left")

pycityschools_data_complete.head()
287/5:
# Count total schools
total_schools = len(pycityschools_data_complete["School ID"].unique())
total_schools
287/6:
# Calculate the total number of students
total_students = len(pycityschools_data_complete["student_name"].unique())
total_students
287/7:
# Calculate Total Budget
total_budget = pycityschools_data_df ["budget"].sum()
total_budget
287/8:
# Calculate the average math score
average_mathscore = round(pycityschools_data_complete["math_score"].mean(), 2)
average_mathscore
287/9:
# Calculate the average reading score
average_readingscore = round(pycityschools_data_complete["reading_score"].mean(), 2)
average_readingscore
287/10:
# Calculate the percentage of students with a passing math score (70 or greater)
unique_math_passing = len(pycityschools_data_complete.loc[pycityschools_data_complete["math_score"] >=70]["student_name"].unique())
percent_math_passing = round((unique_math_passing/total_students)*100, 2)
percent_math_passing
287/11:
# Calculate the percentage of students with a passing reading score (70 or greater)
unique_reading_passing = len(pycityschools_data_complete.loc[pycityschools_data_complete["reading_score"] >=70]["student_name"].unique())
percent_reading_passing = round((unique_reading_passing/total_students)*100, 2)
percent_reading_passing
287/12:
# Calculate the percentage of students who passed math and reading (% Overall Passing)
overall_passing = round((percent_reading_passing + percent_math_passing) / 2, 2)
overall_passing
287/13:
# Create a dataframe to hold the above results
summary_df = pd.DataFrame({"Total Schools": [total_schools],
                          "Total Students": [total_students],
                          "Total Budget": [total_budget],
                          "Average Math Score": [average_mathscore],
                          "Average Reading Score": [average_readingscore],
                          "% of Students with Passing Math Score": [percent_math_passing],
                          "% of Students with Passing Reading Score": [percent_reading_passing],
                          "% of Students with Passing Reading and Math Score": [overall_passing]
                          })
summary_df
287/14:
# Give the displayed data cleaner formatting
summary_df["Total Students"] = summary_df["Total Students"].map("{:,.2f}".format)
summary_df["Total Budget"] = summary_df["Total Budget"].map("{:,.2f}".format)
summary_df["Average Math Score"] = summary_df["Average Math Score"].map("{:,.2f}%".format)
summary_df["Average Reading Score"] = summary_df["Average Reading Score"].map("{:,.2f}%".format)
summary_df["% of Students with Passing Math Score"] = summary_df["% of Students with Passing Math Score"].map("{:,.2f}%".format)
summary_df["% of Students with Passing Reading Score"] = summary_df["% of Students with Passing Reading Score"].map("{:,.2f}%".format)
summary_df["% of Students with Passing Reading and Math Score"] = summary_df["% of Students with Passing Reading and Math Score"].map("{:,.2f}%".format)
summary_df
287/15:
# Create an overview table that summarizes key metrics about each school
    # School Name
    # School Type
    # Total Students
    # Total School Budget
    # Per Student Budget
    # Average Math Score
    # Average Reading Score
    # % Passing Math
    # % Passing Reading
    # % Overall Passing (The percentage of students that passed math and reading.)
    
# Create a dataframe to hold the above results
287/16: schools_overview_df = summary_df.groupby(["school_name"])
287/17:
schools_overview_group = summary_df.groupby(["school_name"])
schools_overview_group
287/18:
schools_overview_group = pycityschools_data_complete.groupby(["school_name"])
schools_overview_group.agg({"Student ID": "count"})
287/19:
schools_overview_group = pycityschools_data_complete.groupby(["school_name"])
schools_overview_group.agg({"Student ID": "count",
                            "budget": "max"})
287/20:
schools_overview_group = pycityschools_data_complete.groupby(["school_name"])
school_summary_df= schools_overview_group.agg({"Student ID": "count",
                            "budget": "max",
                           "math_score": "mean",
                            "reading_score": "mean",
                           })
school_summary_df
287/21:
schools_overview_group = pycityschools_data_complete.groupby(["school_name"])
school_summary_df = schools_overview_group.agg({"Student ID": "count",
                            "budget": "max",
                           "math_score": "mean",
                            "reading_score": "mean",
                           })
school_summary_df["Per Student Budget"] = school_summary_df["budget"] / school_summary_df["Student ID"]
287/22:
schools_overview_group = pycityschools_data_complete.groupby(["school_name"])
school_summary_df = schools_overview_group.agg({"Student ID": "count",
                            "budget": "max",
                           "math_score": "mean",
                            "reading_score": "mean",
                           })
school_summary_df["Per Student Budget"] = school_summary_df["budget"] / school_summary_df["Student ID"]
school_summary_df
287/23:
# Give the displayed data cleaner formatting
summary_df["Total Students"] = summary_df["Total Students"].map("{:,.2f}".format)
summary_df["Total Budget"] = summary_df["Total Budget"].map("${:,.2f}".format)
summary_df["Average Math Score"] = summary_df["Average Math Score"].map("{:,.2f}%".format)
summary_df["Average Reading Score"] = summary_df["Average Reading Score"].map("{:,.2f}%".format)
summary_df["% of Students with Passing Math Score"] = summary_df["% of Students with Passing Math Score"].map("{:,.2f}%".format)
summary_df["% of Students with Passing Reading Score"] = summary_df["% of Students with Passing Reading Score"].map("{:,.2f}%".format)
summary_df["% of Students with Passing Reading and Math Score"] = summary_df["% of Students with Passing Reading and Math Score"].map("{:,.2f}%".format)
summary_df
288/1:
# Dependencies
import pandas as pd
import numpy as np
288/2:
# load in file
pycityschools_file = "Resources/schools_complete.csv"
student_file = "Resources/students_complete.csv"
288/3:
# Read School and Student Data File and store into Pandas DataFrames
pycityschools_data_df = pd.read_csv(pycityschools_file)
student_data_df = pd.read_csv(student_file)

pycityschools_data_df.head()
student_data_df.head()
288/4:
pycityschools_data_complete = pd.merge(student_data_df, pycityschools_data_df, 
                                       on=["school_name", "school_name"], how="left")

pycityschools_data_complete.head()
288/5:
# Count total schools
total_schools = len(pycityschools_data_complete["School ID"].unique())
total_schools
288/6:
# Calculate the total number of students
total_students = len(pycityschools_data_complete["student_name"].unique())
total_students
288/7:
# Calculate Total Budget
total_budget = pycityschools_data_df ["budget"].sum()
total_budget
288/8:
# Calculate the average math score
average_mathscore = round(pycityschools_data_complete["math_score"].mean(), 2)
average_mathscore
288/9:
# Calculate the average reading score
average_readingscore = round(pycityschools_data_complete["reading_score"].mean(), 2)
average_readingscore
288/10:
# Calculate the percentage of students with a passing math score (70 or greater)
unique_math_passing = len(pycityschools_data_complete.loc[pycityschools_data_complete["math_score"] >=70]["student_name"].unique())
percent_math_passing = round((unique_math_passing/total_students)*100, 2)
percent_math_passing
288/11:
# Calculate the percentage of students with a passing reading score (70 or greater)
unique_reading_passing = len(pycityschools_data_complete.loc[pycityschools_data_complete["reading_score"] >=70]["student_name"].unique())
percent_reading_passing = round((unique_reading_passing/total_students)*100, 2)
percent_reading_passing
288/12:
# Calculate the percentage of students who passed math and reading (% Overall Passing)
overall_passing = round((percent_reading_passing + percent_math_passing) / 2, 2)
overall_passing
288/13:
# Create a dataframe to hold the above results
summary_df = pd.DataFrame({"Total Schools": [total_schools],
                          "Total Students": [total_students],
                          "Total Budget": [total_budget],
                          "Average Math Score": [average_mathscore],
                          "Average Reading Score": [average_readingscore],
                          "% of Students with Passing Math Score": [percent_math_passing],
                          "% of Students with Passing Reading Score": [percent_reading_passing],
                          "% of Students with Passing Reading and Math Score": [overall_passing]
                          })
summary_df
288/14:
# Give the displayed data cleaner formatting
summary_df["Total Students"] = summary_df["Total Students"].map("{:,.2f}".format)
summary_df["Total Budget"] = summary_df["Total Budget"].map("${:,.2f}".format)
summary_df["Average Math Score"] = summary_df["Average Math Score"].map("{:,.2f}%".format)
summary_df["Average Reading Score"] = summary_df["Average Reading Score"].map("{:,.2f}%".format)
summary_df["% of Students with Passing Math Score"] = summary_df["% of Students with Passing Math Score"].map("{:,.2f}%".format)
summary_df["% of Students with Passing Reading Score"] = summary_df["% of Students with Passing Reading Score"].map("{:,.2f}%".format)
summary_df["% of Students with Passing Reading and Math Score"] = summary_df["% of Students with Passing Reading and Math Score"].map("{:,.2f}%".format)
summary_df
288/15:
# Create an overview table that summarizes key metrics about each school
    # School Name
    # School Type
    # Total Students
    # Total School Budget
    # Per Student Budget
    # Average Math Score
    # Average Reading Score
    # % Passing Math
    # % Passing Reading
    # % Overall Passing (The percentage of students that passed math and reading.)
    
# Create a dataframe to hold the above results
288/16:
schools_overview_group = pycityschools_data_complete.groupby(["school_name"])
school_summary_df = schools_overview_group.agg({"Student ID": "count",
                            "budget": "max",
                           "math_score": "mean",
                            "reading_score": "mean",
                           })
school_summary_df["Per Student Budget"] = school_summary_df["budget"] / school_summary_df["Student ID"]
school_summary_df
288/17:
schools_overview_group = pycityschools_data_complete.groupby(["school_name"])
school_summary_df = schools_overview_group.agg({"Student ID": "count",
                            "budget": "max",
                           "math_score": "mean",
                            "reading_score": "mean",
                           })
school_summary_df["Per Student Budget"] = school_summary_df["budget"] / school_summary_df["Student ID"]
school_summary_df["% Passing Math"] = school_summary_df[math_score] >=70 / school_summary_df["Student ID"] * 100
school_summary_df
288/18:
schools_overview_group = pycityschools_data_complete.groupby(["school_name"])
school_summary_df = schools_overview_group.agg({"Student ID": "count",
                            "budget": "max",
                           "math_score": "mean",
                            "reading_score": "mean",
                           })
school_summary_df["Per Student Budget"] = school_summary_df["budget"] / school_summary_df["Student ID"]
school_summary_df["% Passing Math"] = school_summary_df["math_score"] >=70 / school_summary_df["Student ID"] * 100
school_summary_df
288/19:
passing_math_stu = pycityschools_data_complete.loc[pycityschools_data_complete["math_score"] >=70]
passing_math_stu
288/20:
group_school = pycityschools_data_complete.groupby(["school_name"]).count()
group_school.head()
288/21:
group_school = pycityschools_data_complete.groupby(["school_name"]).count()
group_school
288/22:
group_school = pycityschools_data_complete.groupby(["school_name"]).count()["type"]
group_school
288/23:
passing_math_count = pycityschools_data_complete.groupby(["school_name"]).count()["type"]
passing_math_count
288/24:
school_summary_df["% Passing Math"] = passing_math_count / school_summary_df["Student ID"]
school_summary_df
288/25:
passing_math_count = passing_math_stu.groupby(["school_name"]).count()["math_score"]
passing_math_count
288/26:
school_summary_df["% Passing Math"] = passing_math_count / school_summary_df["Student ID"]
school_summary_df
288/27:
school_summary_df["% Passing Math"] = passing_math_count / school_summary_df["Student ID"] * 100
school_summary_df
289/1:
# Dependencies
import pandas as pd
import numpy as np
289/2:
# load in file
pycityschools_file = "Resources/schools_complete.csv"
student_file = "Resources/students_complete.csv"
289/3:
# Read School and Student Data File and store into Pandas DataFrames
pycityschools_data_df = pd.read_csv(pycityschools_file)
student_data_df = pd.read_csv(student_file)

pycityschools_data_df.head()
student_data_df.head()
289/4:
pycityschools_data_complete = pd.merge(student_data_df, pycityschools_data_df, 
                                       on=["school_name", "school_name"], how="left")

pycityschools_data_complete.head()
289/5:
# Count total schools
total_schools = len(pycityschools_data_complete["School ID"].unique())
total_schools
289/6:
# Calculate the total number of students
total_students = len(pycityschools_data_complete["student_name"].unique())
total_students
289/7:
# Calculate Total Budget
total_budget = pycityschools_data_df ["budget"].sum()
total_budget
289/8:
# Calculate the average math score
average_mathscore = round(pycityschools_data_complete["math_score"].mean(), 2)
average_mathscore
289/9:
# Calculate the average reading score
average_readingscore = round(pycityschools_data_complete["reading_score"].mean(), 2)
average_readingscore
289/10:
# Calculate the percentage of students with a passing math score (70 or greater)
unique_math_passing = len(pycityschools_data_complete.loc[pycityschools_data_complete["math_score"] >=70]["student_name"].unique())
percent_math_passing = round((unique_math_passing/total_students)*100, 2)
percent_math_passing
289/11:
# Calculate the percentage of students with a passing reading score (70 or greater)
unique_reading_passing = len(pycityschools_data_complete.loc[pycityschools_data_complete["reading_score"] >=70]["student_name"].unique())
percent_reading_passing = round((unique_reading_passing/total_students)*100, 2)
percent_reading_passing
289/12:
# Calculate the percentage of students who passed math and reading (% Overall Passing)
overall_passing = round((percent_reading_passing + percent_math_passing) / 2, 2)
overall_passing
289/13:
# Create a dataframe to hold the above results
summary_df = pd.DataFrame({"Total Schools": [total_schools],
                          "Total Students": [total_students],
                          "Total Budget": [total_budget],
                          "Average Math Score": [average_mathscore],
                          "Average Reading Score": [average_readingscore],
                          "% of Students with Passing Math Score": [percent_math_passing],
                          "% of Students with Passing Reading Score": [percent_reading_passing],
                          "% of Students with Passing Reading and Math Score": [overall_passing]
                          })
summary_df
289/14:
# Give the displayed data cleaner formatting
summary_df["Total Students"] = summary_df["Total Students"].map("{:,.2f}".format)
summary_df["Total Budget"] = summary_df["Total Budget"].map("${:,.2f}".format)
summary_df["Average Math Score"] = summary_df["Average Math Score"].map("{:,.2f}%".format)
summary_df["Average Reading Score"] = summary_df["Average Reading Score"].map("{:,.2f}%".format)
summary_df["% of Students with Passing Math Score"] = summary_df["% of Students with Passing Math Score"].map("{:,.2f}%".format)
summary_df["% of Students with Passing Reading Score"] = summary_df["% of Students with Passing Reading Score"].map("{:,.2f}%".format)
summary_df["% of Students with Passing Reading and Math Score"] = summary_df["% of Students with Passing Reading and Math Score"].map("{:,.2f}%".format)
summary_df
289/15:
# Create an overview table that summarizes key metrics about each school
    # School Name
    # School Type
    # Total Students
    # Total School Budget
    # Per Student Budget
    # Average Math Score
    # Average Reading Score
    # % Passing Math
    # % Passing Reading
    # % Overall Passing (The percentage of students that passed math and reading.)
    
# Create a dataframe to hold the above results
289/16:
schools_overview_group = pycityschools_data_complete.groupby(["school_name"])
school_summary_df = schools_overview_group.agg({"Student ID": "count",
                            "budget": "max",
                           "math_score": "mean",
                            "reading_score": "mean",
                           })
school_summary_df["Per Student Budget"] = school_summary_df["budget"] / school_summary_df["Student ID"]
school_summary_df
289/17:
passing_math_stu = pycityschools_data_complete.loc[pycityschools_data_complete["math_score"] >=70]
passing_math_stu
289/18:
passing_reading_stu = pycityschools_data_complete.loc[pycityschools_data_complete["reading_score"] >=70]
passing_reading_stu
289/19:
passing_math_count = passing_math_stu.groupby(["school_name"]).count()["math_score"]
passing_math_count
289/20:
passing_reading_count = passing_reading_stu.groupby(["school_name"]).count()["reading_score"]
passing_reading_count
289/21:
school_summary_df["% Passing Math"] = passing_math_count / school_summary_df["Student ID"] * 100
school_summary_df["% Passing Reading"] = passing_reading_count / school_summary_df["School ID"] * 100
school_summary_df
289/22:
school_summary_df["% Passing Math"] = passing_math_count / school_summary_df["Student ID"] * 100
school_summary_df["% Passing Reading"] = passing_reading_count / school_summary_df["Student ID"] * 100
school_summary_df
289/23:
passing_overall_stu = pycityschools_data_complete.loc[(pycityschools_data_complete["math_score"] >=70)&
                                                  (pycityschools_data_complete["reading_score"] >=70)]
passing_overall_stu
289/24:
passing_reading_stu = pycityschools_data_complete.loc[pycityschools_data_complete["reading_score"] >=70]
passing_reading_stu
289/25:
passing_math_count = passing_math_stu.groupby(["school_name"]).count()["math_score"]
passing_math_count
289/26:
passing_reading_count = passing_reading_stu.groupby(["school_name"]).count()["reading_score"]
passing_reading_count
289/27:
school_summary_df["% Passing Math"] = passing_math_count / school_summary_df["Student ID"] * 100
school_summary_df["% Passing Reading"] = passing_reading_count / school_summary_df["Student ID"] * 100
school_summary_df
290/1:
passing_overall_count = passing_overall_stu.groupby(["school_name"]).count()["reading_score", "math_score"]
passing_overall_count
291/1:
# Dependencies
import pandas as pd
import numpy as np
291/2:
# load in file
pycityschools_file = "Resources/schools_complete.csv"
student_file = "Resources/students_complete.csv"
291/3:
# Read School and Student Data File and store into Pandas DataFrames
pycityschools_data_df = pd.read_csv(pycityschools_file)
student_data_df = pd.read_csv(student_file)

pycityschools_data_df.head()
student_data_df.head()
291/4:
pycityschools_data_complete = pd.merge(student_data_df, pycityschools_data_df, 
                                       on=["school_name", "school_name"], how="left")

pycityschools_data_complete.head()
291/5:
# Count total schools
total_schools = len(pycityschools_data_complete["School ID"].unique())
total_schools
291/6:
# Calculate the total number of students
total_students = len(pycityschools_data_complete["student_name"].unique())
total_students
291/7:
# Calculate Total Budget
total_budget = pycityschools_data_df ["budget"].sum()
total_budget
291/8:
# Calculate the average math score
average_mathscore = round(pycityschools_data_complete["math_score"].mean(), 2)
average_mathscore
291/9:
# Calculate the average reading score
average_readingscore = round(pycityschools_data_complete["reading_score"].mean(), 2)
average_readingscore
291/10:
# Calculate the percentage of students with a passing math score (70 or greater)
unique_math_passing = len(pycityschools_data_complete.loc[pycityschools_data_complete["math_score"] >=70]["student_name"].unique())
percent_math_passing = round((unique_math_passing/total_students)*100, 2)
percent_math_passing
291/11:
# Calculate the percentage of students with a passing reading score (70 or greater)
unique_reading_passing = len(pycityschools_data_complete.loc[pycityschools_data_complete["reading_score"] >=70]["student_name"].unique())
percent_reading_passing = round((unique_reading_passing/total_students)*100, 2)
percent_reading_passing
291/12:
# Calculate the percentage of students who passed math and reading (% Overall Passing)
overall_passing = round((percent_reading_passing + percent_math_passing) / 2, 2)
overall_passing
291/13:
# Create a dataframe to hold the above results
summary_df = pd.DataFrame({"Total Schools": [total_schools],
                          "Total Students": [total_students],
                          "Total Budget": [total_budget],
                          "Average Math Score": [average_mathscore],
                          "Average Reading Score": [average_readingscore],
                          "% of Students with Passing Math Score": [percent_math_passing],
                          "% of Students with Passing Reading Score": [percent_reading_passing],
                          "% of Students with Passing Reading and Math Score": [overall_passing]
                          })
summary_df
291/14:
# Give the displayed data cleaner formatting
summary_df["Total Students"] = summary_df["Total Students"].map("{:,.2f}".format)
summary_df["Total Budget"] = summary_df["Total Budget"].map("${:,.2f}".format)
summary_df["Average Math Score"] = summary_df["Average Math Score"].map("{:,.2f}%".format)
summary_df["Average Reading Score"] = summary_df["Average Reading Score"].map("{:,.2f}%".format)
summary_df["% of Students with Passing Math Score"] = summary_df["% of Students with Passing Math Score"].map("{:,.2f}%".format)
summary_df["% of Students with Passing Reading Score"] = summary_df["% of Students with Passing Reading Score"].map("{:,.2f}%".format)
summary_df["% of Students with Passing Reading and Math Score"] = summary_df["% of Students with Passing Reading and Math Score"].map("{:,.2f}%".format)
summary_df
291/15:
# Create an overview table that summarizes key metrics about each school
    # School Name
    # School Type
    # Total Students
    # Total School Budget
    # Per Student Budget
    # Average Math Score
    # Average Reading Score
    # % Passing Math
    # % Passing Reading
    # % Overall Passing (The percentage of students that passed math and reading.)
    
# Create a dataframe to hold the above results
291/16:
schools_overview_group = pycityschools_data_complete.groupby(["school_name"])
school_summary_df = schools_overview_group.agg({"Student ID": "count",
                            "budget": "max",
                           "math_score": "mean",
                            "reading_score": "mean",
                           })
school_summary_df["Per Student Budget"] = school_summary_df["budget"] / school_summary_df["Student ID"]
school_summary_df
291/17:
passing_math_stu = pycityschools_data_complete.loc[pycityschools_data_complete["math_score"] >=70]
passing_math_stu
291/18:
passing_overall_stu = pycityschools_data_complete.loc[(pycityschools_data_complete["math_score"] >=70)&
                                                  (pycityschools_data_complete["reading_score"] >=70)]
passing_overall_stu
291/19:
passing_reading_stu = pycityschools_data_complete.loc[pycityschools_data_complete["reading_score"] >=70]
passing_reading_stu
291/20:
passing_math_count = passing_math_stu.groupby(["school_name"]).count()["math_score"]
passing_math_count
291/21:
passing_reading_count = passing_reading_stu.groupby(["school_name"]).count()["reading_score"]
passing_reading_count
291/22:
passing_overall_count = passing_overall_stu.groupby(["school_name"]).count()["reading_score", "math_score"]
passing_overall_count
291/23:
passing_overall_count = passing_overall_stu.groupby(["school_name"]).count()["reading_score"]
passing_overall_count
292/1:
# Dependencies
import pandas as pd
import numpy as np
292/2:
# load in file
pycityschools_file = "Resources/schools_complete.csv"
student_file = "Resources/students_complete.csv"
292/3:
# Read School and Student Data File and store into Pandas DataFrames
pycityschools_data_df = pd.read_csv(pycityschools_file)
student_data_df = pd.read_csv(student_file)

pycityschools_data_df.head()
student_data_df.head()
292/4:
pycityschools_data_complete = pd.merge(student_data_df, pycityschools_data_df, 
                                       on=["school_name", "school_name"], how="left")

pycityschools_data_complete.head()
292/5:
# Count total schools
total_schools = len(pycityschools_data_complete["School ID"].unique())
total_schools
292/6:
# Calculate the total number of students
total_students = len(pycityschools_data_complete["student_name"].unique())
total_students
292/7:
# Calculate Total Budget
total_budget = pycityschools_data_df ["budget"].sum()
total_budget
292/8:
# Calculate the average math score
average_mathscore = round(pycityschools_data_complete["math_score"].mean(), 2)
average_mathscore
292/9:
# Calculate the average reading score
average_readingscore = round(pycityschools_data_complete["reading_score"].mean(), 2)
average_readingscore
292/10:
# Calculate the percentage of students with a passing math score (70 or greater)
unique_math_passing = len(pycityschools_data_complete.loc[pycityschools_data_complete["math_score"] >=70]["student_name"].unique())
percent_math_passing = round((unique_math_passing/total_students)*100, 2)
percent_math_passing
292/11:
# Calculate the percentage of students with a passing reading score (70 or greater)
unique_reading_passing = len(pycityschools_data_complete.loc[pycityschools_data_complete["reading_score"] >=70]["student_name"].unique())
percent_reading_passing = round((unique_reading_passing/total_students)*100, 2)
percent_reading_passing
292/12:
# Calculate the percentage of students who passed math and reading (% Overall Passing)
overall_passing = round((percent_reading_passing + percent_math_passing) / 2, 2)
overall_passing
292/13:
# Create a dataframe to hold the above results
summary_df = pd.DataFrame({"Total Schools": [total_schools],
                          "Total Students": [total_students],
                          "Total Budget": [total_budget],
                          "Average Math Score": [average_mathscore],
                          "Average Reading Score": [average_readingscore],
                          "% of Students with Passing Math Score": [percent_math_passing],
                          "% of Students with Passing Reading Score": [percent_reading_passing],
                          "% of Students with Passing Reading and Math Score": [overall_passing]
                          })
summary_df
292/14:
# Give the displayed data cleaner formatting
summary_df["Total Students"] = summary_df["Total Students"].map("{:,.2f}".format)
summary_df["Total Budget"] = summary_df["Total Budget"].map("${:,.2f}".format)
summary_df["Average Math Score"] = summary_df["Average Math Score"].map("{:,.2f}%".format)
summary_df["Average Reading Score"] = summary_df["Average Reading Score"].map("{:,.2f}%".format)
summary_df["% of Students with Passing Math Score"] = summary_df["% of Students with Passing Math Score"].map("{:,.2f}%".format)
summary_df["% of Students with Passing Reading Score"] = summary_df["% of Students with Passing Reading Score"].map("{:,.2f}%".format)
summary_df["% of Students with Passing Reading and Math Score"] = summary_df["% of Students with Passing Reading and Math Score"].map("{:,.2f}%".format)
summary_df
292/15:
# Create an overview table that summarizes key metrics about each school
    # School Name
    # School Type
    # Total Students
    # Total School Budget
    # Per Student Budget
    # Average Math Score
    # Average Reading Score
    # % Passing Math
    # % Passing Reading
    # % Overall Passing (The percentage of students that passed math and reading.)
    
# Create a dataframe to hold the above results
292/16:
schools_overview_group = pycityschools_data_complete.groupby(["school_name"])
school_summary_df = schools_overview_group.agg({"Student ID": "count",
                            "budget": "max",
                           "math_score": "mean",
                            "reading_score": "mean",
                           })
school_summary_df["Per Student Budget"] = school_summary_df["budget"] / school_summary_df["Student ID"]
school_summary_df
292/17:
passing_math_stu = pycityschools_data_complete.loc[pycityschools_data_complete["math_score"] >=70]
passing_math_stu
292/18:
passing_overall_stu = pycityschools_data_complete.loc[(pycityschools_data_complete["math_score"] >=70)&
                                                  (pycityschools_data_complete["reading_score"] >=70)]
passing_overall_stu
292/19:
passing_reading_stu = pycityschools_data_complete.loc[pycityschools_data_complete["reading_score"] >=70]
passing_reading_stu
292/20:
passing_math_count = passing_math_stu.groupby(["school_name"]).count()["math_score"]
passing_math_count
292/21:
passing_reading_count = passing_reading_stu.groupby(["school_name"]).count()["reading_score"]
passing_reading_count
292/22:
passing_overall_count = passing_overall_stu.groupby(["school_name"]).count()["reading_score"]
passing_overall_count
292/23:
school_summary_df["% Passing Math"] = passing_math_count / school_summary_df["Student ID"] * 100
school_summary_df["% Passing Reading"] = passing_reading_count / school_summary_df["Student ID"] * 100
school_summary_df["% Passing Overall"] = passing_overall_count / school_summary_df["Student ID"] * 100
school_summary_df
292/24: school_summary_df = school_summary_df.rename(columns={"budget": "Total School Budget",}
292/25:
school_summary = pycityschools_data_complete.groupby(['school_name'])
school_summary
293/1:
# Dependencies
import pandas as pd
import numpy as np
293/2:
# load in file
pycityschools_file = "Resources/schools_complete.csv"
student_file = "Resources/students_complete.csv"
293/3:
# Read School and Student Data File and store into Pandas DataFrames
pycityschools_data_df = pd.read_csv(pycityschools_file)
student_data_df = pd.read_csv(student_file)

pycityschools_data_df.head()
student_data_df.head()
293/4:
pycityschools_data_complete = pd.merge(student_data_df, pycityschools_data_df, 
                                       on=["school_name", "school_name"], how="left")

pycityschools_data_complete.head()
293/5:
# Count total schools
total_schools = len(pycityschools_data_complete["School ID"].unique())
total_schools
293/6:
# Calculate the total number of students
total_students = len(pycityschools_data_complete["student_name"].unique())
total_students
293/7:
# Calculate Total Budget
total_budget = pycityschools_data_df ["budget"].sum()
total_budget
293/8:
# Calculate the average math score
average_mathscore = round(pycityschools_data_complete["math_score"].mean(), 2)
average_mathscore
293/9:
# Calculate the average reading score
average_readingscore = round(pycityschools_data_complete["reading_score"].mean(), 2)
average_readingscore
293/10:
# Calculate the percentage of students with a passing math score (70 or greater)
unique_math_passing = len(pycityschools_data_complete.loc[pycityschools_data_complete["math_score"] >=70]["student_name"].unique())
percent_math_passing = round((unique_math_passing/total_students)*100, 2)
percent_math_passing
293/11:
# Calculate the percentage of students with a passing reading score (70 or greater)
unique_reading_passing = len(pycityschools_data_complete.loc[pycityschools_data_complete["reading_score"] >=70]["student_name"].unique())
percent_reading_passing = round((unique_reading_passing/total_students)*100, 2)
percent_reading_passing
293/12:
# Calculate the percentage of students who passed math and reading (% Overall Passing)
overall_passing = round((percent_reading_passing + percent_math_passing) / 2, 2)
overall_passing
293/13:
# Create a dataframe to hold the above results
summary_df = pd.DataFrame({"Total Schools": [total_schools],
                          "Total Students": [total_students],
                          "Total Budget": [total_budget],
                          "Average Math Score": [average_mathscore],
                          "Average Reading Score": [average_readingscore],
                          "% of Students with Passing Math Score": [percent_math_passing],
                          "% of Students with Passing Reading Score": [percent_reading_passing],
                          "% of Students with Passing Reading and Math Score": [overall_passing]
                          })
summary_df
293/14:
# Give the displayed data cleaner formatting
summary_df["Total Students"] = summary_df["Total Students"].map("{:,.2f}".format)
summary_df["Total Budget"] = summary_df["Total Budget"].map("${:,.2f}".format)
summary_df["Average Math Score"] = summary_df["Average Math Score"].map("{:,.2f}%".format)
summary_df["Average Reading Score"] = summary_df["Average Reading Score"].map("{:,.2f}%".format)
summary_df["% of Students with Passing Math Score"] = summary_df["% of Students with Passing Math Score"].map("{:,.2f}%".format)
summary_df["% of Students with Passing Reading Score"] = summary_df["% of Students with Passing Reading Score"].map("{:,.2f}%".format)
summary_df["% of Students with Passing Reading and Math Score"] = summary_df["% of Students with Passing Reading and Math Score"].map("{:,.2f}%".format)
summary_df
293/15:
# Create an overview table that summarizes key metrics about each school
    # School Name
    # School Type
    # Total Students
    # Total School Budget
    # Per Student Budget
    # Average Math Score
    # Average Reading Score
    # % Passing Math
    # % Passing Reading
    # % Overall Passing (The percentage of students that passed math and reading.)
    
# Create a dataframe to hold the above results
293/16:
school_summary = pycityschools_data_complete.groupby(['school_name'])
school_summary
293/17:
passing_math_stu = pycityschools_data_complete.loc[pycityschools_data_complete["math_score"] >=70]
passing_math_stu
293/18:
passing_overall_stu = pycityschools_data_complete.loc[(pycityschools_data_complete["math_score"] >=70)&
                                                  (pycityschools_data_complete["reading_score"] >=70)]
passing_overall_stu
293/19:
passing_reading_stu = pycityschools_data_complete.loc[pycityschools_data_complete["reading_score"] >=70]
passing_reading_stu
293/20:
passing_math_count = passing_math_stu.groupby(["school_name"]).count()["math_score"]
passing_math_count
293/21:
passing_reading_count = passing_reading_stu.groupby(["school_name"]).count()["reading_score"]
passing_reading_count
293/22:
passing_overall_count = passing_overall_stu.groupby(["school_name"]).count()["reading_score"]
passing_overall_count
293/23:
school_summary_df["% Passing Math"] = passing_math_count / school_summary_df["Student ID"] * 100
school_summary_df["% Passing Reading"] = passing_reading_count / school_summary_df["Student ID"] * 100
school_summary_df["% Passing Overall"] = passing_overall_count / school_summary_df["Student ID"] * 100
school_summary_df
293/24:
school_type = pycityschools_data_complete.set_index('school_name')['type']
school_type
293/25:
school_budget_total = pycityschools_data_complete.set_index('school_name')['budget']
school_budget_total
293/26:
# Per Student Budget
per_student_budget = pycityschools_data_complete.set_index('school_name')['budget'] / pycityschools_data_complete.set_index('school_name')['size']
per_student_budget
293/27:
# Total Students Per School
total_students = pycityschools_data_complete('Student ID').count()
total_students
293/28:
# Total Students Per School
total_students = summary_df('Student ID').count()
total_students
294/1:
# Dependencies
import pandas as pd
import numpy as np
294/2:
# load in file
pycityschools_file = "Resources/schools_complete.csv"
student_file = "Resources/students_complete.csv"
294/3:
# Read School and Student Data File and store into Pandas DataFrames
pycityschools_data_df = pd.read_csv(pycityschools_file)
student_data_df = pd.read_csv(student_file)

pycityschools_data_df.head()
student_data_df.head()
294/4:
pycityschools_data_complete = pd.merge(student_data_df, pycityschools_data_df, 
                                       on=["school_name", "school_name"], how="left")

pycityschools_data_complete.head()
294/5:
# Count total schools
total_schools = len(pycityschools_data_complete["School ID"].unique())
total_schools
294/6:
# Calculate the total number of students
total_students = len(pycityschools_data_complete["student_name"].unique())
total_students
294/7:
# Calculate Total Budget
total_budget = pycityschools_data_df ["budget"].sum()
total_budget
294/8:
# Calculate the average math score
average_mathscore = round(pycityschools_data_complete["math_score"].mean(), 2)
average_mathscore
294/9:
# Calculate the average reading score
average_readingscore = round(pycityschools_data_complete["reading_score"].mean(), 2)
average_readingscore
294/10:
# Calculate the percentage of students with a passing math score (70 or greater)
unique_math_passing = len(pycityschools_data_complete.loc[pycityschools_data_complete["math_score"] >=70]["student_name"].unique())
percent_math_passing = round((unique_math_passing/total_students)*100, 2)
percent_math_passing
294/11:
# Calculate the percentage of students with a passing reading score (70 or greater)
unique_reading_passing = len(pycityschools_data_complete.loc[pycityschools_data_complete["reading_score"] >=70]["student_name"].unique())
percent_reading_passing = round((unique_reading_passing/total_students)*100, 2)
percent_reading_passing
294/12:
# Calculate the percentage of students who passed math and reading (% Overall Passing)
overall_passing = round((percent_reading_passing + percent_math_passing) / 2, 2)
overall_passing
294/13:
# Create a dataframe to hold the above results
summary_df = pd.DataFrame({"Total Schools": [total_schools],
                          "Total Students": [total_students],
                          "Total Budget": [total_budget],
                          "Average Math Score": [average_mathscore],
                          "Average Reading Score": [average_readingscore],
                          "% of Students with Passing Math Score": [percent_math_passing],
                          "% of Students with Passing Reading Score": [percent_reading_passing],
                          "% of Students with Passing Reading and Math Score": [overall_passing]
                          })
summary_df
294/14:
# Give the displayed data cleaner formatting
summary_df["Total Students"] = summary_df["Total Students"].map("{:,.2f}".format)
summary_df["Total Budget"] = summary_df["Total Budget"].map("${:,.2f}".format)
summary_df["Average Math Score"] = summary_df["Average Math Score"].map("{:,.2f}%".format)
summary_df["Average Reading Score"] = summary_df["Average Reading Score"].map("{:,.2f}%".format)
summary_df["% of Students with Passing Math Score"] = summary_df["% of Students with Passing Math Score"].map("{:,.2f}%".format)
summary_df["% of Students with Passing Reading Score"] = summary_df["% of Students with Passing Reading Score"].map("{:,.2f}%".format)
summary_df["% of Students with Passing Reading and Math Score"] = summary_df["% of Students with Passing Reading and Math Score"].map("{:,.2f}%".format)
summary_df
294/15:
# Create an overview table that summarizes key metrics about each school
    # Total Students
    # Average Math Score
    # Average Reading Score
    # % Passing Math
    # % Passing Reading
    # % Overall Passing (The percentage of students that passed math and reading.)
    
# Create a dataframe to hold the above results
294/16:
# School Name
school_name = pycityschools_data_complete.groupby(['school_name'])
school_name
294/17:
# School Type
school_type = pycityschools_data_complete.set_index('school_name')['type']
school_type
294/18:
# Per School Budget 
school_budget_total = pycityschools_data_complete.set_index('school_name')['budget']
school_budget_total
294/19:
# Per Student Budget
per_student_budget = pycityschools_data_complete.set_index('school_name')['budget'] / pycityschools_data_complete.set_index('school_name')['size']
per_student_budget
294/20:
# Total Students Per School
total_students = summary_df('Student ID').count()
total_students
294/21:
# Total Students Per School
total_students = student_data_df('Student ID').count()
total_students
294/22:
# Total Students Per School
total_students = school_name('Student ID').count()
total_students
295/1:
# Dependencies
import pandas as pd
import numpy as np
295/2:
# load in file
pycityschools_file = "Resources/schools_complete.csv"
student_file = "Resources/students_complete.csv"
295/3:
# Read School and Student Data File and store into Pandas DataFrames
pycityschools_data_df = pd.read_csv(pycityschools_file)
student_data_df = pd.read_csv(student_file)

pycityschools_data_df.head()
student_data_df.head()
295/4:
pycityschools_data_complete = pd.merge(student_data_df, pycityschools_data_df, 
                                       on=["school_name", "school_name"], how="left")

pycityschools_data_complete.head()
295/5:
# Count total schools
total_schools = len(pycityschools_data_complete["School ID"].unique())
total_schools
295/6:
# Calculate the total number of students
total_students = len(pycityschools_data_complete["student_name"].unique())
total_students
295/7:
# Calculate Total Budget
total_budget = pycityschools_data_df ["budget"].sum()
total_budget
295/8:
# Calculate the average math score
average_mathscore = round(pycityschools_data_complete["math_score"].mean(), 2)
average_mathscore
295/9:
# Calculate the average reading score
average_readingscore = round(pycityschools_data_complete["reading_score"].mean(), 2)
average_readingscore
295/10:
# Calculate the percentage of students with a passing math score (70 or greater)
unique_math_passing = len(pycityschools_data_complete.loc[pycityschools_data_complete["math_score"] >=70]["student_name"].unique())
percent_math_passing = round((unique_math_passing/total_students)*100, 2)
percent_math_passing
295/11:
# Calculate the percentage of students with a passing reading score (70 or greater)
unique_reading_passing = len(pycityschools_data_complete.loc[pycityschools_data_complete["reading_score"] >=70]["student_name"].unique())
percent_reading_passing = round((unique_reading_passing/total_students)*100, 2)
percent_reading_passing
295/12:
# Calculate the percentage of students who passed math and reading (% Overall Passing)
overall_passing = round((percent_reading_passing + percent_math_passing) / 2, 2)
overall_passing
295/13:
# Create a dataframe to hold the above results
summary_df = pd.DataFrame({"Total Schools": [total_schools],
                          "Total Students": [total_students],
                          "Total Budget": [total_budget],
                          "Average Math Score": [average_mathscore],
                          "Average Reading Score": [average_readingscore],
                          "% of Students with Passing Math Score": [percent_math_passing],
                          "% of Students with Passing Reading Score": [percent_reading_passing],
                          "% of Students with Passing Reading and Math Score": [overall_passing]
                          })
summary_df
295/14:
# Give the displayed data cleaner formatting
summary_df["Total Students"] = summary_df["Total Students"].map("{:,.2f}".format)
summary_df["Total Budget"] = summary_df["Total Budget"].map("${:,.2f}".format)
summary_df["Average Math Score"] = summary_df["Average Math Score"].map("{:,.2f}%".format)
summary_df["Average Reading Score"] = summary_df["Average Reading Score"].map("{:,.2f}%".format)
summary_df["% of Students with Passing Math Score"] = summary_df["% of Students with Passing Math Score"].map("{:,.2f}%".format)
summary_df["% of Students with Passing Reading Score"] = summary_df["% of Students with Passing Reading Score"].map("{:,.2f}%".format)
summary_df["% of Students with Passing Reading and Math Score"] = summary_df["% of Students with Passing Reading and Math Score"].map("{:,.2f}%".format)
summary_df
295/15:
# Create an overview table that summarizes key metrics about each school
    # Total Students
    # Average Math Score
    # Average Reading Score
    # % Passing Math
    # % Passing Reading
    # % Overall Passing (The percentage of students that passed math and reading.)
    
# Create a dataframe to hold the above results
295/16:
# School Name
school_name = pycityschools_data_complete.groupby(['school_name'])
school_name
295/17:
# School Type
school_type = pycityschools_data_complete.set_index('school_name')['type']
school_type
295/18:
# Per School Budget 
school_budget_total = pycityschools_data_complete.set_index('school_name')['budget']
school_budget_total
295/19:
# Per Student Budget
per_student_budget = pycityschools_data_complete.set_index('school_name')['budget'] / pycityschools_data_complete.set_index('school_name')['size']
per_student_budget
295/20:
# Total Students Per School
total_students = school_name('Student ID').count()
total_students
296/1:
# Dependencies
import pandas as pd
import numpy as np
296/2:
# load in file
pycityschools_file = "Resources/schools_complete.csv"
student_file = "Resources/students_complete.csv"
296/3:
# Read School and Student Data File and store into Pandas DataFrames
pycityschools_data_df = pd.read_csv(pycityschools_file)
student_data_df = pd.read_csv(student_file)

pycityschools_data_df.head()
student_data_df.head()
296/4:
pycityschools_data_complete = pd.merge(student_data_df, pycityschools_data_df, 
                                       on=["school_name", "school_name"], how="left")

pycityschools_data_complete.head()
296/5:
# Count total schools
total_schools = len(pycityschools_data_complete["School ID"].unique())
total_schools
296/6:
# Calculate the total number of students
total_students = len(pycityschools_data_complete["student_name"].unique())
total_students
296/7:
# Calculate Total Budget
total_budget = pycityschools_data_df ["budget"].sum()
total_budget
296/8:
# Calculate the average math score
average_mathscore = round(pycityschools_data_complete["math_score"].mean(), 2)
average_mathscore
296/9:
# Calculate the average reading score
average_readingscore = round(pycityschools_data_complete["reading_score"].mean(), 2)
average_readingscore
296/10:
# Calculate the percentage of students with a passing math score (70 or greater)
unique_math_passing = len(pycityschools_data_complete.loc[pycityschools_data_complete["math_score"] >=70]["student_name"].unique())
percent_math_passing = round((unique_math_passing/total_students)*100, 2)
percent_math_passing
296/11:
# Calculate the percentage of students with a passing reading score (70 or greater)
unique_reading_passing = len(pycityschools_data_complete.loc[pycityschools_data_complete["reading_score"] >=70]["student_name"].unique())
percent_reading_passing = round((unique_reading_passing/total_students)*100, 2)
percent_reading_passing
296/12:
# Calculate the percentage of students who passed math and reading (% Overall Passing)
overall_passing = round((percent_reading_passing + percent_math_passing) / 2, 2)
overall_passing
296/13:
# Create a dataframe to hold the above results
summary_df = pd.DataFrame({"Total Schools": [total_schools],
                          "Total Students": [total_students],
                          "Total Budget": [total_budget],
                          "Average Math Score": [average_mathscore],
                          "Average Reading Score": [average_readingscore],
                          "% of Students with Passing Math Score": [percent_math_passing],
                          "% of Students with Passing Reading Score": [percent_reading_passing],
                          "% of Students with Passing Reading and Math Score": [overall_passing]
                          })
summary_df
296/14:
# Give the displayed data cleaner formatting
summary_df["Total Students"] = summary_df["Total Students"].map("{:,.2f}".format)
summary_df["Total Budget"] = summary_df["Total Budget"].map("${:,.2f}".format)
summary_df["Average Math Score"] = summary_df["Average Math Score"].map("{:,.2f}%".format)
summary_df["Average Reading Score"] = summary_df["Average Reading Score"].map("{:,.2f}%".format)
summary_df["% of Students with Passing Math Score"] = summary_df["% of Students with Passing Math Score"].map("{:,.2f}%".format)
summary_df["% of Students with Passing Reading Score"] = summary_df["% of Students with Passing Reading Score"].map("{:,.2f}%".format)
summary_df["% of Students with Passing Reading and Math Score"] = summary_df["% of Students with Passing Reading and Math Score"].map("{:,.2f}%".format)
summary_df
296/15:
# Create an overview table that summarizes key metrics about each school
    # School Name
# School Type
# Total Students
# Total School Budget
# Per Student Budget
# Average Math Score
# Average Reading Score
# % Passing Math (The percentage of students that passed math.)
# % Passing Reading (The percentage of students that passed reading.)
# % Overall Passing (The percentage of students that passed math and reading.)
    
# Create a dataframe to hold the above results
296/16:
school_summary_df = pycityschools_data_complete.group_by(["school_name"]).agg({"budget": "max",
                                                                              "size": "count",
                                                                              })
school_summary_df
296/17:
school_summary_df = pycityschools_data_complete.group_by(["school_name"]).agg({"budget": ["max"],
                                                                              "size": ["count"],
                                                                              })
school_summary_df
296/18: %history
296/19: %history
296/20: %history -g -f PyCitySchools
296/21: %history -g -f PyCitySchools
296/22: % history -g -f filename
296/23: %history -g -f filename
296/24: %history -g -f filename
296/25: %history -g -f filename
296/26: %history -g -f PyCitySchools
297/1:
# Dependencies
import pandas as pd
import numpy as np
297/2:
# load in file
pycityschools_file = "Resources/schools_complete.csv"
student_file = "Resources/students_complete.csv"
297/3:
# Read School and Student Data File and store into Pandas DataFrames
pycityschools_data_df = pd.read_csv(pycityschools_file)
student_data_df = pd.read_csv(student_file)

pycityschools_data_df.head()
student_data_df.head()
297/4:
pycityschools_data_complete = pd.merge(student_data_df, pycityschools_data_df, 
                                       on=["school_name", "school_name"], how="left")

pycityschools_data_complete.head()
297/5:
# Count total schools
total_schools = len(pycityschools_data_complete["School ID"].unique())
total_schools
297/6:
# Calculate the total number of students
total_students = len(pycityschools_data_complete["student_name"].unique())
total_students
297/7:
# Calculate Total Budget
total_budget = pycityschools_data_df ["budget"].sum()
total_budget
297/8:
# Calculate the average math score
average_mathscore = round(pycityschools_data_complete["math_score"].mean(), 2)
average_mathscore
297/9:
# Calculate the average reading score
average_readingscore = round(pycityschools_data_complete["reading_score"].mean(), 2)
average_readingscore
297/10:
# Calculate the percentage of students with a passing math score (70 or greater)
unique_math_passing = len(pycityschools_data_complete.loc[pycityschools_data_complete["math_score"] >=70]["student_name"].unique())
percent_math_passing = round((unique_math_passing/total_students)*100, 2)
percent_math_passing
297/11:
# Calculate the percentage of students with a passing reading score (70 or greater)
unique_reading_passing = len(pycityschools_data_complete.loc[pycityschools_data_complete["reading_score"] >=70]["student_name"].unique())
percent_reading_passing = round((unique_reading_passing/total_students)*100, 2)
percent_reading_passing
297/12:
# Calculate the percentage of students who passed math and reading (% Overall Passing)
overall_passing = round((percent_reading_passing + percent_math_passing) / 2, 2)
overall_passing
297/13:
# Create a dataframe to hold the above results
summary_df = pd.DataFrame({"Total Schools": [total_schools],
                          "Total Students": [total_students],
                          "Total Budget": [total_budget],
                          "Average Math Score": [average_mathscore],
                          "Average Reading Score": [average_readingscore],
                          "% of Students with Passing Math Score": [percent_math_passing],
                          "% of Students with Passing Reading Score": [percent_reading_passing],
                          "% of Students with Passing Reading and Math Score": [overall_passing]
                          })
summary_df
297/14:
# Give the displayed data cleaner formatting
summary_df["Total Students"] = summary_df["Total Students"].map("{:,.2f}".format)
summary_df["Total Budget"] = summary_df["Total Budget"].map("${:,.2f}".format)
summary_df["Average Math Score"] = summary_df["Average Math Score"].map("{:,.2f}%".format)
summary_df["Average Reading Score"] = summary_df["Average Reading Score"].map("{:,.2f}%".format)
summary_df["% of Students with Passing Math Score"] = summary_df["% of Students with Passing Math Score"].map("{:,.2f}%".format)
summary_df["% of Students with Passing Reading Score"] = summary_df["% of Students with Passing Reading Score"].map("{:,.2f}%".format)
summary_df["% of Students with Passing Reading and Math Score"] = summary_df["% of Students with Passing Reading and Math Score"].map("{:,.2f}%".format)
summary_df
297/15:
# Create an overview table that summarizes key metrics about each school
    # School Name
# School Type
# Total Students
# Total School Budget
# Per Student Budget
# Average Math Score
# Average Reading Score
# % Passing Math (The percentage of students that passed math.)
# % Passing Reading (The percentage of students that passed reading.)
# % Overall Passing (The percentage of students that passed math and reading.)
    
# Create a dataframe to hold the above results
297/16:
school_summary_df = pycityschools_data_complete.group_by(["school_name"]).agg({"budget": ["max"],
                                                                              "size": ["count"],
                                                                              })
school_summary_df
297/17: %history -g -f PyCitySchools
297/18: %history -g -f PyCitySchools
297/19:
schools_overview_group = pycityschools_data_complete.groupby(["school_name"])
school_summary_df = schools_overview_group.agg({"Student ID": "count",
                            "budget": "max",
                           "math_score": "mean",
                            "reading_score": "mean",
                           })
school_summary_df
297/20:
# Passing Math Score
passing_math_stu = pycityschools_data_complete.loc[pycityschools_data_complete["math_score"] >=70]
passing_math_stu
297/21:
passing_overall_stu = pycityschools_data_complete.loc[(pycityschools_data_complete["math_score"] >=70)&
                                                  (pycityschools_data_complete["reading_score"] >=70)]
passing_overall_stu
297/22:
passing_reading_stu = pycityschools_data_complete.loc[pycityschools_data_complete["reading_score"] >=70]
passing_reading_stu
297/23:
passing_math_count = passing_math_stu.groupby(["school_name"]).count()["math_score"]
passing_math_count
297/24:
passing_reading_count = passing_reading_stu.groupby(["school_name"]).count()["reading_score"]
passing_reading_count
297/25:
passing_overall_count = passing_overall_stu.groupby(["school_name"]).count()["reading_score"]
passing_overall_count
297/26:
school_summary_df["% Passing Math"] = passing_math_count / school_summary_df["Student ID"] * 100
school_summary_df["% Passing Reading"] = passing_reading_count / school_summary_df["Student ID"] * 100
school_summary_df["% Passing Overall"] = passing_overall_count / school_summary_df["Student ID"] * 100
school_summary_df
297/27:
school_summary_df = school_summary_df.rename(columns={"budget": "Total School Budget",
                                                     "school_name": "School Name"})
297/28:
school_summary_df = school_summary_df.rename(columns={"budget": "Total School Budget",
                                                     "school_name": "School Name"})
school_summary_df
297/29:
school_summary_df = school_summary_df.rename(columns={"budget": "Total School Budget",
                                                     "school_name": "School Name"
                                                     })
school_summary_df
297/30:
school_summary_df = school_summary_df.rename(columns={"budget": "Total School Budget",
                                                     "math_score": "Average Math Score",
                                                      "reading_score": "Average Reading Score",
                                                     })
school_summary_df
297/31:
school_summary_df = school_summary_df.rename(columns={"budget": "Total School Budget",
                                                      "Student ID": "Total Students"
                                                     "math_score": "Average Math Score",
                                                      "reading_score": "Average Reading Score",
                                                     })
school_summary_df
297/32:
school_summary_df = school_summary_df.rename(columns={"budget": "Total School Budget",
                                                      "Student ID": "Total Students",
                                                     "math_score": "Average Math Score",
                                                      "reading_score": "Average Reading Score",
                                                     })
school_summary_df
297/33:
school_summary_df = school_summary_df.rename(columns={"budget": "Total School Budget",
                                                      "Student ID": "Total Students",
                                                     "math_score": "Average Math Score",
                                                      "reading_score": "Average Reading Score",
                                                     })
school_summary_df["Total Students"] = school_summary_df["Total Students"].map("{:,.2f}".format)
school_summary_df["Total School Budget"] = school_summary_df["Total School Budget"].map("${:,.2f}".format)
school_summary_df["Average Math Score"] = school_summary_df["Average Math Score"].map("{:,.2f}%".format)
school_summary_df["Average Reading Score"] = school_summary_df["Average Reading Score"].map("{:,.2f}%".format)
school_summary_df["% Passing Math"] = school_summary_df["% Passing"].map("{:,.2f}%".format)
school_summary_df["% Passing Reading"] = summary_df["% Passing Reading"].map("{:,.2f}%".format)
school_summary_df["% Passing Overall"] = summary_df["% Passing Overall"].map("{:,.2f}%".format)
school_summary_df
297/34:
school_summary_df = school_summary_df.rename(columns={"budget": "Total School Budget",
                                                      "Student ID": "Total Students",
                                                     "math_score": "Average Math Score",
                                                      "reading_score": "Average Reading Score",
                                                     })
school_summary_df["Total Students"] = school_summary_df["Total Students"].map("{:,.2f}".format)
school_summary_df["Total School Budget"] = school_summary_df["Total School Budget"].map("${:,.2f}".format)
school_summary_df["Average Math Score"] = school_summary_df["Average Math Score"].map("{:,.2f}%".format)
school_summary_df["Average Reading Score"] = school_summary_df["Average Reading Score"].map("{:,.2f}%".format)
school_summary_df["% Passing Math"] = school_summary_df["% Passing Math"].map("{:,.2f}%".format)
school_summary_df["% Passing Reading"] = summary_df["% Passing Reading"].map("{:,.2f}%".format)
school_summary_df["% Passing Overall"] = summary_df["% Passing Overall"].map("{:,.2f}%".format)
school_summary_df
298/1:
# Dependencies
import pandas as pd
import numpy as np
298/2:
# load in file
pycityschools_file = "Resources/schools_complete.csv"
student_file = "Resources/students_complete.csv"
298/3:
# Read School and Student Data File and store into Pandas DataFrames
pycityschools_data_df = pd.read_csv(pycityschools_file)
student_data_df = pd.read_csv(student_file)

pycityschools_data_df.head()
student_data_df.head()
298/4:
pycityschools_data_complete = pd.merge(student_data_df, pycityschools_data_df, 
                                       on=["school_name", "school_name"], how="left")

pycityschools_data_complete.head()
298/5:
# Count total schools
total_schools = len(pycityschools_data_complete["School ID"].unique())
total_schools
298/6:
# Calculate the total number of students
total_students = len(pycityschools_data_complete["student_name"].unique())
total_students
298/7:
# Calculate Total Budget
total_budget = pycityschools_data_df ["budget"].sum()
total_budget
298/8:
# Calculate the average math score
average_mathscore = round(pycityschools_data_complete["math_score"].mean(), 2)
average_mathscore
298/9:
# Calculate the average reading score
average_readingscore = round(pycityschools_data_complete["reading_score"].mean(), 2)
average_readingscore
298/10:
# Calculate the percentage of students with a passing math score (70 or greater)
unique_math_passing = len(pycityschools_data_complete.loc[pycityschools_data_complete["math_score"] >=70]["student_name"].unique())
percent_math_passing = round((unique_math_passing/total_students)*100, 2)
percent_math_passing
298/11:
# Calculate the percentage of students with a passing reading score (70 or greater)
unique_reading_passing = len(pycityschools_data_complete.loc[pycityschools_data_complete["reading_score"] >=70]["student_name"].unique())
percent_reading_passing = round((unique_reading_passing/total_students)*100, 2)
percent_reading_passing
298/12:
# Calculate the percentage of students who passed math and reading (% Overall Passing)
overall_passing = round((percent_reading_passing + percent_math_passing) / 2, 2)
overall_passing
298/13:
# Create a dataframe to hold the above results
summary_df = pd.DataFrame({"Total Schools": [total_schools],
                          "Total Students": [total_students],
                          "Total Budget": [total_budget],
                          "Average Math Score": [average_mathscore],
                          "Average Reading Score": [average_readingscore],
                          "% of Students with Passing Math Score": [percent_math_passing],
                          "% of Students with Passing Reading Score": [percent_reading_passing],
                          "% of Students with Passing Reading and Math Score": [overall_passing]
                          })
summary_df
298/14:
# Give the displayed data cleaner formatting
summary_df["Total Students"] = summary_df["Total Students"].map("{:,.2f}".format)
summary_df["Total Budget"] = summary_df["Total Budget"].map("${:,.2f}".format)
summary_df["Average Math Score"] = summary_df["Average Math Score"].map("{:,.2f}%".format)
summary_df["Average Reading Score"] = summary_df["Average Reading Score"].map("{:,.2f}%".format)
summary_df["% of Students with Passing Math Score"] = summary_df["% of Students with Passing Math Score"].map("{:,.2f}%".format)
summary_df["% of Students with Passing Reading Score"] = summary_df["% of Students with Passing Reading Score"].map("{:,.2f}%".format)
summary_df["% of Students with Passing Reading and Math Score"] = summary_df["% of Students with Passing Reading and Math Score"].map("{:,.2f}%".format)
summary_df
298/15:
# Create an overview table that summarizes key metrics about each school
    # School Name
# School Type
# Total Students
# Total School Budget
# Per Student Budget
# Average Math Score
# Average Reading Score
# % Passing Math (The percentage of students that passed math.)
# % Passing Reading (The percentage of students that passed reading.)
# % Overall Passing (The percentage of students that passed math and reading.)
    
# Create a dataframe to hold the above results
298/16:
schools_overview_group = pycityschools_data_complete.groupby(["school_name"])
school_summary_df = schools_overview_group.agg({"Student ID": "count",
                            "budget": "max",
                           "math_score": "mean",
                            "reading_score": "mean",
                           })
school_summary_df
298/17:
# Passing Math Score
passing_math_stu = pycityschools_data_complete.loc[pycityschools_data_complete["math_score"] >=70]
passing_math_stu
298/18:
passing_overall_stu = pycityschools_data_complete.loc[(pycityschools_data_complete["math_score"] >=70)&
                                                  (pycityschools_data_complete["reading_score"] >=70)]
passing_overall_stu
298/19:
passing_reading_stu = pycityschools_data_complete.loc[pycityschools_data_complete["reading_score"] >=70]
passing_reading_stu
298/20:
passing_math_count = passing_math_stu.groupby(["school_name"]).count()["math_score"]
passing_math_count
298/21:
passing_reading_count = passing_reading_stu.groupby(["school_name"]).count()["reading_score"]
passing_reading_count
298/22:
passing_overall_count = passing_overall_stu.groupby(["school_name"]).count()["reading_score"]
passing_overall_count
298/23:
school_summary_df["% Passing Math"] = passing_math_count / school_summary_df["Student ID"] * 100
school_summary_df["% Passing Reading"] = passing_reading_count / school_summary_df["Student ID"] * 100
school_summary_df["% Passing Overall"] = passing_overall_count / school_summary_df["Student ID"] * 100
school_summary_df
298/24:
# school_summary_df = pd.DataFrame({
#})
298/25:
school_summary_df = school_summary_df.rename(columns={"budget": "Total School Budget",
                                                      "Student ID": "Total Students",
                                                     "math_score": "Average Math Score",
                                                      "reading_score": "Average Reading Score",
                                                     })
school_summary_df["Total Students"] = school_summary_df["Total Students"].map("{:,.2f}".format)
school_summary_df["Total School Budget"] = school_summary_df["Total School Budget"].map("${:,.2f}".format)
school_summary_df["Average Math Score"] = school_summary_df["Average Math Score"].map("{:,.2f}%".format)
school_summary_df["Average Reading Score"] = school_summary_df["Average Reading Score"].map("{:,.2f}%".format)
school_summary_df["% Passing Math"] = school_summary_df["% Passing Math"].map("{:,.2f}%".format)
school_summary_df["% Passing Reading"] = summary_df["% Passing Reading"].map("{:,.2f}%".format)
school_summary_df["% Passing Overall"] = summary_df["% Passing Overall"].map("{:,.2f}%".format)
school_summary_df
298/26:
school_summary_df = school_summary_df.rename(columns={"budget": "Total School Budget",
                                                      "Student ID": "Total Students",
                                                     "math_score": "Average Math Score",
                                                      "reading_score": "Average Reading Score",
                                                     })
school_summary_df
298/27:
school_summary_df["Total Students"] = school_summary_df["Total Students"].map("{:,.2f}".format)
school_summary_df["Total School Budget"] = school_summary_df["Total School Budget"].map("${:,.2f}".format)
school_summary_df["Average Math Score"] = school_summary_df["Average Math Score"].map("{:,.2f}%".format)
school_summary_df["Average Reading Score"] = school_summary_df["Average Reading Score"].map("{:,.2f}%".format)
school_summary_df["% Passing Math"] = school_summary_df["% Passing Math"].map("{:,.2f}%".format)
school_summary_df["% Passing Reading"] = summary_df["% Passing Reading"].map("{:,.2f}%".format)
school_summary_df["% Passing Overall"] = summary_df["% Passing Overall"].map("{:,.2f}%".format)
school_summary_df
298/28:
school_summary_df["% Passing Math"] = school_summary_df["% Passing Math"].map("{:,.2f}%".format)
school_summary_df["% Passing Reading"] = summary_df["% Passing Reading"].map("{:,.2f}%".format)
school_summary_df["% Passing Overall"] = summary_df["% Passing Overall"].map("{:,.2f}%".format)
school_summary_df
298/29:
school_summary_df["% Passing Reading"] = summary_df["% Passing Reading"].map("{:,.2f}%".format)
school_summary_df["% Passing Overall"] = summary_df["% Passing Overall"].map("{:,.2f}%".format)
school_summary_df
298/30:
school_summary_df["% Passing Math"] = round((passing_math_count / school_summary_df["Student ID"]) * 100, 2)
school_summary_df["% Passing Reading"] = passing_reading_count / school_summary_df["Student ID"] * 100
school_summary_df["% Passing Overall"] = passing_overall_count / school_summary_df["Student ID"] * 100
school_summary_df
298/31:
school_summary_df["% Passing Math"] = passing_math_count / school_summary_df["Student ID"] * 100
school_summary_df["% Passing Reading"] = passing_reading_count / school_summary_df["Student ID"] * 100
school_summary_df["% Passing Overall"] = passing_overall_count / school_summary_df["Student ID"] * 100
school_summary_df
299/1:
# Dependencies
import pandas as pd
import numpy as np
299/2:
# load in file
pycityschools_file = "Resources/schools_complete.csv"
student_file = "Resources/students_complete.csv"
299/3:
# Read School and Student Data File and store into Pandas DataFrames
pycityschools_data_df = pd.read_csv(pycityschools_file)
student_data_df = pd.read_csv(student_file)

pycityschools_data_df.head()
student_data_df.head()
299/4:
pycityschools_data_complete = pd.merge(student_data_df, pycityschools_data_df, 
                                       on=["school_name", "school_name"], how="left")

pycityschools_data_complete.head()
299/5:
# Count total schools
total_schools = len(pycityschools_data_complete["School ID"].unique())
total_schools
299/6:
# Calculate the total number of students
total_students = len(pycityschools_data_complete["student_name"].unique())
total_students
299/7:
# Calculate Total Budget
total_budget = pycityschools_data_df ["budget"].sum()
total_budget
299/8:
# Calculate the average math score
average_mathscore = round(pycityschools_data_complete["math_score"].mean(), 2)
average_mathscore
299/9:
# Calculate the average reading score
average_readingscore = round(pycityschools_data_complete["reading_score"].mean(), 2)
average_readingscore
299/10:
# Calculate the percentage of students with a passing math score (70 or greater)
unique_math_passing = len(pycityschools_data_complete.loc[pycityschools_data_complete["math_score"] >=70]["student_name"].unique())
percent_math_passing = round((unique_math_passing/total_students)*100, 2)
percent_math_passing
299/11:
# Calculate the percentage of students with a passing reading score (70 or greater)
unique_reading_passing = len(pycityschools_data_complete.loc[pycityschools_data_complete["reading_score"] >=70]["student_name"].unique())
percent_reading_passing = round((unique_reading_passing/total_students)*100, 2)
percent_reading_passing
299/12:
# Calculate the percentage of students who passed math and reading (% Overall Passing)
overall_passing = round((percent_reading_passing + percent_math_passing) / 2, 2)
overall_passing
299/13:
# Create a dataframe to hold the above results
summary_df = pd.DataFrame({"Total Schools": [total_schools],
                          "Total Students": [total_students],
                          "Total Budget": [total_budget],
                          "Average Math Score": [average_mathscore],
                          "Average Reading Score": [average_readingscore],
                          "% of Students with Passing Math Score": [percent_math_passing],
                          "% of Students with Passing Reading Score": [percent_reading_passing],
                          "% of Students with Passing Reading and Math Score": [overall_passing]
                          })
summary_df
299/14:
# Give the displayed data cleaner formatting
summary_df["Total Students"] = summary_df["Total Students"].map("{:,.2f}".format)
summary_df["Total Budget"] = summary_df["Total Budget"].map("${:,.2f}".format)
summary_df["Average Math Score"] = summary_df["Average Math Score"].map("{:,.2f}%".format)
summary_df["Average Reading Score"] = summary_df["Average Reading Score"].map("{:,.2f}%".format)
summary_df["% of Students with Passing Math Score"] = summary_df["% of Students with Passing Math Score"].map("{:,.2f}%".format)
summary_df["% of Students with Passing Reading Score"] = summary_df["% of Students with Passing Reading Score"].map("{:,.2f}%".format)
summary_df["% of Students with Passing Reading and Math Score"] = summary_df["% of Students with Passing Reading and Math Score"].map("{:,.2f}%".format)
summary_df
299/15:
# Create an overview table that summarizes key metrics about each school
    # School Name
# School Type
# Total Students
# Total School Budget
# Per Student Budget
# Average Math Score
# Average Reading Score
# % Passing Math (The percentage of students that passed math.)
# % Passing Reading (The percentage of students that passed reading.)
# % Overall Passing (The percentage of students that passed math and reading.)
    
# Create a dataframe to hold the above results
299/16:
schools_overview_group = pycityschools_data_complete.groupby(["school_name"])
school_summary_df = schools_overview_group.agg({"Student ID": "count",
                            "budget": "max",
                           "math_score": "mean",
                            "reading_score": "mean",
                           })
school_summary_df
299/17:
# Passing Math Score
passing_math_stu = pycityschools_data_complete.loc[pycityschools_data_complete["math_score"] >=70]
passing_math_stu
299/18:
passing_overall_stu = pycityschools_data_complete.loc[(pycityschools_data_complete["math_score"] >=70)&
                                                  (pycityschools_data_complete["reading_score"] >=70)]
passing_overall_stu
299/19:
passing_reading_stu = pycityschools_data_complete.loc[pycityschools_data_complete["reading_score"] >=70]
passing_reading_stu
299/20:
passing_math_count = passing_math_stu.groupby(["school_name"]).count()["math_score"]
passing_math_count
299/21:
passing_reading_count = passing_reading_stu.groupby(["school_name"]).count()["reading_score"]
passing_reading_count
299/22:
passing_overall_count = passing_overall_stu.groupby(["school_name"]).count()["reading_score"]
passing_overall_count
299/23:
school_summary_df["% Passing Math"] = passing_math_count / school_summary_df["Student ID"] * 100
school_summary_df["% Passing Reading"] = passing_reading_count / school_summary_df["Student ID"] * 100
school_summary_df["% Passing Overall"] = passing_overall_count / school_summary_df["Student ID"] * 100
school_summary_df
299/24:
# school_summary_df = pd.DataFrame({
#})
299/25:
school_summary_df = school_summary_df.rename(columns={"budget": "Total School Budget",
                                                      "Student ID": "Total Students",
                                                     "math_score": "Average Math Score",
                                                      "reading_score": "Average Reading Score",
                                                     })
school_summary_df
299/26:
school_summary_df["% Passing Reading"] = summary_df["% Passing Reading"].map("{:,.2f}%".format)
school_summary_df["% Passing Overall"] = summary_df["% Passing Overall"].map("{:,.2f}%".format)
school_summary_df
299/27:
school_summary_df["Total Students"] = school_summary_df["Total Students"].map("{:,.2f}".format)
school_summary_df["Total Budget"] = school_summary_df["Total Budget"].map("${:,.2f}".format)
school_summary_df["Average Math Score"] = school_summary_df["Average Math Score"].map("{:,.2f}%".format)
school_summary_df["Average Reading Score"] = school_summary_df["Average Reading Score"].map("{:,.2f}%".format)
school_summary_df["% of Students with Passing Math Score"] = school_summary_df["% of Students with Passing Math Score"].map("{:,.2f}%".format)
school_summary_df["% of Students with Passing Reading Score"] = school_summary_df["% of Students with Passing Reading Score"].map("{:,.2f}%".format)
school_summary_df["% of Students with Passing Reading and Math Score"] = school_summary_df["% of Students with Passing Reading and Math Score"].map("{:,.2f}%".format)
school_summary_df
299/28:
school_summary_df["Total Students"] = school_summary_df["Total Students"].map("{:,.2f}".format)
school_summary_df["Total School Budget"] = school_summary_df["Total School Budget"].map("${:,.2f}".format)
school_summary_df["Average Math Score"] = school_summary_df["Average Math Score"].map("{:,.2f}%".format)
school_summary_df["Average Reading Score"] = school_summary_df["Average Reading Score"].map("{:,.2f}%".format)
school_summary_df["% Passing Math"] = school_summary_df["% Passing Math"].map("{:,.2f}%".format)
school_summary_df["% Passing Reading"] = school_summary_df["% Passing Reading"].map("{:,.2f}%".format)
school_summary_df["% Passing Overall"] = school_summary_df["% Passing Overall"].map("{:,.2f}%".format)
school_summary_df
299/29: school_summary_df["Per Student Budget"] = school_summary_df["budget"] / school_summary_df["Student ID"]
299/30:
schools_overview_group = pycityschools_data_complete.groupby(["school_name"])
school_summary_df = schools_overview_group.agg({"Student ID": "count",
                            "budget": "max",
                           "math_score": "mean",
                            "reading_score": "mean",
                           })
school_summary_df["Per Student Budget"] = school_summary_df["budget"] / school_summary_df["Student ID"]
school_summary_df
300/1:
# Dependencies
import pandas as pd
import numpy as np
300/2:
# load in file
pycityschools_file = "Resources/schools_complete.csv"
student_file = "Resources/students_complete.csv"
300/3:
# Read School and Student Data File and store into Pandas DataFrames
pycityschools_data_df = pd.read_csv(pycityschools_file)
student_data_df = pd.read_csv(student_file)

pycityschools_data_df.head()
student_data_df.head()
300/4:
pycityschools_data_complete = pd.merge(student_data_df, pycityschools_data_df, 
                                       on=["school_name", "school_name"], how="left")

pycityschools_data_complete.head()
300/5:
# Count total schools
total_schools = len(pycityschools_data_complete["School ID"].unique())
total_schools
300/6:
# Calculate the total number of students
total_students = len(pycityschools_data_complete["student_name"].unique())
total_students
300/7:
# Calculate Total Budget
total_budget = pycityschools_data_df ["budget"].sum()
total_budget
300/8:
# Calculate the average math score
average_mathscore = round(pycityschools_data_complete["math_score"].mean(), 2)
average_mathscore
300/9:
# Calculate the average reading score
average_readingscore = round(pycityschools_data_complete["reading_score"].mean(), 2)
average_readingscore
300/10:
# Calculate the percentage of students with a passing math score (70 or greater)
unique_math_passing = len(pycityschools_data_complete.loc[pycityschools_data_complete["math_score"] >=70]["student_name"].unique())
percent_math_passing = round((unique_math_passing/total_students)*100, 2)
percent_math_passing
300/11:
# Calculate the percentage of students with a passing reading score (70 or greater)
unique_reading_passing = len(pycityschools_data_complete.loc[pycityschools_data_complete["reading_score"] >=70]["student_name"].unique())
percent_reading_passing = round((unique_reading_passing/total_students)*100, 2)
percent_reading_passing
300/12:
# Calculate the percentage of students who passed math and reading (% Overall Passing)
overall_passing = round((percent_reading_passing + percent_math_passing) / 2, 2)
overall_passing
300/13:
# Create a dataframe to hold the above results
summary_df = pd.DataFrame({"Total Schools": [total_schools],
                          "Total Students": [total_students],
                          "Total Budget": [total_budget],
                          "Average Math Score": [average_mathscore],
                          "Average Reading Score": [average_readingscore],
                          "% of Students with Passing Math Score": [percent_math_passing],
                          "% of Students with Passing Reading Score": [percent_reading_passing],
                          "% of Students with Passing Reading and Math Score": [overall_passing]
                          })
summary_df
300/14:
# Give the displayed data cleaner formatting
summary_df["Total Students"] = summary_df["Total Students"].map("{:,.2f}".format)
summary_df["Total Budget"] = summary_df["Total Budget"].map("${:,.2f}".format)
summary_df["Average Math Score"] = summary_df["Average Math Score"].map("{:,.2f}%".format)
summary_df["Average Reading Score"] = summary_df["Average Reading Score"].map("{:,.2f}%".format)
summary_df["% of Students with Passing Math Score"] = summary_df["% of Students with Passing Math Score"].map("{:,.2f}%".format)
summary_df["% of Students with Passing Reading Score"] = summary_df["% of Students with Passing Reading Score"].map("{:,.2f}%".format)
summary_df["% of Students with Passing Reading and Math Score"] = summary_df["% of Students with Passing Reading and Math Score"].map("{:,.2f}%".format)
summary_df
300/15:
# Create an overview table that summarizes key metrics about each school
    # School Name
# School Type
# Total Students
# Total School Budget
# Per Student Budget
# Average Math Score
# Average Reading Score
# % Passing Math (The percentage of students that passed math.)
# % Passing Reading (The percentage of students that passed reading.)
# % Overall Passing (The percentage of students that passed math and reading.)
    
# Create a dataframe to hold the above results
300/16:
schools_overview_group = pycityschools_data_complete.groupby(["school_name"])
school_summary_df = schools_overview_group.agg({"Student ID": "count",
                            "budget": "max",
                           "math_score": "mean",
                            "reading_score": "mean",
                           })
school_summary_df["Per Student Budget"] = school_summary_df["budget"] / school_summary_df["Student ID"]
school_summary_df
300/17:
# Passing Math Score
passing_math_stu = pycityschools_data_complete.loc[pycityschools_data_complete["math_score"] >=70]
passing_math_stu
300/18:
passing_overall_stu = pycityschools_data_complete.loc[(pycityschools_data_complete["math_score"] >=70)&
                                                  (pycityschools_data_complete["reading_score"] >=70)]
passing_overall_stu
300/19:
passing_reading_stu = pycityschools_data_complete.loc[pycityschools_data_complete["reading_score"] >=70]
passing_reading_stu
300/20:
passing_math_count = passing_math_stu.groupby(["school_name"]).count()["math_score"]
passing_math_count
300/21:
passing_reading_count = passing_reading_stu.groupby(["school_name"]).count()["reading_score"]
passing_reading_count
300/22:
passing_overall_count = passing_overall_stu.groupby(["school_name"]).count()["reading_score"]
passing_overall_count
300/23:
school_summary_df["% Passing Math"] = passing_math_count / school_summary_df["Student ID"] * 100
school_summary_df["% Passing Reading"] = passing_reading_count / school_summary_df["Student ID"] * 100
school_summary_df["% Passing Overall"] = passing_overall_count / school_summary_df["Student ID"] * 100
school_summary_df
300/24:
school_summary_df = school_summary_df.rename(columns={"budget": "Total School Budget",
                                                      "Student ID": "Total Students",
                                                     "math_score": "Average Math Score",
                                                      "reading_score": "Average Reading Score",
                                                     })
school_summary_df
300/25:
school_summary_df.format({'Total Students': '{:,}', 
                          "Total School Budget": "${:,}", 
                          "Per Student Budget": "${:.0f}",
                          'Average Math Score': "{:.1f}", 
                          'Average Reading Score': "{:.1f}", 
                          "% Passing Math": "{:.1%}", 
                          "% Passing Reading": "{:.1%}", 
                          "Overall Passing Rate": "{:.1%}"})
300/26:
school_summary_df["% Passing Math"] = passing_math_count / school_summary_df["Student ID"] * 100
school_summary_df["% Passing Reading"] = passing_reading_count / school_summary_df["Student ID"] * 100
school_summary_df["% Passing Overall"] = passing_overall_count / school_summary_df["Student ID"] * 100
school_summary_df
300/27:
school_summary_df.format({'Total Students': '{:,}', 
                          "Total School Budget": "${:,}", 
                          "Per Student Budget": "${:.0f}",
                          "Average Math Score": "{:.1f}", 
                          "Average Reading Score": "{:.1f}", 
                          "% Passing Math": "{:.1%}", 
                          "% Passing Reading": "{:.1%}", 
                          "Overall Passing Rate": "{:.1%}"
                         })
school_summary_df
300/28:
school_summary_df.style.format({'Total Students': '{:,}', 
                          "Total School Budget": "${:,}", 
                          "Per Student Budget": "${:.0f}",
                          "Average Math Score": "{:.1f}", 
                          "Average Reading Score": "{:.1f}", 
                          "% Passing Math": "{:.1%}", 
                          "% Passing Reading": "{:.1%}", 
                          "Overall Passing Rate": "{:.1%}"
                         })
school_summary_df
300/29:
school_summary_df.style.format({'Total Students': '{:,}', 
                          "Total School Budget": "${:,}", 
                          "Per Student Budget": "${:.0f}",
                          "Average Math Score": "{:.1f}", 
                          "Average Reading Score": "{:.1f}", 
                          "% Passing Math": "{:.1%}", 
                          "% Passing Reading": "{:.1%}", 
                          "Overall Passing Rate": "{:.1%}"
                         })
school_summary_df.style.format
300/30:
school_summary_df = school_summary_df.style.format({'Total Students': '{:,}', 
                          "Total School Budget": "${:,}", 
                          "Per Student Budget": "${:.0f}",
                          "Average Math Score": "{:.1f}", 
                          "Average Reading Score": "{:.1f}", 
                          "% Passing Math": "{:.1%}", 
                          "% Passing Reading": "{:.1%}", 
                          "Overall Passing Rate": "{:.1%}"
                         })
school_summary_df
300/31:
school_summary_df = school_summary_df.style.format({'Total Students': '{:,}', 
                          "Total School Budget": "${:,}", 
                          "Per Student Budget": "${:.0f}",
                          'Average Math Score': "{:.1f}", 
                          'Average Reading Score': "{:.1f}", 
                          "% Passing Math": "{:.1%}", 
                          "% Passing Reading": "{:.1%}", 
                          "Overall Passing Rate": "{:.1%}"
                         })
school_summary_df
300/32:
school_summary_df = school_summary_df.style.format({
                          'Total Students': '{:,}', 
                          "Total School Budget": "${:,}", 
                          "Per Student Budget": "${:.0f}",
                          'Average Math Score': "{:.1f}", 
                          'Average Reading Score': "{:.1f}", 
                          "% Passing Math": "{:.1%}", 
                          "% Passing Reading": "{:.1%}", 
                          "Overall Passing Rate": "{:.1%}"
                         })
school_summary_df
301/1:
# Dependencies
import pandas as pd
import numpy as np
301/2:
# load in file
pycityschools_file = "Resources/schools_complete.csv"
student_file = "Resources/students_complete.csv"
301/3:
# Read School and Student Data File and store into Pandas DataFrames
pycityschools_data_df = pd.read_csv(pycityschools_file)
student_data_df = pd.read_csv(student_file)

pycityschools_data_df.head()
student_data_df.head()
301/4:
pycityschools_data_complete = pd.merge(student_data_df, pycityschools_data_df, 
                                       on=["school_name", "school_name"], how="left")

pycityschools_data_complete.head()
301/5:
# Count total schools
total_schools = len(pycityschools_data_complete["School ID"].unique())
total_schools
301/6:
# Calculate the total number of students
total_students = len(pycityschools_data_complete["student_name"].unique())
total_students
301/7:
# Calculate Total Budget
total_budget = pycityschools_data_df ["budget"].sum()
total_budget
301/8:
# Calculate the average math score
average_mathscore = round(pycityschools_data_complete["math_score"].mean(), 2)
average_mathscore
301/9:
# Calculate the average reading score
average_readingscore = round(pycityschools_data_complete["reading_score"].mean(), 2)
average_readingscore
301/10:
# Calculate the percentage of students with a passing math score (70 or greater)
unique_math_passing = len(pycityschools_data_complete.loc[pycityschools_data_complete["math_score"] >=70]["student_name"].unique())
percent_math_passing = round((unique_math_passing/total_students)*100, 2)
percent_math_passing
301/11:
# Calculate the percentage of students with a passing reading score (70 or greater)
unique_reading_passing = len(pycityschools_data_complete.loc[pycityschools_data_complete["reading_score"] >=70]["student_name"].unique())
percent_reading_passing = round((unique_reading_passing/total_students)*100, 2)
percent_reading_passing
301/12:
# Calculate the percentage of students who passed math and reading (% Overall Passing)
overall_passing = round((percent_reading_passing + percent_math_passing) / 2, 2)
overall_passing
301/13:
# Create a dataframe to hold the above results
summary_df = pd.DataFrame({"Total Schools": [total_schools],
                          "Total Students": [total_students],
                          "Total Budget": [total_budget],
                          "Average Math Score": [average_mathscore],
                          "Average Reading Score": [average_readingscore],
                          "% of Students with Passing Math Score": [percent_math_passing],
                          "% of Students with Passing Reading Score": [percent_reading_passing],
                          "% of Students with Passing Reading and Math Score": [overall_passing]
                          })
summary_df
301/14:
# Give the displayed data cleaner formatting
summary_df["Total Students"] = summary_df["Total Students"].map("{:,.2f}".format)
summary_df["Total Budget"] = summary_df["Total Budget"].map("${:,.2f}".format)
summary_df["Average Math Score"] = summary_df["Average Math Score"].map("{:,.2f}%".format)
summary_df["Average Reading Score"] = summary_df["Average Reading Score"].map("{:,.2f}%".format)
summary_df["% of Students with Passing Math Score"] = summary_df["% of Students with Passing Math Score"].map("{:,.2f}%".format)
summary_df["% of Students with Passing Reading Score"] = summary_df["% of Students with Passing Reading Score"].map("{:,.2f}%".format)
summary_df["% of Students with Passing Reading and Math Score"] = summary_df["% of Students with Passing Reading and Math Score"].map("{:,.2f}%".format)
summary_df
301/15:
# Create an overview table that summarizes key metrics about each school
    # School Name
# School Type
# Total Students
# Total School Budget
# Per Student Budget
# Average Math Score
# Average Reading Score
# % Passing Math (The percentage of students that passed math.)
# % Passing Reading (The percentage of students that passed reading.)
# % Overall Passing (The percentage of students that passed math and reading.)
    
# Create a dataframe to hold the above results
301/16:
schools_overview_group = pycityschools_data_complete.groupby(["school_name"])
school_summary_df = schools_overview_group.agg({"Student ID": "count",
                            "budget": "max",
                           "math_score": "mean",
                            "reading_score": "mean",
                           })
school_summary_df["Per Student Budget"] = school_summary_df["budget"] / school_summary_df["Student ID"]
school_summary_df
301/17:
# Passing Math Score
passing_math_stu = pycityschools_data_complete.loc[pycityschools_data_complete["math_score"] >=70]
passing_math_stu
301/18:
passing_overall_stu = pycityschools_data_complete.loc[(pycityschools_data_complete["math_score"] >=70)&
                                                  (pycityschools_data_complete["reading_score"] >=70)]
passing_overall_stu
301/19:
passing_reading_stu = pycityschools_data_complete.loc[pycityschools_data_complete["reading_score"] >=70]
passing_reading_stu
301/20:
passing_math_count = passing_math_stu.groupby(["school_name"]).count()["math_score"]
passing_math_count
301/21:
passing_reading_count = passing_reading_stu.groupby(["school_name"]).count()["reading_score"]
passing_reading_count
301/22:
passing_overall_count = passing_overall_stu.groupby(["school_name"]).count()["reading_score"]
passing_overall_count
301/23:
school_summary_df["% Passing Math"] = passing_math_count / school_summary_df["Student ID"] * 100
school_summary_df["% Passing Reading"] = passing_reading_count / school_summary_df["Student ID"] * 100
school_summary_df["% Passing Overall"] = passing_overall_count / school_summary_df["Student ID"] * 100
school_summary_df
301/24:
school_summary_df = school_summary_df.rename(columns={"budget": "Total School Budget",
                                                      "Student ID": "Total Students",
                                                     "math_score": "Average Math Score",
                                                      "reading_score": "Average Reading Score",
                                                     })
school_summary_df
301/25:
school_summary_df = school_summary_df.style.format({
                          'Total Students': '{:,}', 
                          "Total School Budget": "${:,}", 
                          "Per Student Budget": "${:.0f}",
                          'Average Math Score': "{:.1f}", 
                          'Average Reading Score': "{:.1f}", 
                          "% Passing Math": "{:.2}%", 
                          "% Passing Reading": "{:.2}%", 
                          "Overall Passing Rate": "{:.2}%"
                         })
school_summary_df
301/26:
school_summary_df = school_summary_df.style.format({
                          'Total Students': '{:,}', 
                          "Total School Budget": "${:,}", 
                          "Per Student Budget": "${:.0f}",
                          'Average Math Score': "{:.1f}", 
                          'Average Reading Score': "{:.1f}", 
                          "% Passing Math": "{:.1}%", 
                          "% Passing Reading": "{:.1}%", 
                          "Overall Passing Rate": "{:.1}%"
                         })
school_summary_df
302/1:
# Dependencies
import pandas as pd
import numpy as np
302/2:
# load in file
pycityschools_file = "Resources/schools_complete.csv"
student_file = "Resources/students_complete.csv"
302/3:
# Read School and Student Data File and store into Pandas DataFrames
pycityschools_data_df = pd.read_csv(pycityschools_file)
student_data_df = pd.read_csv(student_file)

pycityschools_data_df.head()
student_data_df.head()
302/4:
pycityschools_data_complete = pd.merge(student_data_df, pycityschools_data_df, 
                                       on=["school_name", "school_name"], how="left")

pycityschools_data_complete.head()
302/5:
# Count total schools
total_schools = len(pycityschools_data_complete["School ID"].unique())
total_schools
302/6:
# Calculate the total number of students
total_students = len(pycityschools_data_complete["student_name"].unique())
total_students
302/7:
# Calculate Total Budget
total_budget = pycityschools_data_df ["budget"].sum()
total_budget
302/8:
# Calculate the average math score
average_mathscore = round(pycityschools_data_complete["math_score"].mean(), 2)
average_mathscore
302/9:
# Calculate the average reading score
average_readingscore = round(pycityschools_data_complete["reading_score"].mean(), 2)
average_readingscore
302/10:
# Calculate the percentage of students with a passing math score (70 or greater)
unique_math_passing = len(pycityschools_data_complete.loc[pycityschools_data_complete["math_score"] >=70]["student_name"].unique())
percent_math_passing = round((unique_math_passing/total_students)*100, 2)
percent_math_passing
302/11:
# Calculate the percentage of students with a passing reading score (70 or greater)
unique_reading_passing = len(pycityschools_data_complete.loc[pycityschools_data_complete["reading_score"] >=70]["student_name"].unique())
percent_reading_passing = round((unique_reading_passing/total_students)*100, 2)
percent_reading_passing
302/12:
# Calculate the percentage of students who passed math and reading (% Overall Passing)
overall_passing = round((percent_reading_passing + percent_math_passing) / 2, 2)
overall_passing
302/13:
# Create a dataframe to hold the above results
summary_df = pd.DataFrame({"Total Schools": [total_schools],
                          "Total Students": [total_students],
                          "Total Budget": [total_budget],
                          "Average Math Score": [average_mathscore],
                          "Average Reading Score": [average_readingscore],
                          "% of Students with Passing Math Score": [percent_math_passing],
                          "% of Students with Passing Reading Score": [percent_reading_passing],
                          "% of Students with Passing Reading and Math Score": [overall_passing]
                          })
summary_df
302/14:
# Give the displayed data cleaner formatting
summary_df["Total Students"] = summary_df["Total Students"].map("{:,.2f}".format)
summary_df["Total Budget"] = summary_df["Total Budget"].map("${:,.2f}".format)
summary_df["Average Math Score"] = summary_df["Average Math Score"].map("{:,.2f}%".format)
summary_df["Average Reading Score"] = summary_df["Average Reading Score"].map("{:,.2f}%".format)
summary_df["% of Students with Passing Math Score"] = summary_df["% of Students with Passing Math Score"].map("{:,.2f}%".format)
summary_df["% of Students with Passing Reading Score"] = summary_df["% of Students with Passing Reading Score"].map("{:,.2f}%".format)
summary_df["% of Students with Passing Reading and Math Score"] = summary_df["% of Students with Passing Reading and Math Score"].map("{:,.2f}%".format)
summary_df
302/15:
# Create an overview table that summarizes key metrics about each school
    # School Name
# School Type
# Total Students
# Total School Budget
# Per Student Budget
# Average Math Score
# Average Reading Score
# % Passing Math (The percentage of students that passed math.)
# % Passing Reading (The percentage of students that passed reading.)
# % Overall Passing (The percentage of students that passed math and reading.)
    
# Create a dataframe to hold the above results
302/16:
schools_overview_group = pycityschools_data_complete.groupby(["school_name"])
school_summary_df = schools_overview_group.agg({"Student ID": "count",
                            "budget": "max",
                           "math_score": "mean",
                            "reading_score": "mean",
                           })
school_summary_df["Per Student Budget"] = school_summary_df["budget"] / school_summary_df["Student ID"]
school_summary_df
302/17:
# Passing Math Score
passing_math_stu = pycityschools_data_complete.loc[pycityschools_data_complete["math_score"] >=70]
passing_math_stu
302/18:
passing_overall_stu = pycityschools_data_complete.loc[(pycityschools_data_complete["math_score"] >=70)&
                                                  (pycityschools_data_complete["reading_score"] >=70)]
passing_overall_stu
302/19:
passing_reading_stu = pycityschools_data_complete.loc[pycityschools_data_complete["reading_score"] >=70]
passing_reading_stu
302/20:
passing_math_count = passing_math_stu.groupby(["school_name"]).count()["math_score"]
passing_math_count
302/21:
passing_reading_count = passing_reading_stu.groupby(["school_name"]).count()["reading_score"]
passing_reading_count
302/22:
passing_overall_count = passing_overall_stu.groupby(["school_name"]).count()["reading_score"]
passing_overall_count
302/23:
school_summary_df["% Passing Math"] = passing_math_count / school_summary_df["Student ID"] * 100
school_summary_df["% Passing Reading"] = passing_reading_count / school_summary_df["Student ID"] * 100
school_summary_df["% Passing Overall"] = passing_overall_count / school_summary_df["Student ID"] * 100
school_summary_df
302/24:
school_summary_df = school_summary_df.rename(columns={"budget": "Total School Budget",
                                                      "Student ID": "Total Students",
                                                     "math_score": "Average Math Score",
                                                      "reading_score": "Average Reading Score",
                                                     })
school_summary_df
302/25:
school_summary_df = school_summary_df.style.format({
                          'Total Students': '{:,}', 
                          "Total School Budget": "${:,}", 
                          "Per Student Budget": "${:.0f}",
                          'Average Math Score': "{:.1f}", 
                          'Average Reading Score': "{:.1f}", 
                          "% Passing Math": "{:.1}%", 
                          "% Passing Reading": "{:.1}%", 
                          "Overall Passing Rate": "{:.1}%"
                         })
school_summary_df
302/26:
school_summary_df = school_summary_df.style.format({
                          'Total Students': '{:,}', 
                          "Total School Budget": "${:,}", 
                          "Per Student Budget": "${:.0f}",
                          'Average Math Score': "{:.1f}%", 
                          'Average Reading Score': "{:.1f}%", 
                          "% Passing Math": "{:.1f}%", 
                          "% Passing Reading": "{:.1f}%", 
                          "Overall Passing Rate": "{:.1f}%"
                         })
school_summary_df
303/1:
# Dependencies
import pandas as pd
import numpy as np
303/2:
# load in file
pycityschools_file = "Resources/schools_complete.csv"
student_file = "Resources/students_complete.csv"
303/3:
# Read School and Student Data File and store into Pandas DataFrames
pycityschools_data_df = pd.read_csv(pycityschools_file)
student_data_df = pd.read_csv(student_file)

pycityschools_data_df.head()
student_data_df.head()
303/4:
pycityschools_data_complete = pd.merge(student_data_df, pycityschools_data_df, 
                                       on=["school_name", "school_name"], how="left")

pycityschools_data_complete.head()
303/5:
# Count total schools
total_schools = len(pycityschools_data_complete["School ID"].unique())
total_schools
303/6:
# Calculate the total number of students
total_students = len(pycityschools_data_complete["student_name"].unique())
total_students
303/7:
# Calculate Total Budget
total_budget = pycityschools_data_df ["budget"].sum()
total_budget
303/8:
# Calculate the average math score
average_mathscore = round(pycityschools_data_complete["math_score"].mean(), 2)
average_mathscore
303/9:
# Calculate the average reading score
average_readingscore = round(pycityschools_data_complete["reading_score"].mean(), 2)
average_readingscore
303/10:
# Calculate the percentage of students with a passing math score (70 or greater)
unique_math_passing = len(pycityschools_data_complete.loc[pycityschools_data_complete["math_score"] >=70]["student_name"].unique())
percent_math_passing = round((unique_math_passing/total_students)*100, 2)
percent_math_passing
303/11:
# Calculate the percentage of students with a passing reading score (70 or greater)
unique_reading_passing = len(pycityschools_data_complete.loc[pycityschools_data_complete["reading_score"] >=70]["student_name"].unique())
percent_reading_passing = round((unique_reading_passing/total_students)*100, 2)
percent_reading_passing
303/12:
# Calculate the percentage of students who passed math and reading (% Overall Passing)
overall_passing = round((percent_reading_passing + percent_math_passing) / 2, 2)
overall_passing
303/13:
# Create a dataframe to hold the above results
summary_df = pd.DataFrame({"Total Schools": [total_schools],
                          "Total Students": [total_students],
                          "Total Budget": [total_budget],
                          "Average Math Score": [average_mathscore],
                          "Average Reading Score": [average_readingscore],
                          "% of Students with Passing Math Score": [percent_math_passing],
                          "% of Students with Passing Reading Score": [percent_reading_passing],
                          "% of Students with Passing Reading and Math Score": [overall_passing]
                          })
summary_df
303/14:
# Give the displayed data cleaner formatting
summary_df["Total Students"] = summary_df["Total Students"].map("{:,.2f}".format)
summary_df["Total Budget"] = summary_df["Total Budget"].map("${:,.2f}".format)
summary_df["Average Math Score"] = summary_df["Average Math Score"].map("{:,.2f}%".format)
summary_df["Average Reading Score"] = summary_df["Average Reading Score"].map("{:,.2f}%".format)
summary_df["% of Students with Passing Math Score"] = summary_df["% of Students with Passing Math Score"].map("{:,.2f}%".format)
summary_df["% of Students with Passing Reading Score"] = summary_df["% of Students with Passing Reading Score"].map("{:,.2f}%".format)
summary_df["% of Students with Passing Reading and Math Score"] = summary_df["% of Students with Passing Reading and Math Score"].map("{:,.2f}%".format)
summary_df
303/15:
# Create an overview table that summarizes key metrics about each school
    # School Name
# School Type
# Total Students
# Total School Budget
# Per Student Budget
# Average Math Score
# Average Reading Score
# % Passing Math (The percentage of students that passed math.)
# % Passing Reading (The percentage of students that passed reading.)
# % Overall Passing (The percentage of students that passed math and reading.)
    
# Create a dataframe to hold the above results
303/16:
schools_overview_group = pycityschools_data_complete.groupby(["school_name"])
school_summary_df = schools_overview_group.agg({"Student ID": "count",
                            "budget": "max",
                           "math_score": "mean",
                            "reading_score": "mean",
                           })
school_summary_df["Per Student Budget"] = school_summary_df["budget"] / school_summary_df["Student ID"]
school_summary_df
303/17:
# Passing Math Score
passing_math_stu = pycityschools_data_complete.loc[pycityschools_data_complete["math_score"] >=70]
passing_math_stu
303/18:
passing_overall_stu = pycityschools_data_complete.loc[(pycityschools_data_complete["math_score"] >=70)&
                                                  (pycityschools_data_complete["reading_score"] >=70)]
passing_overall_stu
303/19:
passing_reading_stu = pycityschools_data_complete.loc[pycityschools_data_complete["reading_score"] >=70]
passing_reading_stu
303/20:
passing_math_count = passing_math_stu.groupby(["school_name"]).count()["math_score"]
passing_math_count
303/21:
passing_reading_count = passing_reading_stu.groupby(["school_name"]).count()["reading_score"]
passing_reading_count
303/22:
passing_overall_count = passing_overall_stu.groupby(["school_name"]).count()["reading_score"]
passing_overall_count
303/23:
school_summary_df["% Passing Math"] = passing_math_count / school_summary_df["Student ID"] * 100
school_summary_df["% Passing Reading"] = passing_reading_count / school_summary_df["Student ID"] * 100
school_summary_df["% Passing Overall"] = passing_overall_count / school_summary_df["Student ID"] * 100
school_summary_df
303/24:
school_summary_df = school_summary_df.rename(columns={"budget": "Total School Budget",
                                                      "Student ID": "Total Students",
                                                     "math_score": "Average Math Score",
                                                      "reading_score": "Average Reading Score",
                                                     })
school_summary_df
303/25:
school_summary_df = school_summary_df.style.format({
                          'Total Students': '{:,}', 
                          "Total School Budget": "${:,}", 
                          "Per Student Budget": "${:.0f}",
                          'Average Math Score': "{:.1f}%", 
                          'Average Reading Score': "{:.1f}%", 
                          "% Passing Math": "{:.1f}%", 
                          "% Passing Reading": "{:.1f}%", 
                          "Overall Passing Rate": "{:.1f}%"
                         })
school_summary_df
304/1:
# Dependencies
import pandas as pd
import numpy as np
304/2:
# load in file
pycityschools_file = "Resources/schools_complete.csv"
student_file = "Resources/students_complete.csv"
304/3:
# Read School and Student Data File and store into Pandas DataFrames
pycityschools_data_df = pd.read_csv(pycityschools_file)
student_data_df = pd.read_csv(student_file)

pycityschools_data_df.head()
student_data_df.head()
304/4:
pycityschools_data_complete = pd.merge(student_data_df, pycityschools_data_df, 
                                       on=["school_name", "school_name"], how="left")

pycityschools_data_complete.head()
304/5:
# Count total schools
total_schools = len(pycityschools_data_complete["School ID"].unique())
total_schools
304/6:
# Calculate the total number of students
total_students = len(pycityschools_data_complete["student_name"].unique())
total_students
304/7:
# Calculate Total Budget
total_budget = pycityschools_data_df ["budget"].sum()
total_budget
304/8:
# Calculate the average math score
average_mathscore = round(pycityschools_data_complete["math_score"].mean(), 2)
average_mathscore
304/9:
# Calculate the average reading score
average_readingscore = round(pycityschools_data_complete["reading_score"].mean(), 2)
average_readingscore
304/10:
# Calculate the percentage of students with a passing math score (70 or greater)
unique_math_passing = len(pycityschools_data_complete.loc[pycityschools_data_complete["math_score"] >=70]["student_name"].unique())
percent_math_passing = round((unique_math_passing/total_students)*100, 2)
percent_math_passing
304/11:
# Calculate the percentage of students with a passing reading score (70 or greater)
unique_reading_passing = len(pycityschools_data_complete.loc[pycityschools_data_complete["reading_score"] >=70]["student_name"].unique())
percent_reading_passing = round((unique_reading_passing/total_students)*100, 2)
percent_reading_passing
304/12:
# Calculate the percentage of students who passed math and reading (% Overall Passing)
overall_passing = round((percent_reading_passing + percent_math_passing) / 2, 2)
overall_passing
304/13:
# Create a dataframe to hold the above results
summary_df = pd.DataFrame({"Total Schools": [total_schools],
                          "Total Students": [total_students],
                          "Total Budget": [total_budget],
                          "Average Math Score": [average_mathscore],
                          "Average Reading Score": [average_readingscore],
                          "% of Students with Passing Math Score": [percent_math_passing],
                          "% of Students with Passing Reading Score": [percent_reading_passing],
                          "% of Students with Passing Reading and Math Score": [overall_passing]
                          })
summary_df
304/14:
# Give the displayed data cleaner formatting
summary_df["Total Students"] = summary_df["Total Students"].map("{:,.2f}".format)
summary_df["Total Budget"] = summary_df["Total Budget"].map("${:,.2f}".format)
summary_df["Average Math Score"] = summary_df["Average Math Score"].map("{:,.2f}%".format)
summary_df["Average Reading Score"] = summary_df["Average Reading Score"].map("{:,.2f}%".format)
summary_df["% of Students with Passing Math Score"] = summary_df["% of Students with Passing Math Score"].map("{:,.2f}%".format)
summary_df["% of Students with Passing Reading Score"] = summary_df["% of Students with Passing Reading Score"].map("{:,.2f}%".format)
summary_df["% of Students with Passing Reading and Math Score"] = summary_df["% of Students with Passing Reading and Math Score"].map("{:,.2f}%".format)
summary_df
304/15:
# Create an overview table that summarizes key metrics about each school
    # School Name
# School Type
# Total Students
# Total School Budget
# Per Student Budget
# Average Math Score
# Average Reading Score
# % Passing Math (The percentage of students that passed math.)
# % Passing Reading (The percentage of students that passed reading.)
# % Overall Passing (The percentage of students that passed math and reading.)
    
# Create a dataframe to hold the above results
304/16:
schools_overview_group = pycityschools_data_complete.groupby(["school_name"])
school_summary_df = schools_overview_group.agg({"Student ID": "count",
                            "budget": "max",
                           "math_score": "mean",
                            "reading_score": "mean",
                           })
school_summary_df["Per Student Budget"] = school_summary_df["budget"] / school_summary_df["Student ID"]
school_summary_df
304/17:
# Passing Math Score
passing_math_stu = pycityschools_data_complete.loc[pycityschools_data_complete["math_score"] >=70]
passing_math_stu
304/18:
passing_overall_stu = pycityschools_data_complete.loc[(pycityschools_data_complete["math_score"] >=70)&
                                                  (pycityschools_data_complete["reading_score"] >=70)]
passing_overall_stu
304/19:
passing_reading_stu = pycityschools_data_complete.loc[pycityschools_data_complete["reading_score"] >=70]
passing_reading_stu
304/20:
passing_math_count = passing_math_stu.groupby(["school_name"]).count()["math_score"]
passing_math_count
304/21:
passing_reading_count = passing_reading_stu.groupby(["school_name"]).count()["reading_score"]
passing_reading_count
304/22:
passing_overall_count = passing_overall_stu.groupby(["school_name"]).count()["reading_score"]
passing_overall_count
304/23:
school_summary_df["% Passing Math"] = passing_math_count / school_summary_df["Student ID"] * 100
school_summary_df["% Passing Reading"] = passing_reading_count / school_summary_df["Student ID"] * 100
school_summary_df["% Passing Overall"] = passing_overall_count / school_summary_df["Student ID"] * 100
school_summary_df
304/24:
school_summary_df = school_summary_df.rename(columns={"budget": "Total School Budget",
                                                      "Student ID": "Total Students",
                                                     "math_score": "Average Math Score",
                                                      "reading_score": "Average Reading Score",
                                                     })
school_summary_df
304/25:
school_summary_df = school_summary_df.style.format({
                          'Total Students': '{:,}', 
                          "Total School Budget": "${:,}", 
                          "Per Student Budget": "${:.0f}",
                          'Average Math Score': "{:.1f}%", 
                          'Average Reading Score': "{:.1f}%", 
                          "% Passing Math": "{:.1f}%", 
                          "% Passing Reading": "{:.1f}%", 
                          "% Passing Overall": "{:.1f}%"
                         })
school_summary_df
304/26:
passing_overall_count = passing_overall_stu.groupby(["school_name"]).count()["reading_score"]
passing_overall_count = passing_overall_count.rename(columns={"school_name": "School Name"})
305/1:
# Dependencies
import pandas as pd
import numpy as np
305/2:
# load in file
pycityschools_file = "Resources/schools_complete.csv"
student_file = "Resources/students_complete.csv"
305/3:
# Read School and Student Data File and store into Pandas DataFrames
pycityschools_data_df = pd.read_csv(pycityschools_file)
student_data_df = pd.read_csv(student_file)

pycityschools_data_df.head()
student_data_df.head()
305/4:
pycityschools_data_complete = pd.merge(student_data_df, pycityschools_data_df, 
                                       on=["school_name", "school_name"], how="left")

pycityschools_data_complete.head()
305/5:
# Count total schools
total_schools = len(pycityschools_data_complete["School ID"].unique())
total_schools
305/6:
# Calculate the total number of students
total_students = len(pycityschools_data_complete["student_name"].unique())
total_students
305/7:
# Calculate Total Budget
total_budget = pycityschools_data_df ["budget"].sum()
total_budget
305/8:
# Calculate the average math score
average_mathscore = round(pycityschools_data_complete["math_score"].mean(), 2)
average_mathscore
305/9:
# Calculate the average reading score
average_readingscore = round(pycityschools_data_complete["reading_score"].mean(), 2)
average_readingscore
305/10:
# Calculate the percentage of students with a passing math score (70 or greater)
unique_math_passing = len(pycityschools_data_complete.loc[pycityschools_data_complete["math_score"] >=70]["student_name"].unique())
percent_math_passing = round((unique_math_passing/total_students)*100, 2)
percent_math_passing
305/11:
# Calculate the percentage of students with a passing reading score (70 or greater)
unique_reading_passing = len(pycityschools_data_complete.loc[pycityschools_data_complete["reading_score"] >=70]["student_name"].unique())
percent_reading_passing = round((unique_reading_passing/total_students)*100, 2)
percent_reading_passing
305/12:
# Calculate the percentage of students who passed math and reading (% Overall Passing)
overall_passing = round((percent_reading_passing + percent_math_passing) / 2, 2)
overall_passing
305/13:
# Create a dataframe to hold the above results
summary_df = pd.DataFrame({"Total Schools": [total_schools],
                          "Total Students": [total_students],
                          "Total Budget": [total_budget],
                          "Average Math Score": [average_mathscore],
                          "Average Reading Score": [average_readingscore],
                          "% of Students with Passing Math Score": [percent_math_passing],
                          "% of Students with Passing Reading Score": [percent_reading_passing],
                          "% of Students with Passing Reading and Math Score": [overall_passing]
                          })
summary_df
305/14:
# Give the displayed data cleaner formatting
summary_df["Total Students"] = summary_df["Total Students"].map("{:,.2f}".format)
summary_df["Total Budget"] = summary_df["Total Budget"].map("${:,.2f}".format)
summary_df["Average Math Score"] = summary_df["Average Math Score"].map("{:,.2f}%".format)
summary_df["Average Reading Score"] = summary_df["Average Reading Score"].map("{:,.2f}%".format)
summary_df["% of Students with Passing Math Score"] = summary_df["% of Students with Passing Math Score"].map("{:,.2f}%".format)
summary_df["% of Students with Passing Reading Score"] = summary_df["% of Students with Passing Reading Score"].map("{:,.2f}%".format)
summary_df["% of Students with Passing Reading and Math Score"] = summary_df["% of Students with Passing Reading and Math Score"].map("{:,.2f}%".format)
summary_df
305/15:
# Create an overview table that summarizes key metrics about each school
    # School Name
# School Type
# Total Students
# Total School Budget
# Per Student Budget
# Average Math Score
# Average Reading Score
# % Passing Math (The percentage of students that passed math.)
# % Passing Reading (The percentage of students that passed reading.)
# % Overall Passing (The percentage of students that passed math and reading.)
    
# Create a dataframe to hold the above results
305/16:
schools_overview_group = pycityschools_data_complete.groupby(["school_name"])
school_summary_df = schools_overview_group.agg({"Student ID": "count",
                            "budget": "max",
                           "math_score": "mean",
                            "reading_score": "mean",
                           })
school_summary_df["Per Student Budget"] = school_summary_df["budget"] / school_summary_df["Student ID"]
school_summary_df
305/17:
# Passing Math Score
passing_math_stu = pycityschools_data_complete.loc[pycityschools_data_complete["math_score"] >=70]
passing_math_stu
305/18:
passing_overall_stu = pycityschools_data_complete.loc[(pycityschools_data_complete["math_score"] >=70)&
                                                  (pycityschools_data_complete["reading_score"] >=70)]
passing_overall_stu
305/19:
passing_reading_stu = pycityschools_data_complete.loc[pycityschools_data_complete["reading_score"] >=70]
passing_reading_stu
305/20:
passing_math_count = passing_math_stu.groupby(["school_name"]).count()["math_score"]
passing_math_count
305/21:
passing_reading_count = passing_reading_stu.groupby(["school_name"]).count()["reading_score"]
passing_reading_count
305/22:
passing_overall_count = passing_overall_stu.groupby(["school_name"]).count()["reading_score"]
passing_overall_count = passing_overall_count.rename(column={"school_name": "School Name"})
305/23:
passing_overall_count = passing_overall_stu.groupby(["school_name"]).count()["reading_score"]
passing_overall_count
305/24:
school_summary_df["% Passing Math"] = passing_math_count / school_summary_df["Student ID"] * 100
school_summary_df["% Passing Reading"] = passing_reading_count / school_summary_df["Student ID"] * 100
school_summary_df["% Passing Overall"] = passing_overall_count / school_summary_df["Student ID"] * 100
school_summary_df
305/25:
school_summary_df = school_summary_df.rename(columns={"budget": "Total School Budget",
                                                      "Student ID": "Total Students",
                                                     "math_score": "Average Math Score",
                                                      "reading_score": "Average Reading Score",
                                                     })
school_summary_df
305/26:
school_summary_df = school_summary_df.style.format({
                          'Total Students': '{:,}', 
                          "Total School Budget": "${:,}", 
                          "Per Student Budget": "${:.0f}",
                          'Average Math Score': "{:.1f}%", 
                          'Average Reading Score': "{:.1f}%", 
                          "% Passing Math": "{:.1f}%", 
                          "% Passing Reading": "{:.1f}%", 
                          "% Passing Overall": "{:.1f}%"
                         })
school_summary_df
305/27:
school_summary_df = school_summary_df.style.format({
                          'Total Students': '{:,}', 
                          "Total School Budget": "${:,}", 
                          "Per Student Budget": "${:.0f}",
                          'Average Math Score': "{:.1f}%", 
                          'Average Reading Score': "{:.1f}%", 
                          "% Passing Math": "{:.1f}%", 
                          "% Passing Reading": "{:.1f}%", 
                          "% Passing Overall": "{:.1f}%"
                         })
school_summary_df = school_summary_df.rename_axis("school_name": "School Name")
school_summary_df
305/28:
school_summary_df = school_summary_df.style.format({
                          'Total Students': '{:,}', 
                          "Total School Budget": "${:,}", 
                          "Per Student Budget": "${:.0f}",
                          'Average Math Score': "{:.1f}%", 
                          'Average Reading Score': "{:.1f}%", 
                          "% Passing Math": "{:.1f}%", 
                          "% Passing Reading": "{:.1f}%", 
                          "% Passing Overall": "{:.1f}%"
                         })
school_summary_df
306/1:
# Dependencies
import pandas as pd
import numpy as np
306/2:
# load in file
pycityschools_file = "Resources/schools_complete.csv"
student_file = "Resources/students_complete.csv"
306/3:
# Read School and Student Data File and store into Pandas DataFrames
pycityschools_data_df = pd.read_csv(pycityschools_file)
student_data_df = pd.read_csv(student_file)

pycityschools_data_df.head()
student_data_df.head()
306/4:
pycityschools_data_complete = pd.merge(student_data_df, pycityschools_data_df, 
                                       on=["school_name", "school_name"], how="left")

pycityschools_data_complete.head()
306/5:
# Count total schools
total_schools = len(pycityschools_data_complete["School ID"].unique())
total_schools
306/6:
# Calculate the total number of students
total_students = len(pycityschools_data_complete["student_name"].unique())
total_students
306/7:
# Calculate Total Budget
total_budget = pycityschools_data_df ["budget"].sum()
total_budget
306/8:
# Calculate the average math score
average_mathscore = round(pycityschools_data_complete["math_score"].mean(), 2)
average_mathscore
306/9:
# Calculate the average reading score
average_readingscore = round(pycityschools_data_complete["reading_score"].mean(), 2)
average_readingscore
306/10:
# Calculate the percentage of students with a passing math score (70 or greater)
unique_math_passing = len(pycityschools_data_complete.loc[pycityschools_data_complete["math_score"] >=70]["student_name"].unique())
percent_math_passing = round((unique_math_passing/total_students)*100, 2)
percent_math_passing
306/11:
# Calculate the percentage of students with a passing reading score (70 or greater)
unique_reading_passing = len(pycityschools_data_complete.loc[pycityschools_data_complete["reading_score"] >=70]["student_name"].unique())
percent_reading_passing = round((unique_reading_passing/total_students)*100, 2)
percent_reading_passing
306/12:
# Calculate the percentage of students who passed math and reading (% Overall Passing)
overall_passing = round((percent_reading_passing + percent_math_passing) / 2, 2)
overall_passing
306/13:
# Create a dataframe to hold the above results
summary_df = pd.DataFrame({"Total Schools": [total_schools],
                          "Total Students": [total_students],
                          "Total Budget": [total_budget],
                          "Average Math Score": [average_mathscore],
                          "Average Reading Score": [average_readingscore],
                          "% of Students with Passing Math Score": [percent_math_passing],
                          "% of Students with Passing Reading Score": [percent_reading_passing],
                          "% of Students with Passing Reading and Math Score": [overall_passing]
                          })
summary_df
306/14:
# Give the displayed data cleaner formatting
summary_df["Total Students"] = summary_df["Total Students"].map("{:,.2f}".format)
summary_df["Total Budget"] = summary_df["Total Budget"].map("${:,.2f}".format)
summary_df["Average Math Score"] = summary_df["Average Math Score"].map("{:,.2f}%".format)
summary_df["Average Reading Score"] = summary_df["Average Reading Score"].map("{:,.2f}%".format)
summary_df["% of Students with Passing Math Score"] = summary_df["% of Students with Passing Math Score"].map("{:,.2f}%".format)
summary_df["% of Students with Passing Reading Score"] = summary_df["% of Students with Passing Reading Score"].map("{:,.2f}%".format)
summary_df["% of Students with Passing Reading and Math Score"] = summary_df["% of Students with Passing Reading and Math Score"].map("{:,.2f}%".format)
summary_df
306/15:
# Create an overview table that summarizes key metrics about each school
    # School Name
# School Type
# Total Students
# Total School Budget
# Per Student Budget
# Average Math Score
# Average Reading Score
# % Passing Math (The percentage of students that passed math.)
# % Passing Reading (The percentage of students that passed reading.)
# % Overall Passing (The percentage of students that passed math and reading.)
    
# Create a dataframe to hold the above results
306/16:
schools_overview_group = pycityschools_data_complete.groupby(["school_name"])
school_summary_df = schools_overview_group.agg({"Student ID": "count",
                            "budget": "max",
                           "math_score": "mean",
                            "reading_score": "mean",
                           })
school_summary_df["Per Student Budget"] = school_summary_df["budget"] / school_summary_df["Student ID"]
school_summary_df
306/17:
# Passing Math Score
passing_math_stu = pycityschools_data_complete.loc[pycityschools_data_complete["math_score"] >=70]
passing_math_stu
306/18:
passing_overall_stu = pycityschools_data_complete.loc[(pycityschools_data_complete["math_score"] >=70)&
                                                  (pycityschools_data_complete["reading_score"] >=70)]
passing_overall_stu
306/19:
passing_reading_stu = pycityschools_data_complete.loc[pycityschools_data_complete["reading_score"] >=70]
passing_reading_stu
306/20:
passing_math_count = passing_math_stu.groupby(["school_name"]).count()["math_score"]
passing_math_count
306/21:
passing_reading_count = passing_reading_stu.groupby(["school_name"]).count()["reading_score"]
passing_reading_count
306/22:
passing_overall_count = passing_overall_stu.groupby(["school_name"]).count()["reading_score"]
passing_overall_count
306/23:
school_summary_df["% Passing Math"] = passing_math_count / school_summary_df["Student ID"] * 100
school_summary_df["% Passing Reading"] = passing_reading_count / school_summary_df["Student ID"] * 100
school_summary_df["% Passing Overall"] = passing_overall_count / school_summary_df["Student ID"] * 100
school_summary_df
306/24:
school_summary_df = school_summary_df.rename(columns={"budget": "Total School Budget",
                                                      "Student ID": "Total Students",
                                                     "math_score": "Average Math Score",
                                                      "reading_score": "Average Reading Score",
                                                     })
school_summary_df
306/25:
school_summary_df = school_summary_df.style.format({
                          'Total Students': '{:,}', 
                          "Total School Budget": "${:,}", 
                          "Per Student Budget": "${:.0f}",
                          'Average Math Score': "{:.1f}%", 
                          'Average Reading Score': "{:.1f}%", 
                          "% Passing Math": "{:.1f}%", 
                          "% Passing Reading": "{:.1f}%", 
                          "% Passing Overall": "{:.1f}%"
                         })
school_summary_df
306/26:
schools_overview_group = pycityschools_data_complete.groupby(["school_name"])
school_summary_df = schools_overview_group.agg({"Student ID": "count",
                            "budget": "max",
                           "math_score": "mean",
                            "reading_score": "mean",
                           })
school_summary_df["Per Student Budget"] = school_summary_df["budget"] / school_summary_df["Student ID"]
school_summary_df["School Type"] = pycityschools_data_complete.set_index("school_name")["type"]
school_summary_df
307/1:
# Dependencies
import pandas as pd
import numpy as np
307/2:
# load in file
pycityschools_file = "Resources/schools_complete.csv"
student_file = "Resources/students_complete.csv"
307/3:
# Read School and Student Data File and store into Pandas DataFrames
pycityschools_data_df = pd.read_csv(pycityschools_file)
student_data_df = pd.read_csv(student_file)

pycityschools_data_df.head()
student_data_df.head()
307/4:
pycityschools_data_complete = pd.merge(student_data_df, pycityschools_data_df, 
                                       on=["school_name", "school_name"], how="left")

pycityschools_data_complete.head()
307/5:
# Count total schools
total_schools = len(pycityschools_data_complete["School ID"].unique())
total_schools
307/6:
# Calculate the total number of students
total_students = len(pycityschools_data_complete["student_name"].unique())
total_students
307/7:
# Calculate Total Budget
total_budget = pycityschools_data_df ["budget"].sum()
total_budget
307/8:
# Calculate the average math score
average_mathscore = round(pycityschools_data_complete["math_score"].mean(), 2)
average_mathscore
307/9:
# Calculate the average reading score
average_readingscore = round(pycityschools_data_complete["reading_score"].mean(), 2)
average_readingscore
307/10:
# Calculate the percentage of students with a passing math score (70 or greater)
unique_math_passing = len(pycityschools_data_complete.loc[pycityschools_data_complete["math_score"] >=70]["student_name"].unique())
percent_math_passing = round((unique_math_passing/total_students)*100, 2)
percent_math_passing
307/11:
# Calculate the percentage of students with a passing reading score (70 or greater)
unique_reading_passing = len(pycityschools_data_complete.loc[pycityschools_data_complete["reading_score"] >=70]["student_name"].unique())
percent_reading_passing = round((unique_reading_passing/total_students)*100, 2)
percent_reading_passing
307/12:
# Calculate the percentage of students who passed math and reading (% Overall Passing)
overall_passing = round((percent_reading_passing + percent_math_passing) / 2, 2)
overall_passing
307/13:
# Create a dataframe to hold the above results
summary_df = pd.DataFrame({"Total Schools": [total_schools],
                          "Total Students": [total_students],
                          "Total Budget": [total_budget],
                          "Average Math Score": [average_mathscore],
                          "Average Reading Score": [average_readingscore],
                          "% of Students with Passing Math Score": [percent_math_passing],
                          "% of Students with Passing Reading Score": [percent_reading_passing],
                          "% of Students with Passing Reading and Math Score": [overall_passing]
                          })
summary_df
307/14:
# Give the displayed data cleaner formatting
summary_df["Total Students"] = summary_df["Total Students"].map("{:,.2f}".format)
summary_df["Total Budget"] = summary_df["Total Budget"].map("${:,.2f}".format)
summary_df["Average Math Score"] = summary_df["Average Math Score"].map("{:,.2f}%".format)
summary_df["Average Reading Score"] = summary_df["Average Reading Score"].map("{:,.2f}%".format)
summary_df["% of Students with Passing Math Score"] = summary_df["% of Students with Passing Math Score"].map("{:,.2f}%".format)
summary_df["% of Students with Passing Reading Score"] = summary_df["% of Students with Passing Reading Score"].map("{:,.2f}%".format)
summary_df["% of Students with Passing Reading and Math Score"] = summary_df["% of Students with Passing Reading and Math Score"].map("{:,.2f}%".format)
summary_df
307/15:
schools_overview_group = pycityschools_data_complete.groupby(["school_name"])
school_summary_df = schools_overview_group.agg({"Student ID": "count",
                            "budget": "max",
                           "math_score": "mean",
                            "reading_score": "mean",
                           })
school_summary_df["Per Student Budget"] = school_summary_df["budget"] / school_summary_df["Student ID"]
school_summary_df["School Type"] = pycityschools_data_complete.set_index("school_name")["type"]
school_summary_df
307/16:
schools_overview_group = pycityschools_data_complete.groupby(["school_name"])
school_summary_df = schools_overview_group.agg({"Student ID": "count",
                            "budget": "max",
                           "math_score": "mean",
                            "reading_score": "mean",
                           })
school_summary_df["Per Student Budget"] = school_summary_df["budget"] / school_summary_df["Student ID"]
school_summary_df
307/17:
# Passing Math Score
passing_math_stu = pycityschools_data_complete.loc[pycityschools_data_complete["math_score"] >=70]
passing_math_stu
307/18:
passing_overall_stu = pycityschools_data_complete.loc[(pycityschools_data_complete["math_score"] >=70)&
                                                  (pycityschools_data_complete["reading_score"] >=70)]
passing_overall_stu
307/19:
passing_reading_stu = pycityschools_data_complete.loc[pycityschools_data_complete["reading_score"] >=70]
passing_reading_stu
307/20:
passing_math_count = passing_math_stu.groupby(["school_name"]).count()["math_score"]
passing_math_count
307/21:
passing_reading_count = passing_reading_stu.groupby(["school_name"]).count()["reading_score"]
passing_reading_count
307/22:
passing_overall_count = passing_overall_stu.groupby(["school_name"]).count()["reading_score"]
passing_overall_count
307/23:
school_summary_df["% Passing Math"] = passing_math_count / school_summary_df["Student ID"] * 100
school_summary_df["% Passing Reading"] = passing_reading_count / school_summary_df["Student ID"] * 100
school_summary_df["% Passing Overall"] = passing_overall_count / school_summary_df["Student ID"] * 100
school_summary_df
307/24:
school_summary_df = school_summary_df.rename(columns={"budget": "Total School Budget",
                                                      "Student ID": "Total Students",
                                                     "math_score": "Average Math Score",
                                                      "reading_score": "Average Reading Score",
                                                     })
school_summary_df
307/25:
school_summary_df = school_summary_df.style.format({
                          'Total Students': '{:,}', 
                          "Total School Budget": "${:,}", 
                          "Per Student Budget": "${:.0f}",
                          'Average Math Score': "{:.1f}%", 
                          'Average Reading Score': "{:.1f}%", 
                          "% Passing Math": "{:.1f}%", 
                          "% Passing Reading": "{:.1f}%", 
                          "% Passing Overall": "{:.1f}%"
                         })
school_summary_df
307/26:
# School Type
school_type = pycityschools_data_complete.loc[pycityschools_data_complete["type"]]
school_type
307/27:
# Find top 5 performing schools based on % Overall Passing
top_five_schools = school_summary_df("% Overall Passing", ascending = False)
top_five_schools.head()
307/28:
# Find top 5 performing schools based on % Overall Passing
top_five_schools = school_summary_df("% Passing Overall", ascending = False)
top_five_schools.head()
307/29:
# Find top 5 performing schools based on % Overall Passing
top_five_schools = school_summary_df("% Passing Overall", ascending=False)
top_five_schools.head()
309/1:
# Dependencies
import pandas as pd
import numpy as np
309/2:
# load in file
pycityschools_file = "Resources/schools_complete.csv"
student_file = "Resources/students_complete.csv"
309/3:
# Read School and Student Data File and store into Pandas DataFrames
pycityschools_data_df = pd.read_csv(pycityschools_file)
student_data_df = pd.read_csv(student_file)

pycityschools_data_df.head()
student_data_df.head()
309/4:
pycityschools_data_complete = pd.merge(student_data_df, pycityschools_data_df, 
                                       on=["school_name", "school_name"], how="left")

pycityschools_data_complete.head()
309/5:
# Count total schools
total_schools = len(pycityschools_data_complete["School ID"].unique())
total_schools
309/6:
# Calculate the total number of students
total_students = len(pycityschools_data_complete["student_name"].unique())
total_students
309/7:
# Calculate Total Budget
total_budget = pycityschools_data_df ["budget"].sum()
total_budget
309/8:
# Calculate the average math score
average_mathscore = round(pycityschools_data_complete["math_score"].mean(), 2)
average_mathscore
309/9:
# Calculate the average reading score
average_readingscore = round(pycityschools_data_complete["reading_score"].mean(), 2)
average_readingscore
309/10:
# Calculate the percentage of students with a passing math score (70 or greater)
unique_math_passing = len(pycityschools_data_complete.loc[pycityschools_data_complete["math_score"] >=70]["student_name"].unique())
percent_math_passing = round((unique_math_passing/total_students)*100, 2)
percent_math_passing
309/11:
# Calculate the percentage of students with a passing reading score (70 or greater)
unique_reading_passing = len(pycityschools_data_complete.loc[pycityschools_data_complete["reading_score"] >=70]["student_name"].unique())
percent_reading_passing = round((unique_reading_passing/total_students)*100, 2)
percent_reading_passing
309/12:
# Calculate the percentage of students who passed math and reading (% Overall Passing)
overall_passing = round((percent_reading_passing + percent_math_passing) / 2, 2)
overall_passing
309/13:
# Create a dataframe to hold the above results
summary_df = pd.DataFrame({"Total Schools": [total_schools],
                          "Total Students": [total_students],
                          "Total Budget": [total_budget],
                          "Average Math Score": [average_mathscore],
                          "Average Reading Score": [average_readingscore],
                          "% of Students with Passing Math Score": [percent_math_passing],
                          "% of Students with Passing Reading Score": [percent_reading_passing],
                          "% of Students with Passing Reading and Math Score": [overall_passing]
                          })
summary_df
309/14:
# Give the displayed data cleaner formatting
summary_df["Total Students"] = summary_df["Total Students"].map("{:,.2f}".format)
summary_df["Total Budget"] = summary_df["Total Budget"].map("${:,.2f}".format)
summary_df["Average Math Score"] = summary_df["Average Math Score"].map("{:,.2f}%".format)
summary_df["Average Reading Score"] = summary_df["Average Reading Score"].map("{:,.2f}%".format)
summary_df["% of Students with Passing Math Score"] = summary_df["% of Students with Passing Math Score"].map("{:,.2f}%".format)
summary_df["% of Students with Passing Reading Score"] = summary_df["% of Students with Passing Reading Score"].map("{:,.2f}%".format)
summary_df["% of Students with Passing Reading and Math Score"] = summary_df["% of Students with Passing Reading and Math Score"].map("{:,.2f}%".format)
summary_df
309/15:
schools_overview_group = pycityschools_data_complete.groupby(["school_name"])
school_summary_df = schools_overview_group.agg({"Student ID": "count",
                            "budget": "max",
                           "math_score": "mean",
                            "reading_score": "mean",
                           })
school_summary_df["Per Student Budget"] = school_summary_df["budget"] / school_summary_df["Student ID"]
school_summary_df
309/16:
# Passing Math Score
passing_math_stu = pycityschools_data_complete.loc[pycityschools_data_complete["math_score"] >=70]
passing_math_stu
309/17:
passing_overall_stu = pycityschools_data_complete.loc[(pycityschools_data_complete["math_score"] >=70)&
                                                  (pycityschools_data_complete["reading_score"] >=70)]
passing_overall_stu
309/18:
passing_reading_stu = pycityschools_data_complete.loc[pycityschools_data_complete["reading_score"] >=70]
passing_reading_stu
309/19:
passing_math_count = passing_math_stu.groupby(["school_name"]).count()["math_score"]
passing_math_count
309/20:
passing_reading_count = passing_reading_stu.groupby(["school_name"]).count()["reading_score"]
passing_reading_count
309/21:
passing_overall_count = passing_overall_stu.groupby(["school_name"]).count()["reading_score"]
passing_overall_count
309/22:
school_summary_df["% Passing Math"] = passing_math_count / school_summary_df["Student ID"] * 100
school_summary_df["% Passing Reading"] = passing_reading_count / school_summary_df["Student ID"] * 100
school_summary_df["% Passing Overall"] = passing_overall_count / school_summary_df["Student ID"] * 100
school_summary_df
309/23:
school_summary_df = school_summary_df.rename(columns={"budget": "Total School Budget",
                                                      "Student ID": "Total Students",
                                                     "math_score": "Average Math Score",
                                                      "reading_score": "Average Reading Score",
                                                     })
school_summary_df
309/24:
school_summary_df = school_summary_df.style.format({
                          'Total Students': '{:,}', 
                          "Total School Budget": "${:,}", 
                          "Per Student Budget": "${:.0f}",
                          'Average Math Score': "{:.1f}%", 
                          'Average Reading Score': "{:.1f}%", 
                          "% Passing Math": "{:.1f}%", 
                          "% Passing Reading": "{:.1f}%", 
                          "% Passing Overall": "{:.1f}%"
                         })
school_summary_df
309/25:
# Create a table that highlights the top 5 performing schools based on % Overall Passing. Include:

# School Name
# School Type
# Total Students
# Total School Budget
# Per Student Budget
# Average Math Score
# Average Reading Score
# % Passing Math (The percentage of students that passed math.)
# % Passing Reading (The percentage of students that passed reading.)
# % Overall Passing (The percentage of students that passed math and reading.)
309/26:
# Find top 5 performing schools based on % Overall Passing
top_five_schools = school_summary_df("% Passing Overall", ascending=False)
top_five_schools.head()
309/27:
# Find top 5 performing schools based on % Overall Passing
top_five_schools = school_summary_df.sort_values("% Passing Overall", ascending=False)
top_five_schools.head()
309/28:
# Find top 5 performing schools based on % Overall Passing
top_five_schools = school_summary_df.sort_values("% Passing Overall", ascending = False)
top_five_schools.head()
310/1:
# Dependencies
import pandas as pd
import numpy as np
310/2:
# load in file
pycityschools_file = "Resources/schools_complete.csv"
student_file = "Resources/students_complete.csv"
310/3:
# Read School and Student Data File and store into Pandas DataFrames
pycityschools_data_df = pd.read_csv(pycityschools_file)
student_data_df = pd.read_csv(student_file)

pycityschools_data_df.head()
student_data_df.head()
310/4:
pycityschools_data_complete = pd.merge(student_data_df, pycityschools_data_df, 
                                       on=["school_name", "school_name"], how="left")

pycityschools_data_complete.head()
310/5:
# Count total schools
total_schools = len(pycityschools_data_complete["School ID"].unique())
total_schools
310/6:
# Calculate the total number of students
total_students = len(pycityschools_data_complete["student_name"].unique())
total_students
310/7:
# Calculate Total Budget
total_budget = pycityschools_data_df ["budget"].sum()
total_budget
310/8:
# Calculate the average math score
average_mathscore = round(pycityschools_data_complete["math_score"].mean(), 2)
average_mathscore
310/9:
# Calculate the average reading score
average_readingscore = round(pycityschools_data_complete["reading_score"].mean(), 2)
average_readingscore
310/10:
# Calculate the percentage of students with a passing math score (70 or greater)
unique_math_passing = len(pycityschools_data_complete.loc[pycityschools_data_complete["math_score"] >=70]["student_name"].unique())
percent_math_passing = round((unique_math_passing/total_students)*100, 2)
percent_math_passing
310/11:
# Calculate the percentage of students with a passing reading score (70 or greater)
unique_reading_passing = len(pycityschools_data_complete.loc[pycityschools_data_complete["reading_score"] >=70]["student_name"].unique())
percent_reading_passing = round((unique_reading_passing/total_students)*100, 2)
percent_reading_passing
310/12:
# Calculate the percentage of students who passed math and reading (% Overall Passing)
overall_passing = round((percent_reading_passing + percent_math_passing) / 2, 2)
overall_passing
310/13:
# Create a dataframe to hold the above results
summary_df = pd.DataFrame({"Total Schools": [total_schools],
                          "Total Students": [total_students],
                          "Total Budget": [total_budget],
                          "Average Math Score": [average_mathscore],
                          "Average Reading Score": [average_readingscore],
                          "% of Students with Passing Math Score": [percent_math_passing],
                          "% of Students with Passing Reading Score": [percent_reading_passing],
                          "% of Students with Passing Reading and Math Score": [overall_passing]
                          })
summary_df
310/14:
# Give the displayed data cleaner formatting
summary_df["Total Students"] = summary_df["Total Students"].map("{:,.2f}".format)
summary_df["Total Budget"] = summary_df["Total Budget"].map("${:,.2f}".format)
summary_df["Average Math Score"] = summary_df["Average Math Score"].map("{:,.2f}%".format)
summary_df["Average Reading Score"] = summary_df["Average Reading Score"].map("{:,.2f}%".format)
summary_df["% of Students with Passing Math Score"] = summary_df["% of Students with Passing Math Score"].map("{:,.2f}%".format)
summary_df["% of Students with Passing Reading Score"] = summary_df["% of Students with Passing Reading Score"].map("{:,.2f}%".format)
summary_df["% of Students with Passing Reading and Math Score"] = summary_df["% of Students with Passing Reading and Math Score"].map("{:,.2f}%".format)
summary_df
310/15:
schools_overview_group = pycityschools_data_complete.groupby(["school_name"])
school_summary_df = schools_overview_group.agg({"Student ID": "count",
                            "budget": "max",
                           "math_score": "mean",
                            "reading_score": "mean",
                           })
school_summary_df["Per Student Budget"] = school_summary_df["budget"] / school_summary_df["Student ID"]
school_summary_df
310/16:
# Passing Math Score
passing_math_stu = pycityschools_data_complete.loc[pycityschools_data_complete["math_score"] >=70]
passing_math_stu
310/17:
passing_overall_stu = pycityschools_data_complete.loc[(pycityschools_data_complete["math_score"] >=70)&
                                                  (pycityschools_data_complete["reading_score"] >=70)]
passing_overall_stu
310/18:
passing_reading_stu = pycityschools_data_complete.loc[pycityschools_data_complete["reading_score"] >=70]
passing_reading_stu
310/19:
passing_math_count = passing_math_stu.groupby(["school_name"]).count()["math_score"]
passing_math_count
310/20:
passing_reading_count = passing_reading_stu.groupby(["school_name"]).count()["reading_score"]
passing_reading_count
310/21:
passing_overall_count = passing_overall_stu.groupby(["school_name"]).count()["reading_score"]
passing_overall_count
310/22:
school_summary_df["% Passing Math"] = passing_math_count / school_summary_df["Student ID"] * 100
school_summary_df["% Passing Reading"] = passing_reading_count / school_summary_df["Student ID"] * 100
school_summary_df["% Passing Overall"] = passing_overall_count / school_summary_df["Student ID"] * 100
school_summary_df
310/23:
school_summary_df = school_summary_df.rename(columns={"budget": "Total School Budget",
                                                      "Student ID": "Total Students",
                                                     "math_score": "Average Math Score",
                                                      "reading_score": "Average Reading Score",
                                                     })
school_summary_df
310/24:
school_summary_df = school_summary_df.style.format({
                          'Total Students': '{:,}', 
                          "Total School Budget": "${:,}", 
                          "Per Student Budget": "${:.0f}",
                          'Average Math Score': "{:.1f}%", 
                          'Average Reading Score': "{:.1f}%", 
                          "% Passing Math": "{:.1f}%", 
                          "% Passing Reading": "{:.1f}%", 
                          "% Passing Overall": "{:.1f}%"
                         })
school_summary_df
310/25:
# Create a table that highlights the top 5 performing schools based on % Overall Passing. Include:

# School Name
# School Type
# Total Students
# Total School Budget
# Per Student Budget
# Average Math Score
# Average Reading Score
# % Passing Math (The percentage of students that passed math.)
# % Passing Reading (The percentage of students that passed reading.)
# % Overall Passing (The percentage of students that passed math and reading.)
310/26:
# Find top 5 performing schools based on % Overall Passing
top_five_schools = school_summary_df.sort_values("% Passing Overall", ascending = False)
top_five_schools.head()
310/27:
# Find top 5 performing schools based on % Overall Passing
top_five_schools = school_summary_df.loc[("% Passing Overall", ascending = False)]
top_five_schools.head()
310/28:
# Find top 5 performing schools based on % Overall Passing
top_five_schools = school_summary_df.loc[("% Passing Overall", ascending=False)]
top_five_schools.head()
311/1:
# Dependencies
import pandas as pd
import numpy as np
311/2:
# load in file
pycityschools_file = "Resources/schools_complete.csv"
student_file = "Resources/students_complete.csv"
311/3:
# Read School and Student Data File and store into Pandas DataFrames
pycityschools_data_df = pd.read_csv(pycityschools_file)
student_data_df = pd.read_csv(student_file)

pycityschools_data_df.head()
student_data_df.head()
311/4:
pycityschools_data_complete = pd.merge(student_data_df, pycityschools_data_df, 
                                       on=["school_name", "school_name"], how="left")

pycityschools_data_complete.head()
311/5:
# Count total schools
total_schools = len(pycityschools_data_complete["School ID"].unique())
total_schools
311/6:
# Calculate the total number of students
total_students = len(pycityschools_data_complete["student_name"].unique())
total_students
311/7:
# Calculate Total Budget
total_budget = pycityschools_data_df ["budget"].sum()
total_budget
311/8:
# Calculate the average math score
average_mathscore = round(pycityschools_data_complete["math_score"].mean(), 2)
average_mathscore
311/9:
# Calculate the average reading score
average_readingscore = round(pycityschools_data_complete["reading_score"].mean(), 2)
average_readingscore
311/10:
# Calculate the percentage of students with a passing math score (70 or greater)
unique_math_passing = len(pycityschools_data_complete.loc[pycityschools_data_complete["math_score"] >=70]["student_name"].unique())
percent_math_passing = round((unique_math_passing/total_students)*100, 2)
percent_math_passing
311/11:
# Calculate the percentage of students with a passing reading score (70 or greater)
unique_reading_passing = len(pycityschools_data_complete.loc[pycityschools_data_complete["reading_score"] >=70]["student_name"].unique())
percent_reading_passing = round((unique_reading_passing/total_students)*100, 2)
percent_reading_passing
311/12:
# Calculate the percentage of students who passed math and reading (% Overall Passing)
overall_passing = round((percent_reading_passing + percent_math_passing) / 2, 2)
overall_passing
311/13:
# Create a dataframe to hold the above results
summary_df = pd.DataFrame({"Total Schools": [total_schools],
                          "Total Students": [total_students],
                          "Total Budget": [total_budget],
                          "Average Math Score": [average_mathscore],
                          "Average Reading Score": [average_readingscore],
                          "% of Students with Passing Math Score": [percent_math_passing],
                          "% of Students with Passing Reading Score": [percent_reading_passing],
                          "% of Students with Passing Reading and Math Score": [overall_passing]
                          })
summary_df
311/14:
# Give the displayed data cleaner formatting
summary_df["Total Students"] = summary_df["Total Students"].map("{:,.2f}".format)
summary_df["Total Budget"] = summary_df["Total Budget"].map("${:,.2f}".format)
summary_df["Average Math Score"] = summary_df["Average Math Score"].map("{:,.2f}%".format)
summary_df["Average Reading Score"] = summary_df["Average Reading Score"].map("{:,.2f}%".format)
summary_df["% of Students with Passing Math Score"] = summary_df["% of Students with Passing Math Score"].map("{:,.2f}%".format)
summary_df["% of Students with Passing Reading Score"] = summary_df["% of Students with Passing Reading Score"].map("{:,.2f}%".format)
summary_df["% of Students with Passing Reading and Math Score"] = summary_df["% of Students with Passing Reading and Math Score"].map("{:,.2f}%".format)
summary_df
311/15:
schools_overview_group = pycityschools_data_complete.groupby(["school_name"])
school_summary_df = schools_overview_group.agg({"Student ID": "count",
                            "budget": "max",
                           "math_score": "mean",
                            "reading_score": "mean",
                           })
school_summary_df["Per Student Budget"] = school_summary_df["budget"] / school_summary_df["Student ID"]
school_summary_df
311/16:
# Passing Math Score
passing_math_stu = pycityschools_data_complete.loc[pycityschools_data_complete["math_score"] >=70]
passing_math_stu
311/17:
passing_overall_stu = pycityschools_data_complete.loc[(pycityschools_data_complete["math_score"] >=70)&
                                                  (pycityschools_data_complete["reading_score"] >=70)]
passing_overall_stu
311/18:
passing_reading_stu = pycityschools_data_complete.loc[pycityschools_data_complete["reading_score"] >=70]
passing_reading_stu
311/19:
passing_math_count = passing_math_stu.groupby(["school_name"]).count()["math_score"]
passing_math_count
311/20:
passing_reading_count = passing_reading_stu.groupby(["school_name"]).count()["reading_score"]
passing_reading_count
311/21:
passing_overall_count = passing_overall_stu.groupby(["school_name"]).count()["reading_score"]
passing_overall_count
311/22:
school_summary_df["% Passing Math"] = passing_math_count / school_summary_df["Student ID"] * 100
school_summary_df["% Passing Reading"] = passing_reading_count / school_summary_df["Student ID"] * 100
school_summary_df["% Passing Overall"] = passing_overall_count / school_summary_df["Student ID"] * 100
school_summary_df
311/23:
school_summary_df = school_summary_df.rename(columns={"budget": "Total School Budget",
                                                      "Student ID": "Total Students",
                                                     "math_score": "Average Math Score",
                                                      "reading_score": "Average Reading Score",
                                                     })
school_summary_df
311/24:
school_summary_df = school_summary_df.style.format({
                          'Total Students': '{:,}', 
                          "Total School Budget": "${:,}", 
                          "Per Student Budget": "${:.0f}",
                          'Average Math Score': "{:.1f}%", 
                          'Average Reading Score': "{:.1f}%", 
                          "% Passing Math": "{:.1f}%", 
                          "% Passing Reading": "{:.1f}%", 
                          "% Passing Overall": "{:.1f}%"
                         })
school_summary_df
311/25:
# Create a table that highlights the top 5 performing schools based on % Overall Passing. Include:

# School Name
# School Type
# Total Students
# Total School Budget
# Per Student Budget
# Average Math Score
# Average Reading Score
# % Passing Math (The percentage of students that passed math.)
# % Passing Reading (The percentage of students that passed reading.)
# % Overall Passing (The percentage of students that passed math and reading.)
311/26:
# Find top 5 performing schools based on % Overall Passing
top_five_schools = school_summary_df.loc[("% Passing Overall", ascending=False)]
top_five_schools.head()
311/27:
# Find top 5 performing schools based on % Overall Passing
top_five_schools = school_summary_df.sort_values("% Passing Overall")
top_five_schools.head()
312/1:
# Dependencies
import pandas as pd
import numpy as np
312/2:
# load in file
pycityschools_file = "Resources/schools_complete.csv"
student_file = "Resources/students_complete.csv"
312/3:
# Read School and Student Data File and store into Pandas DataFrames
pycityschools_data_df = pd.read_csv(pycityschools_file)
student_data_df = pd.read_csv(student_file)

pycityschools_data_df.head()
student_data_df.head()
312/4:
pycityschools_data_complete = pd.merge(student_data_df, pycityschools_data_df, 
                                       on=["school_name", "school_name"], how="left")

pycityschools_data_complete.head()
312/5:
# Count total schools
total_schools = len(pycityschools_data_complete["School ID"].unique())
total_schools
312/6:
# Calculate the total number of students
total_students = len(pycityschools_data_complete["student_name"].unique())
total_students
312/7:
# Calculate Total Budget
total_budget = pycityschools_data_df ["budget"].sum()
total_budget
312/8:
# Calculate the average math score
average_mathscore = round(pycityschools_data_complete["math_score"].mean(), 2)
average_mathscore
312/9:
# Calculate the average reading score
average_readingscore = round(pycityschools_data_complete["reading_score"].mean(), 2)
average_readingscore
312/10:
# Calculate the percentage of students with a passing math score (70 or greater)
unique_math_passing = len(pycityschools_data_complete.loc[pycityschools_data_complete["math_score"] >=70]["student_name"].unique())
percent_math_passing = round((unique_math_passing/total_students)*100, 2)
percent_math_passing
312/11:
# Calculate the percentage of students with a passing reading score (70 or greater)
unique_reading_passing = len(pycityschools_data_complete.loc[pycityschools_data_complete["reading_score"] >=70]["student_name"].unique())
percent_reading_passing = round((unique_reading_passing/total_students)*100, 2)
percent_reading_passing
312/12:
# Calculate the percentage of students who passed math and reading (% Overall Passing)
overall_passing = round((percent_reading_passing + percent_math_passing) / 2, 2)
overall_passing
312/13:
# Create a dataframe to hold the above results
summary_df = pd.DataFrame({"Total Schools": [total_schools],
                          "Total Students": [total_students],
                          "Total Budget": [total_budget],
                          "Average Math Score": [average_mathscore],
                          "Average Reading Score": [average_readingscore],
                          "% of Students with Passing Math Score": [percent_math_passing],
                          "% of Students with Passing Reading Score": [percent_reading_passing],
                          "% of Students with Passing Reading and Math Score": [overall_passing]
                          })
summary_df
312/14:
# Give the displayed data cleaner formatting
summary_df["Total Students"] = summary_df["Total Students"].map("{:,.2f}".format)
summary_df["Total Budget"] = summary_df["Total Budget"].map("${:,.2f}".format)
summary_df["Average Math Score"] = summary_df["Average Math Score"].map("{:,.2f}%".format)
summary_df["Average Reading Score"] = summary_df["Average Reading Score"].map("{:,.2f}%".format)
summary_df["% of Students with Passing Math Score"] = summary_df["% of Students with Passing Math Score"].map("{:,.2f}%".format)
summary_df["% of Students with Passing Reading Score"] = summary_df["% of Students with Passing Reading Score"].map("{:,.2f}%".format)
summary_df["% of Students with Passing Reading and Math Score"] = summary_df["% of Students with Passing Reading and Math Score"].map("{:,.2f}%".format)
summary_df
312/15:
schools_overview_group = pycityschools_data_complete.groupby(["school_name"])
school_summary_df = schools_overview_group.agg({"Student ID": "count",
                            "budget": "max",
                           "math_score": "mean",
                            "reading_score": "mean",
                           })
school_summary_df["Per Student Budget"] = school_summary_df["budget"] / school_summary_df["Student ID"]
school_summary_df
312/16:
# Passing Math Score
passing_math_stu = pycityschools_data_complete.loc[pycityschools_data_complete["math_score"] >=70]
passing_math_stu
312/17:
passing_overall_stu = pycityschools_data_complete.loc[(pycityschools_data_complete["math_score"] >=70)&
                                                  (pycityschools_data_complete["reading_score"] >=70)]
passing_overall_stu
312/18:
passing_reading_stu = pycityschools_data_complete.loc[pycityschools_data_complete["reading_score"] >=70]
passing_reading_stu
312/19:
passing_math_count = passing_math_stu.groupby(["school_name"]).count()["math_score"]
passing_math_count
312/20:
passing_reading_count = passing_reading_stu.groupby(["school_name"]).count()["reading_score"]
passing_reading_count
312/21:
passing_overall_count = passing_overall_stu.groupby(["school_name"]).count()["reading_score"]
passing_overall_count
312/22:
school_summary_df["% Passing Math"] = passing_math_count / school_summary_df["Student ID"] * 100
school_summary_df["% Passing Reading"] = passing_reading_count / school_summary_df["Student ID"] * 100
school_summary_df["% Passing Overall"] = passing_overall_count / school_summary_df["Student ID"] * 100
school_summary_df
312/23:
school_summary_df = school_summary_df.rename(columns={"budget": "Total School Budget",
                                                      "Student ID": "Total Students",
                                                     "math_score": "Average Math Score",
                                                      "reading_score": "Average Reading Score",
                                                     })
school_summary_df
312/24:
school_summary_df = school_summary_df.style.format({
                          'Total Students': '{:,}', 
                          "Total School Budget": "${:,}", 
                          "Per Student Budget": "${:.0f}",
                          'Average Math Score': "{:.1f}%", 
                          'Average Reading Score': "{:.1f}%", 
                          "% Passing Math": "{:.1f}%", 
                          "% Passing Reading": "{:.1f}%", 
                          "% Passing Overall": "{:.1f}%"
                         })
school_summary_df
312/25:
# Create a table that highlights the top 5 performing schools based on % Overall Passing. Include:

# School Name
# School Type
# Total Students
# Total School Budget
# Per Student Budget
# Average Math Score
# Average Reading Score
# % Passing Math (The percentage of students that passed math.)
# % Passing Reading (The percentage of students that passed reading.)
# % Overall Passing (The percentage of students that passed math and reading.)
312/26:
# Find top 5 performing schools based on % Overall Passing
top_five_schools = school_summary_df.sort_values("% Passing Overall")
top_five_schools.head()
312/27:
# Find top 5 performing schools based on % Overall Passing
top_five_schools = school_summary_df.sort_values("Overall Passing Rate", ascending = False)
top_five_schools
312/28:
# Find top 5 performing schools based on % Overall Passing
top_five_schools = school_summary_df.sort_values("% Passing Overall", ascending = False)
top_five_schools
313/1:
# Dependencies
import pandas as pd
import numpy as np
313/2:
# load in file
pycityschools_file = "Resources/schools_complete.csv"
student_file = "Resources/students_complete.csv"
313/3:
# Read School and Student Data File and store into Pandas DataFrames
pycityschools_data_df = pd.read_csv(pycityschools_file)
student_data_df = pd.read_csv(student_file)

pycityschools_data_df.head()
student_data_df.head()
313/4:
pycityschools_data_complete = pd.merge(student_data_df, pycityschools_data_df, 
                                       on=["school_name", "school_name"], how="left")

pycityschools_data_complete.head()
313/5:
# Count total schools
total_schools = len(pycityschools_data_complete["School ID"].unique())
total_schools
313/6:
# Calculate the total number of students
total_students = len(pycityschools_data_complete["student_name"].unique())
total_students
313/7:
# Calculate Total Budget
total_budget = pycityschools_data_df ["budget"].sum()
total_budget
313/8:
# Calculate the average math score
average_mathscore = round(pycityschools_data_complete["math_score"].mean(), 2)
average_mathscore
313/9:
# Calculate the average reading score
average_readingscore = round(pycityschools_data_complete["reading_score"].mean(), 2)
average_readingscore
313/10:
# Calculate the percentage of students with a passing math score (70 or greater)
unique_math_passing = len(pycityschools_data_complete.loc[pycityschools_data_complete["math_score"] >=70]["student_name"].unique())
percent_math_passing = round((unique_math_passing/total_students)*100, 2)
percent_math_passing
313/11:
# Calculate the percentage of students with a passing reading score (70 or greater)
unique_reading_passing = len(pycityschools_data_complete.loc[pycityschools_data_complete["reading_score"] >=70]["student_name"].unique())
percent_reading_passing = round((unique_reading_passing/total_students)*100, 2)
percent_reading_passing
313/12:
# Calculate the percentage of students who passed math and reading (% Overall Passing)
overall_passing = round((percent_reading_passing + percent_math_passing) / 2, 2)
overall_passing
313/13:
# Create a dataframe to hold the above results
summary_df = pd.DataFrame({"Total Schools": [total_schools],
                          "Total Students": [total_students],
                          "Total Budget": [total_budget],
                          "Average Math Score": [average_mathscore],
                          "Average Reading Score": [average_readingscore],
                          "% of Students with Passing Math Score": [percent_math_passing],
                          "% of Students with Passing Reading Score": [percent_reading_passing],
                          "% of Students with Passing Reading and Math Score": [overall_passing]
                          })
summary_df
313/14:
# Give the displayed data cleaner formatting
summary_df["Total Students"] = summary_df["Total Students"].map("{:,.2f}".format)
summary_df["Total Budget"] = summary_df["Total Budget"].map("${:,.2f}".format)
summary_df["Average Math Score"] = summary_df["Average Math Score"].map("{:,.2f}%".format)
summary_df["Average Reading Score"] = summary_df["Average Reading Score"].map("{:,.2f}%".format)
summary_df["% of Students with Passing Math Score"] = summary_df["% of Students with Passing Math Score"].map("{:,.2f}%".format)
summary_df["% of Students with Passing Reading Score"] = summary_df["% of Students with Passing Reading Score"].map("{:,.2f}%".format)
summary_df["% of Students with Passing Reading and Math Score"] = summary_df["% of Students with Passing Reading and Math Score"].map("{:,.2f}%".format)
summary_df
313/15:
schools_overview_group = pycityschools_data_complete.groupby(["school_name"])
school_summary_df = schools_overview_group.agg({"Student ID": "count",
                            "budget": "max",
                           "math_score": "mean",
                            "reading_score": "mean",
                           })
school_summary_df["Per Student Budget"] = school_summary_df["budget"] / school_summary_df["Student ID"]
school_summary_df
313/16:
# Passing Math Score
passing_math_stu = pycityschools_data_complete.loc[pycityschools_data_complete["math_score"] >=70]
passing_math_stu
313/17:
passing_overall_stu = pycityschools_data_complete.loc[(pycityschools_data_complete["math_score"] >=70)&
                                                  (pycityschools_data_complete["reading_score"] >=70)]
passing_overall_stu
313/18:
passing_reading_stu = pycityschools_data_complete.loc[pycityschools_data_complete["reading_score"] >=70]
passing_reading_stu
313/19:
passing_math_count = passing_math_stu.groupby(["school_name"]).count()["math_score"]
passing_math_count
313/20:
passing_reading_count = passing_reading_stu.groupby(["school_name"]).count()["reading_score"]
passing_reading_count
313/21:
passing_overall_count = passing_overall_stu.groupby(["school_name"]).count()["reading_score"]
passing_overall_count
313/22:
school_summary_df["% Passing Math"] = passing_math_count / school_summary_df["Student ID"] * 100
school_summary_df["% Passing Reading"] = passing_reading_count / school_summary_df["Student ID"] * 100
school_summary_df["% Passing Overall"] = passing_overall_count / school_summary_df["Student ID"] * 100
school_summary_df
313/23:
school_summary_df = school_summary_df.rename(columns={"budget": "Total School Budget",
                                                      "Student ID": "Total Students",
                                                     "math_score": "Average Math Score",
                                                      "reading_score": "Average Reading Score",
                                                     })
school_summary_df
313/24:
school_summary_df = school_summary_df.style.format({
                          'Total Students': '{:,}', 
                          "Total School Budget": "${:,}", 
                          "Per Student Budget": "${:.0f}",
                          'Average Math Score': "{:.1f}%", 
                          'Average Reading Score': "{:.1f}%", 
                          "% Passing Math": "{:.1f}%", 
                          "% Passing Reading": "{:.1f}%", 
                          "% Passing Overall": "{:.1f}%"
                         })
school_summary_df
313/25:
# Create a table that highlights the top 5 performing schools based on % Overall Passing. Include:

# School Name
# School Type
# Total Students
# Total School Budget
# Per Student Budget
# Average Math Score
# Average Reading Score
# % Passing Math (The percentage of students that passed math.)
# % Passing Reading (The percentage of students that passed reading.)
# % Overall Passing (The percentage of students that passed math and reading.)
313/26:
# Find top 5 performing schools based on % Overall Passing
top_five_schools = school_summary_df.sort_values("% Passing Overall", ascending = False)
top_five_schools
313/27:
# Find top 5 performing schools based on % Overall Passing
top_five_schools = school_summary_df.sort_values(by="% Passing Overall", ascending = False)
top_five_schools
313/28:
# Find top 5 performing schools based on % Overall Passing
top_five_schools = school_summary_df.sort(by="% Passing Overall", ascending = False)
top_five_schools
314/1:
# Dependencies
import pandas as pd
import numpy as np
314/2:
# load in file
pycityschools_file = "Resources/schools_complete.csv"
student_file = "Resources/students_complete.csv"
314/3:
# Read School and Student Data File and store into Pandas DataFrames
pycityschools_data_df = pd.read_csv(pycityschools_file)
student_data_df = pd.read_csv(student_file)

pycityschools_data_df.head()
student_data_df.head()
314/4:
pycityschools_data_complete = pd.merge(student_data_df, pycityschools_data_df, 
                                       on=["school_name", "school_name"], how="left")

pycityschools_data_complete.head()
314/5:
# Count total schools
total_schools = len(pycityschools_data_complete["School ID"].unique())
total_schools
314/6:
# Calculate the total number of students
total_students = len(pycityschools_data_complete["student_name"].unique())
total_students
314/7:
# Calculate Total Budget
total_budget = pycityschools_data_df ["budget"].sum()
total_budget
314/8:
# Calculate the average math score
average_mathscore = round(pycityschools_data_complete["math_score"].mean(), 2)
average_mathscore
314/9:
# Calculate the average reading score
average_readingscore = round(pycityschools_data_complete["reading_score"].mean(), 2)
average_readingscore
314/10:
# Calculate the percentage of students with a passing math score (70 or greater)
unique_math_passing = len(pycityschools_data_complete.loc[pycityschools_data_complete["math_score"] >=70]["student_name"].unique())
percent_math_passing = round((unique_math_passing/total_students)*100, 2)
percent_math_passing
314/11:
# Calculate the percentage of students with a passing reading score (70 or greater)
unique_reading_passing = len(pycityschools_data_complete.loc[pycityschools_data_complete["reading_score"] >=70]["student_name"].unique())
percent_reading_passing = round((unique_reading_passing/total_students)*100, 2)
percent_reading_passing
314/12:
# Calculate the percentage of students who passed math and reading (% Overall Passing)
overall_passing = round((percent_reading_passing + percent_math_passing) / 2, 2)
overall_passing
314/13:
# Create a dataframe to hold the above results
summary_df = pd.DataFrame({"Total Schools": [total_schools],
                          "Total Students": [total_students],
                          "Total Budget": [total_budget],
                          "Average Math Score": [average_mathscore],
                          "Average Reading Score": [average_readingscore],
                          "% of Students with Passing Math Score": [percent_math_passing],
                          "% of Students with Passing Reading Score": [percent_reading_passing],
                          "% of Students with Passing Reading and Math Score": [overall_passing]
                          })
summary_df
314/14:
# Give the displayed data cleaner formatting
summary_df["Total Students"] = summary_df["Total Students"].map("{:,.2f}".format)
summary_df["Total Budget"] = summary_df["Total Budget"].map("${:,.2f}".format)
summary_df["Average Math Score"] = summary_df["Average Math Score"].map("{:,.2f}%".format)
summary_df["Average Reading Score"] = summary_df["Average Reading Score"].map("{:,.2f}%".format)
summary_df["% of Students with Passing Math Score"] = summary_df["% of Students with Passing Math Score"].map("{:,.2f}%".format)
summary_df["% of Students with Passing Reading Score"] = summary_df["% of Students with Passing Reading Score"].map("{:,.2f}%".format)
summary_df["% of Students with Passing Reading and Math Score"] = summary_df["% of Students with Passing Reading and Math Score"].map("{:,.2f}%".format)
summary_df
314/15:
schools_overview_group = pycityschools_data_complete.groupby(["school_name"])
school_summary_df = schools_overview_group.agg({"Student ID": "count",
                            "budget": "max",
                           "math_score": "mean",
                            "reading_score": "mean",
                           })
school_summary_df["Per Student Budget"] = school_summary_df["budget"] / school_summary_df["Student ID"]
school_summary_df
314/16:
# Passing Math Score
passing_math_stu = pycityschools_data_complete.loc[pycityschools_data_complete["math_score"] >=70]
passing_math_stu
314/17:
passing_overall_stu = pycityschools_data_complete.loc[(pycityschools_data_complete["math_score"] >=70)&
                                                  (pycityschools_data_complete["reading_score"] >=70)]
passing_overall_stu
314/18:
passing_reading_stu = pycityschools_data_complete.loc[pycityschools_data_complete["reading_score"] >=70]
passing_reading_stu
314/19:
passing_math_count = passing_math_stu.groupby(["school_name"]).count()["math_score"]
passing_math_count
314/20:
passing_reading_count = passing_reading_stu.groupby(["school_name"]).count()["reading_score"]
passing_reading_count
314/21:
passing_overall_count = passing_overall_stu.groupby(["school_name"]).count()["reading_score"]
passing_overall_count
314/22:
school_summary_df["% Passing Math"] = passing_math_count / school_summary_df["Student ID"] * 100
school_summary_df["% Passing Reading"] = passing_reading_count / school_summary_df["Student ID"] * 100
school_summary_df["% Passing Overall"] = passing_overall_count / school_summary_df["Student ID"] * 100
school_summary_df
314/23:
school_summary_df = school_summary_df.rename(columns={"budget": "Total School Budget",
                                                      "Student ID": "Total Students",
                                                     "math_score": "Average Math Score",
                                                      "reading_score": "Average Reading Score",
                                                     })
school_summary_df
314/24:
school_summary_df = school_summary_df.style.format({
                          'Total Students': '{:,}', 
                          "Total School Budget": "${:,}", 
                          "Per Student Budget": "${:.0f}",
                          'Average Math Score': "{:.1f}%", 
                          'Average Reading Score': "{:.1f}%", 
                          "% Passing Math": "{:.1f}%", 
                          "% Passing Reading": "{:.1f}%", 
                          "% Passing Overall": "{:.1f}%"
                         })
school_summary_df
314/25:
# Create a table that highlights the top 5 performing schools based on % Overall Passing. Include:

# School Name
# School Type
# Total Students
# Total School Budget
# Per Student Budget
# Average Math Score
# Average Reading Score
# % Passing Math (The percentage of students that passed math.)
# % Passing Reading (The percentage of students that passed reading.)
# % Overall Passing (The percentage of students that passed math and reading.)
314/26:
# Find top 5 performing schools based on % Overall Passing
top_five_schools = school_summary_df.sort(by="% Passing Overall", ascending = False)
top_five_schools
314/27:
# Find top 5 performing schools based on % Overall Passing
top_five_schools = school_summary_df.sort("% Passing Overall", ascending = False)
top_five_schools
315/1:
# Dependencies
import pandas as pd
import numpy as np
315/2:
# load in file
pycityschools_file = "Resources/schools_complete.csv"
student_file = "Resources/students_complete.csv"
315/3:
# Read School and Student Data File and store into Pandas DataFrames
pycityschools_data_df = pd.read_csv(pycityschools_file)
student_data_df = pd.read_csv(student_file)

pycityschools_data_df.head()
student_data_df.head()
315/4:
pycityschools_data_complete = pd.merge(student_data_df, pycityschools_data_df, 
                                       on=["school_name", "school_name"], how="left")

pycityschools_data_complete.head()
315/5:
# Count total schools
total_schools = len(pycityschools_data_complete["School ID"].unique())
total_schools
315/6:
# Calculate the total number of students
total_students = len(pycityschools_data_complete["student_name"].unique())
total_students
315/7:
# Calculate Total Budget
total_budget = pycityschools_data_df ["budget"].sum()
total_budget
315/8:
# Calculate the average math score
average_mathscore = round(pycityschools_data_complete["math_score"].mean(), 2)
average_mathscore
315/9:
# Calculate the average reading score
average_readingscore = round(pycityschools_data_complete["reading_score"].mean(), 2)
average_readingscore
315/10:
# Calculate the percentage of students with a passing math score (70 or greater)
unique_math_passing = len(pycityschools_data_complete.loc[pycityschools_data_complete["math_score"] >=70]["student_name"].unique())
percent_math_passing = round((unique_math_passing/total_students)*100, 2)
percent_math_passing
315/11:
# Calculate the percentage of students with a passing reading score (70 or greater)
unique_reading_passing = len(pycityschools_data_complete.loc[pycityschools_data_complete["reading_score"] >=70]["student_name"].unique())
percent_reading_passing = round((unique_reading_passing/total_students)*100, 2)
percent_reading_passing
315/12:
# Calculate the percentage of students who passed math and reading (% Overall Passing)
overall_passing = round((percent_reading_passing + percent_math_passing) / 2, 2)
overall_passing
315/13:
# Create a dataframe to hold the above results
summary_df = pd.DataFrame({"Total Schools": [total_schools],
                          "Total Students": [total_students],
                          "Total Budget": [total_budget],
                          "Average Math Score": [average_mathscore],
                          "Average Reading Score": [average_readingscore],
                          "% of Students with Passing Math Score": [percent_math_passing],
                          "% of Students with Passing Reading Score": [percent_reading_passing],
                          "% of Students with Passing Reading and Math Score": [overall_passing]
                          })
summary_df
315/14:
# Give the displayed data cleaner formatting
summary_df["Total Students"] = summary_df["Total Students"].map("{:,.2f}".format)
summary_df["Total Budget"] = summary_df["Total Budget"].map("${:,.2f}".format)
summary_df["Average Math Score"] = summary_df["Average Math Score"].map("{:,.2f}%".format)
summary_df["Average Reading Score"] = summary_df["Average Reading Score"].map("{:,.2f}%".format)
summary_df["% of Students with Passing Math Score"] = summary_df["% of Students with Passing Math Score"].map("{:,.2f}%".format)
summary_df["% of Students with Passing Reading Score"] = summary_df["% of Students with Passing Reading Score"].map("{:,.2f}%".format)
summary_df["% of Students with Passing Reading and Math Score"] = summary_df["% of Students with Passing Reading and Math Score"].map("{:,.2f}%".format)
summary_df
315/15:
schools_overview_group = pycityschools_data_complete.groupby(["school_name"])
school_summary_df = schools_overview_group.agg({"Student ID": "count",
                            "budget": "max",
                           "math_score": "mean",
                            "reading_score": "mean",
                           })
school_summary_df["Per Student Budget"] = school_summary_df["budget"] / school_summary_df["Student ID"]
school_summary_df
315/16:
# Passing Math Score
passing_math_stu = pycityschools_data_complete.loc[pycityschools_data_complete["math_score"] >=70]
passing_math_stu
315/17:
passing_overall_stu = pycityschools_data_complete.loc[(pycityschools_data_complete["math_score"] >=70)&
                                                  (pycityschools_data_complete["reading_score"] >=70)]
passing_overall_stu
315/18:
passing_reading_stu = pycityschools_data_complete.loc[pycityschools_data_complete["reading_score"] >=70]
passing_reading_stu
315/19:
passing_math_count = passing_math_stu.groupby(["school_name"]).count()["math_score"]
passing_math_count
315/20:
passing_reading_count = passing_reading_stu.groupby(["school_name"]).count()["reading_score"]
passing_reading_count
315/21:
passing_overall_count = passing_overall_stu.groupby(["school_name"]).count()["reading_score"]
passing_overall_count
315/22:
school_summary_df["% Passing Math"] = passing_math_count / school_summary_df["Student ID"] * 100
school_summary_df["% Passing Reading"] = passing_reading_count / school_summary_df["Student ID"] * 100
school_summary_df["% Passing Overall"] = passing_overall_count / school_summary_df["Student ID"] * 100
school_summary_df
315/23:
school_summary_df = school_summary_df.rename(columns={"budget": "Total School Budget",
                                                      "Student ID": "Total Students",
                                                     "math_score": "Average Math Score",
                                                      "reading_score": "Average Reading Score",
                                                     })
school_summary_df
315/24:
school_summary_df = school_summary_df.style.format({
                          'Total Students': '{:,}', 
                          "Total School Budget": "${:,}", 
                          "Per Student Budget": "${:.0f}",
                          'Average Math Score': "{:.1f}%", 
                          'Average Reading Score': "{:.1f}%", 
                          "% Passing Math": "{:.1f}%", 
                          "% Passing Reading": "{:.1f}%", 
                          "% Passing Overall": "{:.1f}%"
                         })
school_summary_df
315/25:
# Create a table that highlights the top 5 performing schools based on % Overall Passing. Include:

# School Name
# School Type
# Total Students
# Total School Budget
# Per Student Budget
# Average Math Score
# Average Reading Score
# % Passing Math (The percentage of students that passed math.)
# % Passing Reading (The percentage of students that passed reading.)
# % Overall Passing (The percentage of students that passed math and reading.)
315/26:
# Find top 5 performing schools based on % Overall Passing
top_five_schools = school_summary_df.sort("% Passing Overall", ascending = False)
top_five_schools.head()
315/27:
# Find top 5 performing schools based on % Overall Passing
top_five_schools = school_summary_df.sort_index("% Passing Overall", ascending = False)
top_five_schools.head()
315/28:
# Find top 5 performing schools based on % Overall Passing
top_five_schools = school_summary_df.sort_values("% Passing Overall", ascending = False)
top_five_schools.head()
315/29:
# Find top 5 performing schools based on % Overall Passing
top_five_schools = school_summary_df.sort_values("% Passing Overall", ascending = False)
top_five_schools.head()
315/30:
# Find top 5 performing schools based on % Overall Passing
top_five_schools = school_summary.sort_values("% Passing Overall", ascending = False)
top_five_schools.head()
315/31:
# Find top 5 performing schools based on % Overall Passing
top_five_schools = school_summary_df.sort_values("% Passing Overall", ascending = False)
top_five_schools.head()
315/32:
# Find top 5 performing schools based on % Overall Passing
top_five_schools = school_summary_df.sort("% Passing Overall", ascending = False)
top_five_schools.head()
315/33:
# Create a table that highlights the bottom 5 performing schools based on % Overall Passing. Include all of the same metrics as above.
bottom_five = school_summary_df.sort_value("% Passing Overall")
bottom_five
315/34:
# Find top 5 performing schools based on % Overall Passing
top_five_schools = school_summary_df.sort_value(by=["% Passing Overall"], ascending = False)
top_five_schools.head()
315/35:
# Find top 5 performing schools based on % Overall Passing
top_five_schools = school_summary_df.sort_values(by=["% Passing Overall"], ascending = False)
top_five_schools.head()
316/1:
# Dependencies
import pandas as pd
import numpy as np
316/2:
# load in file
pycityschools_file = "Resources/schools_complete.csv"
student_file = "Resources/students_complete.csv"
316/3:
# Read School and Student Data File and store into Pandas DataFrames
pycityschools_data_df = pd.read_csv(pycityschools_file)
student_data_df = pd.read_csv(student_file)

pycityschools_data_df.head()
student_data_df.head()
316/4:
pycityschools_data_complete = pd.merge(student_data_df, pycityschools_data_df, 
                                       on=["school_name", "school_name"], how="left")

pycityschools_data_complete.head()
316/5:
# Count total schools
total_schools = len(pycityschools_data_complete["School ID"].unique())
total_schools
316/6:
# Calculate the total number of students
total_students = len(pycityschools_data_complete["student_name"].unique())
total_students
316/7:
# Calculate Total Budget
total_budget = pycityschools_data_df ["budget"].sum()
total_budget
316/8:
# Calculate the average math score
average_mathscore = round(pycityschools_data_complete["math_score"].mean(), 2)
average_mathscore
316/9:
# Calculate the average reading score
average_readingscore = round(pycityschools_data_complete["reading_score"].mean(), 2)
average_readingscore
316/10:
# Calculate the percentage of students with a passing math score (70 or greater)
unique_math_passing = len(pycityschools_data_complete.loc[pycityschools_data_complete["math_score"] >=70]["student_name"].unique())
percent_math_passing = round((unique_math_passing/total_students)*100, 2)
percent_math_passing
316/11:
# Calculate the percentage of students with a passing reading score (70 or greater)
unique_reading_passing = len(pycityschools_data_complete.loc[pycityschools_data_complete["reading_score"] >=70]["student_name"].unique())
percent_reading_passing = round((unique_reading_passing/total_students)*100, 2)
percent_reading_passing
316/12:
# Calculate the percentage of students who passed math and reading (% Overall Passing)
overall_passing = round((percent_reading_passing + percent_math_passing) / 2, 2)
overall_passing
316/13:
# Create a dataframe to hold the above results
summary_df = pd.DataFrame({"Total Schools": [total_schools],
                          "Total Students": [total_students],
                          "Total Budget": [total_budget],
                          "Average Math Score": [average_mathscore],
                          "Average Reading Score": [average_readingscore],
                          "% of Students with Passing Math Score": [percent_math_passing],
                          "% of Students with Passing Reading Score": [percent_reading_passing],
                          "% of Students with Passing Reading and Math Score": [overall_passing]
                          })
summary_df
316/14:
# Give the displayed data cleaner formatting
summary_df["Total Students"] = summary_df["Total Students"].map("{:,.2f}".format)
summary_df["Total Budget"] = summary_df["Total Budget"].map("${:,.2f}".format)
summary_df["Average Math Score"] = summary_df["Average Math Score"].map("{:,.2f}%".format)
summary_df["Average Reading Score"] = summary_df["Average Reading Score"].map("{:,.2f}%".format)
summary_df["% of Students with Passing Math Score"] = summary_df["% of Students with Passing Math Score"].map("{:,.2f}%".format)
summary_df["% of Students with Passing Reading Score"] = summary_df["% of Students with Passing Reading Score"].map("{:,.2f}%".format)
summary_df["% of Students with Passing Reading and Math Score"] = summary_df["% of Students with Passing Reading and Math Score"].map("{:,.2f}%".format)
summary_df
316/15:
schools_overview_group = pycityschools_data_complete.groupby(["school_name"])
school_summary_df = schools_overview_group.agg({"Student ID": "count",
                            "budget": "max",
                           "math_score": "mean",
                            "reading_score": "mean",
                           })
school_summary_df["Per Student Budget"] = school_summary_df["budget"] / school_summary_df["Student ID"]
school_summary_df
316/16:
# Passing Math Score
passing_math_stu = pycityschools_data_complete.loc[pycityschools_data_complete["math_score"] >=70]
passing_math_stu
316/17:
passing_overall_stu = pycityschools_data_complete.loc[(pycityschools_data_complete["math_score"] >=70)&
                                                  (pycityschools_data_complete["reading_score"] >=70)]
passing_overall_stu
316/18:
passing_reading_stu = pycityschools_data_complete.loc[pycityschools_data_complete["reading_score"] >=70]
passing_reading_stu
316/19:
passing_math_count = passing_math_stu.groupby(["school_name"]).count()["math_score"]
passing_math_count
316/20:
passing_reading_count = passing_reading_stu.groupby(["school_name"]).count()["reading_score"]
passing_reading_count
316/21:
passing_overall_count = passing_overall_stu.groupby(["school_name"]).count()["reading_score"]
passing_overall_count
316/22:
school_summary_df["% Passing Math"] = passing_math_count / school_summary_df["Student ID"] * 100
school_summary_df["% Passing Reading"] = passing_reading_count / school_summary_df["Student ID"] * 100
school_summary_df["% Passing Overall"] = passing_overall_count / school_summary_df["Student ID"] * 100
school_summary_df
316/23:
school_summary_df = school_summary_df.rename(columns={"budget": "Total School Budget",
                                                      "Student ID": "Total Students",
                                                     "math_score": "Average Math Score",
                                                      "reading_score": "Average Reading Score",
                                                     })
school_summary_df
316/24:
school_summary_df = school_summary_df.style.format({
                          'Total Students': '{:,}', 
                          "Total School Budget": "${:,}", 
                          "Per Student Budget": "${:.0f}",
                          'Average Math Score': "{:.1f}%", 
                          'Average Reading Score': "{:.1f}%", 
                          "% Passing Math": "{:.1f}%", 
                          "% Passing Reading": "{:.1f}%", 
                          "% Passing Overall": "{:.1f}%"
                         })
school_summary_df
316/25:
# Create a table that highlights the top 5 performing schools based on % Overall Passing. Include:

# School Name
# School Type
# Total Students
# Total School Budget
# Per Student Budget
# Average Math Score
# Average Reading Score
# % Passing Math (The percentage of students that passed math.)
# % Passing Reading (The percentage of students that passed reading.)
# % Overall Passing (The percentage of students that passed math and reading.)
316/26:
# Find top 5 performing schools based on % Overall Passing
top_five_schools = school_summary_df.sort_values(by=["% Passing Overall"], ascending = False)
top_five_schools.head()
317/1:
# Dependencies
import pandas as pd
import numpy as np
317/2:
# load in file
pycityschools_file = "Resources/schools_complete.csv"
student_file = "Resources/students_complete.csv"
317/3:
# Read School and Student Data File and store into Pandas DataFrames
pycityschools_data_df = pd.read_csv(pycityschools_file)
student_data_df = pd.read_csv(student_file)

pycityschools_data_df.head()
student_data_df.head()
317/4:
pycityschools_data_complete = pd.merge(student_data_df, pycityschools_data_df, 
                                       on=["school_name", "school_name"], how="left")

pycityschools_data_complete.head()
317/5:
# Count total schools
total_schools = len(pycityschools_data_complete["School ID"].unique())
total_schools
317/6:
# Calculate the total number of students
total_students = len(pycityschools_data_complete["student_name"].unique())
total_students
317/7:
# Calculate Total Budget
total_budget = pycityschools_data_df ["budget"].sum()
total_budget
317/8:
# Calculate the average math score
average_mathscore = round(pycityschools_data_complete["math_score"].mean(), 2)
average_mathscore
317/9:
# Calculate the average reading score
average_readingscore = round(pycityschools_data_complete["reading_score"].mean(), 2)
average_readingscore
317/10:
# Calculate the percentage of students with a passing math score (70 or greater)
unique_math_passing = len(pycityschools_data_complete.loc[pycityschools_data_complete["math_score"] >=70]["student_name"].unique())
percent_math_passing = round((unique_math_passing/total_students)*100, 2)
percent_math_passing
317/11:
# Calculate the percentage of students with a passing reading score (70 or greater)
unique_reading_passing = len(pycityschools_data_complete.loc[pycityschools_data_complete["reading_score"] >=70]["student_name"].unique())
percent_reading_passing = round((unique_reading_passing/total_students)*100, 2)
percent_reading_passing
317/12:
# Calculate the percentage of students who passed math and reading (% Overall Passing)
overall_passing = round((percent_reading_passing + percent_math_passing) / 2, 2)
overall_passing
317/13:
# Create a dataframe to hold the above results
summary_df = pd.DataFrame({"Total Schools": [total_schools],
                          "Total Students": [total_students],
                          "Total Budget": [total_budget],
                          "Average Math Score": [average_mathscore],
                          "Average Reading Score": [average_readingscore],
                          "% of Students with Passing Math Score": [percent_math_passing],
                          "% of Students with Passing Reading Score": [percent_reading_passing],
                          "% of Students with Passing Reading and Math Score": [overall_passing]
                          })
summary_df
317/14:
# Give the displayed data cleaner formatting
summary_df["Total Students"] = summary_df["Total Students"].map("{:,.2f}".format)
summary_df["Total Budget"] = summary_df["Total Budget"].map("${:,.2f}".format)
summary_df["Average Math Score"] = summary_df["Average Math Score"].map("{:,.2f}%".format)
summary_df["Average Reading Score"] = summary_df["Average Reading Score"].map("{:,.2f}%".format)
summary_df["% of Students with Passing Math Score"] = summary_df["% of Students with Passing Math Score"].map("{:,.2f}%".format)
summary_df["% of Students with Passing Reading Score"] = summary_df["% of Students with Passing Reading Score"].map("{:,.2f}%".format)
summary_df["% of Students with Passing Reading and Math Score"] = summary_df["% of Students with Passing Reading and Math Score"].map("{:,.2f}%".format)
summary_df
317/15:
schools_overview_group = pycityschools_data_complete.groupby(["school_name"])
school_summary_df = schools_overview_group.agg({"Student ID": "count",
                            "budget": "max",
                           "math_score": "mean",
                            "reading_score": "mean",
                           })
school_summary_df["Per Student Budget"] = school_summary_df["budget"] / school_summary_df["Student ID"]
school_summary_df
317/16:
# Passing Math Score
passing_math_stu = pycityschools_data_complete.loc[pycityschools_data_complete["math_score"] >=70]
passing_math_stu
317/17:
passing_overall_stu = pycityschools_data_complete.loc[(pycityschools_data_complete["math_score"] >=70)&
                                                  (pycityschools_data_complete["reading_score"] >=70)]
passing_overall_stu
317/18:
passing_reading_stu = pycityschools_data_complete.loc[pycityschools_data_complete["reading_score"] >=70]
passing_reading_stu
317/19:
passing_math_count = passing_math_stu.groupby(["school_name"]).count()["math_score"]
passing_math_count
317/20:
passing_reading_count = passing_reading_stu.groupby(["school_name"]).count()["reading_score"]
passing_reading_count
317/21:
passing_overall_count = passing_overall_stu.groupby(["school_name"]).count()["reading_score"]
passing_overall_count
317/22:
school_summary_df["% Passing Math"] = passing_math_count / school_summary_df["Student ID"] * 100
school_summary_df["% Passing Reading"] = passing_reading_count / school_summary_df["Student ID"] * 100
school_summary_df["% Passing Overall"] = passing_overall_count / school_summary_df["Student ID"] * 100
school_summary_df
317/23:
school_summary_df = school_summary_df.rename(columns={"budget": "Total School Budget",
                                                      "Student ID": "Total Students",
                                                     "math_score": "Average Math Score",
                                                      "reading_score": "Average Reading Score",
                                                     })
school_summary_df
317/24:
school_summary_df = school_summary_df.style.format({
                          'Total Students': '{:,}', 
                          "Total School Budget": "${:,}", 
                          "Per Student Budget": "${:.0f}",
                          'Average Math Score': "{:.1f}%", 
                          'Average Reading Score': "{:.1f}%", 
                          "% Passing Math": "{:.1f}%", 
                          "% Passing Reading": "{:.1f}%", 
                          "% Passing Overall": "{:.1f}%"
                         })
school_summary_df
317/25:
# Create a table that highlights the top 5 performing schools based on % Overall Passing. Include:

# School Name
# School Type
# Total Students
# Total School Budget
# Per Student Budget
# Average Math Score
# Average Reading Score
# % Passing Math (The percentage of students that passed math.)
# % Passing Reading (The percentage of students that passed reading.)
# % Overall Passing (The percentage of students that passed math and reading.)
317/26:
# Find top 5 performing schools based on % Overall Passing
top_five_schools = school_summary_df.sort_values(by=["% Passing Overall"], ascending = False)
top_five_schools.head()
317/27:
# Find top 5 performing schools based on % Overall Passing
top_five_schools = school_summary_df.sort_values("% Passing Overall", ascending = False)
top_five_schools.head()
317/28:
# Find top 5 performing schools based on % Overall Passing
top_five_schools = pycityschools_data_complete.sort_values("% Passing Overall", ascending = False)
top_five_schools.head()
317/29:
# Find top 5 performing schools based on % Overall Passing
top_five_schools = school_summary_df.sort_values("% Passing Overall", ascending = False)
top_five_schools.head()
317/30:
# Create a table that highlights the bottom 5 performing schools based on % Overall Passing. Include all of the same metrics as above.
bottom_five = school_summary_df.sort_values("% Passing Overall")
bottom_five.head()
317/31:
# Find top 5 performing schools based on % Overall Passing
#top_five_schools = school_summary_df.sort_values("% Passing Overall", ascending = False)
#top_five_schools.head()
317/32:
# Create a table that highlights the bottom 5 performing schools based on % Overall Passing. Include all of the same metrics as above.
#bottom_five = school_summary_df.sort_values("% Passing Overall")
#bottom_five.head()
317/33:
# Create a table that lists the average Math Score for students of each grade level (9th, 10th, 11th, 12th) at each school.

average_math_score_9th = student_data_df[student_df['grade'] == '9th'].groupby('school')["math_score"].mean()
average_math_score_10th = student_data_df[student_df['grade'] == '10th'].groupby('school')["math_score"].mean()
average_math_score_11th = student_data_df[student_df['grade'] == '11th'].groupby('school')["math_score"].mean()
average_math_score_12th = student_data_df[student_df['grade'] == '12th'].groupby('school')["math_score"].mean()

average_math_scores_grade = pd.DataFrame({
        "9th Grade": average_math_score_9th,
        "10th Grade": average_math_score_10th,
        "11th Grade": average_math_score_11th,
        "12th Grade": average_math_score_12th
        })

average_math_scores_grade.head()
317/34:
# Create a table that lists the average Math Score for students of each grade level (9th, 10th, 11th, 12th) at each school.

average_math_score_9th = student_data_df[student_data_df['grade'] == '9th'].groupby('school')["math_score"].mean()
average_math_score_10th = student_data_df[student_data_df['grade'] == '10th'].groupby('school')["math_score"].mean()
average_math_score_11th = student_data_df[student_data_df['grade'] == '11th'].groupby('school')["math_score"].mean()
average_math_score_12th = student_data_df[student_data_df['grade'] == '12th'].groupby('school')["math_score"].mean()

average_math_scores_grade = pd.DataFrame({
        "9th Grade": average_math_score_9th,
        "10th Grade": average_math_score_10th,
        "11th Grade": average_math_score_11th,
        "12th Grade": average_math_score_12th
        })

average_math_scores_grade.head()
317/35:
# Create a table that lists the average Math Score for students of each grade level (9th, 10th, 11th, 12th) at each school.

average_math_score_9th = student_data_df[student_data_df['grade'] == '9th'].mean()
average_math_score_10th = student_data_df[student_data_df['grade'] == '10th'].mean()
average_math_score_11th = student_data_df[student_data_df['grade'] == '11th'].mean()
average_math_score_12th = student_data_df[student_data_df['grade'] == '12th'].mean()

average_math_scores_grade = pd.DataFrame({
        "9th Grade": average_math_score_9th,
        "10th Grade": average_math_score_10th,
        "11th Grade": average_math_score_11th,
        "12th Grade": average_math_score_12th
        })

average_math_scores_grade.head()
317/36:
# Create a table that lists the average Math Score for students of each grade level (9th, 10th, 11th, 12th) at each school.

average_math_score_9th = student_data_df[student_data_df['grade'] == '9th'].mean()
average_math_score_10th = student_data_df[student_data_df['grade'] == '10th'].mean()
average_math_score_11th = student_data_df[student_data_df['grade'] == '11th'].mean()
average_math_score_12th = student_data_df[student_data_df['grade'] == '12th'].mean()

grouped_average_math_score_9th = student_data_df.groupby('school')["math_score"]

average_math_scores_grade = pd.DataFrame({
        "9th Grade": average_math_score_9th,
        "10th Grade": average_math_score_10th,
        "11th Grade": average_math_score_11th,
        "12th Grade": average_math_score_12th
        })

average_math_scores_grade.head()
317/37:
# Create a table that lists the average Math Score for students of each grade level (9th, 10th, 11th, 12th) at each school.

average_math_score_9th = student_data_df.loc[student_data_df['grade'] == '9th'].groupby('school')["math_score"].mean()
average_math_score_10th = student_data_df.loc[student_data_df['grade'] == '10th'].groupby('school')["math_score"].mean()
average_math_score_11th = student_data_df.loc[student_data_df['grade'] == '11th'].groupby('school')["math_score"].mean()
average_math_score_12th = student_data_df.loc[student_data_df['grade'] == '12th'].groupby('school')["math_score"].mean()

average_math_scores_grade = pd.DataFrame({
        "9th Grade": average_math_score_9th,
        "10th Grade": average_math_score_10th,
        "11th Grade": average_math_score_11th,
        "12th Grade": average_math_score_12th
        })

average_math_scores_grade.head()
317/38:
# Create a table that lists the average Math Score for students of each grade level (9th, 10th, 11th, 12th) at each school.

average_math_score_9th = student_data_df.loc[student_data_df['grade'] == '9th'].groupby('school_name')["math_score"].mean()
average_math_score_10th = student_data_df.loc[student_data_df['grade'] == '10th'].groupby('school_name')["math_score"].mean()
average_math_score_11th = student_data_df.loc[student_data_df['grade'] == '11th'].groupby('school_name')["math_score"].mean()
average_math_score_12th = student_data_df.loc[student_data_df['grade'] == '12th'].groupby('school_name')["math_score"].mean()

average_math_scores_grade = pd.DataFrame({
        "9th Grade": average_math_score_9th,
        "10th Grade": average_math_score_10th,
        "11th Grade": average_math_score_11th,
        "12th Grade": average_math_score_12th
        })

average_math_scores_grade.head()
317/39:
# Reading Score 
average_reading_score_9th = student_data_df.loc[student_data_df['grade'] == '9th'].groupby('school_name')["reading_score"].mean()
average_reading_score_10th = student_data_df.loc[student_data_df['grade'] == '10th'].groupby('school_name')["reading_score"].mean()
average_reading_score_11th = student_data_df.loc[student_data_df['grade'] == '11th'].groupby('school_name')["reading_score"].mean()
average_reading_score_12th = student_data_df.loc[student_data_df['grade'] == '12th'].groupby('school_name')["reading_score"].mean()

average_math_scores_grade = pd.DataFrame({
        "9th Grade": average_reading_score_9th,
        "10th Grade": average_reading_score_10th,
        "11th Grade": average_reading_score_11th,
        "12th Grade": average_reading_score_12th
        })

average_reading_scores_grade.head()
317/40:
# Reading Score 
average_reading_score_9th = student_data_df.loc[student_data_df['grade'] == '9th'].groupby('school_name')["reading_score"].mean()
average_reading_score_10th = student_data_df.loc[student_data_df['grade'] == '10th'].groupby('school_name')["reading_score"].mean()
average_reading_score_11th = student_data_df.loc[student_data_df['grade'] == '11th'].groupby('school_name')["reading_score"].mean()
average_reading_score_12th = student_data_df.loc[student_data_df['grade'] == '12th'].groupby('school_name')["reading_score"].mean()

average_reading_scores_grade = pd.DataFrame({
        "9th Grade": average_reading_score_9th,
        "10th Grade": average_reading_score_10th,
        "11th Grade": average_reading_score_11th,
        "12th Grade": average_reading_score_12th
        })

average_reading_scores_grade.head()
289/28:
school_summary_df["% Passing Math"] = passing_math_count / school_summary_df["Student ID"] * 100
school_summary_df["% Passing Reading"] = passing_reading_count / school_summary_df["Student ID"] * 100
school_summary_df["% Passing Overall"] = passing_overall_count / school_summary_df["Student ID"] * 100
school_summary_df["School Type"] = pycityschools_data_complete.join("type")
school_summary_df
321/1:
# Dependencies
import pandas as pd
import numpy as np
321/2:
# load in file
pycityschools_file = "Resources/schools_complete.csv"
student_file = "Resources/students_complete.csv"
321/3:
# Read School and Student Data File and store into Pandas DataFrames
pycityschools_data_df = pd.read_csv(pycityschools_file)
student_data_df = pd.read_csv(student_file)

pycityschools_data_df.head()
student_data_df.head()
321/4:
pycityschools_data_complete = pd.merge(student_data_df, pycityschools_data_df, 
                                       on=["school_name", "school_name"], how="left")

pycityschools_data_complete.head()
321/5:
# Count total schools
total_schools = len(pycityschools_data_complete["School ID"].unique())
total_schools
321/6:
# Calculate the total number of students
total_students = len(pycityschools_data_complete["student_name"].unique())
total_students
321/7:
# Calculate Total Budget
total_budget = pycityschools_data_df ["budget"].sum()
total_budget
321/8:
# Calculate the average math score
average_mathscore = round(pycityschools_data_complete["math_score"].mean(), 2)
average_mathscore
321/9:
# Calculate the average reading score
average_readingscore = round(pycityschools_data_complete["reading_score"].mean(), 2)
average_readingscore
321/10:
# Calculate the percentage of students with a passing math score (70 or greater)
unique_math_passing = len(pycityschools_data_complete.loc[pycityschools_data_complete["math_score"] >=70]["student_name"].unique())
percent_math_passing = round((unique_math_passing/total_students)*100, 2)
percent_math_passing
321/11:
# Calculate the percentage of students with a passing reading score (70 or greater)
unique_reading_passing = len(pycityschools_data_complete.loc[pycityschools_data_complete["reading_score"] >=70]["student_name"].unique())
percent_reading_passing = round((unique_reading_passing/total_students)*100, 2)
percent_reading_passing
321/12:
# Calculate the percentage of students who passed math and reading (% Overall Passing)
overall_passing = round((percent_reading_passing + percent_math_passing) / 2, 2)
overall_passing
321/13:
# Create a dataframe to hold the above results
summary_df = pd.DataFrame({"Total Schools": [total_schools],
                          "Total Students": [total_students],
                          "Total Budget": [total_budget],
                          "Average Math Score": [average_mathscore],
                          "Average Reading Score": [average_readingscore],
                          "% of Students with Passing Math Score": [percent_math_passing],
                          "% of Students with Passing Reading Score": [percent_reading_passing],
                          "% of Students with Passing Reading and Math Score": [overall_passing]
                          })
summary_df
321/14:
# Give the displayed data cleaner formatting
summary_df["Total Students"] = summary_df["Total Students"].map("{:,.2f}".format)
summary_df["Total Budget"] = summary_df["Total Budget"].map("${:,.2f}".format)
summary_df["Average Math Score"] = summary_df["Average Math Score"].map("{:,.2f}%".format)
summary_df["Average Reading Score"] = summary_df["Average Reading Score"].map("{:,.2f}%".format)
summary_df["% of Students with Passing Math Score"] = summary_df["% of Students with Passing Math Score"].map("{:,.2f}%".format)
summary_df["% of Students with Passing Reading Score"] = summary_df["% of Students with Passing Reading Score"].map("{:,.2f}%".format)
summary_df["% of Students with Passing Reading and Math Score"] = summary_df["% of Students with Passing Reading and Math Score"].map("{:,.2f}%".format)
summary_df
321/15:
schools_overview_group = pycityschools_data_complete.groupby(["school_name"])
school_summary_df = schools_overview_group.agg({"Student ID": "count",
                            "budget": "max",
                           "math_score": "mean",
                            "reading_score": "mean",
                           })
school_summary_df["Per Student Budget"] = school_summary_df["budget"] / school_summary_df["Student ID"]
school_summary_df
321/16:
# Passing Math Score
passing_math_stu = pycityschools_data_complete.loc[pycityschools_data_complete["math_score"] >=70]
passing_math_stu
321/17:
passing_overall_stu = pycityschools_data_complete.loc[(pycityschools_data_complete["math_score"] >=70)&
                                                  (pycityschools_data_complete["reading_score"] >=70)]
passing_overall_stu
321/18:
passing_reading_stu = pycityschools_data_complete.loc[pycityschools_data_complete["reading_score"] >=70]
passing_reading_stu
321/19:
passing_math_count = passing_math_stu.groupby(["school_name"]).count()["math_score"]
passing_math_count
321/20:
passing_reading_count = passing_reading_stu.groupby(["school_name"]).count()["reading_score"]
passing_reading_count
321/21:
passing_overall_count = passing_overall_stu.groupby(["school_name"]).count()["reading_score"]
passing_overall_count
321/22:
school_summary_df["% Passing Math"] = passing_math_count / school_summary_df["Student ID"] * 100
school_summary_df["% Passing Reading"] = passing_reading_count / school_summary_df["Student ID"] * 100
school_summary_df["% Passing Overall"] = passing_overall_count / school_summary_df["Student ID"] * 100
school_summary_df["School Type"] = pycityschools_data_complete.join("type")
school_summary_df
321/23:
school_summary_df["% Passing Math"] = passing_math_count / school_summary_df["Student ID"] * 100
school_summary_df["% Passing Reading"] = passing_reading_count / school_summary_df["Student ID"] * 100
school_summary_df["% Passing Overall"] = passing_overall_count / school_summary_df["Student ID"] * 100
school_summary_df
321/24:
school_summary_df = school_summary_df.rename(columns={"budget": "Total School Budget",
                                                      "Student ID": "Total Students",
                                                     "math_score": "Average Math Score",
                                                      "reading_score": "Average Reading Score",
                                                     })
school_summary_df
321/25:
school_summary_df = school_summary_df.style.format({
                          'Total Students': '{:,}', 
                          "Total School Budget": "${:,}", 
                          "Per Student Budget": "${:.0f}",
                          'Average Math Score': "{:.1f}%", 
                          'Average Reading Score': "{:.1f}%", 
                          "% Passing Math": "{:.1f}%", 
                          "% Passing Reading": "{:.1f}%", 
                          "% Passing Overall": "{:.1f}%"
                         })
school_summary_df
321/26:
school_summary_df["School Type"] = pycityschools_data_complete[type]
school_summary_df
321/27:
school_summary_df["School Type"] = pycityschools_data_complete['type']
school_summary_df
321/28:
school_summary_df = pycityschools_data_complete[["type"]]
school_summary_df
321/29:
school_type = pycityschools_data_complete[["type"]]
school_type
321/30:
school_summary_df["% Passing Math"] = passing_math_count / school_summary_df["Student ID"] * 100
school_summary_df["% Passing Reading"] = passing_reading_count / school_summary_df["Student ID"] * 100
school_summary_df["% Passing Overall"] = passing_overall_count / school_summary_df["Student ID"] * 100
school_summary_df["School Type"] = school_type
school_summary_df
321/31:
school_summary_df["% Passing Math"] = passing_math_count / school_summary_df["Student ID"] * 100
school_summary_df["% Passing Reading"] = passing_reading_count / school_summary_df["Student ID"] * 100
school_summary_df["% Passing Overall"] = passing_overall_count / school_summary_df["Student ID"] * 100
school_summary_df["School Type"] = school_type.count()
school_summary_df
321/32:
school_summary_df["% Passing Math"] = passing_math_count / school_summary_df["Student ID"] * 100
school_summary_df["% Passing Reading"] = passing_reading_count / school_summary_df["Student ID"] * 100
school_summary_df["% Passing Overall"] = passing_overall_count / school_summary_df["Student ID"] * 100
school_summary_df
322/1:
# Dependencies
import pandas as pd
import numpy as np
322/2:
# load in file
pycityschools_file = "Resources/schools_complete.csv"
student_file = "Resources/students_complete.csv"
322/3:
# Read School and Student Data File and store into Pandas DataFrames
pycityschools_data_df = pd.read_csv(pycityschools_file)
student_data_df = pd.read_csv(student_file)

pycityschools_data_df.head()
student_data_df.head()
322/4:
pycityschools_data_complete = pd.merge(student_data_df, pycityschools_data_df, 
                                       on=["school_name", "school_name"], how="left")

pycityschools_data_complete.head()
322/5:
# Count total schools
total_schools = len(pycityschools_data_complete["School ID"].unique())
total_schools
322/6:
# Calculate the total number of students
total_students = len(pycityschools_data_complete["student_name"].unique())
total_students
322/7:
# Calculate Total Budget
total_budget = pycityschools_data_df ["budget"].sum()
total_budget
322/8:
# Calculate the average math score
average_mathscore = round(pycityschools_data_complete["math_score"].mean(), 2)
average_mathscore
322/9:
# Calculate the average reading score
average_readingscore = round(pycityschools_data_complete["reading_score"].mean(), 2)
average_readingscore
322/10:
# Calculate the percentage of students with a passing math score (70 or greater)
unique_math_passing = len(pycityschools_data_complete.loc[pycityschools_data_complete["math_score"] >=70]["student_name"].unique())
percent_math_passing = round((unique_math_passing/total_students)*100, 2)
percent_math_passing
322/11:
# Calculate the percentage of students with a passing reading score (70 or greater)
unique_reading_passing = len(pycityschools_data_complete.loc[pycityschools_data_complete["reading_score"] >=70]["student_name"].unique())
percent_reading_passing = round((unique_reading_passing/total_students)*100, 2)
percent_reading_passing
322/12:
# Calculate the percentage of students who passed math and reading (% Overall Passing)
overall_passing = round((percent_reading_passing + percent_math_passing) / 2, 2)
overall_passing
322/13:
# Create a dataframe to hold the above results
summary_df = pd.DataFrame({"Total Schools": [total_schools],
                          "Total Students": [total_students],
                          "Total Budget": [total_budget],
                          "Average Math Score": [average_mathscore],
                          "Average Reading Score": [average_readingscore],
                          "% of Students with Passing Math Score": [percent_math_passing],
                          "% of Students with Passing Reading Score": [percent_reading_passing],
                          "% of Students with Passing Reading and Math Score": [overall_passing]
                          })
summary_df
322/14:
# Give the displayed data cleaner formatting
summary_df["Total Students"] = summary_df["Total Students"].map("{:,.2f}".format)
summary_df["Total Budget"] = summary_df["Total Budget"].map("${:,.2f}".format)
summary_df["Average Math Score"] = summary_df["Average Math Score"].map("{:,.2f}%".format)
summary_df["Average Reading Score"] = summary_df["Average Reading Score"].map("{:,.2f}%".format)
summary_df["% of Students with Passing Math Score"] = summary_df["% of Students with Passing Math Score"].map("{:,.2f}%".format)
summary_df["% of Students with Passing Reading Score"] = summary_df["% of Students with Passing Reading Score"].map("{:,.2f}%".format)
summary_df["% of Students with Passing Reading and Math Score"] = summary_df["% of Students with Passing Reading and Math Score"].map("{:,.2f}%".format)
summary_df
322/15:
schools_overview_group = pycityschools_data_complete.groupby(["school_name"])
school_summary_df = schools_overview_group.agg({"Student ID": "count",
                            "budget": "max",
                           "math_score": "mean",
                            "reading_score": "mean",
                           })
school_summary_df["Per Student Budget"] = school_summary_df["budget"] / school_summary_df["Student ID"]
school_summary_df
322/16:
school_type = pycityschools_data_complete[["type"]]
school_type
322/17:
# Passing Math Score
passing_math_stu = pycityschools_data_complete.loc[pycityschools_data_complete["math_score"] >=70]
passing_math_stu
322/18:
passing_overall_stu = pycityschools_data_complete.loc[(pycityschools_data_complete["math_score"] >=70)&
                                                  (pycityschools_data_complete["reading_score"] >=70)]
passing_overall_stu
322/19:
passing_reading_stu = pycityschools_data_complete.loc[pycityschools_data_complete["reading_score"] >=70]
passing_reading_stu
322/20:
passing_math_count = passing_math_stu.groupby(["school_name"]).count()["math_score"]
passing_math_count
322/21:
passing_reading_count = passing_reading_stu.groupby(["school_name"]).count()["reading_score"]
passing_reading_count
322/22:
passing_overall_count = passing_overall_stu.groupby(["school_name"]).count()["reading_score"]
passing_overall_count
322/23:
school_summary_df["% Passing Math"] = passing_math_count / school_summary_df["Student ID"] * 100
school_summary_df["% Passing Reading"] = passing_reading_count / school_summary_df["Student ID"] * 100
school_summary_df["% Passing Overall"] = passing_overall_count / school_summary_df["Student ID"] * 100
school_summary_df["School Type"] = school_type
school_summary_df
322/24:
school_summary_df = school_summary_df.rename(columns={"budget": "Total School Budget",
                                                      "Student ID": "Total Students",
                                                     "math_score": "Average Math Score",
                                                      "reading_score": "Average Reading Score",
                                                     })
school_summary_df
322/25:
school_summary_df = school_summary_df.style.format({
                          'Total Students': '{:,}', 
                          "Total School Budget": "${:,}", 
                          "Per Student Budget": "${:.0f}",
                          'Average Math Score': "{:.1f}%", 
                          'Average Reading Score': "{:.1f}%", 
                          "% Passing Math": "{:.1f}%", 
                          "% Passing Reading": "{:.1f}%", 
                          "% Passing Overall": "{:.1f}%"
                         })
school_summary_df
322/26:
# Create a table that highlights the top 5 performing schools based on % Overall Passing. Include:

# School Name
# School Type
# Total Students
# Total School Budget
# Per Student Budget
# Average Math Score
# Average Reading Score
# % Passing Math (The percentage of students that passed math.)
# % Passing Reading (The percentage of students that passed reading.)
# % Overall Passing (The percentage of students that passed math and reading.)
322/27:
# Find top 5 performing schools based on % Overall Passing
#top_five_schools = school_summary_df.sort_values("% Passing Overall", ascending = False)
#top_five_schools.head()
322/28:
# Create a table that highlights the bottom 5 performing schools based on % Overall Passing. Include all of the same metrics as above.
#bottom_five = school_summary_df.sort_values("% Passing Overall")
#bottom_five.head()
322/29:
# Create a table that lists the average Math Score for students of each grade level (9th, 10th, 11th, 12th) at each school.

average_math_score_9th = student_data_df.loc[student_data_df['grade'] == '9th'].groupby('school_name')["math_score"].mean()
average_math_score_10th = student_data_df.loc[student_data_df['grade'] == '10th'].groupby('school_name')["math_score"].mean()
average_math_score_11th = student_data_df.loc[student_data_df['grade'] == '11th'].groupby('school_name')["math_score"].mean()
average_math_score_12th = student_data_df.loc[student_data_df['grade'] == '12th'].groupby('school_name')["math_score"].mean()

average_math_scores_grade = pd.DataFrame({
        "9th Grade": average_math_score_9th,
        "10th Grade": average_math_score_10th,
        "11th Grade": average_math_score_11th,
        "12th Grade": average_math_score_12th
        })

average_math_scores_grade.head()
322/30:
# Reading Score 
average_reading_score_9th = student_data_df.loc[student_data_df['grade'] == '9th'].groupby('school_name')["reading_score"].mean()
average_reading_score_10th = student_data_df.loc[student_data_df['grade'] == '10th'].groupby('school_name')["reading_score"].mean()
average_reading_score_11th = student_data_df.loc[student_data_df['grade'] == '11th'].groupby('school_name')["reading_score"].mean()
average_reading_score_12th = student_data_df.loc[student_data_df['grade'] == '12th'].groupby('school_name')["reading_score"].mean()

average_reading_scores_grade = pd.DataFrame({
        "9th Grade": average_reading_score_9th,
        "10th Grade": average_reading_score_10th,
        "11th Grade": average_reading_score_11th,
        "12th Grade": average_reading_score_12th
        })

average_reading_scores_grade.head()
322/31:
# Create a table that breaks down school performances based on average Spending Ranges (Per Student). 
#Use 4 reasonable bins to group school spending. Include in the table each of the following:

# Average Math Score
# Average Reading Score
# % Passing Math (The percentage of students that passed math.)
# % Passing Reading (The percentage of students that passed reading.)
# % Overall Passing (The percentage of students that passed math and reading.)
322/32:
# Create Bins and their labels
bins = [0, 550, 600, 650, 700]
group_labels = ["Les than $549", "$550 to $599", "$600 to $650", "Greater than $700"]
322/33:
school_type = pycityschools_data_complete[["type", "school_name"]]
school_type
323/1:
# Dependencies
import pandas as pd
import numpy as np
323/2:
# load in file
pycityschools_file = "Resources/schools_complete.csv"
student_file = "Resources/students_complete.csv"
323/3:
# Read School and Student Data File and store into Pandas DataFrames
pycityschools_data_df = pd.read_csv(pycityschools_file)
student_data_df = pd.read_csv(student_file)

pycityschools_data_df.head()
student_data_df.head()
323/4:
pycityschools_data_complete = pd.merge(student_data_df, pycityschools_data_df, 
                                       on=["school_name", "school_name"], how="left")

pycityschools_data_complete.head()
323/5:
# Count total schools
total_schools = len(pycityschools_data_complete["School ID"].unique())
total_schools
323/6:
# Calculate the total number of students
total_students = len(pycityschools_data_complete["student_name"].unique())
total_students
323/7:
# Calculate Total Budget
total_budget = pycityschools_data_df ["budget"].sum()
total_budget
323/8:
# Calculate the average math score
average_mathscore = round(pycityschools_data_complete["math_score"].mean(), 2)
average_mathscore
323/9:
# Calculate the average reading score
average_readingscore = round(pycityschools_data_complete["reading_score"].mean(), 2)
average_readingscore
323/10:
# Calculate the percentage of students with a passing math score (70 or greater)
unique_math_passing = len(pycityschools_data_complete.loc[pycityschools_data_complete["math_score"] >=70]["student_name"].unique())
percent_math_passing = round((unique_math_passing/total_students)*100, 2)
percent_math_passing
323/11:
# Calculate the percentage of students with a passing reading score (70 or greater)
unique_reading_passing = len(pycityschools_data_complete.loc[pycityschools_data_complete["reading_score"] >=70]["student_name"].unique())
percent_reading_passing = round((unique_reading_passing/total_students)*100, 2)
percent_reading_passing
323/12:
# Calculate the percentage of students who passed math and reading (% Overall Passing)
overall_passing = round((percent_reading_passing + percent_math_passing) / 2, 2)
overall_passing
323/13:
# Create a dataframe to hold the above results
summary_df = pd.DataFrame({"Total Schools": [total_schools],
                          "Total Students": [total_students],
                          "Total Budget": [total_budget],
                          "Average Math Score": [average_mathscore],
                          "Average Reading Score": [average_readingscore],
                          "% of Students with Passing Math Score": [percent_math_passing],
                          "% of Students with Passing Reading Score": [percent_reading_passing],
                          "% of Students with Passing Reading and Math Score": [overall_passing]
                          })
summary_df
323/14:
# Give the displayed data cleaner formatting
summary_df["Total Students"] = summary_df["Total Students"].map("{:,.2f}".format)
summary_df["Total Budget"] = summary_df["Total Budget"].map("${:,.2f}".format)
summary_df["Average Math Score"] = summary_df["Average Math Score"].map("{:,.2f}%".format)
summary_df["Average Reading Score"] = summary_df["Average Reading Score"].map("{:,.2f}%".format)
summary_df["% of Students with Passing Math Score"] = summary_df["% of Students with Passing Math Score"].map("{:,.2f}%".format)
summary_df["% of Students with Passing Reading Score"] = summary_df["% of Students with Passing Reading Score"].map("{:,.2f}%".format)
summary_df["% of Students with Passing Reading and Math Score"] = summary_df["% of Students with Passing Reading and Math Score"].map("{:,.2f}%".format)
summary_df
323/15:
schools_overview_group = pycityschools_data_complete.groupby(["school_name"])
school_summary_df = schools_overview_group.agg({"Student ID": "count",
                            "budget": "max",
                           "math_score": "mean",
                            "reading_score": "mean",
                           })
school_summary_df["Per Student Budget"] = school_summary_df["budget"] / school_summary_df["Student ID"]
school_summary_df
323/16:
school_type = pycityschools_data_complete[["type", "school_name"]]
school_type
323/17:
# Passing Math Score
passing_math_stu = pycityschools_data_complete.loc[pycityschools_data_complete["math_score"] >=70]
passing_math_stu
323/18:
passing_overall_stu = pycityschools_data_complete.loc[(pycityschools_data_complete["math_score"] >=70)&
                                                  (pycityschools_data_complete["reading_score"] >=70)]
passing_overall_stu
323/19:
passing_reading_stu = pycityschools_data_complete.loc[pycityschools_data_complete["reading_score"] >=70]
passing_reading_stu
323/20:
passing_math_count = passing_math_stu.groupby(["school_name"]).count()["math_score"]
passing_math_count
323/21:
passing_reading_count = passing_reading_stu.groupby(["school_name"]).count()["reading_score"]
passing_reading_count
323/22:
passing_overall_count = passing_overall_stu.groupby(["school_name"]).count()["reading_score"]
passing_overall_count
323/23:
school_summary_df["% Passing Math"] = passing_math_count / school_summary_df["Student ID"] * 100
school_summary_df["% Passing Reading"] = passing_reading_count / school_summary_df["Student ID"] * 100
school_summary_df["% Passing Overall"] = passing_overall_count / school_summary_df["Student ID"] * 100
school_summary_df["School Type"] = school_type
school_summary_df
323/24:
school_summary_df["% Passing Math"] = passing_math_count / school_summary_df["Student ID"] * 100
school_summary_df["% Passing Reading"] = passing_reading_count / school_summary_df["Student ID"] * 100
school_summary_df["% Passing Overall"] = passing_overall_count / school_summary_df["Student ID"] * 100
school_summary_df
323/25:
school_summary_df = school_summary_df.rename(columns={"budget": "Total School Budget",
                                                      "Student ID": "Total Students",
                                                     "math_score": "Average Math Score",
                                                      "reading_score": "Average Reading Score",
                                                     })
school_summary_df
323/26:
school_summary_df = school_summary_df.style.format({
                          'Total Students': '{:,}', 
                          "Total School Budget": "${:,}", 
                          "Per Student Budget": "${:.0f}",
                          'Average Math Score': "{:.1f}%", 
                          'Average Reading Score': "{:.1f}%", 
                          "% Passing Math": "{:.1f}%", 
                          "% Passing Reading": "{:.1f}%", 
                          "% Passing Overall": "{:.1f}%"
                         })
school_summary_df
323/27:
# Find top 5 performing schools based on % Overall Passing
top_five_schools = school_summary_df.nlargest(5, "% Passing Overall")
top_five_schools
323/28:
# Find top 5 performing schools based on % Overall Passing
top_five_schools = school_summary_df.sort_values("% Passing Overall")
top_five_schools
323/29:
# Find top 5 performing schools based on % Overall Passing
top_five_schools = school_summary_df.sort_values('% Passing Overall', ascending=False)
top_five_schools
324/1:
# Dependencies
import pandas as pd
import numpy as np
324/2:
# load in file
pycityschools_file = "Resources/schools_complete.csv"
student_file = "Resources/students_complete.csv"
324/3:
# Read School and Student Data File and store into Pandas DataFrames
pycityschools_data_df = pd.read_csv(pycityschools_file)
student_data_df = pd.read_csv(student_file)

pycityschools_data_df.head()
student_data_df.head()
324/4:
pycityschools_data_complete = pd.merge(student_data_df, pycityschools_data_df, 
                                       on=["school_name", "school_name"], how="left")

pycityschools_data_complete.head()
324/5:
# Count total schools
total_schools = len(pycityschools_data_complete["School ID"].unique())
total_schools
324/6:
# Calculate the total number of students
total_students = len(pycityschools_data_complete["student_name"].unique())
total_students
324/7:
# Calculate Total Budget
total_budget = pycityschools_data_df ["budget"].sum()
total_budget
324/8:
# Calculate the average math score
average_mathscore = round(pycityschools_data_complete["math_score"].mean(), 2)
average_mathscore
324/9:
# Calculate the average reading score
average_readingscore = round(pycityschools_data_complete["reading_score"].mean(), 2)
average_readingscore
324/10:
# Calculate the percentage of students with a passing math score (70 or greater)
unique_math_passing = len(pycityschools_data_complete.loc[pycityschools_data_complete["math_score"] >=70]["student_name"].unique())
percent_math_passing = round((unique_math_passing/total_students)*100, 2)
percent_math_passing
324/11:
# Calculate the percentage of students with a passing reading score (70 or greater)
unique_reading_passing = len(pycityschools_data_complete.loc[pycityschools_data_complete["reading_score"] >=70]["student_name"].unique())
percent_reading_passing = round((unique_reading_passing/total_students)*100, 2)
percent_reading_passing
324/12:
# Calculate the percentage of students who passed math and reading (% Overall Passing)
overall_passing = round((percent_reading_passing + percent_math_passing) / 2, 2)
overall_passing
324/13:
# Create a dataframe to hold the above results
summary_df = pd.DataFrame({"Total Schools": [total_schools],
                          "Total Students": [total_students],
                          "Total Budget": [total_budget],
                          "Average Math Score": [average_mathscore],
                          "Average Reading Score": [average_readingscore],
                          "% of Students with Passing Math Score": [percent_math_passing],
                          "% of Students with Passing Reading Score": [percent_reading_passing],
                          "% of Students with Passing Reading and Math Score": [overall_passing]
                          })
summary_df
324/14:
# Give the displayed data cleaner formatting
summary_df["Total Students"] = summary_df["Total Students"].map("{:,.2f}".format)
summary_df["Total Budget"] = summary_df["Total Budget"].map("${:,.2f}".format)
summary_df["Average Math Score"] = summary_df["Average Math Score"].map("{:,.2f}%".format)
summary_df["Average Reading Score"] = summary_df["Average Reading Score"].map("{:,.2f}%".format)
summary_df["% of Students with Passing Math Score"] = summary_df["% of Students with Passing Math Score"].map("{:,.2f}%".format)
summary_df["% of Students with Passing Reading Score"] = summary_df["% of Students with Passing Reading Score"].map("{:,.2f}%".format)
summary_df["% of Students with Passing Reading and Math Score"] = summary_df["% of Students with Passing Reading and Math Score"].map("{:,.2f}%".format)
summary_df
324/15:
schools_overview_group = pycityschools_data_complete.groupby(["school_name"])
school_summary_df = schools_overview_group.agg({"Student ID": "count",
                            "budget": "max",
                           "math_score": "mean",
                            "reading_score": "mean",
                           })
school_summary_df["Per Student Budget"] = school_summary_df["budget"] / school_summary_df["Student ID"]
school_summary_df
324/16:
school_type = pycityschools_data_complete[["type", "school_name"]]
school_type
324/17:
# Passing Math Score
passing_math_stu = pycityschools_data_complete.loc[pycityschools_data_complete["math_score"] >=70]
passing_math_stu
324/18:
passing_overall_stu = pycityschools_data_complete.loc[(pycityschools_data_complete["math_score"] >=70)&
                                                  (pycityschools_data_complete["reading_score"] >=70)]
passing_overall_stu
324/19:
passing_reading_stu = pycityschools_data_complete.loc[pycityschools_data_complete["reading_score"] >=70]
passing_reading_stu
324/20:
passing_math_count = passing_math_stu.groupby(["school_name"]).count()["math_score"]
passing_math_count
324/21:
passing_reading_count = passing_reading_stu.groupby(["school_name"]).count()["reading_score"]
passing_reading_count
324/22:
passing_overall_count = passing_overall_stu.groupby(["school_name"]).count()["reading_score"]
passing_overall_count
324/23:
school_summary_df["% Passing Math"] = passing_math_count / school_summary_df["Student ID"] * 100
school_summary_df["% Passing Reading"] = passing_reading_count / school_summary_df["Student ID"] * 100
school_summary_df["% Passing Overall"] = passing_overall_count / school_summary_df["Student ID"] * 100
school_summary_df
324/24:
school_summary_df = school_summary_df.rename(columns={"budget": "Total School Budget",
                                                      "Student ID": "Total Students",
                                                     "math_score": "Average Math Score",
                                                      "reading_score": "Average Reading Score",
                                                     })
school_summary_df
324/25:
school_summary_df = school_summary_df.style.format({
                          'Total Students': '{:,}', 
                          "Total School Budget": "${:,}", 
                          "Per Student Budget": "${:.0f}",
                          'Average Math Score': "{:.1f}%", 
                          'Average Reading Score': "{:.1f}%", 
                          "% Passing Math": "{:.1f}%", 
                          "% Passing Reading": "{:.1f}%", 
                          "% Passing Overall": "{:.1f}%"
                         })
school_summary_df
324/26:
# Create a table that highlights the top 5 performing schools based on % Overall Passing. Include:

# School Name
# School Type
# Total Students
# Total School Budget
# Per Student Budget
# Average Math Score
# Average Reading Score
# % Passing Math (The percentage of students that passed math.)
# % Passing Reading (The percentage of students that passed reading.)
# % Overall Passing (The percentage of students that passed math and reading.)
324/27:
# Find top 5 performing schools based on % Overall Passing
top_five_schools = school_summary_df.sort_values('% Passing Overall', ascending=False)
top_five_schools
324/28:
# Reading Score 
average_reading_score_9th = student_data_df.loc[student_data_df['grade'] == '9th'].groupby('school_name')["reading_score"].mean()
average_reading_score_10th = student_data_df.loc[student_data_df['grade'] == '10th'].groupby('school_name')["reading_score"].mean()
average_reading_score_11th = student_data_df.loc[student_data_df['grade'] == '11th'].groupby('school_name')["reading_score"].mean()
average_reading_score_12th = student_data_df.loc[student_data_df['grade'] == '12th'].groupby('school_name')["reading_score"].mean()

average_reading_scores_grade = pd.DataFrame({
        "9th Grade": average_reading_score_9th,
        "10th Grade": average_reading_score_10th,
        "11th Grade": average_reading_score_11th,
        "12th Grade": average_reading_score_12th
        })

average_reading_scores_grade.head()
324/29:
# Create a table that breaks down school performances based on average Spending Ranges (Per Student). 
#Use 4 reasonable bins to group school spending. Include in the table each of the following:

# Average Math Score
# Average Reading Score
# % Passing Math (The percentage of students that passed math.)
# % Passing Reading (The percentage of students that passed reading.)
# % Overall Passing (The percentage of students that passed math and reading.)
324/30:
school_spending = school_summary_df.copy()

# Create Bins and their labels
bins = [0, 550, 600, 650, 700]
group_labels = ["Less than $549", "$550 to $599", "$600 to $650", "Greater than $700"]
324/31:
school_spending['Spending Per Student'] = pd.cut(school_spending['Per Student Budget'],bins, labels=group_labels)
school_spending
325/1:
# Dependencies
import pandas as pd
import numpy as np
325/2:
# load in file
pycityschools_file = "Resources/schools_complete.csv"
student_file = "Resources/students_complete.csv"
325/3:
# Read School and Student Data File and store into Pandas DataFrames
pycityschools_data_df = pd.read_csv(pycityschools_file)
student_data_df = pd.read_csv(student_file)

pycityschools_data_df.head()
student_data_df.head()
325/4:
pycityschools_data_complete = pd.merge(student_data_df, pycityschools_data_df, 
                                       on=["school_name", "school_name"], how="left")

pycityschools_data_complete.head()
325/5:
# Count total schools
total_schools = len(pycityschools_data_complete["School ID"].unique())
total_schools
325/6:
# Calculate the total number of students
total_students = len(pycityschools_data_complete["student_name"].unique())
total_students
325/7:
# Calculate Total Budget
total_budget = pycityschools_data_df ["budget"].sum()
total_budget
325/8:
# Calculate the average math score
average_mathscore = round(pycityschools_data_complete["math_score"].mean(), 2)
average_mathscore
325/9:
# Calculate the average reading score
average_readingscore = round(pycityschools_data_complete["reading_score"].mean(), 2)
average_readingscore
325/10:
# Calculate the percentage of students with a passing math score (70 or greater)
unique_math_passing = len(pycityschools_data_complete.loc[pycityschools_data_complete["math_score"] >=70]["student_name"].unique())
percent_math_passing = round((unique_math_passing/total_students)*100, 2)
percent_math_passing
325/11:
# Calculate the percentage of students with a passing reading score (70 or greater)
unique_reading_passing = len(pycityschools_data_complete.loc[pycityschools_data_complete["reading_score"] >=70]["student_name"].unique())
percent_reading_passing = round((unique_reading_passing/total_students)*100, 2)
percent_reading_passing
325/12:
# Calculate the percentage of students who passed math and reading (% Overall Passing)
overall_passing = round((percent_reading_passing + percent_math_passing) / 2, 2)
overall_passing
325/13:
# Create a dataframe to hold the above results
summary_df = pd.DataFrame({"Total Schools": [total_schools],
                          "Total Students": [total_students],
                          "Total Budget": [total_budget],
                          "Average Math Score": [average_mathscore],
                          "Average Reading Score": [average_readingscore],
                          "% of Students with Passing Math Score": [percent_math_passing],
                          "% of Students with Passing Reading Score": [percent_reading_passing],
                          "% of Students with Passing Reading and Math Score": [overall_passing]
                          })
summary_df
325/14:
# Give the displayed data cleaner formatting
summary_df["Total Students"] = summary_df["Total Students"].map("{:,.2f}".format)
summary_df["Total Budget"] = summary_df["Total Budget"].map("${:,.2f}".format)
summary_df["Average Math Score"] = summary_df["Average Math Score"].map("{:,.2f}%".format)
summary_df["Average Reading Score"] = summary_df["Average Reading Score"].map("{:,.2f}%".format)
summary_df["% of Students with Passing Math Score"] = summary_df["% of Students with Passing Math Score"].map("{:,.2f}%".format)
summary_df["% of Students with Passing Reading Score"] = summary_df["% of Students with Passing Reading Score"].map("{:,.2f}%".format)
summary_df["% of Students with Passing Reading and Math Score"] = summary_df["% of Students with Passing Reading and Math Score"].map("{:,.2f}%".format)
summary_df
325/15:
schools_overview_group = pycityschools_data_complete.groupby(["school_name"])
school_summary_df = schools_overview_group.agg({"Student ID": "count",
                            "budget": "max",
                           "math_score": "mean",
                            "reading_score": "mean",
                           })
school_summary_df["Per Student Budget"] = school_summary_df["budget"] / school_summary_df["Student ID"]
school_summary_df
325/16:
school_type = pycityschools_data_complete[["type", "school_name"]]
school_type
325/17:
# Passing Math Score
passing_math_stu = pycityschools_data_complete.loc[pycityschools_data_complete["math_score"] >=70]
passing_math_stu
325/18:
passing_overall_stu = pycityschools_data_complete.loc[(pycityschools_data_complete["math_score"] >=70)&
                                                  (pycityschools_data_complete["reading_score"] >=70)]
passing_overall_stu
325/19:
passing_reading_stu = pycityschools_data_complete.loc[pycityschools_data_complete["reading_score"] >=70]
passing_reading_stu
325/20:
passing_math_count = passing_math_stu.groupby(["school_name"]).count()["math_score"]
passing_math_count
325/21:
passing_reading_count = passing_reading_stu.groupby(["school_name"]).count()["reading_score"]
passing_reading_count
325/22:
passing_overall_count = passing_overall_stu.groupby(["school_name"]).count()["reading_score"]
passing_overall_count
325/23:
school_summary_df["% Passing Math"] = passing_math_count / school_summary_df["Student ID"] * 100
school_summary_df["% Passing Reading"] = passing_reading_count / school_summary_df["Student ID"] * 100
school_summary_df["% Passing Overall"] = passing_overall_count / school_summary_df["Student ID"] * 100
school_summary_df
325/24:
school_summary_df = school_summary_df.rename(columns={"budget": "Total School Budget",
                                                      "Student ID": "Total Students",
                                                     "math_score": "Average Math Score",
                                                      "reading_score": "Average Reading Score",
                                                     })
school_summary_df
325/25:
school_summary_df = school_summary_df.style.format({
                          'Total Students': '{:,}', 
                          "Total School Budget": "${:,}", 
                          "Per Student Budget": "${:.0f}",
                          'Average Math Score': "{:.1f}%", 
                          'Average Reading Score': "{:.1f}%", 
                          "% Passing Math": "{:.1f}%", 
                          "% Passing Reading": "{:.1f}%", 
                          "% Passing Overall": "{:.1f}%"
                         })
school_summary_df
325/26:
# Create a table that highlights the top 5 performing schools based on % Overall Passing. Include:

# School Name
# School Type
# Total Students
# Total School Budget
# Per Student Budget
# Average Math Score
# Average Reading Score
# % Passing Math (The percentage of students that passed math.)
# % Passing Reading (The percentage of students that passed reading.)
# % Overall Passing (The percentage of students that passed math and reading.)
325/27:
# Find top 5 performing schools based on % Overall Passing
top_five_schools = school_summary_df.sort_values('% Passing Overall', ascending=False)
top_five_schools
325/28:
# Create Bins and their labels
bins = [0, 550, 600, 650, 700]
group_labels = ["Less than $549", "$550 to $599", "$600 to $650", "Greater than $700"]
325/29:
school_spending['Spending Per Student'] = pd.cut(school_summary_df['Per Student Budget'],bins, labels=group_labels)
school_spending
328/1:
school_spending['Spending Per Student'] = pd.cut(school_summary_df["Per Student Budget"], bins, labels=group_labels)
school_spending
331/1:
# Dependencies
import pandas as pd
import numpy as np
331/2:
# load in file
pycityschools_file = "Resources/schools_complete.csv"
student_file = "Resources/students_complete.csv"
331/3:
# Read School and Student Data File and store into Pandas DataFrames
pycityschools_data_df = pd.read_csv(pycityschools_file)
student_data_df = pd.read_csv(student_file)

pycityschools_data_df.head()
student_data_df.head()
331/4:
pycityschools_data_complete = pd.merge(student_data_df, pycityschools_data_df, 
                                       on=["school_name", "school_name"], how="left")

pycityschools_data_complete.head()
331/5:
# Count total schools
total_schools = len(pycityschools_data_complete["School ID"].unique())
total_schools
331/6:
# Calculate the total number of students
total_students = len(pycityschools_data_complete["student_name"].unique())
total_students
331/7:
# Calculate Total Budget
total_budget = pycityschools_data_df ["budget"].sum()
total_budget
331/8:
# Calculate the average math score
average_mathscore = round(pycityschools_data_complete["math_score"].mean(), 2)
average_mathscore
331/9:
# Calculate the average reading score
average_readingscore = round(pycityschools_data_complete["reading_score"].mean(), 2)
average_readingscore
331/10:
# Calculate the percentage of students with a passing math score (70 or greater)
unique_math_passing = len(pycityschools_data_complete.loc[pycityschools_data_complete["math_score"] >=70]["student_name"].unique())
percent_math_passing = round((unique_math_passing/total_students)*100, 2)
percent_math_passing
331/11:
# Calculate the percentage of students with a passing reading score (70 or greater)
unique_reading_passing = len(pycityschools_data_complete.loc[pycityschools_data_complete["reading_score"] >=70]["student_name"].unique())
percent_reading_passing = round((unique_reading_passing/total_students)*100, 2)
percent_reading_passing
331/12:
# Calculate the percentage of students who passed math and reading (% Overall Passing)
overall_passing = round((percent_reading_passing + percent_math_passing) / 2, 2)
overall_passing
331/13:
# Create a dataframe to hold the above results
summary_df = pd.DataFrame({"Total Schools": [total_schools],
                          "Total Students": [total_students],
                          "Total Budget": [total_budget],
                          "Average Math Score": [average_mathscore],
                          "Average Reading Score": [average_readingscore],
                          "% of Students with Passing Math Score": [percent_math_passing],
                          "% of Students with Passing Reading Score": [percent_reading_passing],
                          "% of Students with Passing Reading and Math Score": [overall_passing]
                          })
summary_df
331/14:
# Give the displayed data cleaner formatting
summary_df["Total Students"] = summary_df["Total Students"].map("{:,.2f}".format)
summary_df["Total Budget"] = summary_df["Total Budget"].map("${:,.2f}".format)
summary_df["Average Math Score"] = summary_df["Average Math Score"].map("{:,.2f}%".format)
summary_df["Average Reading Score"] = summary_df["Average Reading Score"].map("{:,.2f}%".format)
summary_df["% of Students with Passing Math Score"] = summary_df["% of Students with Passing Math Score"].map("{:,.2f}%".format)
summary_df["% of Students with Passing Reading Score"] = summary_df["% of Students with Passing Reading Score"].map("{:,.2f}%".format)
summary_df["% of Students with Passing Reading and Math Score"] = summary_df["% of Students with Passing Reading and Math Score"].map("{:,.2f}%".format)
summary_df
331/15:
schools_overview_group = pycityschools_data_complete.groupby(["school_name"])
school_summary_df = schools_overview_group.agg({"Student ID": "count",
                            "budget": "max",
                           "math_score": "mean",
                            "reading_score": "mean",
                           })
school_summary_df["Per Student Budget"] = school_summary_df["budget"] / school_summary_df["Student ID"]
school_summary_df
331/16:
school_type = pycityschools_data_complete[["type", "school_name"]]
school_type
331/17:
# Passing Math Score
passing_math_stu = pycityschools_data_complete.loc[pycityschools_data_complete["math_score"] >=70]
passing_math_stu
331/18:
passing_overall_stu = pycityschools_data_complete.loc[(pycityschools_data_complete["math_score"] >=70)&
                                                  (pycityschools_data_complete["reading_score"] >=70)]
passing_overall_stu
331/19:
passing_reading_stu = pycityschools_data_complete.loc[pycityschools_data_complete["reading_score"] >=70]
passing_reading_stu
331/20:
passing_math_count = passing_math_stu.groupby(["school_name"]).count()["math_score"]
passing_math_count
331/21:
passing_reading_count = passing_reading_stu.groupby(["school_name"]).count()["reading_score"]
passing_reading_count
331/22:
passing_overall_count = passing_overall_stu.groupby(["school_name"]).count()["reading_score"]
passing_overall_count
331/23:
school_summary_df["% Passing Math"] = passing_math_count / school_summary_df["Student ID"] * 100
school_summary_df["% Passing Reading"] = passing_reading_count / school_summary_df["Student ID"] * 100
school_summary_df["% Passing Overall"] = passing_overall_count / school_summary_df["Student ID"] * 100
school_summary_df
331/24:
school_summary_df = school_summary_df.rename(columns={"budget": "Total School Budget",
                                                      "Student ID": "Total Students",
                                                     "math_score": "Average Math Score",
                                                      "reading_score": "Average Reading Score",
                                                     })
school_summary_df
331/25:
school_summary_df = school_summary_df.style.format({
                          'Total Students': '{:,}', 
                          "Total School Budget": "${:,}", 
                          "Per Student Budget": "${:.0f}",
                          'Average Math Score': "{:.1f}%", 
                          'Average Reading Score': "{:.1f}%", 
                          "% Passing Math": "{:.1f}%", 
                          "% Passing Reading": "{:.1f}%", 
                          "% Passing Overall": "{:.1f}%"
                         })
school_summary_df
331/26:
# Create a table that highlights the top 5 performing schools based on % Overall Passing. Include:

# School Name
# School Type
# Total Students
# Total School Budget
# Per Student Budget
# Average Math Score
# Average Reading Score
# % Passing Math (The percentage of students that passed math.)
# % Passing Reading (The percentage of students that passed reading.)
# % Overall Passing (The percentage of students that passed math and reading.)
331/27:
# Find top 5 performing schools based on % Overall Passing
# top_five_schools = school_summary_df.sort_values('% Passing Overall', ascending=False)
# top_five_schools
331/28:
# Create a table that highlights the bottom 5 performing schools based on % Overall Passing. Include all of the same metrics as above.
# bottom_five = school_summary_df.sort_values("% Passing Overall")
# bottom_five.head()
331/29:
# Create a table that lists the average Math Score for students of each grade level (9th, 10th, 11th, 12th) at each school.

average_math_score_9th = student_data_df.loc[student_data_df['grade'] == '9th'].groupby('school_name')["math_score"].mean()
average_math_score_10th = student_data_df.loc[student_data_df['grade'] == '10th'].groupby('school_name')["math_score"].mean()
average_math_score_11th = student_data_df.loc[student_data_df['grade'] == '11th'].groupby('school_name')["math_score"].mean()
average_math_score_12th = student_data_df.loc[student_data_df['grade'] == '12th'].groupby('school_name')["math_score"].mean()

average_math_scores_grade = pd.DataFrame({
        "9th Grade": average_math_score_9th,
        "10th Grade": average_math_score_10th,
        "11th Grade": average_math_score_11th,
        "12th Grade": average_math_score_12th
        })

average_math_scores_grade.head()
331/30:
# Reading Score 
average_reading_score_9th = student_data_df.loc[student_data_df['grade'] == '9th'].groupby('school_name')["reading_score"].mean()
average_reading_score_10th = student_data_df.loc[student_data_df['grade'] == '10th'].groupby('school_name')["reading_score"].mean()
average_reading_score_11th = student_data_df.loc[student_data_df['grade'] == '11th'].groupby('school_name')["reading_score"].mean()
average_reading_score_12th = student_data_df.loc[student_data_df['grade'] == '12th'].groupby('school_name')["reading_score"].mean()

average_reading_scores_grade = pd.DataFrame({
        "9th Grade": average_reading_score_9th,
        "10th Grade": average_reading_score_10th,
        "11th Grade": average_reading_score_11th,
        "12th Grade": average_reading_score_12th
        })

average_reading_scores_grade.head()
331/31:
# Create a table that breaks down school performances based on average Spending Ranges (Per Student). 
#Use 4 reasonable bins to group school spending. Include in the table each of the following:

# Average Math Score
# Average Reading Score
# % Passing Math (The percentage of students that passed math.)
# % Passing Reading (The percentage of students that passed reading.)
# % Overall Passing (The percentage of students that passed math and reading.)
331/32:
# Create Bins and their labels
bins = [0, 550, 600, 650, 700]
group_labels = ["Less than $549", "$550 to $599", "$600 to $650", "Greater than $700"]
331/33:
school_spending['Spending Per Student'] = pd.cut(school_summary_df["Per Student Budget"], bins, labels=group_labels)
school_spending
331/34:
school_spending = pd.cut(school_summary_df["Per Student Budget"], bins, labels=group_labels)
school_spending
331/35:
school_spending["Spending Per Student"] = pd.cut(school_summary_df["Per Student Budget"], bins, labels=group_labels)
school_spending
331/36: pd.cut(school_summary_df["Per Student Budget"], bins, labels=group_labels)
331/37: pd.cut(school_summary_df["Per Student Budget"], bins, labels=group_labels).head()
332/1:
# Dependencies
import pandas as pd
import numpy as np
332/2:
# load in file
pycityschools_file = "Resources/schools_complete.csv"
student_file = "Resources/students_complete.csv"
332/3:
# Read School and Student Data File and store into Pandas DataFrames
pycityschools_data_df = pd.read_csv(pycityschools_file)
student_data_df = pd.read_csv(student_file)

pycityschools_data_df.head()
student_data_df.head()
332/4:
pycityschools_data_complete = pd.merge(student_data_df, pycityschools_data_df, 
                                       on=["school_name", "school_name"], how="left")

pycityschools_data_complete.head()
332/5:
# Count total schools
total_schools = len(pycityschools_data_complete["School ID"].unique())
total_schools
332/6:
# Calculate the total number of students
total_students = len(pycityschools_data_complete["student_name"].unique())
total_students
332/7:
# Calculate Total Budget
total_budget = pycityschools_data_df ["budget"].sum()
total_budget
332/8:
# Calculate the average math score
average_mathscore = round(pycityschools_data_complete["math_score"].mean(), 2)
average_mathscore
332/9:
# Calculate the average reading score
average_readingscore = round(pycityschools_data_complete["reading_score"].mean(), 2)
average_readingscore
332/10:
# Calculate the percentage of students with a passing math score (70 or greater)
unique_math_passing = len(pycityschools_data_complete.loc[pycityschools_data_complete["math_score"] >=70]["student_name"].unique())
percent_math_passing = round((unique_math_passing/total_students)*100, 2)
percent_math_passing
332/11:
# Calculate the percentage of students with a passing reading score (70 or greater)
unique_reading_passing = len(pycityschools_data_complete.loc[pycityschools_data_complete["reading_score"] >=70]["student_name"].unique())
percent_reading_passing = round((unique_reading_passing/total_students)*100, 2)
percent_reading_passing
332/12:
# Calculate the percentage of students who passed math and reading (% Overall Passing)
overall_passing = round((percent_reading_passing + percent_math_passing) / 2, 2)
overall_passing
332/13:
# Create a dataframe to hold the above results
summary_df = pd.DataFrame({"Total Schools": [total_schools],
                          "Total Students": [total_students],
                          "Total Budget": [total_budget],
                          "Average Math Score": [average_mathscore],
                          "Average Reading Score": [average_readingscore],
                          "% of Students with Passing Math Score": [percent_math_passing],
                          "% of Students with Passing Reading Score": [percent_reading_passing],
                          "% of Students with Passing Reading and Math Score": [overall_passing]
                          })
summary_df
332/14:
# Give the displayed data cleaner formatting
summary_df["Total Students"] = summary_df["Total Students"].map("{:,.2f}".format)
summary_df["Total Budget"] = summary_df["Total Budget"].map("${:,.2f}".format)
summary_df["Average Math Score"] = summary_df["Average Math Score"].map("{:,.2f}%".format)
summary_df["Average Reading Score"] = summary_df["Average Reading Score"].map("{:,.2f}%".format)
summary_df["% of Students with Passing Math Score"] = summary_df["% of Students with Passing Math Score"].map("{:,.2f}%".format)
summary_df["% of Students with Passing Reading Score"] = summary_df["% of Students with Passing Reading Score"].map("{:,.2f}%".format)
summary_df["% of Students with Passing Reading and Math Score"] = summary_df["% of Students with Passing Reading and Math Score"].map("{:,.2f}%".format)
summary_df
332/15:
schools_overview_group = pycityschools_data_complete.groupby(["school_name"])
school_summary_df = schools_overview_group.agg({"Student ID": "count",
                            "budget": "max",
                           "math_score": "mean",
                            "reading_score": "mean",
                           })
school_summary_df["Per Student Budget"] = school_summary_df["budget"] / school_summary_df["Student ID"]
school_summary_df
332/16:
school_type = pycityschools_data_complete[["type", "school_name"]]
school_type
332/17:
# Passing Math Score
passing_math_stu = pycityschools_data_complete.loc[pycityschools_data_complete["math_score"] >=70]
passing_math_stu
332/18:
passing_overall_stu = pycityschools_data_complete.loc[(pycityschools_data_complete["math_score"] >=70)&
                                                  (pycityschools_data_complete["reading_score"] >=70)]
passing_overall_stu
332/19:
passing_reading_stu = pycityschools_data_complete.loc[pycityschools_data_complete["reading_score"] >=70]
passing_reading_stu
332/20:
passing_math_count = passing_math_stu.groupby(["school_name"]).count()["math_score"]
passing_math_count
332/21:
passing_reading_count = passing_reading_stu.groupby(["school_name"]).count()["reading_score"]
passing_reading_count
332/22:
passing_overall_count = passing_overall_stu.groupby(["school_name"]).count()["reading_score"]
passing_overall_count
332/23:
school_summary_df["% Passing Math"] = passing_math_count / school_summary_df["Student ID"] * 100
school_summary_df["% Passing Reading"] = passing_reading_count / school_summary_df["Student ID"] * 100
school_summary_df["% Passing Overall"] = passing_overall_count / school_summary_df["Student ID"] * 100
school_summary_df
332/24:
school_summary_df = school_summary_df.rename(columns={"budget": "Total School Budget",
                                                      "Student ID": "Total Students",
                                                     "math_score": "Average Math Score",
                                                      "reading_score": "Average Reading Score",
                                                     })
school_summary_df
332/25:
school_summary_df = school_summary_df.style.format({
                          'Total Students': '{:,}', 
                          "Total School Budget": "${:,}", 
                          "Per Student Budget": "${:.0f}",
                          'Average Math Score': "{:.1f}%", 
                          'Average Reading Score': "{:.1f}%", 
                          "% Passing Math": "{:.1f}%", 
                          "% Passing Reading": "{:.1f}%", 
                          "% Passing Overall": "{:.1f}%"
                         })
school_summary_df
332/26:
# Create a table that highlights the top 5 performing schools based on % Overall Passing. Include:

# School Name
# School Type
# Total Students
# Total School Budget
# Per Student Budget
# Average Math Score
# Average Reading Score
# % Passing Math (The percentage of students that passed math.)
# % Passing Reading (The percentage of students that passed reading.)
# % Overall Passing (The percentage of students that passed math and reading.)
332/27:
# Find top 5 performing schools based on % Overall Passing
# top_five_schools = school_summary_df.sort_values('% Passing Overall', ascending=False)
# top_five_schools
332/28:
# Create a table that highlights the bottom 5 performing schools based on % Overall Passing. Include all of the same metrics as above.
# bottom_five = school_summary_df.sort_values("% Passing Overall")
# bottom_five.head()
332/29:
# Create a table that lists the average Math Score for students of each grade level (9th, 10th, 11th, 12th) at each school.

average_math_score_9th = student_data_df.loc[student_data_df['grade'] == '9th'].groupby('school_name')["math_score"].mean()
average_math_score_10th = student_data_df.loc[student_data_df['grade'] == '10th'].groupby('school_name')["math_score"].mean()
average_math_score_11th = student_data_df.loc[student_data_df['grade'] == '11th'].groupby('school_name')["math_score"].mean()
average_math_score_12th = student_data_df.loc[student_data_df['grade'] == '12th'].groupby('school_name')["math_score"].mean()

average_math_scores_grade = pd.DataFrame({
        "9th Grade": average_math_score_9th,
        "10th Grade": average_math_score_10th,
        "11th Grade": average_math_score_11th,
        "12th Grade": average_math_score_12th
        })

average_math_scores_grade.head()
332/30:
# Reading Score 
average_reading_score_9th = student_data_df.loc[student_data_df['grade'] == '9th'].groupby('school_name')["reading_score"].mean()
average_reading_score_10th = student_data_df.loc[student_data_df['grade'] == '10th'].groupby('school_name')["reading_score"].mean()
average_reading_score_11th = student_data_df.loc[student_data_df['grade'] == '11th'].groupby('school_name')["reading_score"].mean()
average_reading_score_12th = student_data_df.loc[student_data_df['grade'] == '12th'].groupby('school_name')["reading_score"].mean()

average_reading_scores_grade = pd.DataFrame({
        "9th Grade": average_reading_score_9th,
        "10th Grade": average_reading_score_10th,
        "11th Grade": average_reading_score_11th,
        "12th Grade": average_reading_score_12th
        })

average_reading_scores_grade.head()
332/31:
# Create a table that breaks down school performances based on average Spending Ranges (Per Student). 
#Use 4 reasonable bins to group school spending. Include in the table each of the following:

# Average Math Score
# Average Reading Score
# % Passing Math (The percentage of students that passed math.)
# % Passing Reading (The percentage of students that passed reading.)
# % Overall Passing (The percentage of students that passed math and reading.)
332/32:
# Create Bins and their labels
bins = [0, 550, 600, 650, 700]
group_labels = ["Less than $549", "$550 to $599", "$600 to $650", "Greater than $700"]
332/33: pd.cut(school_summary_df["Per Student Budget"], bins, labels=group_labels).head()
332/34: pd.cut(school_summary_df['Per Student Budget'], bins, labels=group_labels).head()
334/1: %matplotlib notebook
334/2:
# Dependencies
import matplotlib.pyplot as plt
import pandas as pd
import scipy.stats as st
334/3:
# Dependencies
import matplotlib.pyplot as plt
import pandas as pd
import scipy.stats as st
import numpy as np
335/1:
# Dependencies
import pandas as pd
import matplotlib.pyplot as plt
import scipy.stats as sts
336/1:
# Dependencies
import pandas as pd
import matplotlib.pyplot as plt
import scipy.stats as sts
336/2:
# Read in the california housing data set
CAhousing_data = pd.read_csv('../Resources/California_Housing.csv')
CAhousing_data.head()
336/3:
# Determine which measure of central tendency is most appropriate to describe the Population
plt.hist(CAhousing_data['Population'])
plt.xlabel('Population')
plt.ylabel('Counts')
plt.show()
print(CAhousing_data['Population'].mean())
print(CAhousing_data['Population'].median())
print(CAhousing_data['Population'].mode())
336/4:
# Determine if the house age in California is considered normally distributed
plt.hist(CAhousing_data['HouseAge'])
plt.xlabel('House Age (years)')
plt.ylabel('Counts')
plt.show()
print(sts.normaltest(CAhousing_data['HouseAge'].sample(100)))
338/1:
# Dependencies
import pandas as pd
import matplotlib.pyplot as plt
import scipy.stats as sts
338/2:
# Read in the california housing data set
CAhousing_data = pd.read_csv('../Resources/California_Housing.csv')
CAhousing_data.head()
338/3:
# Determine which measure of central tendency is most appropriate to describe the Population
plt.hist(CAhousing_data['Population'])
plt.xlabel('Population')
plt.ylabel('Counts')
plt.show()
print(CAhousing_data['Population'].mean())
print(CAhousing_data['Population'].median())
print(CAhousing_data['Population'].mode())
338/4:
# Determine if the house age in California is considered normally distributed
plt.hist(CAhousing_data['HouseAge'])
plt.xlabel('House Age (years)')
plt.ylabel('Counts')
plt.show()
print(sts.normaltest(CAhousing_data['HouseAge'].sample(100)))
338/5:
# Determine if there are any potential outliers in the average occupancy in California
quartiles = CAhousing_data['AveOccup'].quantile([.25,.5,.75])
lowerq = quartiles[0.25]
upperq = quartiles[0.75]
iqr = upperq-lowerq

print(f"The lower quartile of occupancy is: {lowerq}")
print(f"The upper quartile of occupancy is: {upperq}")
print(f"The interquartile range of occupancy is: {iqr}")
print(f"The the median of occupancy is: {quartiles[0.5]} ")
338/6: # With the potential outliers, what is the lowest and highest median income (in $1000s) observed?
338/7: # Bonus - plot the latitude and longitude of the California housing data using Matplotlib, color the data points using the median income of the block.
338/8:
# Determine if there are any potential outliers in the average occupancy in California
quartiles = CAhousing_data['AveOccup'].quantile([.25,.5,.75])
lowerq = quartiles[0.25]
upperq = quartiles[0.75]
iqr = upperq-lowerq

print(f"The lower quartile of occupancy is: {lowerq}")
print(f"The upper quartile of occupancy is: {upperq}")
print(f"The interquartile range of occupancy is: {iqr}")
print(f"The the median of occupancy is: {quartiles[0.5]} ")

lower_bound = lowerq - (1.5*iqr)
upper_bound = upperq + (1.5*iqr)
print(f"Values below {lower_bound} could be outliers.")
print(f"Values above {upper_bound} could be outliers.")

outlier_occupancy = CAhousing_data.loc[(CAhousing_data['AveOccup'] < lower_bound) | (CAhousing_data['AveOccup'] > upper_bound)]
outlier_occupancy
339/1:
# Read Boston housing data into a Pandas dataframe
housing_data = pd.DataFrame(data=boston_dataset.data,columns=boston_dataset.feature_names)
housing_data['MEDV'] = boston_dataset.target
housing_data.head()
342/1:
# Dependencies
from matplotlib import pyplot as plt
import numpy as np
import pandas as pd
from sklearn.datasets import load_boston
from scipy.stats import sem
343/1:
# Dependencies
from matplotlib import pyplot as plt
import numpy as np
import pandas as pd
from sklearn.datasets import load_boston
from scipy.stats import sem
343/2:
# Import the Boston housing data set from sklearn and get description
boston_dataset = load_boston()
343/3:
# Read Boston housing data into a Pandas dataframe
housing_data = pd.DataFrame(data=boston_dataset.data,columns=boston_dataset.feature_names)
housing_data['MEDV'] = boston_dataset.target
housing_data.head()
343/4: # Create a bunch of samples, each with sample size of 20
343/5: # Calculate standard error of means
343/6: # Determine which sample's mean is closest to the population mean
343/7: # Compare to the population mean
343/8: # Plot sample means with error bars
343/9:
# Determine which sample's mean is closest to the population mean
print(f"The smallest SEM observed was {min(sems)}")
samp_index = sems.index(min(sems))
print(f"The sample with the smallest SEM is sample {samp_index+1}")
343/10:
# Calculate standard error of means
means = [s.MEDV.mean() for s in samples]

sems = [sem(s.MEDV) for s in samples]
343/11:
# Determine which sample's mean is closest to the population mean
print(f"The smallest SEM observed was {min(sems)}")
samp_index = sems.index(min(sems))
print(f"The sample with the smallest SEM is sample {samp_index+1}")
344/1:
# Dependencies
from matplotlib import pyplot as plt
import numpy as np
import pandas as pd
from sklearn.datasets import load_boston
from scipy.stats import sem
344/2:
# Import the Boston housing data set from sklearn and get description
boston_dataset = load_boston()
344/3:
# Read Boston housing data into a Pandas dataframe
housing_data = pd.DataFrame(data=boston_dataset.data,columns=boston_dataset.feature_names)
housing_data['MEDV'] = boston_dataset.target
housing_data.head()
344/4:
# Create a bunch of samples, each with sample size of 20
nsamples = 25
div = 20
samples = [housing_data.sample(div) for x in range(0,nsamples)]
344/5:
# Calculate standard error of means
means = [s.MEDV.mean() for s in samples]

sems = [sem(s.MEDV) for s in samples]
344/6:
# Determine which sample's mean is closest to the population mean
print(f"The smallest SEM observed was {min(sems)}")
samp_index = sems.index(min(sems))
print(f"The sample with the smallest SEM is sample {samp_index+1}")
344/7: # Compare to the population mean
344/8: # Plot sample means with error bars
344/9:
# Compare to the population mean
print(f"The mean of the sample 5 is {samples[samp_index].MEDV.mean()}")
print(f"The mean of the population data set is {housing_data.MEDV.mean()}")
344/10:
# Plot sample means with error bars
fig, ax = plt.subplots()
ax.errorbar(np.arange(0, len(samples), 1)+1,means, yerr=sems, fmt="o", color="p",
            alpha=0.5, label="Mean of House Prices")
ax.set_xlim(0, len(means)+1)
ax.set_xlabel("Sample Number")
ax.set_ylabel("Mean of Median House Prices ($1000)")
plt.legend(loc="best", fontsize="small", fancybox=True)
plt.show()
345/1:
# Dependencies
from matplotlib import pyplot as plt
import numpy as np
import pandas as pd
from sklearn.datasets import load_boston
from scipy.stats import sem
345/2:
# Import the Boston housing data set from sklearn and get description
boston_dataset = load_boston()
345/3:
# Read Boston housing data into a Pandas dataframe
housing_data = pd.DataFrame(data=boston_dataset.data,columns=boston_dataset.feature_names)
housing_data['MEDV'] = boston_dataset.target
housing_data.head()
345/4:
# Create a bunch of samples, each with sample size of 20
nsamples = 25
div = 20
samples = [housing_data.sample(div) for x in range(0,nsamples)]
345/5:
# Calculate standard error of means
means = [s.MEDV.mean() for s in samples]

sems = [sem(s.MEDV) for s in samples]
345/6:
# Determine which sample's mean is closest to the population mean
print(f"The smallest SEM observed was {min(sems)}")
samp_index = sems.index(min(sems))
print(f"The sample with the smallest SEM is sample {samp_index+1}")
345/7:
# Compare to the population mean
print(f"The mean of the sample 5 is {samples[samp_index].MEDV.mean()}")
print(f"The mean of the population data set is {housing_data.MEDV.mean()}")
345/8:
# Plot sample means with error bars
fig, ax = plt.subplots()
ax.errorbar(np.arange(0, len(samples), 1)+1,means, yerr=sems, fmt="o", color="p",
            alpha=0.5, label="Mean of House Prices")
ax.set_xlim(0, len(means)+1)
ax.set_xlabel("Sample Number")
ax.set_ylabel("Mean of Median House Prices ($1000)")
plt.legend(loc="best", fontsize="small", fancybox=True)
plt.show()
345/9:
# Plot sample means with error bars
fig, ax = plt.subplots()
ax.errorbar(np.arange(0, len(samples), 1)+1,means, yerr=sems, fmt="o", color="b",
            alpha=0.5, label="Mean of House Prices")
ax.set_xlim(0, len(means)+1)
ax.set_xlabel("Sample Number")
ax.set_ylabel("Mean of Median House Prices ($1000)")
plt.legend(loc="best", fontsize="small", fancybox=True)
plt.show()
347/1:
# Plot malic_acid versus flavanoids on a scatterplot
malic_acid = wine_data.malic_acid
flavanoids = wine_data.flavanoids
plt.scatter(malic_acid,flavanoids)
349/1:
# Dependencies
import pandas as pd
import sklearn.datasets as dta
import scipy.stats as st
import matplotlib.pyplot as plt
349/2:
# Read in the wine recognition data set from sklearn and load into Pandas
data = dta.load_wine()
wine_data = pd.DataFrame(data.data,columns=data.feature_names)
print(data.DECR)
349/3:
# Read in the wine recognition data set from sklearn and load into Pandas
data = dta.load_wine()
wine_data = pd.DataFrame(data.data,columns=data.feature_names)
349/4:
# Plot malic_acid versus flavanoids on a scatterplot
malic_acid = wine_data.malic_acid
flavanoids = wine_data.flavanoids
plt.scatter(malic_acid,flavanoids)
349/5:
# Calculate the correlation coefficient between malic_acid and flavanoids
print(f"The correlation coefficient between malic acid and flavanoids is {round(st.pearsonr(malic_acid,flavanoids)[0],2)}")
349/6:
# Plot alcohol versus colour_intensity on a scatterplot
alcohol = wine_data.alcohol
color_intensity = wine_data.color_intensity
plt.scatter(alcohol,color_intensity)
plt.xlabel("Amount of Alcohol")
plt.ylabel("Intensity of Color")
plt.show()
349/7:
# Calculate the correlation coefficient between alcohol and color_intensity
print(f"The correlation coefficient between alcohol and color intensity is {round(st.pearsonr(alcohol,color_intensity)[0],2)}")
349/8:
# BONUS: Generate the correlation matrix and find the strongest positive and negative correlations
wine_corr = wine_data.corr()
wine_corr
349/9:
# BONUS: Generate the correlation matrix and find the strongest positive and negative correlations
wine_corr = wine_data.corr()
wine_corr
349/10: wine_corr.unstack().sort_values()
352/1:
# Dependencies
from matplotlib import pyplot as plt
from scipy import stats
import numpy as np
import pandas as pd
352/2:
# Load crime data set into pandas
crime_data = pd.read_csv("../Resources/crime_data.csv")
352/3:
# Generate a scatter plot of violent crime rate versus year
year = crime_data.iloc[:,0]
violent_crime_rate = crime_data.iloc[:,3]
plt.scatter(year,violent_crime_rate)
plt.xticks(year, rotation=90)
plt.xlabel('Year')
plt.ylabel('Violent Crime Rate')
plt.show()
352/4: # Perform a linear regression on violent crime rate versus year
352/5: # Create equation of line to calculate predicted violent crime rate
352/6: # Plot the linear model on top of scatter plot
352/7: # Repeat plotting scatter and linear model for murder rate versus year
352/8: # Repeat plotting scatter and linear model for aggravated assault versus year
352/9: # Generate a facet plot of all 3 figures
352/10: # Calculate the crime rates for 2019
352/11:
# Plot the linear model on top of scatter plot
year = crime_data.iloc[:,0]
violent_crime_rate = crime_data.iloc[:,3]
plt.scatter(year,violent_crime_rate)
plt.plot(year,vc_fit, "--")
plt.xticks(year, rotation=90)
plt.xlabel('Year')
plt.ylabel('Violent Crime Rate')
plt.show()
352/12:
# Plot the linear model on top of scatter plot
year = crime_data.iloc[:,0]
violent_crime_rate = crime_data.iloc[:,3]
plt.scatter(year,violent_crime_rate)
plt.plot(year,vc_fit,"--")
plt.xticks(year, rotation=90)
plt.xlabel('Year')
plt.ylabel('Violent Crime Rate')
plt.show()
353/1:
# Dependencies
from matplotlib import pyplot as plt
from scipy import stats
import numpy as np
import pandas as pd
353/2:
# Load crime data set into pandas
crime_data = pd.read_csv("../Resources/crime_data.csv")
353/3:
# Generate a scatter plot of violent crime rate versus year
year = crime_data.iloc[:,0]
violent_crime_rate = crime_data.iloc[:,3]
plt.scatter(year,violent_crime_rate)
plt.xticks(year, rotation=90)
plt.xlabel('Year')
plt.ylabel('Violent Crime Rate')
plt.show()
353/4:
# Perform a linear regression on violent crime rate versus year
vc_slope, vc_int, vc_r, vc_p, vc_strd_err = stats.linregress(year, violent_crime_rate)
353/5:
# Create equation of line to calculate predicted violent crime rate
vc_fit = vc_slope * year + vc_int
353/6:
# Plot the linear model on top of scatter plot
year = crime_data.iloc[:,0]
violent_crime_rate = crime_data.iloc[:,3]
plt.scatter(year,violent_crime_rate)
plt.plot(year,vc_fit,"--")
plt.xticks(year, rotation=90)
plt.xlabel('Year')
plt.ylabel('Violent Crime Rate')
plt.show()
353/7: # Repeat plotting scatter and linear model for murder rate versus year
353/8: # Repeat plotting scatter and linear model for aggravated assault versus year
353/9: # Generate a facet plot of all 3 figures
353/10: # Calculate the crime rates for 2019
354/1:
# Dependencies
from matplotlib import pyplot as plt
from scipy import stats
import numpy as np
import pandas as pd
354/2:
# Load crime data set into pandas
crime_data = pd.read_csv("../Resources/crime_data.csv")
354/3:
# Generate a scatter plot of violent crime rate versus year
year = crime_data.iloc[:,0]
violent_crime_rate = crime_data.iloc[:,3]
plt.scatter(year,violent_crime_rate)
plt.xticks(year, rotation=90)
plt.xlabel('Year')
plt.ylabel('Violent Crime Rate')
plt.show()
354/4:
# Perform a linear regression on violent crime rate versus year
vc_slope, vc_int, vc_r, vc_p, vc_strd_err = stats.linregress(year, violent_crime_rate)
354/5:
# Create equation of line to calculate predicted violent crime rate
vc_fit = vc_slope * year + vc_int
354/6:
# Plot the linear model on top of scatter plot
year = crime_data.iloc[:,0]
violent_crime_rate = crime_data.iloc[:,3]
plt.scatter(year,violent_crime_rate)
plt.plot(year,vc_fit,"--")
plt.xticks(year, rotation=90)
plt.xlabel('Year')
plt.ylabel('Violent Crime Rate')
plt.show()
354/7:
# Repeat plotting scatter and linear model for murder rate versus year
murder_rate = crime_data.iloc[:, 5]
m_slope, m_int, m_r, m_p, m_std_err = stats.linregress(year, murder_rate)
m_fit = m_slope * year + m_int
plt.scatter(year,murder_rate)
plt.plot(year,m_fit,"--")
plt.xticks(year, rotation=90)
plt.xlabel('Year')
plt.ylabel('Murder Rate')
plt.show()
354/8: # Repeat plotting scatter and linear model for aggravated assault versus year
354/9: # Generate a facet plot of all 3 figures
354/10: # Calculate the crime rates for 2019
354/11:
# Repeat plotting scatter and linear model for aggravated assault versus year
aggravated_assault_rate = crime_data.iloc[:, 9]
aa_slope, aa_int, aa_r, aa_p, aa_std_err = stats.linregress(
    year, aggravated_assault_rate)
aa_fit = aa_slope * year + aa_int
plt.scatter(year,aggravated_assault_rate)
plt.plot(year,aa_fit,"--")
plt.xticks(year, rotation=90)
plt.xlabel('Year')
plt.ylabel('Aggravated Assault Rate')
plt.show()
354/12:
# Generate a facet plot of all 3 figures
fig, (ax1, ax2, ax3) = plt.subplots(3, sharex=True)
fig.suptitle("Crime Rates Over Time", fontsize=16, fontweight="bold")

ax1.set_xlim(min(year), max(year))
ax1.plot(year, violent_crime_rate, linewidth=1, marker="o")
ax1.plot(year, vc_fit, "b--", linewidth=1)
ax1.set_ylabel("Violent Crime Rate")

ax2.plot(year, murder_rate, linewidth=1, marker="o", color="r")
ax2.plot(year, m_fit, "r--", linewidth=1)
ax2.set_ylabel("Murder Rate")

ax3.plot(year, aggravated_assault_rate, linewidth=1, marker="o", color="g")
ax3.plot(year, aa_fit, "g--", linewidth=1)
ax3.set_ylabel("Aggravated Assault Rate")
ax3.set_xlabel("Year")

plt.show()
354/13:
# Calculate the crime rates for 2019
year = 2019
print(f"The violent crime rate in 2019 will be {round(vc_slope * year + vc_int,2)}.")
print(f"The murder rate in 2019 will be {round(m_slope * year + m_int,2)}.")
print(f"The aggravated assault rate in 2019 will be {round(aa_slope * year + aa_int,2)}.")
355/1:
# Find top 5 performing schools based on % Overall Passing
top_five_schools = school_summary_df.sort_values('% Passing Overall', ascending=False)
top_five_schools
356/1:
# Dependencies
import pandas as pd
import numpy as np
356/2:
# load in file
pycityschools_file = "Resources/schools_complete.csv"
student_file = "Resources/students_complete.csv"
356/3:
# Read School and Student Data File and store into Pandas DataFrames
pycityschools_data_df = pd.read_csv(pycityschools_file)
student_data_df = pd.read_csv(student_file)

pycityschools_data_df.head()
student_data_df.head()
356/4:
pycityschools_data_complete = pd.merge(student_data_df, pycityschools_data_df, 
                                       on=["school_name", "school_name"], how="left")

pycityschools_data_complete.head()
356/5:
# Count total schools
total_schools = len(pycityschools_data_complete["School ID"].unique())
total_schools
356/6:
# Calculate the total number of students
total_students = len(pycityschools_data_complete["student_name"].unique())
total_students
356/7:
# Calculate Total Budget
total_budget = pycityschools_data_df ["budget"].sum()
total_budget
356/8:
# Calculate the average math score
average_mathscore = round(pycityschools_data_complete["math_score"].mean(), 2)
average_mathscore
356/9:
# Calculate the average reading score
average_readingscore = round(pycityschools_data_complete["reading_score"].mean(), 2)
average_readingscore
356/10:
# Calculate the percentage of students with a passing math score (70 or greater)
unique_math_passing = len(pycityschools_data_complete.loc[pycityschools_data_complete["math_score"] >=70]["student_name"].unique())
percent_math_passing = round((unique_math_passing/total_students)*100, 2)
percent_math_passing
356/11:
# Calculate the percentage of students with a passing reading score (70 or greater)
unique_reading_passing = len(pycityschools_data_complete.loc[pycityschools_data_complete["reading_score"] >=70]["student_name"].unique())
percent_reading_passing = round((unique_reading_passing/total_students)*100, 2)
percent_reading_passing
356/12:
# Calculate the percentage of students who passed math and reading (% Overall Passing)
overall_passing = round((percent_reading_passing + percent_math_passing) / 2, 2)
overall_passing
356/13:
# Create a dataframe to hold the above results
summary_df = pd.DataFrame({"Total Schools": [total_schools],
                          "Total Students": [total_students],
                          "Total Budget": [total_budget],
                          "Average Math Score": [average_mathscore],
                          "Average Reading Score": [average_readingscore],
                          "% of Students with Passing Math Score": [percent_math_passing],
                          "% of Students with Passing Reading Score": [percent_reading_passing],
                          "% of Students with Passing Reading and Math Score": [overall_passing]
                          })
summary_df
356/14:
# Give the displayed data cleaner formatting
summary_df["Total Students"] = summary_df["Total Students"].map("{:,.2f}".format)
summary_df["Total Budget"] = summary_df["Total Budget"].map("${:,.2f}".format)
summary_df["Average Math Score"] = summary_df["Average Math Score"].map("{:,.2f}%".format)
summary_df["Average Reading Score"] = summary_df["Average Reading Score"].map("{:,.2f}%".format)
summary_df["% of Students with Passing Math Score"] = summary_df["% of Students with Passing Math Score"].map("{:,.2f}%".format)
summary_df["% of Students with Passing Reading Score"] = summary_df["% of Students with Passing Reading Score"].map("{:,.2f}%".format)
summary_df["% of Students with Passing Reading and Math Score"] = summary_df["% of Students with Passing Reading and Math Score"].map("{:,.2f}%".format)
summary_df
356/15:
schools_overview_group = pycityschools_data_complete.groupby(["school_name"])
school_summary_df = schools_overview_group.agg({"Student ID": "count",
                            "budget": "max",
                           "math_score": "mean",
                            "reading_score": "mean",
                           })
school_summary_df["Per Student Budget"] = school_summary_df["budget"] / school_summary_df["Student ID"]
school_summary_df
356/16:
school_type = pycityschools_data_complete[["type", "school_name"]]
school_type
356/17:
# Passing Math Score
passing_math_stu = pycityschools_data_complete.loc[pycityschools_data_complete["math_score"] >=70]
passing_math_stu
356/18:
passing_overall_stu = pycityschools_data_complete.loc[(pycityschools_data_complete["math_score"] >=70)&
                                                  (pycityschools_data_complete["reading_score"] >=70)]
passing_overall_stu
356/19:
passing_reading_stu = pycityschools_data_complete.loc[pycityschools_data_complete["reading_score"] >=70]
passing_reading_stu
356/20:
passing_math_count = passing_math_stu.groupby(["school_name"]).count()["math_score"]
passing_math_count
356/21:
passing_reading_count = passing_reading_stu.groupby(["school_name"]).count()["reading_score"]
passing_reading_count
356/22:
passing_overall_count = passing_overall_stu.groupby(["school_name"]).count()["reading_score"]
passing_overall_count
356/23:
school_summary_df["% Passing Math"] = passing_math_count / school_summary_df["Student ID"] * 100
school_summary_df["% Passing Reading"] = passing_reading_count / school_summary_df["Student ID"] * 100
school_summary_df["% Passing Overall"] = passing_overall_count / school_summary_df["Student ID"] * 100
school_summary_df
356/24:
school_summary_df = school_summary_df.rename(columns={"budget": "Total School Budget",
                                                      "Student ID": "Total Students",
                                                     "math_score": "Average Math Score",
                                                      "reading_score": "Average Reading Score",
                                                     })
school_summary_df
356/25:
school_summary_df = school_summary_df.style.format({
                          'Total Students': '{:,}', 
                          "Total School Budget": "${:,}", 
                          "Per Student Budget": "${:.0f}",
                          'Average Math Score': "{:.1f}%", 
                          'Average Reading Score': "{:.1f}%", 
                          "% Passing Math": "{:.1f}%", 
                          "% Passing Reading": "{:.1f}%", 
                          "% Passing Overall": "{:.1f}%"
                         })
school_summary_df
356/26:
# Create a table that highlights the top 5 performing schools based on % Overall Passing. Include:

# School Name
# School Type
# Total Students
# Total School Budget
# Per Student Budget
# Average Math Score
# Average Reading Score
# % Passing Math (The percentage of students that passed math.)
# % Passing Reading (The percentage of students that passed reading.)
# % Overall Passing (The percentage of students that passed math and reading.)
356/27:
# Find top 5 performing schools based on % Overall Passing
top_five_schools = school_summary_df.sort_values('% Passing Overall', ascending=False)
top_five_schools
357/1: %matplotlib notebook
357/2:
# Dependencies
import matplotlib.pyplot as plt
import pandas as pd
import scipy.stats as st
import numpy as np
357/3:
# Files
mouse_metadata_path = "data/Mouse_metadata.csv"
study_results_path = "data/Study_results.csv"
357/4:
# Read the mouse data and the study results
mouse_metadata = pd.read_csv(mouse_metadata_path)
study_results = pd.read_csv(study_results_path)
357/5:
# Read the mouse data and the study results
mouse_metadata = pd.read_csv(mouse_metadata_path)
study_results = pd.read_csv(study_results_path)
mouse_metadata.head()
study_restults.head()
357/6:
# Dependencies
import matplotlib.pyplot as plt
import pandas as pd
import scipy.stats as st
import numpy as np
357/7:
# Files
mouse_metadata_path = "data/Mouse_metadata.csv"
study_results_path = "data/Study_results.csv"
357/8:
# Read the mouse data and the study results
mouse_metadata = pd.read_csv(mouse_metadata_path)
study_results = pd.read_csv(study_results_path)
mouse_metadata.head()
study_restults.head()
357/9:
# Read the mouse data and the study results
mouse_metadata = pd.read_csv(mouse_metadata_path)
study_results = pd.read_csv(study_results_path)

mouse_metadata.head()
study_restults.head()
359/1: %matplotlib notebook
359/2:
# Dependencies
import matplotlib.pyplot as plt
import pandas as pd
import scipy.stats as st
import numpy as np
359/3:
# Files
mouse_metadata_path = "data/Mouse_metadata.csv"
study_results_path = "data/Study_results.csv"
359/4:
# Read the mouse data and the study results
mouse_metadata = pd.read_csv(mouse_metadata_path)
study_results = pd.read_csv(study_results_path)

mouse_metadata.head()
study_restults.head()
359/5:
# Read the mouse data and the study results
mouse_metadata = pd.read_csv(mouse_metadata_path)
study_results = pd.read_csv(study_results_path)

mouse_metadata.head()
study_results.head()
359/6:
# Combine the data into a single dataset
pymaceuticals_data = pd.merge(mouse_metadata, study_results, on=["Mouse ID", "Mouse ID"])

# Display the data table for preview
pymaceuticals_data
359/7:
 # Checking the number of mice.
number_of_mice = pymaceuticals_data.count_values("Mouse ID")
number_of_mice
359/8:
 # Checking the number of mice.
number_of_mice = pymaceuticals_data.count_value("Mouse ID")
number_of_mice
359/9:
 # Checking the number of mice.
number_of_mice = pymaceuticals_data.loc(pymaceuticals_data["Mouse ID"].count())
number_of_mice
359/10:
 # Checking the number of mice.
number_of_mice = pymaceuticals_data.loc[pymaceuticals_data["Mouse ID"].count()]
number_of_mice
359/11:
 # Checking the number of mice.
number_of_mice = pymaceuticals_data.loc[pymaceuticals_data(len["Mouse ID"])]
number_of_mice
359/12:
 # Checking the number of mice.
number_of_mice = pymaceuticals_data.loc[pymaceuticals_data["Mouse ID"].sum()]
number_of_mice
359/13:
 # Checking the number of mice.
number_of_mice = len[pymaceuticals_data["Mouse ID"].sum()]
number_of_mice
359/14:
 # Checking the number of mice.
number_of_mice = len(pymaceuticals_data["Mouse ID"])
number_of_mice
359/15:
# Getting the duplicate mice by ID number that shows up for Mouse ID and Timepoint. 
duplicate_mice = (pymaceuticals_data.dublicated["Mouse ID", "Timepoint"])
duplicate_mice
359/16:
# Getting the duplicate mice by ID number that shows up for Mouse ID and Timepoint. 
duplicate_mice = (pymaceuticals_data.duplicated["Mouse ID", "Timepoint"])
duplicate_mice
359/17:
# Getting the duplicate mice by ID number that shows up for Mouse ID and Timepoint. 
duplicate_mice = pymaceuticals_data.duplicated("Mouse ID", "Timepoint")
duplicate_mice
359/18:
# Getting the duplicate mice by ID number that shows up for Mouse ID and Timepoint. 
duplicate_mice = pymaceuticals_data.duplicated("Mouse ID", "Timepoint", keep=False)
duplicate_mice
359/19:
# Getting the duplicate mice by ID number that shows up for Mouse ID and Timepoint. 
duplicate_mice = pymaceuticals_data.duplicated(subset=['Mouse ID', 'Timepoint'])
duplicate_mice
359/20:
# Create a clean DataFrame by dropping the duplicate mouse by its ID.
dropped_duplicates = pymaceuticals_data.drop_duplicates(subset=['Timepoint'],keep='first',inplace=False)
dropped_duplicates
359/21:
 # Checking the number of mice in the clean DataFrame.
total_mice = len(dropped_duplicates["Mouse ID"])
total_mice
359/22:
# Create a clean DataFrame by dropping the duplicate mouse by its ID.
pymaceuticals_clean = pymaceuticals_data.drop_duplicates(subset=['Timepoint'],keep='first',inplace=False)
pymaceuticals_clean
359/23:
 # Checking the number of mice in the clean DataFrame.
total_mice = len(pymaceuticals_clean["Mouse ID"])
total_mice
359/24:
# Generate a summary statistics table of mean, median, variance, standard deviation, and SEM of the tumor volume for each regimen

# This method is the most straighforward, creating multiple series and putting them all together at the end.
mean_data = pymaceuticals_clean.groupby('Drug Regimen').mean()['Tumor Volume (mm3)']
median_data = pymaceuticals_clean.groupby('Drug Regimen').median()['Tumor Volume (mm3)']
variance_data = pymaceuticals_clean.groupby('Drug Regimen').var()['Tumor Volume (mm3)']
std_data = pymaceuticals_clean.groupby('Drug Regimen').std()['Tumor Volume (mm3)']
sem_data = pymaceuticals_clean.groupby('Drug Regimen').sem()['Tumor Volume (mm3)']

mean_table = pd.DataFrame(mean_data)
summary_table = mean_table.rename(columns={"Mean: Tumor Volume"})

summary_table["Median: Tumor Volume"] = median_data
summary_table["Variance: Tumor Volume"]= variance_data
summary_table["Standard Deviation: Tumor Volume"] = variance_data
summary_table["SEM: Tumor Volume"] = sem_data

summary_table
359/25:
# Generate a summary statistics table of mean, median, variance, standard deviation, and SEM of the tumor volume for each regimen

# This method is the most straighforward, creating multiple series and putting them all together at the end.
mean_data = pymaceuticals_clean.groupby('Drug Regimen').mean()['Tumor Volume (mm3)']
median_data = pymaceuticals_clean.groupby('Drug Regimen').median()['Tumor Volume (mm3)']
variance_data = pymaceuticals_clean.groupby('Drug Regimen').var()['Tumor Volume (mm3)']
std_data = pymaceuticals_clean.groupby('Drug Regimen').std()['Tumor Volume (mm3)']
sem_data = pymaceuticals_clean.groupby('Drug Regimen').sem()['Tumor Volume (mm3)']

means = pd.DataFrame(mean_data)
summary_table = means.rename(columns={"Mean: Tumor Volume"})

summary_table["Median: Tumor Volume"] = median_data
summary_table["Variance: Tumor Volume"]= variance_data
summary_table["Standard Deviation: Tumor Volume"] = variance_data
summary_table["SEM: Tumor Volume"] = sem_data

summary_table
359/26:
grouped = pymaceuticals_clean.groupby('Drug Regimen')
summary_table = grouped.agg(['mean','median','var','std','sem'])["Tumor Volume (mm3)"]
Summary_table
359/27:
grouped = pymaceuticals_clean.groupby('Drug Regimen')
summary_table = grouped.agg(['mean','median','var','std','sem'])["Tumor Volume (mm3)"]
summary_table
359/28:
# Create a clean DataFrame by dropping the duplicate mouse by its ID.
pymaceuticals_clean = pymaceuticals_data.drop_duplicates(subset=['Timepoint'],keep='last',inplace=False)
pymaceuticals_clean
359/29:
 # Checking the number of mice in the clean DataFrame.
total_mice = len(pymaceuticals_clean["Mouse ID"])
total_mice
359/30:
# Generate a summary statistics table of mean, median, variance, standard deviation, and SEM of the tumor volume for each regimen

# This method is the most straighforward, creating multiple series and putting them all together at the end.
grouped = pymaceuticals_clean.groupby('Drug Regimen')
summary_table = grouped.agg(['mean','median','var','std','sem'])["Tumor Volume (mm3)"]
summary_table
359/31:
# Create a clean DataFrame by dropping the duplicate mouse by its ID.
pymaceuticals_clean = pymaceuticals_data.drop_duplicates(subset=['Mouse ID'],keep='first',inplace=False)
pymaceuticals_clean
359/32:
 # Checking the number of mice in the clean DataFrame.
total_mice = len(pymaceuticals_clean["Mouse ID"])
total_mice
359/33:
# Generate a summary statistics table of mean, median, variance, standard deviation, and SEM of the tumor volume for each regimen

# This method is the most straighforward, creating multiple series and putting them all together at the end.
grouped = pymaceuticals_clean.groupby('Drug Regimen')
summary_table = grouped.agg(['mean','median','var','std','sem'])["Tumor Volume (mm3)"]
summary_table
359/34:
# Create a clean DataFrame by dropping the duplicate mouse by its ID.
pymaceuticals_clean = pymaceuticals_data.drop_duplicates(subset=['Mouse ID', 'Timepoint'],keep='first',inplace=False)
pymaceuticals_clean
359/35:
 # Checking the number of mice in the clean DataFrame.
total_mice = len(pymaceuticals_clean["Mouse ID"])
total_mice
359/36:
# Generate a summary statistics table of mean, median, variance, standard deviation, and SEM of the tumor volume for each regimen

# This method is the most straighforward, creating multiple series and putting them all together at the end.
grouped = pymaceuticals_clean.groupby('Drug Regimen')
summary_table = grouped.agg(['mean','median','var','std','sem'])["Tumor Volume (mm3)"]
summary_table
359/37:
# Generate a summary statistics table of mean, median, variance, standard deviation, and SEM of the tumor volume for each regimen

# This method is the most straighforward, creating multiple series and putting them all together at the end.
grouped = pymaceuticals_clean.groupby('Drug Regimen')
summary_table = round(grouped.agg(['mean','median','var','std','sem'])["Tumor Volume (mm3)"], 2)
summary_table
359/38:
regimen = summary_table.groupby(['Drug Regimen']).count()['Mouse ID']
regimen
359/39:
regimen = pymaceuticals_clean(['Drug Regimen']).count()['Mouse ID']
regimen
359/40:
regimen = pymaceuticals_clean.groupby(['Drug Regimen']).count()['Mouse ID']
regimen
359/41: x_axis = np.arange(len(regimen))
359/42:
# Generate a bar plot showing the total number of mice for each treatment throughout the course of the study using pandas.
plt.bar(x_axis, regimen, color='pink', alpha=1, align='center' )
359/43:
# Generate a bar plot showing the total number of mice for each treatment throughout the course of the study using pandas.
plt.bar(x_axis, regimen, color='pink', alpha=1, align='center' )
tick_locations = [value for value in x_axis]
plt.xticks(tick_locations, ['Capomulin', 'Ceftamin', 'Infubinol', 'Ketapril', 'Naftisol', 'Placebo', 'Propriva', 'Ramicane', 'Stelasyn', 'Zoniferol'])
359/44:
# Generate a bar plot showing the total number of mice for each treatment throughout the course of the study using pandas.
plt.bar(x_axis, regimen, color='pink', alpha=1, align='center' )
tick_locations = [value for value in x_axis]
plt.xticks(tick_locations, ['Capomulin', 'Ceftamin', 'Infubinol', 'Ketapril', 'Naftisol', 'Placebo', 'Propriva', 'Ramicane', 'Stelasyn', 'Zoniferol'])
plt.tight_layout()
359/45:
# Generate a bar plot showing the total number of mice for each treatment throughout the course of the study using pandas.
plt.bar(x_axis, regimen, color='pink', alpha=1, align='center', fig(20,20))
tick_locations = [value for value in x_axis]
plt.xticks(tick_locations, ['Capomulin', 'Ceftamin', 'Infubinol', 'Ketapril', 'Naftisol', 'Placebo', 'Propriva', 'Ramicane', 'Stelasyn', 'Zoniferol'])
plt.tight_layout()
359/46:
# Generate a bar plot showing the total number of mice for each treatment throughout the course of the study using pandas.
plt.bar(x_axis, regimen, color='pink', alpha=1, align='center')
tick_locations = [value for value in x_axis]
plt.xticks(tick_locations, ['Capomulin', 'Ceftamin', 'Infubinol', 'Ketapril', 'Naftisol', 'Placebo', 'Propriva', 'Ramicane', 'Stelasyn', 'Zoniferol'])
plt.tight_layout()
359/47:
# Generate a bar plot showing the total number of mice for each treatment throughout the course of the study using pandas.
# tick locations horizontal?
plt.bar(x_axis, regimen, color='pink', alpha=1, align='center')

tick_locations = [value for value in x_axis]
plt.xticks(tick_locations, ['Capomulin', 'Ceftamin', 'Infubinol', 'Ketapril', 'Naftisol', 'Placebo', 'Propriva', 'Ramicane', 'Stelasyn', 'Zoniferol'])
plt.tight_layout()

plt.title("Total Mice Treated")
plt.xlabel("Drug Regimen")
plt.ylabel("Number of Mice")
364/1:
# Generate a bar plot showing the total number of mice for each treatment throughout the course of the study using pandas.
# tick locations horizontal?
plt.bar(x_axis, regimen, color='pink', alpha=1, align='center')

tick_locations = [value for value in x_axis]
plt.xticks(tick_locations, ['Capomulin', 'Ceftamin', 'Infubinol', 'Ketapril', 'Naftisol', 'Placebo', 'Propriva', 'Ramicane', 'Stelasyn', 'Zoniferol'], rotation='vertical')
plt.tight_layout()

plt.title("Total Mice Treated")
plt.xlabel("Drug Regimen")
plt.ylabel("Number of Mice")
365/1: %matplotlib notebook
365/2:
# Dependencies
import matplotlib.pyplot as plt
import pandas as pd
import scipy.stats as st
import numpy as np
365/3:
# Files
mouse_metadata_path = "data/Mouse_metadata.csv"
study_results_path = "data/Study_results.csv"
365/4:
# Read the mouse data and the study results
mouse_metadata = pd.read_csv(mouse_metadata_path)
study_results = pd.read_csv(study_results_path)

mouse_metadata.head()
study_results.head()
365/5:
# Combine the data into a single dataset
pymaceuticals_data = pd.merge(mouse_metadata, study_results, on=["Mouse ID", "Mouse ID"])

# Display the data table for preview
pymaceuticals_data
365/6:
 # Checking the number of mice.
number_of_mice = len(pymaceuticals_data["Mouse ID"])
number_of_mice
365/7:
# Getting the duplicate mice by ID number that shows up for Mouse ID and Timepoint. 
duplicate_mice = pymaceuticals_data.duplicated(subset=['Mouse ID', 'Timepoint'])
duplicate_mice
365/8:
# Create a clean DataFrame by dropping the duplicate mouse by its ID.
pymaceuticals_clean = pymaceuticals_data.drop_duplicates(subset=['Mouse ID', 'Timepoint'],keep='first',inplace=False)
pymaceuticals_clean
365/9:
 # Checking the number of mice in the clean DataFrame.
total_mice = len(pymaceuticals_clean["Mouse ID"])
total_mice
365/10:
# Generate a summary statistics table of mean, median, variance, standard deviation, and SEM of the tumor volume for each regimen

# This method is the most straighforward, creating multiple series and putting them all together at the end.
grouped = pymaceuticals_clean.groupby('Drug Regimen')
summary_table = round(grouped.agg(['mean','median','var','std','sem'])["Tumor Volume (mm3)"], 2)
summary_table
365/11:
regimen = pymaceuticals_clean.groupby(['Drug Regimen']).count()['Mouse ID']
regimen
365/12: x_axis = np.arange(len(regimen))
365/13:
# Generate a bar plot showing the total number of mice for each treatment throughout the course of the study using pandas.
# tick locations horizontal?
plt.bar(x_axis, regimen, color='pink', alpha=1, align='center')

tick_locations = [value for value in x_axis]
plt.xticks(tick_locations, ['Capomulin', 'Ceftamin', 'Infubinol', 'Ketapril', 'Naftisol', 'Placebo', 'Propriva', 'Ramicane', 'Stelasyn', 'Zoniferol'], rotation='vertical')
plt.tight_layout()

plt.title("Total Mice Treated")
plt.xlabel("Drug Regimen")
plt.ylabel("Number of Mice")
365/14: # Generate a pie plot showing the distribution of female versus male mice using pandas
365/15:
# Calculate the final tumor volume of each mouse across four of the treatment regimens:  
# Capomulin, Ramicane, Infubinol, and Ceftamin

# Start by getting the last (greatest) timepoint for each mouse


# Merge this group df with the original dataframe to get the tumor volume at the last timepoint
365/16:
# Put treatments into a list for for loop (and later for plot labels)


# Create empty list to fill with tumor vol data (for plotting)


# Calculate the IQR and quantitatively determine if there are any potential outliers. 

    
    # Locate the rows which contain mice on each drug and get the tumor volumes
    
    
    # add subset 
    
    
    # Determine outliers using upper and lower bounds
365/17: # Generate a box plot of the final tumor volume of each mouse across four regimens of interest
365/18:  # Generate a line plot of time point versus tumor volume for a mouse treated with Capomulin
365/19: # Generate a scatter plot of mouse weight versus average tumor volume for the Capomulin regimen
365/20:
# Calculate the correlation coefficient and linear regression model 
# for mouse weight and average tumor volume for the Capomulin regimen
366/1:
# Generate a pie plot showing the distribution of female versus male mice using pandas
gender = pymaceuticals_clean.groupby(['Sex']).count()
gender
367/1: %matplotlib notebook
367/2:
# Dependencies
import matplotlib.pyplot as plt
import pandas as pd
import scipy.stats as st
import numpy as np
367/3:
# Files
mouse_metadata_path = "data/Mouse_metadata.csv"
study_results_path = "data/Study_results.csv"
367/4:
# Read the mouse data and the study results
mouse_metadata = pd.read_csv(mouse_metadata_path)
study_results = pd.read_csv(study_results_path)

mouse_metadata.head()
study_results.head()
367/5:
# Combine the data into a single dataset
pymaceuticals_data = pd.merge(mouse_metadata, study_results, on=["Mouse ID", "Mouse ID"])

# Display the data table for preview
pymaceuticals_data
367/6:
 # Checking the number of mice.
number_of_mice = len(pymaceuticals_data["Mouse ID"])
number_of_mice
367/7:
# Getting the duplicate mice by ID number that shows up for Mouse ID and Timepoint. 
duplicate_mice = pymaceuticals_data.duplicated(subset=['Mouse ID', 'Timepoint'])
duplicate_mice
367/8:
# Create a clean DataFrame by dropping the duplicate mouse by its ID.
pymaceuticals_clean = pymaceuticals_data.drop_duplicates(subset=['Mouse ID', 'Timepoint'],keep='first',inplace=False)
pymaceuticals_clean
367/9:
 # Checking the number of mice in the clean DataFrame.
total_mice = len(pymaceuticals_clean["Mouse ID"])
total_mice
367/10:
# Generate a summary statistics table of mean, median, variance, standard deviation, and SEM of the tumor volume for each regimen

# This method is the most straighforward, creating multiple series and putting them all together at the end.
grouped = pymaceuticals_clean.groupby('Drug Regimen')
summary_table = round(grouped.agg(['mean','median','var','std','sem'])["Tumor Volume (mm3)"], 2)
summary_table
367/11:
regimen = pymaceuticals_clean.groupby(['Drug Regimen']).count()['Mouse ID']
regimen
367/12: x_axis = np.arange(len(regimen))
367/13:
# Generate a bar plot showing the total number of mice for each treatment throughout the course of the study using pandas.
# tick locations horizontal?
plt.bar(x_axis, regimen, color='pink', alpha=1, align='center')

tick_locations = [value for value in x_axis]
plt.xticks(tick_locations, ['Capomulin', 'Ceftamin', 'Infubinol', 'Ketapril', 'Naftisol', 'Placebo', 'Propriva', 'Ramicane', 'Stelasyn', 'Zoniferol'], rotation='vertical')
plt.tight_layout()

plt.title("Total Mice Treated")
plt.xlabel("Drug Regimen")
plt.ylabel("Number of Mice")
367/14:
# Generate a pie plot showing the distribution of female versus male mice using pandas
gender = pymaceuticals_clean.groupby(['Sex']).count()
gender
367/15:
# Calculate the final tumor volume of each mouse across four of the treatment regimens:  
# Capomulin, Ramicane, Infubinol, and Ceftamin

# Start by getting the last (greatest) timepoint for each mouse


# Merge this group df with the original dataframe to get the tumor volume at the last timepoint
367/16:
# Put treatments into a list for for loop (and later for plot labels)


# Create empty list to fill with tumor vol data (for plotting)


# Calculate the IQR and quantitatively determine if there are any potential outliers. 

    
    # Locate the rows which contain mice on each drug and get the tumor volumes
    
    
    # add subset 
    
    
    # Determine outliers using upper and lower bounds
367/17: # Generate a box plot of the final tumor volume of each mouse across four regimens of interest
367/18:  # Generate a line plot of time point versus tumor volume for a mouse treated with Capomulin
367/19: # Generate a scatter plot of mouse weight versus average tumor volume for the Capomulin regimen
367/20:
# Calculate the correlation coefficient and linear regression model 
# for mouse weight and average tumor volume for the Capomulin regimen
367/21: gender_percentage = gender / total_mice *100
367/22:
gender_percentage = gender / total_mice *100
gender_percentage
367/23:
pie_df = pd.DataFrame(gender, gender_percentage)
pie_df
367/24:
pie_df = pd.DataFrame(gender.size(), gender_percentage)
pie_df
367/25:
# Generate a pie plot showing the distribution of female versus male mice using pandas
gender = pymaceuticals_clean.groupby(["Mouse ID","Sex"])
gender
367/26:
gender_percentage = gender / total_mice *100
gender_percentage
367/27: gender_df = pd.DataFrame(gender.size())
367/28:
gender_data = pd.DataFrame(mouse_gender_df.groupby(["Sex"]).count())
gender_data.columns = ["Total Count"]
367/29:
gender_data = pd.DataFrame(gender_df.groupby(["Sex"]).count())
gender_data.columns = ["Total Count"]
368/1: %matplotlib notebook
368/2:
# Dependencies
import matplotlib.pyplot as plt
import pandas as pd
import scipy.stats as st
import numpy as np
368/3:
# Files
mouse_metadata_path = "data/Mouse_metadata.csv"
study_results_path = "data/Study_results.csv"
368/4:
# Read the mouse data and the study results
mouse_metadata = pd.read_csv(mouse_metadata_path)
study_results = pd.read_csv(study_results_path)

mouse_metadata.head()
study_results.head()
368/5:
# Combine the data into a single dataset
pymaceuticals_data = pd.merge(mouse_metadata, study_results, on=["Mouse ID", "Mouse ID"])

# Display the data table for preview
pymaceuticals_data
368/6:
 # Checking the number of mice.
number_of_mice = len(pymaceuticals_data["Mouse ID"])
number_of_mice
368/7:
# Getting the duplicate mice by ID number that shows up for Mouse ID and Timepoint. 
duplicate_mice = pymaceuticals_data.duplicated(subset=['Mouse ID', 'Timepoint'])
duplicate_mice
368/8:
# Create a clean DataFrame by dropping the duplicate mouse by its ID.
pymaceuticals_clean = pymaceuticals_data.drop_duplicates(subset=['Mouse ID', 'Timepoint'],keep='first',inplace=False)
pymaceuticals_clean
368/9:
 # Checking the number of mice in the clean DataFrame.
total_mice = len(pymaceuticals_clean["Mouse ID"])
total_mice
368/10:
# Generate a summary statistics table of mean, median, variance, standard deviation, and SEM of the tumor volume for each regimen

# This method is the most straighforward, creating multiple series and putting them all together at the end.
grouped = pymaceuticals_clean.groupby('Drug Regimen')
summary_table = round(grouped.agg(['mean','median','var','std','sem'])["Tumor Volume (mm3)"], 2)
summary_table
368/11:
regimen = pymaceuticals_clean.groupby(['Drug Regimen']).count()['Mouse ID']
regimen
368/12: x_axis = np.arange(len(regimen))
368/13:
# Generate a bar plot showing the total number of mice for each treatment throughout the course of the study using pandas.
# tick locations horizontal?
plt.bar(x_axis, regimen, color='pink', alpha=1, align='center')

tick_locations = [value for value in x_axis]
plt.xticks(tick_locations, ['Capomulin', 'Ceftamin', 'Infubinol', 'Ketapril', 'Naftisol', 'Placebo', 'Propriva', 'Ramicane', 'Stelasyn', 'Zoniferol'], rotation='vertical')
plt.tight_layout()

plt.title("Total Mice Treated")
plt.xlabel("Drug Regimen")
plt.ylabel("Number of Mice")
368/14:
# Generate a pie plot showing the distribution of female versus male mice using pandas
gender = pymaceuticals_clean.groupby(["Mouse ID","Sex"])
gender
368/15:
gender_percentage = gender / total_mice *100
gender_percentage
368/16:
# Generate a bar plot showing the total number of mice for each treatment throughout the course of the study using pandas.
# tick locations horizontal?
plt.bar(x_axis, regimen, color='pink', alpha=1, align='center')

tick_locations = [value for value in x_axis]
plt.xticks(tick_locations, ['Capomulin', 'Ceftamin', 'Infubinol', 'Ketapril', 'Naftisol', 'Placebo', 'Propriva', 'Ramicane', 'Stelasyn', 'Zoniferol'], rotation='vertical')
plt.tight_layout()

plt.title("Total Mice Treated")
plt.xlabel("Drug Regimen")
plt.ylabel("Number of Mice")
368/17:
#gender_percentage = gender / total_mice *100
#gender_percentage
368/18: gender_df = pd.DataFrame(gender.size())
368/19:
gender_data = pd.DataFrame(gender_df.groupby(["Sex"]).count())
gender_data.columns = ["Total Count"]
368/20:
gender_data = pd.DataFrame(gender_df.groupby(["Sex"]).count())
gender_data.columns = ["Total Count"]
gender_data
368/21:
gender_df = pd.DataFrame(gender.size())
gender_df
368/22:
# Generate a bar plot showing the total number of mice for each treatment throughout the course of the study using pandas.
# tick locations horizontal?
plt.bar(x_axis, regimen, color='pink', alpha=1, align='center')

tick_locations = [value for value in x_axis]
plt.xticks(tick_locations, ['Capomulin', 'Ceftamin', 'Infubinol', 'Ketapril', 'Naftisol', 'Placebo', 'Propriva', 'Ramicane', 'Stelasyn', 'Zoniferol'], rotation='vertical')


plt.title("Total Mice Treated")
plt.xlabel("Drug Regimen")
plt.ylabel("Number of Mice")

plt.tight_layout()
plt.show()
369/1:
# Dependencies
import requests
import json
369/2:
# URL for GET requests to retrieve vehicle data
url = "https://api.spacexdata.com/v3/launchpads"
369/3:
# Pretty print JSON for all launchpads
print(reequetts.get(url).json())
369/4:
# Pretty print JSON for all launchpads
print(requests.get(url).json())
369/5:
# Pretty print JSON for a specific launchpad
response = requests.get(url).json()
print(json.dumps(response, indent=4, sort_keys=True))
369/6:
# Pretty print JSON for all launchpads
response = requests.get(url).json()
print(json.dumps(response, indent=4, sort_keys=True))
369/7:
# Pretty print JSON for a specific launchpad
response = requests.get(url + "/Vandenberg_AFB_Space_Launch_Complex_4").json()
print(json.dumps(response, indent=4, sort_keys=True))
369/8:
# Pretty print JSON for a specific launchpad
response = requests.get(url + "/vafb_slc_4e").json()
print(json.dumps(response, indent=4, sort_keys=True))
371/1:
# Storing the JSON response within a variable
data = response.json()
# Use json.dumps to print the json stored in variable
# YOUR CODE HERE
print(json.dumps(data, indent=4, sort_kys=True))
371/2:
# Dependencies
import requests
import json
371/3:
# URL for GET requests to retrieve Star Wars character data
base_url = "https://swapi.dev/api/people/"
371/4:
# Create a url with a specific character id
character_id = '4'
url = base_url + character_id
print(url)
371/5:
# Perform a get request for this character
response = requests.get(url)
print(response.url)
371/6:
# Storing the JSON response within a variable
data = response.json()
# Use json.dumps to print the json stored in variable
# YOUR CODE HERE
print(json.dumps(data, indent=4, sort_kys=True))
371/7:
# Storing the JSON response within a variable
data = response.json()
# Use json.dumps to print the json stored in variable
# YOUR CODE HERE
print(json.dumps(data, indent=4, sort_keys=True))
371/8:
# Print the name of the character retrieved
# YOUR CODE HERE
name = data["name"]
print(name)
371/9:
# Print the number of films that they were in (hint: use len())
# YOUR CODE HERE
total_films = len(data["films"])
print(total_films)
371/10:
# Request the starships URI found in the starships property of the
# previously retreived json, then use the response to figure out what this 
# character's first starship was
# YOUR CODE HERE
first_ship_url = data["startships"][0]
ship_response = requests.get(first_ship_url).json()
ship_response
371/11:
# Request the starships URI found in the starships property of the
# previously retreived json, then use the response to figure out what this 
# character's first starship was
# YOUR CODE HERE
first_ship_url = data["starships"][0]
ship_response = requests.get(first_ship_url).json()
ship_response
371/12:
# Print the name of the character's first starship
# YOUR CODE HERE
first_starship = ship_response["name"]
print(f"Their first ship: {first_starship}")
373/1:
# Ask the user what kind of data they would like to search for
question = ("What type of data would you like?"
           "[Math, Trivia, Date or Year]")
search = input(question)
375/1:
# Dependencies
import requests
import json
375/2:
# Base URL for GET requests to retrieve number/date facts
url = "http://numbersapi.com"
375/3:
# Ask the user what kind of data they would like to search for
question = ("What type of data would you like?"
           "[Math, Trivia, Date or Year]")
search = input(question)
376/1:
# Dependencies
import requests
import json
376/2:
# Dependencies
import requests
import json
376/3:
# Dependencies
import requests
import json
377/1:
# Dependencies
import requests
import json
377/2:
# Base URL for GET requests to retrieve number/date facts
url = "http://numbersapi.com"
377/3:
# Ask the user what kind of data they would like to search for
question = ("What type of data would you like?"
           "[Math, Trivia, Date or Year]")
search = input(question)
378/1:
# Dependencies
import requests
import json
378/2:
# Base URL for GET requests to retrieve number/date facts
url = "http://numbersapi.com"
378/3:
# Ask the user what kind of data they would like to search for
question = ("What type of data would you like?"
           "[Math, Trivia, Date or Year]")
search = input(question)
378/4:
# Create code to return a number fact

if(search == "Date"):
    month = input("Which month are you looking for?")
    day = input("What day are you looking for?")
    
    response = requests.get(f"{url}{month}/{day}/{search}?").json()
    
    print(response["text"])
    
else:
    
    number = input("What number would you like to search for?")
    
    response = requests.get(url + number + "/" + search + "?json").json()
    
    print(response["text"])
379/1:
# Dependencies
import requests
import json
379/2:
# Base URL for GET requests to retrieve number/date facts
url = "http://numbersapi.com"
379/3:
# Ask the user what kind of data they would like to search for
question = ("What type of data would you like?"
           "[Math, Trivia, Date or Year]")
search = input(question)
379/4:
# Create code to return a number fact

if(search == "Date"):
    month = input("Which month are you looking for?")
    day = input("What day are you looking for?")
    
    response = requests.get(f"{url}{month}/{day}/{search}?").json()
    
    print(response["text"])
    
else:
    
    number = input("What number would you like to search for?")
    
    response = requests.get(url + number + "/" + search + "?json").json()
    
    print(response["text"])
380/1:
# Dependencies
import requests
import json
380/2:
# Base URL for GET requests to retrieve number/date facts
url = "http://numbersapi.com"
380/3:
# Ask the user what kind of data they would like to search for
question = ("What type of data would you like?"
           "[Math, Trivia, Date or Year]")
search = input(question)
380/4:
# Create code to return a number fact

if(search == "Date"):
    month = input("Which month are you looking for?")
    day = input("What day are you looking for?")
    
    response = requests.get(f"{url}{month}/{day}/{search}?").json()
    
    print(response["text"])
    
else:
    
    number = input("What number would you like to search for?")
    
    response = requests.get(url + number + "/" + search + "?json").json()
    
    print(response["text"])
381/1:
# Dependencies
import requests
import json
381/2:
# Base URL for GET requests to retrieve number/date facts
url = "http://numbersapi.com"
381/3:
# Ask the user what kind of data they would like to search for
question = ("What type of data would you like?"
           "[Math, Trivia, Date or Year]")
search = input(question)
381/4:
# Create code to return a number fact

if(search.lower() == "Date"):
    month = input("Which month are you looking for?")
    day = input("What day are you looking for?")
    
    response = requests.get(f"{url}{month}/{day}/{search}?").json()
    
    print(response["text"])
    
else:
    
    number = input("What number would you like to search for?")
    
    response = requests.get(url + number + "/" + search.lower() + "?json").json()
    
    print(response["text"])
381/5:
# Create code to return a number fact

if(search == "Date"):
    month = input("Which month are you looking for?")
    day = input("What day are you looking for?")
    
    response = requests.get(f"{url}{month}/{day}/{search}?").json()
    
    print(response["text"])
    
else:
    
    number = input("What number would you like to search for?")
    
    response = requests.get(url + number + "/" + search + "?json").json()
    
    print(response["text"])
374/1:
# Dependencies
import requests
import json
374/2:
# Base URL for GET requests to retrieve number/date facts
url = "http://numbersapi.com/"
374/3:
# Ask the user what kind of data they would like to search for
question = ("What type of data would you like to search for? "
            "[Trivia, Math, Date, or Year] ")
kind_of_search = input(question)
374/4:
# If the kind of search is "date" take in two numbers
if(kind_of_search.lower() == "date"):

  # Collect the month to search for
  month = input("What month would you like to search for? ")
  # Collect the day to search for
  day = input("What day would you like to search for? ")

  # Make an API call to the "date" API and convert response object to JSON
  response = requests.get(f"{url}{month}/{day}/{kind_of_search.lower()}?json").json()
  # Print the fact stored within the response
  print(response["text"])

# If the kind of search is anything but "date" then take one number
else:

  # Collect the number to search for
  number = input("What number would you like to search for? ")

  # Make an API call to the API and convert response object to JSON
  response = requests.get(url + number + "/" +  kind_of_search.lower()+ "?json").json()
  # Print the fact stored within the response
  print(response["text"])
382/1:
# Dependencies
import requests
from config import api_key

url = f"http://www.omdbapi.com/?apikey={api_key}&t="
382/2:
# Who was the director of the movie Aliens?
movie = requests.get(url + "Aliens").json
382/3:
# Who was the director of the movie Aliens?
movie = requests.get(url + "Aliens").json
print(f'The director of Aliens was {movie["Director"]}.')
382/4:
# Dependencies
import requests
from config import api_key

url = f"http://www.omdbapi.com/?apikey={api_key}&t="
382/5:
# Who was the director of the movie Aliens?
movie = requests.get(url + "Aliens").json
print(f'The director of Aliens was {movie["Director"]}.')
384/1:
# Dependencies
import requests
from config import api_key

url = f"http://www.omdbapi.com/?apikey={api_key}&t="
384/2:
# Who was the director of the movie Aliens?
movie = requests.get(url + "Aliens").json
print(f'The director of Aliens was {movie["Director"]}.')
385/1:
# Dependencies
import requests
from config import api_key

url = f"http://www.omdbapi.com/?apikey={api_key}&t="
385/2:
# Who was the director of the movie Aliens?
movie = requests.get(url + "Aliens").json
print(f'The director of Aliens was {movie["Director"]}.')
385/3:
# Dependencies
import requests
from config import api_key

url = f"http://www.omdbapi.com/?apikey={api_key}&t="
385/4:
# Who was the director of the movie Aliens?
movie = requests.get(url + "Aliens").json
print(f'The director of Aliens was {movie["Director"]}.')
385/5: print(api_key)
385/6:
# Dependencies
import requests
from config import api_key

url = f"http://www.omdbapi.com/?apikey={api_key}&t="
385/7:
# Who was the director of the movie Aliens?
movie = requests.get(url + "Aliens").json
print(f'The director of Aliens was {movie["Director"]}.')
386/1:
# Dependencies
import requests
from config import api_key

url = f"http://www.omdbapi.com/?apikey={api_key}&t="
386/2:
# Who was the director of the movie Aliens?
movie = requests.get(url + "Aliens").json
print(f'The director of Aliens was {movie["Director"]}.')
386/3:
# Dependencies
import requests
from config import api_key

url = f"http://www.omdbapi.com/?apikey={api_key}&t="
386/4:
# Who was the director of the movie Aliens?
movie = requests.get(url + "Aliens").json
print(f'The director of Aliens was {movie["Director"]}.')
383/1:
# Who was the director of the movie Aliens?
movie = requests.get(url + "Aliens").json()
print(f'The director of Aliens was {movie["Director"]}.')
383/2:
# Dependencies
import requests
from config import api_key

url = f"http://www.omdbapi.com/?apikey={api_key}&t="
383/3:
# Who was the director of the movie Aliens?
movie = requests.get(url + "Aliens").json()
print(f'The director of Aliens was {movie["Director"]}.')
383/4:
# What was the movie Gladiator rated?
movie = requests.get(url + "Gladiator").json()
print(f'The rating of Gladiator was {movie["Rated"]}.')
386/5:
# Dependencies
import requests
from config import api_key

url = f"http://www.omdbapi.com/?apikey={api_key}&t="
386/6:
# Who was the director of the movie Aliens?
movie = requests.get(url + "Aliens").json
print(f'The director of Aliens was {movie["Director"]}.')
388/1: # What was the movie Gladiator rated?
388/2:
# Dependencies
import requests
from config import api_key

url = f"http://www.omdbapi.com/?apikey={api_key}&t="
388/3:
# Who was the director of the movie Aliens?
movie = requests.get(url + "Aliens").json
print(f'The director of Aliens was {movie["Director"]}.')
383/5:
# Dependencies
import requests
from config import api_key

url = f"http://www.omdbapi.com/?apikey={api_key}&t="
383/6:
# Who was the director of the movie Aliens?
movie = requests.get(url + "Aliens").json()
print(f'The director of Aliens was {movie["Director"]}.')
383/7:
# What was the movie Gladiator rated?
movie = requests.get(url + "Gladiator").json()
print(f'The rating of Gladiator was {movie["Rated"]}.')
383/8:
# What year was 50 First Dates released?
movie = requests.get(url + "50 First Dates").json()
print(f'The movie 50 First Dates was released in {movie["Year"]}.')
383/9:
# Who wrote Moana?
movie = requests.get(url + "Moana").json()
print(f'Moana was written by {movie["Writer"]}.')
383/10:
# What was the plot of the movie Sing?
movie = requests.get(url + "Sing").json()
print(f'The plot of Sing was: {movie["Plot"]}')
389/1:
# Dependencies
import requests
from config import api_key

url = "http://www.omdbapi.com/?apikey=" + api_key + "&t="

movies = ["Aliens", "Sing", "Moana"]

responses = []

for movie in movies:
    movie_data = requests.get(url + movie).json()
    responses.append(movie_data)
    print(f'The director of {movie} is {movie_data["Director"]}')
389/2: responses
391/1:
# Dependencies
import requests
from config import api_key

url = "https://api.nytimes.com/svc/search/v2/articlesearch.json?"

# Store a search term
query = "COVID-19"


# Search for articles published between a begin and end date
begin_date = '20200301'
end_date = '20201010'


# Build url

query_url = f"{url}apikey={api_key}&q={query}&begin_date={begin_date}&end_date={end_date}"
391/2:
# Retrieve articles
articles = requests.get(query_url).json()
articles_list = articles["response"]["docs"]

for article in articles_list:
    print(f'A snippet from the article: {article["snippet"]}')
    print('----------------------------')
391/3:
# Dependencies
import requests
from config import api_key

url = "https://api.nytimes.com/svc/search/v2/articlesearch.json?"

# Store a search term
query = "COVID-19"


# Search for articles published between a begin and end date
begin_date = '20200301'
end_date = '20201010'


# Build url

query_url = f"{url}apikey={api_key}&q={query}&begin_date={begin_date}&end_date={end_date}"
391/4:
# Retrieve articles
articles = requests.get(query_url).json()
articles_list = articles["response"]["docs"]

for article in articles_list:
    print(f'A snippet from the article: {article["snippet"]}')
    print('----------------------------')
391/5:
# Dependencies
import requests
from config import api_key
import time

url = "https://api.nytimes.com/svc/search/v2/articlesearch.json?"

# Store a search term
query = "COVID-19"


# Search for articles published between a begin and end date
begin_date = '20200301'
end_date = '20201010'


# Build url

query_url = f"{url}apikey={api_key}&q={query}&begin_date={begin_date}&end_date={end_date}"
391/6:
# Retrieve articles
articles = requests.get(query_url).json()
articles_list = articles["response"]["docs"]

for article in articles_list:
    print(f'A snippet from the article: {article["snippet"]}')
    print('----------------------------')
391/7:
# Dependencies
import requests
from config import api_key
import time

url = "https://api.nytimes.com/svc/search/v2/articlesearch.json?"

# Store a search term
query = "COVID-19"


# Search for articles published between a begin and end date
begin_date = '20200301'
end_date = '20201010'


# Build url

query_url = f"{url}apikey={api_key}&q={query}&begin_date={begin_date}&end_date={end_date}"
391/8:
# Retrieve articles
articles = requests.get(query_url).json()
articles_list = articles["response"]["docs"]

for article in articles_list:
    print(f'A snippet from the article: {article["snippet"]}')
    print('----------------------------')
392/1:
# Dependencies
import requests
from config import api_key
import time

url = "https://api.nytimes.com/svc/search/v2/articlesearch.json?"

# Store a search term
query = "obama"

# Search for articles published between a begin and end date
begin_date = "20160101"
end_date = "20160130"

query_url = f"{url}api-key={api_key}&q={query}&begin_date={begin_date}&end_date={end_date}"
392/2:
# Retrieve articles
articles = requests.get(query_url).json()
articles_list = articles["response"]["docs"]

for article in articles_list:
    print(f'A snippet from the article: {article["snippet"]}')
    print('---------------------------')
391/9:
# Dependencies
import requests
import json
from config import api_key
import time

url = "https://api.nytimes.com/svc/search/v2/articlesearch.json?"

# Store a search term
query = "COVID-19"


# Search for articles published between a begin and end date
begin_date = '20200301'
end_date = '20201010'


# Build url

query_url = f"{url}apikey={api_key}&q={query}&begin_date={begin_date}&end_date={end_date}"
391/10:
# Retrieve articles
articles = requests.get(query_url).json()
articles_list = articles["response"]["docs"]

for article in articles_list:
    print(f'A snippet from the article: {article["snippet"]}')
    print('----------------------------')
391/11:
# Retrieve articles
articles = requests.get(query_url).json()
articles_list = articles['response']["docs"]

for article in articles_list:
    print(f'A snippet from the article: {article["snippet"]}')
    print('----------------------------')
391/12:
# Retrieve articles
articles = requests.get(query_url).json()
#articles_list = articles['response']["docs"]

for article in articles_list:
    print(f'A snippet from the article: {article["snippet"]}')
    print('----------------------------')
391/13:
# Retrieve articles
articles = requests.get(query_url).json()
articles_list = articles['response']["docs"]

for article in articles_list:
    print(f'A snippet from the article: {article["snippet"]}')
    print('----------------------------')
391/14:
# Dependencies
import requests
import json
from config import api_key
import time

url = "https://api.nytimes.com/svc/search/v2/articlesearch.json?"

# Store a search term
query = "COVID-19"


# Search for articles published between a begin and end date
begin_date = '20200301'
end_date = '20201010'


# Build url

query_url = f"{url}apikey={api_key}&q={query}&begin_date={begin_date}&end_date={end_date}"
391/15:
# Retrieve articles
articles = requests.get(query_url).json()
articles_list = articles['response']["docs"]

for article in articles_list:
    print(f'A snippet from the article: {article["snippet"]}')
    print('----------------------------')
391/16:
# Retrieve articles
articles = requests.get(query_url).json()
pprint.pprint(articles)

    print(f'A snippet from the article: {article["snippet"]}')
    print('----------------------------')
391/17:
# Retrieve articles
articles = requests.get(query_url).json()
pprint.pprint(articles)

print(f'A snippet from the article: {article["snippet"]}')
print('----------------------------')
391/18:
# Dependencies
import requests
import json
from config import api_key
import time

url = "https://api.nytimes.com/svc/search/v2/articlesearch.json?"

# Store a search term
query = "COVID-19"


# Search for articles published between a begin and end date
begin_date = '20200301'
end_date = '20201010'


# Build url

query_url = f"{url}apikey={api_key}&q={query}&begin_date={begin_date}&end_date={end_date}"
391/19:
# Retrieve articles
articles = requests.get(query_url).json()
pprint.pprint(articles)

print(f'A snippet from the article: {article["snippet"]}')
print('----------------------------')
392/3:
# BONUS: How would we get 30 results? 
# HINT: Look up the page query param

# Empty list for articles
articles_list = []

# loop through pages 0-2
for page in range(0, 3):
    query_url = f"{url}api-key={api_key}&q={query}&begin_date={begin_date}&end_date={end_date}"
    # create query with page number
    query_url = f"{query_url}&page={str(page)}"
    articles = requests.get(query_url).json()
    
    # Add a one second interval between queries to stay within API query limits
    time.sleep(1)
    # loop through the response and append each article to the list
    for article in articles["response"]["docs"]:
        articles_list.append(article)
391/20:
# Retrieve articles
articles = requests.get(query_url).json()
articles_list = articles["response"]["docs"]
pprint(articles_list)
391/21:
# Dependencies
import requests
from pprint import pprint
from config import api_key
import time

url = "https://api.nytimes.com/svc/search/v2/articlesearch.json?"

# Store a search term
query = "COVID-19"


# Search for articles published between a begin and end date
begin_date = '20200301'
end_date = '20201010'


# Build url

query_url = f"{url}apikey={api_key}&q={query}&begin_date={begin_date}&end_date={end_date}"
391/22:
# Retrieve articles
articles = requests.get(query_url).json()
articles_list = articles["response"]["docs"]
pprint(articles_list)
393/1:
# Dependencies
import requests
from pprint import pprint
from config import api_key
import time

url = "https://api.nytimes.com/svc/search/v2/articlesearch.json?"

# Store a search term
query = "COVID-19"


# Search for articles published between a begin and end date
begin_date = '20200301'
end_date = '20201010'


# Build url

query_url = f"{url}apikey={api_key}&q={query}&begin_date={begin_date}&end_date={end_date}"
393/2:
# Retrieve articles
articles = requests.get(query_url).json()
articles_list = articles["response"]["docs"]
pprint(articles_list)
393/3:
# Dependencies
import requests
from pprint import pprint
from config import api_key
import time

url = "https://api.nytimes.com/svc/search/v2/articlesearch.json?"

# Store a search term
query = "COVID-19"


# Search for articles published between a begin and end date
begin_date = '20200301'
end_date = '20201010'


# Build url

query_url = url + "api-key=" + api_key + "&q=" + query + "begin_date" + begin_date + "end_date" + end_date
393/4:
# Retrieve articles
articles = requests.get(query_url).json()
articles_list = articles["response"]["docs"]
pprint(articles_list)
393/5:
# Dependencies
import requests
from pprint import pprint
from config import api_key
import time

url = "https://api.nytimes.com/svc/search/v2/articlesearch.json?"

# Store a search term
query = "pandemic"


# Search for articles published between a begin and end date
begin_date = '20200301'
end_date = '20201010'


# Build url

query_url = url + "api-key=" + api_key + "&q=" + query + "begin_date" + begin_date + "end_date" + end_date
393/6:
# Retrieve articles
articles = requests.get(query_url).json()
articles_list = articles["response"]["docs"]
pprint(articles_list)
393/7:
# Dependencies
import requests
from pprint import pprint
from config import api_key
import time

url = "https://api.nytimes.com/svc/search/v2/articlesearch.json?"

# Store a search term
query = "COVID-19"


# Search for articles published between a begin and end date
begin_date = '20200301'
end_date = '20201010'


# Build url

query_url = url + "api-key=" + api_key + "&q=" + query + "begin_date" + begin_date + "end_date" + end_date
393/8:
# Retrieve articles
articles = requests.get(query_url).json()
articles_list = articles["response"]["docs"]
pprint(articles_list)
395/1: %matplotlib notebook
395/2:
# Dependencies
import matplotlib.pyplot as plt
import pandas as pd
import scipy.stats as st
import numpy as np
395/3:
# Files
mouse_metadata_path = "data/Mouse_metadata.csv"
study_results_path = "data/Study_results.csv"
395/4:
# Read the mouse data and the study results
mouse_metadata = pd.read_csv(mouse_metadata_path)
study_results = pd.read_csv(study_results_path)

mouse_metadata.head()
study_results.head()
395/5:
# Combine the data into a single dataset
pymaceuticals_data = pd.merge(mouse_metadata, study_results, on=["Mouse ID", "Mouse ID"])

# Display the data table for preview
pymaceuticals_data
395/6:
 # Checking the number of mice.
number_of_mice = len(pymaceuticals_data["Mouse ID"])
number_of_mice
395/7:
# Getting the duplicate mice by ID number that shows up for Mouse ID and Timepoint. 
duplicate_mice = pymaceuticals_data.duplicated(subset=['Mouse ID', 'Timepoint'])
duplicate_mice
395/8:
# Create a clean DataFrame by dropping the duplicate mouse by its ID.
pymaceuticals_clean = pymaceuticals_data.drop_duplicates(subset=['Mouse ID', 'Timepoint'],keep='first',inplace=False)
pymaceuticals_clean
395/9:
 # Checking the number of mice in the clean DataFrame.
total_mice = len(pymaceuticals_clean["Mouse ID"])
total_mice
395/10:
# Generate a summary statistics table of mean, median, variance, standard deviation, and SEM of the tumor volume for each regimen

# This method is the most straighforward, creating multiple series and putting them all together at the end.
grouped = pymaceuticals_clean.groupby('Drug Regimen')
summary_table = round(grouped.agg(['mean','median','var','std','sem'])["Tumor Volume (mm3)"], 2)
summary_table
395/11:
regimen = pymaceuticals_clean.groupby(['Drug Regimen']).count()['Mouse ID']
regimen
395/12: x_axis = np.arange(len(regimen))
395/13:
# Generate a bar plot showing the total number of mice for each treatment throughout the course of the study using pandas.
# tick locations horizontal?
plt.bar(x_axis, regimen, color='pink', alpha=1, align='center')

tick_locations = [value for value in x_axis]
plt.xticks(tick_locations, ['Capomulin', 'Ceftamin', 'Infubinol', 'Ketapril', 'Naftisol', 'Placebo', 'Propriva', 'Ramicane', 'Stelasyn', 'Zoniferol'], rotation='vertical')


plt.title("Total Mice Treated")
plt.xlabel("Drug Regimen")
plt.ylabel("Number of Mice")

plt.tight_layout()
plt.show()
395/14:
# Generate a pie plot showing the distribution of female versus male mice using pandas
gender = pymaceuticals_clean.groupby(["Mouse ID","Sex"])
gender
395/15:
gender_percentage = gender / total_mice *100
gender_percentage
395/16:
# Generate a pie plot showing the distribution of female versus male mice using pandas
gender = pymaceuticals_clean.groupby(["Mouse ID","Sex"])
gender
395/17:
gender_df = pd.DataFrame(gender.size())
gender_df
395/18:
gender_data = pd.DataFrame(gender_df.groupby(["Sex"]).count())
gender_data.columns = ["Total Count"]
gender_data
395/19:
gender_data["Sex by Percentage"] = (100*(gender_data["Total Count"]/gender_data["Total Count"].sum()))
gender_data
395/20:
gender_data["Sex by Percentage"] = round(100*(gender_data["Total Count"]/gender_data["Total Count"].sum()), 2)
gender_data
395/21:
labels = ["Sex", "Gender by Percentage"]
colors = ["lightblue", "pink"]
395/22:
plt.pie(gender_data, explode=explodee, labels=labels, colors=colors)
pltt.show()
395/23:
labels = ["Sex", "Gender by Percentage"]
colors = ["lightblue", "pink"]
explode = (0.1, 0)
395/24:
plt.pie(gender_data, explode=explode, labels=labels, colors=colors)
pltt.show()
395/25:
plot = gender_data.plt.pie(explode=explode, labels=labels, colors=colors)
pltt.show()
395/26:
plot = gender_data.plot.pie(explode=explode, labels=labels, colors=colors)
pltt.show()
395/27:
plot = gender_data.plot.pie(y='Total Coun', explode=explode, labels=labels, colors=colors)
pltt.show()
395/28:
labels = ["Sex", "Gender by Percentage"]
colors = ["lightblue", "pink"]
explode = (0.1, 0)
395/29:
plot = gender_data.plot.pie(y='Total Count', explode=explode, labels=labels, colors=colors)
pltt.show()
395/30:
labels = ["Male", "Female"]
colors = ["lightblue", "pink"]
explode = (0.1, 0)
395/31:
plot = gender_data.plot.pie(y='Total Count', explode=explode, labels=labels, colors=colors)
pltt.show()
395/32:
plot = gender_data.plot.pie(y='Total Count', explode=explode, labels=labels, colors=colors)
plt.show()
395/33:
plot = gender_data.plot.pie(y='Total Count', explode=explode, labels=labels, colors=colors, autopct="%1.1f%%", shadow=True, startangle=140)
plt.show()
397/1: %matplotlib notebook
397/2:
# Dependencies
import matplotlib.pyplot as plt
import pandas as pd
import scipy.stats as st
import numpy as np
397/3:
# Files
mouse_metadata_path = "data/Mouse_metadata.csv"
study_results_path = "data/Study_results.csv"
397/4:
# Read the mouse data and the study results
mouse_metadata = pd.read_csv(mouse_metadata_path)
study_results = pd.read_csv(study_results_path)

mouse_metadata.head()
study_results.head()
397/5:
# Combine the data into a single dataset
pymaceuticals_data = pd.merge(mouse_metadata, study_results, on=["Mouse ID", "Mouse ID"])

# Display the data table for preview
pymaceuticals_data
397/6:
 # Checking the number of mice.
number_of_mice = len(pymaceuticals_data["Mouse ID"])
number_of_mice
397/7:
# Getting the duplicate mice by ID number that shows up for Mouse ID and Timepoint. 
duplicate_mice = pymaceuticals_data.duplicated(subset=['Mouse ID', 'Timepoint'])
duplicate_mice
397/8:
# Create a clean DataFrame by dropping the duplicate mouse by its ID.
pymaceuticals_clean = pymaceuticals_data.drop_duplicates(subset=['Mouse ID', 'Timepoint'],keep='first',inplace=False)
pymaceuticals_clean
397/9:
 # Checking the number of mice in the clean DataFrame.
total_mice = len(pymaceuticals_clean["Mouse ID"])
total_mice
397/10:
# Generate a summary statistics table of mean, median, variance, standard deviation, and SEM of the tumor volume for each regimen

# This method is the most straighforward, creating multiple series and putting them all together at the end.
grouped = pymaceuticals_clean.groupby('Drug Regimen')
summary_table = round(grouped.agg(['mean','median','var','std','sem'])["Tumor Volume (mm3)"], 2)
summary_table
397/11:
regimen = pymaceuticals_clean.groupby(['Drug Regimen']).count()['Mouse ID']
regimen
397/12: x_axis = np.arange(len(regimen))
397/13:
# Generate a bar plot showing the total number of mice for each treatment throughout the course of the study using pandas.
# tick locations horizontal?
plt.bar(x_axis, regimen, color='pink', alpha=1, align='center')

tick_locations = [value for value in x_axis]
plt.xticks(tick_locations, ['Capomulin', 'Ceftamin', 'Infubinol', 'Ketapril', 'Naftisol', 'Placebo', 'Propriva', 'Ramicane', 'Stelasyn', 'Zoniferol'], rotation='vertical')


plt.title("Total Mice Treated")
plt.xlabel("Drug Regimen")
plt.ylabel("Number of Mice")

plt.tight_layout()
plt.show()
397/14:
# Generate a pie plot showing the distribution of female versus male mice using pandas
gender = pymaceuticals_clean.groupby(["Mouse ID","Sex"])
gender
397/15:
gender_df = pd.DataFrame(gender.size())
gender_df
397/16:
gender_data = pd.DataFrame(gender_df.groupby(["Sex"]).count())
gender_data.columns = ["Total Count"]
gender_data
397/17:
gender_data["Gender by Percentage"] = round(100*(gender_data["Total Count"]/gender_data["Total Count"].sum()), 2)
gender_data
397/18:
labels = ["Male", "Female"]
colors = ["lightblue", "pink"]
explode = (0.1, 0)
397/19:
plot = gender_data.plot.pie(y='Total Count', explode=explode, labels=labels, colors=colors, autopct="%1.1f%%", shadow=True, startangle=140)
plt.show()
397/20:
# Calculate the final tumor volume of each mouse across four of the treatment regimens:  
# Capomulin, Ramicane, Infubinol, and Ceftamin

# Start by getting the last (greatest) timepoint for each mouse


# Merge this group df with the original dataframe to get the tumor volume at the last timepoint
397/21:
# Put treatments into a list for for loop (and later for plot labels)


# Create empty list to fill with tumor vol data (for plotting)


# Calculate the IQR and quantitatively determine if there are any potential outliers. 

    
    # Locate the rows which contain mice on each drug and get the tumor volumes
    
    
    # add subset 
    
    
    # Determine outliers using upper and lower bounds
397/22: # Generate a box plot of the final tumor volume of each mouse across four regimens of interest
397/23:  # Generate a line plot of time point versus tumor volume for a mouse treated with Capomulin
397/24: # Generate a scatter plot of mouse weight versus average tumor volume for the Capomulin regimen
397/25:
# Calculate the correlation coefficient and linear regression model 
# for mouse weight and average tumor volume for the Capomulin regimen
398/1:
labels = ["Female", "Male"]
colors = ["pink", "lightblue"]
explode = (0.1, 0)
398/2:
plot = gender_data.plot.pie(y='Total Count', explode=explode, labels=labels, colors=colors, autopct="%1.1f%%", shadow=True, startangle=140)
plt.show()
399/1: %matplotlib notebook
399/2:
# Dependencies
import matplotlib.pyplot as plt
import pandas as pd
import scipy.stats as st
import numpy as np
399/3:
# Files
mouse_metadata_path = "data/Mouse_metadata.csv"
study_results_path = "data/Study_results.csv"
399/4:
# Read the mouse data and the study results
mouse_metadata = pd.read_csv(mouse_metadata_path)
study_results = pd.read_csv(study_results_path)

mouse_metadata.head()
study_results.head()
399/5:
# Combine the data into a single dataset
pymaceuticals_data = pd.merge(mouse_metadata, study_results, on=["Mouse ID", "Mouse ID"])

# Display the data table for preview
pymaceuticals_data
399/6:
 # Checking the number of mice.
number_of_mice = len(pymaceuticals_data["Mouse ID"])
number_of_mice
399/7:
# Getting the duplicate mice by ID number that shows up for Mouse ID and Timepoint. 
duplicate_mice = pymaceuticals_data.duplicated(subset=['Mouse ID', 'Timepoint'])
duplicate_mice
399/8:
# Create a clean DataFrame by dropping the duplicate mouse by its ID.
pymaceuticals_clean = pymaceuticals_data.drop_duplicates(subset=['Mouse ID', 'Timepoint'],keep='first',inplace=False)
pymaceuticals_clean
399/9:
 # Checking the number of mice in the clean DataFrame.
total_mice = len(pymaceuticals_clean["Mouse ID"])
total_mice
399/10:
# Generate a summary statistics table of mean, median, variance, standard deviation, and SEM of the tumor volume for each regimen

# This method is the most straighforward, creating multiple series and putting them all together at the end.
grouped = pymaceuticals_clean.groupby('Drug Regimen')
summary_table = round(grouped.agg(['mean','median','var','std','sem'])["Tumor Volume (mm3)"], 2)
summary_table
399/11:
regimen = pymaceuticals_clean.groupby(['Drug Regimen']).count()['Mouse ID']
regimen
399/12: x_axis = np.arange(len(regimen))
399/13:
# Generate a bar plot showing the total number of mice for each treatment throughout the course of the study using pandas.
# tick locations horizontal?
plt.bar(x_axis, regimen, color='pink', alpha=1, align='center')

tick_locations = [value for value in x_axis]
plt.xticks(tick_locations, ['Capomulin', 'Ceftamin', 'Infubinol', 'Ketapril', 'Naftisol', 'Placebo', 'Propriva', 'Ramicane', 'Stelasyn', 'Zoniferol'], rotation='vertical')


plt.title("Total Mice Treated")
plt.xlabel("Drug Regimen")
plt.ylabel("Number of Mice")

plt.tight_layout()
plt.show()
399/14:
# Generate a pie plot showing the distribution of female versus male mice using pandas
gender = pymaceuticals_clean.groupby(["Mouse ID","Sex"])
gender
399/15:
gender_df = pd.DataFrame(gender.size())
gender_df
399/16:
gender_data = pd.DataFrame(gender_df.groupby(["Sex"]).count())
gender_data.columns = ["Total Count"]
gender_data
399/17:
gender_data["Gender by Percentage"] = round(100*(gender_data["Total Count"]/gender_data["Total Count"].sum()), 2)
gender_data
399/18:
labels = ["Female", "Male"]
colors = ["pink", "lightblue"]
explode = (0.1, 0)
399/19:
plot = gender_data.plot.pie(y='Total Count', explode=explode, labels=labels, colors=colors, autopct="%1.1f%%", shadow=True, startangle=140)
plt.show()
399/20:
# Calculate the final tumor volume of each mouse across four of the treatment regimens:  
# Capomulin, Ramicane, Infubinol, and Ceftamin

# Start by getting the last (greatest) timepoint for each mouse


# Merge this group df with the original dataframe to get the tumor volume at the last timepoint
399/21:
# Put treatments into a list for for loop (and later for plot labels)


# Create empty list to fill with tumor vol data (for plotting)


# Calculate the IQR and quantitatively determine if there are any potential outliers. 

    
    # Locate the rows which contain mice on each drug and get the tumor volumes
    
    
    # add subset 
    
    
    # Determine outliers using upper and lower bounds
399/22: # Generate a box plot of the final tumor volume of each mouse across four regimens of interest
399/23:  # Generate a line plot of time point versus tumor volume for a mouse treated with Capomulin
399/24: # Generate a scatter plot of mouse weight versus average tumor volume for the Capomulin regimen
399/25:
# Calculate the correlation coefficient and linear regression model 
# for mouse weight and average tumor volume for the Capomulin regimen
399/26:
# Calculate the final tumor volume of each mouse across four of the treatment regimens:  
# Capomulin, Ramicane, Infubinol, and Ceftamin
treatment = pymaceuticals_clean.groupby(["Mouse ID","Drug Regimen"])
treatment

# Start by getting the last (greatest) timepoint for each mouse


# Merge this group df with the original dataframe to get the tumor volume at the last timepoint
399/27:
# Calculate the final tumor volume of each mouse across four of the treatment regimens:  
# Capomulin, Ramicane, Infubinol, and Ceftamin
regimens_df = pymaceuticals_clean.loc[pymaceuticals_clean["Drug Regimen"] == "Capomulin"]
regimens_df
# Start by getting the last (greatest) timepoint for each mouse


# Merge this group df with the original dataframe to get the tumor volume at the last timepoint
399/28:
# Calculate the final tumor volume of each mouse across four of the treatment regimens:  
# Capomulin, Ramicane, Infubinol, and Ceftamin
regimens_df = pymaceuticals_clean.loc[pymaceuticals_clean["Drug Regimen"] == "Capomulin", "Ramicane", "Infubinol", "Ceftamin"]
regimens_df
# Start by getting the last (greatest) timepoint for each mouse


# Merge this group df with the original dataframe to get the tumor volume at the last timepoint
399/29:
# Calculate the final tumor volume of each mouse across four of the treatment regimens:  
# Capomulin, Ramicane, Infubinol, and Ceftamin
regimens_df = pymaceuticals_clean.loc[pymaceuticals_clean["Drug Regimen"] == "Capomulin"]
regimens_df = pymaceuticals_clean.loc[pymaceuticals_clean["Drug Regimen"] == "Remicane"]
regimens_df = pymaceuticals_clean.loc[pymaceuticals_clean["Drug Regimen"] == "Infubinol"]
regimens_df = pymaceuticals_clean.loc[pymaceuticals_clean["Drug Regimen"] == "Ceftamin"]
regimens_df
# Start by getting the last (greatest) timepoint for each mouse


# Merge this group df with the original dataframe to get the tumor volume at the last timepoint
399/30:
# Calculate the final tumor volume of each mouse across four of the treatment regimens:  
# Capomulin, Ramicane, Infubinol, and Ceftamin
regimens_df = pymaceuticals_clean.loc[(pymaceuticals_clean["Drug Regimen"] == "Capomulin") | (pymaceuticals_clean["Drug Regimen"] == "Ramicane") | 
                                      (pymaceuticals_clean["Drug Regimen"] == "Infubinol") | (pymaceuticals_clean["Drug Regimen"] == "Ceftamin")]
regimens_df
# Start by getting the last (greatest) timepoint for each mouse


# Merge this group df with the original dataframe to get the tumor volume at the last timepoint
399/31:
# Calculate the final tumor volume of each mouse across four of the treatment regimens:  
# Capomulin, Ramicane, Infubinol, and Ceftamin
regimens_df = pymaceuticals_clean.loc[(pymaceuticals_clean["Drug Regimen"] == "Capomulin") | (pymaceuticals_clean["Drug Regimen"] == "Ramicane") | 
                                      (pymaceuticals_clean["Drug Regimen"] == "Infubinol") | (pymaceuticals_clean["Drug Regimen"] == "Ceftamin")]
regimens_df.head()
# Start by getting the last (greatest) timepoint for each mouse


# Merge this group df with the original dataframe to get the tumor volume at the last timepoint
399/32:
# Calculate the final tumor volume of each mouse across four of the treatment regimens:  
# Capomulin, Ramicane, Infubinol, and Ceftamin
regimens_df = pymaceuticals_clean.loc[(pymaceuticals_clean["Drug Regimen"] == "Capomulin") | (pymaceuticals_clean["Drug Regimen"] == "Ramicane") | 
                                      (pymaceuticals_clean["Drug Regimen"] == "Infubinol") | (pymaceuticals_clean["Drug Regimen"] == "Ceftamin")]
regimens_df.head()
# Start by getting the last (greatest) timepoint for each mouse
regimens_df = regimens.sort_values(["Timepoint"], ascendin=True)
regimens_df

# Merge this group df with the original dataframe to get the tumor volume at the last timepoint
399/33:
# Calculate the final tumor volume of each mouse across four of the treatment regimens:  
# Capomulin, Ramicane, Infubinol, and Ceftamin
regimens_df = pymaceuticals_clean.loc[(pymaceuticals_clean["Drug Regimen"] == "Capomulin") | (pymaceuticals_clean["Drug Regimen"] == "Ramicane") | 
                                      (pymaceuticals_clean["Drug Regimen"] == "Infubinol") | (pymaceuticals_clean["Drug Regimen"] == "Ceftamin")]
regimens_df.head()
# Start by getting the last (greatest) timepoint for each mouse
regimens_df = regimens.sort_values(["Timepoint"], ascending=True)
regimens_df

# Merge this group df with the original dataframe to get the tumor volume at the last timepoint
399/34:
# Calculate the final tumor volume of each mouse across four of the treatment regimens:  
# Capomulin, Ramicane, Infubinol, and Ceftamin
regimens_df = pymaceuticals_clean.loc[(pymaceuticals_clean["Drug Regimen"] == "Capomulin") | (pymaceuticals_clean["Drug Regimen"] == "Ramicane") | 
                                      (pymaceuticals_clean["Drug Regimen"] == "Infubinol") | (pymaceuticals_clean["Drug Regimen"] == "Ceftamin")]
regimens_df.head()
# Start by getting the last (greatest) timepoint for each mouse
#regimens_df = regimens.sort_values(["Timepoint"], ascending=True)
#regimens_df

# Merge this group df with the original dataframe to get the tumor volume at the last timepoint
399/35:
# Calculate the final tumor volume of each mouse across four of the treatment regimens:  
# Capomulin, Ramicane, Infubinol, and Ceftamin
regimens_df = pymaceuticals_clean.loc[(pymaceuticals_clean["Drug Regimen"] == "Capomulin") | (pymaceuticals_clean["Drug Regimen"] == "Ramicane") | 
                                      (pymaceuticals_clean["Drug Regimen"] == "Infubinol") | (pymaceuticals_clean["Drug Regimen"] == "Ceftamin")]
regimens_df.head()
# Start by getting the last (greatest) timepoint for each mouse
regimens_df = regimens_df.sort_values(["Timepoint"], ascending=True)
regimens_df

# Merge this group df with the original dataframe to get the tumor volume at the last timepoint
399/36:
# Calculate the final tumor volume of each mouse across four of the treatment regimens:  
# Capomulin, Ramicane, Infubinol, and Ceftamin
regimens_df = pymaceuticals_clean.loc[(pymaceuticals_clean["Drug Regimen"] == "Capomulin") | (pymaceuticals_clean["Drug Regimen"] == "Ramicane") | 
                                      (pymaceuticals_clean["Drug Regimen"] == "Infubinol") | (pymaceuticals_clean["Drug Regimen"] == "Ceftamin")]
regimens_df.head()
# Start by getting the last (greatest) timepoint for each mouse
regimens_df = regimens_df.sort_values(["Timepoint"], ascending=True)
regimens_df.head()

# Merge this group df with the original dataframe to get the tumor volume at the last timepoint
399/37:
# Calculate the final tumor volume of each mouse across four of the treatment regimens:  
# Capomulin, Ramicane, Infubinol, and Ceftamin
regimens_df = pymaceuticals_clean.loc[(pymaceuticals_clean["Drug Regimen"] == "Capomulin") | (pymaceuticals_clean["Drug Regimen"] == "Ramicane") | 
                                      (pymaceuticals_clean["Drug Regimen"] == "Infubinol") | (pymaceuticals_clean["Drug Regimen"] == "Ceftamin")]
regimens_df.head()
# Start by getting the last (greatest) timepoint for each mouse
regimens_df = regimens_df.sort_values(["Timepoint"], ascending=False)
regimens_df.head()

# Merge this group df with the original dataframe to get the tumor volume at the last timepoint
399/38:
# Calculate the final tumor volume of each mouse across four of the treatment regimens:  
# Capomulin, Ramicane, Infubinol, and Ceftamin
regimens_df = pymaceuticals_clean.loc[(pymaceuticals_clean["Drug Regimen"] == "Capomulin") | (pymaceuticals_clean["Drug Regimen"] == "Ramicane") | 
                                      (pymaceuticals_clean["Drug Regimen"] == "Infubinol") | (pymaceuticals_clean["Drug Regimen"] == "Ceftamin")]
regimens_df.head()
# Start by getting the last (greatest) timepoint for each mouse
regimens_df = regimens_df.sort_values(["Timepoint"], ascending=False)
regimens_df.head()

# Merge this group df with the original dataframe to get the tumor volume at the last timepoint
tumor_df = pd.merge(pymaceuticals_clean, regimens_df, on=["Tumor Volume(mm3)","Tumor Volume(mm3)"])
tumor_df
399/39:
# Calculate the final tumor volume of each mouse across four of the treatment regimens:  
# Capomulin, Ramicane, Infubinol, and Ceftamin
regimens_df = pymaceuticals_clean.loc[(pymaceuticals_clean["Drug Regimen"] == "Capomulin") | (pymaceuticals_clean["Drug Regimen"] == "Ramicane") | 
                                      (pymaceuticals_clean["Drug Regimen"] == "Infubinol") | (pymaceuticals_clean["Drug Regimen"] == "Ceftamin")]
regimens_df.head()
# Start by getting the last (greatest) timepoint for each mouse
regimens_df = regimens_df.sort_values(["Timepoint"], ascending=False)
regimens_df.head()

# Merge this group df with the original dataframe to get the tumor volume at the last timepoint
tumor_df = pd.merge(pymaceuticals_data, regimens_df, on=["Tumor Volume(mm3)","Tumor Volume(mm3)"])
tumor_df
399/40:
# Calculate the final tumor volume of each mouse across four of the treatment regimens:  
# Capomulin, Ramicane, Infubinol, and Ceftamin
regimens_df = pymaceuticals_clean.loc[(pymaceuticals_clean["Drug Regimen"] == "Capomulin") | (pymaceuticals_clean["Drug Regimen"] == "Ramicane") | 
                                      (pymaceuticals_clean["Drug Regimen"] == "Infubinol") | (pymaceuticals_clean["Drug Regimen"] == "Ceftamin")]
regimens_df.head()
# Start by getting the last (greatest) timepoint for each mouse
regimens_df = regimens_df.sort_values(["Timepoint"], ascending=False)
regimens_df.head()

# Merge this group df with the original dataframe to get the tumor volume at the last timepoint
tumor_df = pd.merge(pymaceuticals_data, regimens_df, on=["Tumor Volume(mm3)","Tumor Volume(mm3)"], how=left)
tumor_df
399/41:
# Calculate the final tumor volume of each mouse across four of the treatment regimens:  
# Capomulin, Ramicane, Infubinol, and Ceftamin
regimens_df = pymaceuticals_clean.loc[(pymaceuticals_clean["Drug Regimen"] == "Capomulin") | (pymaceuticals_clean["Drug Regimen"] == "Ramicane") | 
                                      (pymaceuticals_clean["Drug Regimen"] == "Infubinol") | (pymaceuticals_clean["Drug Regimen"] == "Ceftamin")]
regimens_df.head()
# Start by getting the last (greatest) timepoint for each mouse
regimens_df = regimens_df.sort_values(["Timepoint"], ascending=False)
regimens_df.head()

# Merge this group df with the original dataframe to get the tumor volume at the last timepoint
tumor_df = pd.merge(pymaceuticals_data, regimens_df, on=["Tumor Volume(mm3)","Tumor Volume(mm3)"], how=right)
tumor_df
399/42:
# Calculate the final tumor volume of each mouse across four of the treatment regimens:  
# Capomulin, Ramicane, Infubinol, and Ceftamin
regimens_df = pymaceuticals_clean.loc[(pymaceuticals_clean["Drug Regimen"] == "Capomulin") | (pymaceuticals_clean["Drug Regimen"] == "Ramicane") | 
                                      (pymaceuticals_clean["Drug Regimen"] == "Infubinol") | (pymaceuticals_clean["Drug Regimen"] == "Ceftamin")]
regimens_df.head()
# Start by getting the last (greatest) timepoint for each mouse
regimens_df = regimens_df.sort_values(["Timepoint"], ascending=False)
regimens_df.head()

# Merge this group df with the original dataframe to get the tumor volume at the last timepoint
tumor_df = pd.merge(pymaceuticals_data, regimens_df, on=["Tumor Volume(mm3)","Tumor Volume(mm3)"])
tumor_df
399/43:
# Calculate the final tumor volume of each mouse across four of the treatment regimens:  
# Capomulin, Ramicane, Infubinol, and Ceftamin
regimens_df = pymaceuticals_clean.loc[(pymaceuticals_clean["Drug Regimen"] == "Capomulin") | (pymaceuticals_clean["Drug Regimen"] == "Ramicane") | 
                                      (pymaceuticals_clean["Drug Regimen"] == "Infubinol") | (pymaceuticals_clean["Drug Regimen"] == "Ceftamin")]
regimens_df.head()
# Start by getting the last (greatest) timepoint for each mouse
regimens_df = regimens_df.sort_values(["Timepoint"], ascending=False)
regimens_df.head()

# Merge this group df with the original dataframe to get the tumor volume at the last timepoint
#tumor_df = pd.merge(pymaceuticals_data, regimens_df, on=["Tumor Volume(mm3)","Tumor Volume(mm3)"])
#tumor_df
399/44:
# Calculate the final tumor volume of each mouse across four of the treatment regimens:  
# Capomulin, Ramicane, Infubinol, and Ceftamin
regimens_df = pymaceuticals_clean.loc[(pymaceuticals_clean["Drug Regimen"] == "Capomulin") | (pymaceuticals_clean["Drug Regimen"] == "Ramicane") | 
                                      (pymaceuticals_clean["Drug Regimen"] == "Infubinol") | (pymaceuticals_clean["Drug Regimen"] == "Ceftamin")]
regimens_df.head()
# Start by getting the last (greatest) timepoint for each mouse
regimens_df = regimens_df.sort_values(["Timepoint"], ascending=False)
regimens_df.head()
pymaceuticals_data.head()

# Merge this group df with the original dataframe to get the tumor volume at the last timepoint
#tumor_df = pd.merge(pymaceuticals_data, regimens_df, on=["Tumor Volume(mm3)","Tumor Volume(mm3)"])
#tumor_df
399/45:
# Calculate the final tumor volume of each mouse across four of the treatment regimens:  
# Capomulin, Ramicane, Infubinol, and Ceftamin
regimens_df = pymaceuticals_clean.loc[(pymaceuticals_clean["Drug Regimen"] == "Capomulin") | (pymaceuticals_clean["Drug Regimen"] == "Ramicane") | 
                                      (pymaceuticals_clean["Drug Regimen"] == "Infubinol") | (pymaceuticals_clean["Drug Regimen"] == "Ceftamin")]
regimens_df.head()
# Start by getting the last (greatest) timepoint for each mouse
regimens_df = regimens_df.sort_values(["Timepoint"], ascending=False)
regimens_df.head()

# Merge this group df with the original dataframe to get the tumor volume at the last timepoint
tumor_df = regimens_df.merge(pymaceuticals_data, on=["Tumor Volume(mm3)","Tumor Volume(mm3)"])
#tumor_df
399/46:
# Calculate the final tumor volume of each mouse across four of the treatment regimens:  
# Capomulin, Ramicane, Infubinol, and Ceftamin
regimens_df = pymaceuticals_clean.loc[(pymaceuticals_clean["Drug Regimen"] == "Capomulin") | (pymaceuticals_clean["Drug Regimen"] == "Ramicane") | 
                                      (pymaceuticals_clean["Drug Regimen"] == "Infubinol") | (pymaceuticals_clean["Drug Regimen"] == "Ceftamin")]
regimens_df.head()
# Start by getting the last (greatest) timepoint for each mouse
regimens_df = regimens_df.sort_values(["Timepoint"], ascending=False)
regimens_df.head()

# Merge this group df with the original dataframe to get the tumor volume at the last timepoint
tumor_df = regimens_df.merge(pymaceuticals_data, left_on='Tumor Volume(mm3)', right_on='Tumor Volume(mm3)')
tumor_df
399/47:
# Calculate the final tumor volume of each mouse across four of the treatment regimens:  
# Capomulin, Ramicane, Infubinol, and Ceftamin
regimens_df = pymaceuticals_clean.loc[(pymaceuticals_clean["Drug Regimen"] == "Capomulin") | (pymaceuticals_clean["Drug Regimen"] == "Ramicane") | 
                                      (pymaceuticals_clean["Drug Regimen"] == "Infubinol") | (pymaceuticals_clean["Drug Regimen"] == "Ceftamin")]
regimens_df.head()
# Start by getting the last (greatest) timepoint for each mouse
regimens_df = regimens_df.sort_values(["Timepoint"], ascending=False)
regimens_df.head()

# Merge this group df with the original dataframe to get the tumor volume at the last timepoint
tumor_df = regimens_df.merge(pymaceuticals_data, on='Tumor Volume(mm3)')
tumor_df
399/48:
# Calculate the final tumor volume of each mouse across four of the treatment regimens:  
# Capomulin, Ramicane, Infubinol, and Ceftamin
regimens_df = pymaceuticals_clean.loc[(pymaceuticals_clean["Drug Regimen"] == "Capomulin") | (pymaceuticals_clean["Drug Regimen"] == "Ramicane") | 
                                      (pymaceuticals_clean["Drug Regimen"] == "Infubinol") | (pymaceuticals_clean["Drug Regimen"] == "Ceftamin")]
regimens_df.head()
# Start by getting the last (greatest) timepoint for each mouse
regimens_df = regimens_df.sort_values(["Timepoint"], ascending=False)
regimens_df.head()

# Merge this group df with the original dataframe to get the tumor volume at the last timepoint
tumor_df = regimens_df.merge(pymaceuticals_data, on='Tumor Volume(mm3)').sum()
tumor_df
399/49:
# Calculate the final tumor volume of each mouse across four of the treatment regimens:  
# Capomulin, Ramicane, Infubinol, and Ceftamin
regimens_df = pymaceuticals_clean.loc[(pymaceuticals_clean["Drug Regimen"] == "Capomulin") | (pymaceuticals_clean["Drug Regimen"] == "Ramicane") | 
                                      (pymaceuticals_clean["Drug Regimen"] == "Infubinol") | (pymaceuticals_clean["Drug Regimen"] == "Ceftamin")]
regimens_df.head()
# Start by getting the last (greatest) timepoint for each mouse
regimens_df = regimens_df.sort_values(["Timepoint"], ascending=False)
regimens_df.head()

# Merge this group df with the original dataframe to get the tumor volume at the last timepoint
# tumor_df = regimens_df.merge(pymaceuticals_data, on='Tumor Volume(mm3)')
# tumor_df
399/50:
# Calculate the final tumor volume of each mouse across four of the treatment regimens:  
# Capomulin, Ramicane, Infubinol, and Ceftamin
regimens_df = pymaceuticals_clean.loc[(pymaceuticals_clean["Drug Regimen"] == "Capomulin") | (pymaceuticals_clean["Drug Regimen"] == "Ramicane") | 
                                      (pymaceuticals_clean["Drug Regimen"] == "Infubinol") | (pymaceuticals_clean["Drug Regimen"] == "Ceftamin")]
regimens_df.head()
# Start by getting the last (greatest) timepoint for each mouse
regimens_df = regimens_df.sort_values(["Timepoint"], ascending=False)
regimens_df.head()

# Merge this group df with the original dataframe to get the tumor volume at the last timepoint
tumor_df = pd.merge(pymaceuticals_data, regimens_df, on=("Mouse ID","Timepoint"),how="left")
tumor_df
399/51:
# Calculate the final tumor volume of each mouse across four of the treatment regimens:  
# Capomulin, Ramicane, Infubinol, and Ceftamin
regimens_df = pymaceuticals_clean.loc[(pymaceuticals_clean["Drug Regimen"] == "Capomulin") | (pymaceuticals_clean["Drug Regimen"] == "Ramicane") | 
                                      (pymaceuticals_clean["Drug Regimen"] == "Infubinol") | (pymaceuticals_clean["Drug Regimen"] == "Ceftamin")]
regimens_df.head()
# Start by getting the last (greatest) timepoint for each mouse
regimens_df = regimens_df.groupby('Mouse ID').max()['Timepoint']
regimens_df.head()

# Merge this group df with the original dataframe to get the tumor volume at the last timepoint
#tumor_df = pd.merge(pymaceuticals_data, regimens_df, on=("Mouse ID","Timepoint"),how="left")
#umor_df
399/52:
# Calculate the final tumor volume of each mouse across four of the treatment regimens:  
# Capomulin, Ramicane, Infubinol, and Ceftamin
regimens_df = pymaceuticals_clean.loc[(pymaceuticals_clean["Drug Regimen"] == "Capomulin") | (pymaceuticals_clean["Drug Regimen"] == "Ramicane") | 
                                      (pymaceuticals_clean["Drug Regimen"] == "Infubinol") | (pymaceuticals_clean["Drug Regimen"] == "Ceftamin")]
regimens_df.head()
# Start by getting the last (greatest) timepoint for each mouse
regimens_df = regimens_df.groupby('Mouse ID').max()['Timepoint']
regimens_df.head()

# Merge this group df with the original dataframe to get the tumor volume at the last timepoint
tumor_df = pd.merge(pymaceuticals_data, regimens_df, on=("Mouse ID","Timepoint"),how="left")
tumor_df
399/53:
# Calculate the final tumor volume of each mouse across four of the treatment regimens:  
# Capomulin, Ramicane, Infubinol, and Ceftamin
regimens_df = pymaceuticals_clean.loc[(pymaceuticals_clean["Drug Regimen"] == "Capomulin") | (pymaceuticals_clean["Drug Regimen"] == "Ramicane") | 
                                      (pymaceuticals_clean["Drug Regimen"] == "Infubinol") | (pymaceuticals_clean["Drug Regimen"] == "Ceftamin")]
regimens_df.head()
# Start by getting the last (greatest) timepoint for each mouse
regimens_df = regimens_df.groupby('Mouse ID').max()['Timepoint']
regimens_df.head()

# Merge this group df with the original dataframe to get the tumor volume at the last timepoint
tumor_df = pd.merge(pymaceuticals_data, regimens_df, on=("Mouse ID","Timepoint"),how="right")
tumor_df
399/54:
# Put treatments into a list for for loop (and later for plot labels)
treatments = ['Capomulin', 'Ramicane', 'Infubinol', 'Ceftamin']

# Create empty list to fill with tumor vol data (for plotting)
tumor_vol = []

# Calculate the IQR and quantitatively determine if there are any potential outliers. 

quartiles = tumor_df.quantile([.25,.5,.75])
lowerq = quartiles[0.25]
upperq = quartiles[0.75]
iqr = upperq-lowerq
    # Locate the rows which contain mice on each drug and get the tumor volumes
    
    
    # add subset 
    
    
    # Determine outliers using upper and lower bounds
399/55:
# Put treatments into a list for for loop (and later for plot labels)
treatments = ['Capomulin', 'Ramicane', 'Infubinol', 'Ceftamin']

# Create empty list to fill with tumor vol data (for plotting)
tumor_vol = []

# Calculate the IQR and quantitatively determine if there are any potential outliers. 

quartiles = tumor_df.quantile([.25,.5,.75])
lowerq = quartiles[0.25]
upperq = quartiles[0.75]
iqr = upperq-lowerq
quartiles

    # Locate the rows which contain mice on each drug and get the tumor volumes
    
    
    # add subset 
    
    
    # Determine outliers using upper and lower bounds
399/56:
# Put treatments into a list for for loop (and later for plot labels)
treatments = ['Capomulin', 'Ramicane', 'Infubinol', 'Ceftamin']

# Create empty list to fill with tumor vol data (for plotting)
tumor_vol = []

# Calculate the IQR and quantitatively determine if there are any potential outliers. 

quartiles = tumor_df.quantile([.25,.5,.75])
lowerq = quartiles[.25]
upperq = quartiles[.75]
iqr = upperq-lowerq
quartiles

    # Locate the rows which contain mice on each drug and get the tumor volumes
    
    
    # add subset 
    
    
    # Determine outliers using upper and lower bounds
399/57:
# Put treatments into a list for for loop (and later for plot labels)
treatments = ['Capomulin', 'Ramicane', 'Infubinol', 'Ceftamin']

# Create empty list to fill with tumor vol data (for plotting)
tumor_vol = []

# Calculate the IQR and quantitatively determine if there are any potential outliers. 

quartiles = tumor_df.quantile([.25,.5,.75])
#lowerq = quartiles[.25]
#upperq = quartiles[.75]
#iqr = upperq-lowerq


    # Locate the rows which contain mice on each drug and get the tumor volumes
    
    
    # add subset 
    
    
    # Determine outliers using upper and lower bounds
399/58:
# Put treatments into a list for for loop (and later for plot labels)
treatments = ['Capomulin', 'Ramicane', 'Infubinol', 'Ceftamin']

# Create empty list to fill with tumor vol data (for plotting)
tumor_vol = []

# Calculate the IQR and quantitatively determine if there are any potential outliers. 

quartiles = tumor_df.quantile([.25,.5,.75])
#lowerq = quartiles[.25]
#upperq = quartiles[.75]
#iqr = upperq-lowerq
quartiles

    # Locate the rows which contain mice on each drug and get the tumor volumes
    
    
    # add subset 
    
    
    # Determine outliers using upper and lower bounds
399/59:
# Put treatments into a list for for loop (and later for plot labels)
treatments = ['Capomulin', 'Ramicane', 'Infubinol', 'Ceftamin']

# Create empty list to fill with tumor vol data (for plotting)
tumor_vol = []

# Calculate the IQR and quantitatively determine if there are any potential outliers. 

quartiles = tumor_df.quantile([.25,.5,.75])
lowerq = quartiles[.25]
#upperq = quartiles[.75]
#iqr = upperq-lowerq
quartiles

    # Locate the rows which contain mice on each drug and get the tumor volumes
    
    
    # add subset 
    
    
    # Determine outliers using upper and lower bounds
399/60:
# Put treatments into a list for for loop (and later for plot labels)
treatments = ['Capomulin', 'Ramicane', 'Infubinol', 'Ceftamin']

# Create empty list to fill with tumor vol data (for plotting)
tumor_vol = []

# Calculate the IQR and quantitatively determine if there are any potential outliers. 

quartiles = tumor_df.quantile([0.25,0.5,0.75])
lowerq = quartiles[0.25]
upperq = quartiles[0.75]
iqr = upperq-lowerq

print(f"The lower quartile is: {lowerq}")
print(f"The upper quartile is: {upperq}")
print(f"The interquartile range is: {iqr}")
print(f"The the median is: {quartiles[0.5]} ")

    # Locate the rows which contain mice on each drug and get the tumor volumes
    
    
    # add subset 
    
    
    # Determine outliers using upper and lower bounds
399/61:
# Put treatments into a list for for loop (and later for plot labels)
treatments = ['Capomulin', 'Ramicane', 'Infubinol', 'Ceftamin']

# Create empty list to fill with tumor vol data (for plotting)
tumor_vol = []

# Calculate the IQR and quantitatively determine if there are any potential outliers. 

quartiles = tumor_df.quantile([0.25,0.5,0.75])
lowerq = quartiles[0.25]
upperq = quartiles[0.75]
iqr = upperq-lowerq

print(f"The lower quartile is: {lowerq}")
print(f"The upper quartile is: {upperq}")
print(f"The interquartile range is: {iqr}")
print(f"The the median is: {quartiles[0.5]}")

    # Locate the rows which contain mice on each drug and get the tumor volumes
    
    
    # add subset 
    
    
    # Determine outliers using upper and lower bounds
406/1:
# Dependencies
import json
import os

# Load JSON
filepath = os.path.join("..", "Resources", "youtube_response.json")
with open(filepath) as jsonfile:
    json_data = json.load(jsonfile)
406/2:
data = video_json['data']
data_items = data['items']

title = data_items[0]['title']
print("Title: ", title)
406/3:
# Dependencies
import json
import os

# Load JSON
filepath = os.path.join("..", "Resources", "youtube_response.json")
with open(filepath) as jsonfile:
    json_data = json.load(jsonfile)
406/4:
data = json_data['data']
data_items = data['items']

title = data_items[0]['title']
print("Title: ", title)
406/5:
rating = data_items[0]['rating']
print("Rating: ", rating)
406/6:
tumbnail = data_items[0]['thumbnail']['default']
tumbnail
406/7:
thumbnail = data_items[0]['thumbnail']['default']
print("Thumbnail: ", thumbnail)
406/8:
view_count = data_items[0]['viewcCount']
print(f"View count: {view_count}")
406/9:
view_count = data_items[0]['viewCount']
print(f"View count: {view_count}")
406/10:
# Dependencies
import json
import os

# Load JSON
filepath = os.path.join("..", "Resources", "youtube_response.json")
with open(filepath) as jsonfile:
    json_data = json.load(jsonfile)
406/11:
data = json_data['data']
data_items = data['items']

title = data_items[0]['title']
print("Title: ", title)
406/12:
thumbnail = data_items[0]['thumbnail']['default']
thumbnail
406/13:
thumbnail = data_items[0]['thumbnail']['default']
print(thumbnail)
406/14:
thumbnail = data_items[0]['thumbnail']['default']
print("The thumbnail is: " thumbnail)
406/15:
thumbnail = data_items[0]['thumbnail']['default']
print("The thumbnail is: ", thumbnail)
408/1:
# Dependencies
import json
import requests
408/2:
# Specify the URL
url = "http://nyt-mongo-scraper.herokuapp.com/api/headlines"

# Make request and store response
response = requests.get(url)
408/3:
# JSON-ify response
response_json = response.json()
408/4:
# Print first and last articles
print(f"The first response is {json.dumps(response_json[0], indent=2)}.")
print(f"The last response is {json.dumps(response_json[-1], indent=2)}.")
408/5:
#Print the number of responses received.
print(f"We received {len(response_json)} responses.")
408/6:
# Dependencies
import json
import requests
408/7:
# Specify the URL
url = "http://nyt-mongo-scraper.herokuapp.com/api/headlines"

# Make request and store response
response = requests.get(url)

print(response.status_code)
408/8:
# JSON-ify response
response_json = response.json()
408/9:
# Print first and last articles
print(f"The first response is {json.dumps(response_json[0], indent=2)}.")
print(f"The last response is {json.dumps(response_json[-1], indent=2)}.")
408/10:
# Specify the URL
url = "http://nyt-mongo-scraper.herokuapp.com/api/healines"

# Make request and store response
response = requests.get(url)

print(response.status_code)
408/11:
# Specify the URL
url = "http://nyt-mongo-scraper.herokuapp.com/api/headlines"

# Make request and store response
response = requests.get(url)

print(response.status_code)
408/12:
# JSON-ify response
response_json = response.json()
408/13:
# Print first and last articles
print(f"The first response is {json.dumps(response_json[0], indent=2)}.")
print(f"The last response is {json.dumps(response_json[-1], indent=2)}.")
408/14:
#Print the number of responses received.
print(f"We received {len(response_json)} responses.")
408/15:
# Print first and last articles
print(f"The first response is {json.dumps(response_json[0], indent=4)}.")
print(f"The last response is {json.dumps(response_json[-1], indent=4)}.")
411/1:
# Dependencies
import requests
from config import api_key

# Save config information.
url = "https://www.weather.gov/documentation/services-web-api#"
city = "Hawaii"
units = "metric"
411/2:
# Build query URL and request your results in Celsius
query_url = f"{url}appid={api_key}&q={city}&units={units}"

# Get weather data
weather_response = requests.get(query_url)
weather_json = weather_response.json()
411/3:
# Report temperature
print(f"The temperature in Hawaii is {temperature} C.")
411/4:
# Get temperature from JSON response
temperature = weather_json["main"]["temp"]
411/5:
# Dependencies
import requests
from config import api_key

# Save config information.
url = "https://www.weather.gov/documentation/services-web-api#"
city = "Hawaii"
units = "metric"
411/6:
# Build query URL and request your results in Celsius
query_url = f"{url}appid={api_key}&q={city}&units={units}"

# Get weather data
weather_response = requests.get(query_url)
weather_json = weather_response.json()
411/7:
# Dependencies
import requests
from config import api_key

# Save config information.
url = "https://www.weather.gov/documentation/services-web-api#/default/get_alerts"
city = "Hawaii"
units = "metric"
411/8:
# Build query URL and request your results in Celsius
query_url = f"{url}appid={api_key}&q={city}&units={units}"

# Get weather data
weather_response = requests.get(query_url)
weather_json = weather_response.json()
412/1:
# Dependencies
import requests
from config import api_key

# Save config information.
url = "https://www.weather.gov/documentation/services-web-api#/default/get_alerts"
city = "Hawaii"
units = "metric"
412/2:
# Build query URL and request your results in Celsius
query_url = f"{url}appid={api_key}&q={city}&units={units}"

# Get weather data
weather_response = requests.get(query_url)
weather_json = weather_response.json()
413/1:
# Dependencies
import requests
from config import api_key

# Save config information.
url = "https://www.weather.gov/documentation/services-web-api#/default/get_alerts"
city = "Hawaii"
units = "metric"
413/2:
# Build query URL and request your results in Celsius
query_url = f"{url}appid={api_key}&q={city}&units={units}"

# Get weather data
weather_response = requests.get(query_url)
weather_json = weather_response.json()
413/3:
# Dependencies
import requests
from config import api_key

# Save config information.
url = "https://www.weather.gov/documentation/services-web-api#/default/get_alerts"
city = "Hawaii"
units = "metric"
413/4:
# Build query URL and request your results in Celsius
query_url = f"{url}appid={api_key}&q={city}&units={units}"

# Get weather data
weather_response = requests.get(query_url)
weather_json = weather_response.json()
413/5:
# Dependencies
import requests
from config import api_key

# Save config information.
url = "https://www.weather.gov/documentation/services-web-api#/default/get_alerts"
city = "Hawaii"
units = "metric"
413/6:
# Build query URL and request your results in Celsius
query_url = f"{url}appid={api_key}&q={city}&units={units}"

# Get weather data
weather_response = requests.get(query_url)
weather_json = weather_response.json()
413/7:
# Dependencies
import requests
from config import api_key

# Save config information.
url = "https://www.weather.gov/documentation/services-web-api#"
city = "Hawaii"
units = "metric"
413/8:
# Build query URL and request your results in Celsius
query_url = f"{url}appid={api_key}&q={city}&units={units}"

# Get weather data
weather_response = requests.get(query_url)
weather_json = weather_response.json()
413/9:
# Dependencies
import requests
from config import api_key

# Save config information.
url = "https://www.weather.gov/documentation/services-web-api#"
city = "Hawaii"
units = "metric"
413/10:
# Build query URL and request your results in Celsius
query_url = f"{url}appid={api_key}&q={city}&units={units}"

# Get weather data
weather_response = requests.get(query_url)
weather_json = weather_response.json()
413/11:
# Dependencies
import requests
from config import api_key

# Save config information.
url = "https://api.weather.gov/alerts/active?area={state}"
state = "KS"
units = "metric"
413/12:
# Build query URL and request your results in Celsius
query_url = f"{url}appid={api_key}&q={city}&units={units}"

# Get weather data
weather_response = requests.get(query_url)
weather_json = weather_response.json()
413/13:
# Build query URL and request your results in Celsius
query_url = f"{url}appid={api_key}&q={city}&units={units}"

# Get weather data
weather_response = requests.get(query_url)
weather_json = weather_response.json()
413/14:
# Get temperature from JSON response
temperature = weather_json["main"]["temp"]
413/15:
# Report temperature
print(f"The temperature in Hawaii is {temperature} C.")
413/16:
# Dependencies
import requests
from config import api_key

# Save config information.
url = f"https://api.weather.gov/alerts/active?area={state}"
state = "KS"
units = "metric"
413/17:
# Build query URL and request your results in Celsius

# Get weather data
weather_response = requests.get(query_url)
weather_json = weather_response.json()
413/18:
# Get temperature from JSON response
weather_json
413/19:
# Dependencies
import requests
from config import api_key

# Save config information.
url = f"https://api.weather.gov/alerts/active?area={state}"
state = "KS"
units = "metric"
413/20:
# Build query URL and request your results in Celsius

# Get weather data
weather_response = requests.get(url)
weather_json = weather_response.json()
413/21:
# Get temperature from JSON response
weather_json
413/22:
# Dependencies
import requests
import json
from config import api_key

# Save config information.
url = f"https://api.weather.gov/alerts/active?area={state}"
state = "KS"
units = "metric"
413/23:
# Build query URL and request your results in Celsius

# Get weather data
weather_response = requests.get(url)
weather_json = weather_response.json()
413/24:
# Get temperature from JSON response
weather_json
399/62:
plot = gender_data.plot.pie(y='Total Count', explode=explode, labels=labels, colors=colors, autopct="%1.1f%%", shadow=True, startangle=140)
plt.show()
plt.savfig("Mice count piechart.png")
414/1: %matplotlib notebook
414/2:
# Dependencies
import matplotlib.pyplot as plt
import pandas as pd
import scipy.stats as st
import numpy as np
414/3:
# Files
mouse_metadata_path = "data/Mouse_metadata.csv"
study_results_path = "data/Study_results.csv"
414/4:
# Read the mouse data and the study results
mouse_metadata = pd.read_csv(mouse_metadata_path)
study_results = pd.read_csv(study_results_path)

mouse_metadata.head()
study_results.head()
414/5:
# Combine the data into a single dataset
pymaceuticals_data = pd.merge(mouse_metadata, study_results, on=["Mouse ID", "Mouse ID"])

# Display the data table for preview
pymaceuticals_data
414/6:
 # Checking the number of mice.
number_of_mice = len(pymaceuticals_data["Mouse ID"])
number_of_mice
414/7:
# Getting the duplicate mice by ID number that shows up for Mouse ID and Timepoint. 
duplicate_mice = pymaceuticals_data.duplicated(subset=['Mouse ID', 'Timepoint'])
duplicate_mice
414/8:
# Create a clean DataFrame by dropping the duplicate mouse by its ID.
pymaceuticals_clean = pymaceuticals_data.drop_duplicates(subset=['Mouse ID', 'Timepoint'],keep='first',inplace=False)
pymaceuticals_clean
414/9:
 # Checking the number of mice in the clean DataFrame.
total_mice = len(pymaceuticals_clean["Mouse ID"])
total_mice
414/10:
# Generate a summary statistics table of mean, median, variance, standard deviation, and SEM of the tumor volume for each regimen

# This method is the most straighforward, creating multiple series and putting them all together at the end.
grouped = pymaceuticals_clean.groupby('Drug Regimen')
summary_table = round(grouped.agg(['mean','median','var','std','sem'])["Tumor Volume (mm3)"], 2)
summary_table
414/11:
regimen = pymaceuticals_clean.groupby(['Drug Regimen']).count()['Mouse ID']
regimen
414/12: x_axis = np.arange(len(regimen))
414/13:
# Generate a bar plot showing the total number of mice for each treatment throughout the course of the study using pandas.
# tick locations horizontal?
plt.bar(x_axis, regimen, color='pink', alpha=1, align='center')

tick_locations = [value for value in x_axis]
plt.xticks(tick_locations, ['Capomulin', 'Ceftamin', 'Infubinol', 'Ketapril', 'Naftisol', 'Placebo', 'Propriva', 'Ramicane', 'Stelasyn', 'Zoniferol'], rotation='vertical')


plt.title("Total Mice Treated")
plt.xlabel("Drug Regimen")
plt.ylabel("Number of Mice")

plt.tight_layout()
plt.show()
414/14:
# Generate a pie plot showing the distribution of female versus male mice using pandas
gender = pymaceuticals_clean.groupby(["Mouse ID","Sex"])
gender
414/15:
gender_df = pd.DataFrame(gender.size())
gender_df
414/16:
gender_data = pd.DataFrame(gender_df.groupby(["Sex"]).count())
gender_data.columns = ["Total Count"]
gender_data
414/17:
gender_data["Gender by Percentage"] = round(100*(gender_data["Total Count"]/gender_data["Total Count"].sum()), 2)
gender_data
414/18:
labels = ["Female", "Male"]
colors = ["pink", "lightblue"]
explode = (0.1, 0)
414/19:
plot = gender_data.plot.pie(y='Total Count', explode=explode, labels=labels, colors=colors, autopct="%1.1f%%", shadow=True, startangle=140)
plt.show()
plt.savfig("Mice count piechart.png")
414/20:
plot = gender_data.plot.pie(y='Total Count', explode=explode, labels=labels, colors=colors, autopct="%1.1f%%", shadow=True, startangle=140)
plt.show()
plt.savfig("Mice_count_piechart.png")
414/21:
plot = gender_data.plot.pie(y='Total Count', explode=explode, labels=labels, colors=colors, autopct="%1.1f%%", shadow=True, startangle=140)
plt.show()
plt.savefig("Mice_count_piechart.png")
415/1:
#Dependencies
import requests
import json
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
415/2:
#list of tv show titles to query
tv_shows = ["Altered Carbon", "Grey's Anatomy", "This is Us", "The Flash", "Vikings", "Shameless", "Arrow", "Peaky Blinders", "Dirk Gently", "The Crown"]

# make iterative requests to TVmaze search endpoint
base_url = "http://api.tvmaze.com/search/shows?q="
titles = []
ratings = []

# For loop
for show in tv_shows:
    target_url = base_url + show
    response = requests.get(target_url).json()
    titles.append(response[0]['show']['average'])
    ratings.append(response[0]['show']['rating']['average'])
415/3:
#list of tv show titles to query
tv_shows = ["Altered Carbon", "Grey's Anatomy", "This is Us", "The Flash", "Vikings", "Shameless", "Arrow", "Peaky Blinders", "Dirk Gently", "The Crown"]

# make iterative requests to TVmaze search endpoint
base_url = "http://api.tvmaze.com/search/shows?q="
titles = []
ratings = []

# For loop
for show in tv_shows:
    target_url = base_url + show
    response = requests.get(target_url).json()
    titles.append(response[0]['show']['name'])
    ratings.append(response[0]['show']['rating']['average'])
415/4:
# create dataframe
shows_df = pd.DataFrame({
        "title": titles,
        "rating": ratings
})

shows_df
415/5:
# use pandas to create a bar chart from the dataframe
shows_df.plot('title', 'rating', kind='bar', figsize=(10,5), rot=45)
415/6:
# use pandas to create a bar chart from the dataframe
shows_df.plot('title', 'rating', kind='bar', figsize=(10,5), rot=45)
plt.title("Show Ratings")
415/7:
# use pandas to create a bar chart from the dataframe
shows_df.plot('title', 'rating', kind='bar', figsize=(10,5), rot=45)
plt.title("Show Ratings")
plt.xtitle('Shows')
415/8:
# use pandas to create a bar chart from the dataframe
shows_df.plot('title', 'rating', kind='bar', figsize=(10,5), rot=45)
plt.title("Show Ratings")
plt.xtitle("Shows")
415/9:
# use pandas to create a bar chart from the dataframe
shows_df.plot(kind='bar', figsize=(10,5), rot=45)
plt.title("Show Ratings")
415/10:
# use pandas to create a bar chart from the dataframe
shows_df.plot('title', 'rating', kind='bar', figsize=(10,5), rot=45)
plt.title("Show Ratings")
415/11:
# use pandas to create a bar chart from the dataframe
shows_df.plot('title', 'rating', kind='bar', figsize=(10,5))
plt.title("Show Ratings")
417/1:
# Dependencies
import matplotlib.pyplot as plt
import requests
from scipy import stats
import pandas as pd
from config import api_key
417/2:
# Save config information.
url = "http://api.openweathermap.org/data/2.5/weather?"
units = "metric"

# Build partial query URL
query_url = f"{url}appid={api_key}&units={units}&q="
417/3:
# Get latitude and temperature for cities
cities = ["Paris", "London", "Oslo", "Beijing", "Mumbai", "Manila", "New York", "Seattle", "Dallas", "Taipei"]

# set up lists to hold reponse info
lat = []
temp = []

# Loop through the list of cities and perform a request for data on each
for city in cities:
    response = requests.get(query_url + city).json()
    lat.append(response['coord']['lat'])
    temp.append(response['main']['temp'])

print(f"The latitude information received is: {lat}")
print(f"The temperature information received is: {temp}")
417/4:
# create a data frame from cities, lat, and temp
weather_dict = {
    "city": cities,
    "lat": lat,
    "temp": temp
}
weather_data = pd.DataFrame(weather_dict)
weather_data
417/5:
# Create a Scatter Plot for temperature vs. latitude
weather_dict = {
    "city": cities,
    "lat": lat,
    "temp": temp
}
weather_data = pd.DataFrame(weather_dict)
weather_data
417/6:
# Create a Scatter Plot for temperature vs. latitude
x_values = weather_data['lat']
y_values = weather_data['temp']
plt.scatter(x_values,y_values)
plt.xlabel('Latitude')
plt.ylabel('Temperature')
plt.show()
417/7:
# Perform a linear regression on temperature vs. latitude
(slope, intercept, rvalue, pvalue, stderr) = stats.linregress(x_values, y_values)
# Get regression values
regress_vlaues = x_value *slope + intercept
print(regress_values)
417/8:
# Create a Scatter Plot for temperature vs. latitude
x_values = weather_data['lat']
y_values = weather_data['temp']
plt.scatter(x_values,y_values)
plt.xlabel('Latitude')
plt.ylabel('Temperature')
plt.show()
417/9:
# Perform a linear regression on temperature vs. latitude
(slope, intercept, rvalue, pvalue, stderr) = stats.linregress(x_values, y_values)
# Get regression values
regress_vlaues = x_value *slope + intercept
print(regress_values)
417/10:
# Perform a linear regression on temperature vs. latitude
(slope, intercept, rvalue, pvalue, stderr) = stats.linregress(x_values, y_values)
# Get regression values
regress_vlaues = x_values *slope + intercept
print(regress_values)
417/11:
# Perform a linear regression on temperature vs. latitude
(slope, intercept, rvalue, pvalue, stderr) = stats.linregress(x_values, y_values)
# Get regression values
regress_values = x_values * slope + intercept
print(regress_values)
417/12:
# Create line equation string
line_eq = "y =" + str(round(slope,2)) + "x +" + str(round(intercept,2))
print(line_eq)
417/13:
# Create Plot
plt.scatter(x_values,y_values)
plt.plot(x_values,regress_values, "r-")

# Label plot and annotate the line equation
plt.xlabel('Latitude')
plt.ylabel('Temperature')
plt.annotate(line_eq,(20,15),fontsize=15,color="blue")

# Print r square value
print(f"The r-value is: {rvalue**2}"")

# Show plot
plt.show()
417/14:
# Create line equation string
line_eq = "y =" + str(round(slope,2)) + "x +" + str(round(intercept,2))
print(line_eq)
417/15:
# Create Plot
plt.scatter(x_values,y_values)
plt.plot(x_values,regress_values, "r-")

# Label plot and annotate the line equation
plt.xlabel('Latitude')
plt.ylabel('Temperature')
plt.annotate(line_eq,(20,15),fontsize=15,color="blue")

# Print r square value
print(f"The r-value is: {rvalue**2}"")

# Show plot
plt.show()
417/16:
# Create Plot
plt.scatter(x_values,y_values)
plt.plot(x_values,regress_values, "r-")

# Label plot and annotate the line equation
plt.xlabel('Latitude')
plt.ylabel('Temperature')
plt.annotate(line_eq,(20,15),fontsize=15,color="blue")

# Print r square value
print(f"The r-value is: {rvalue**2}")

# Show plot
plt.show()
417/17:
# Use the line equation to predict the temperature for Florence at a latitude of 43.77 degrees
florence_lat = 43.77
florence_predicted_temp = round(slope * florence_lat + intercept,2)

print(f"The predicted temperature for Florence will be {florence_predicted_ttem}.")
417/18:
# Use the line equation to predict the temperature for Florence at a latitude of 43.77 degrees
florence_lat = 43.77
florence_predicted_temp = round(slope * florence_lat + intercept,2)

print(f"The predicted temperature for Florence will be {florence_predicted_temp}.")
417/19:
# Use API to determine actual temperature
response = requests.get(query_url + "Florence").json()
florence_actual_temp = response['main']['temp']

print(f"The sctual temperature of Florence is {florence_actual_temp}")
419/1: %matplotlib notebook
419/2:
# Dependencies
import matplotlib.pyplot as plt
import pandas as pd
import scipy.stats as st
import numpy as np
419/3:
# Files
mouse_metadata_path = "data/Mouse_metadata.csv"
study_results_path = "data/Study_results.csv"
419/4:
# Read the mouse data and the study results
mouse_metadata = pd.read_csv(mouse_metadata_path)
study_results = pd.read_csv(study_results_path)

mouse_metadata.head()
study_results.head()
419/5:
# Combine the data into a single dataset
pymaceuticals_data = pd.merge(mouse_metadata, study_results, on=["Mouse ID", "Mouse ID"])

# Display the data table for preview
pymaceuticals_data
419/6:
 # Checking the number of mice.
number_of_mice = len(pymaceuticals_data["Mouse ID"])
number_of_mice
419/7:
# Getting the duplicate mice by ID number that shows up for Mouse ID and Timepoint. 
duplicate_mice = pymaceuticals_data.duplicated(subset=['Mouse ID', 'Timepoint'])
duplicate_mice
419/8:
# Create a clean DataFrame by dropping the duplicate mouse by its ID.
pymaceuticals_clean = pymaceuticals_data.drop_duplicates(subset=['Mouse ID', 'Timepoint'],keep='first',inplace=False)
pymaceuticals_clean
419/9:
 # Checking the number of mice in the clean DataFrame.
total_mice = len(pymaceuticals_clean["Mouse ID"])
total_mice
419/10:
# Generate a summary statistics table of mean, median, variance, standard deviation, and SEM of the tumor volume for each regimen

# This method is the most straighforward, creating multiple series and putting them all together at the end.
grouped = pymaceuticals_clean.groupby('Drug Regimen')
summary_table = round(grouped.agg(['mean','median','var','std','sem'])["Tumor Volume (mm3)"], 2)
summary_table
419/11:
regimen = pymaceuticals_clean.groupby(['Drug Regimen']).count()['Mouse ID']
regimen
419/12: x_axis = np.arange(len(regimen))
419/13:
# Generate a bar plot showing the total number of mice for each treatment throughout the course of the study using pandas.
# tick locations horizontal?
plt.bar(x_axis, regimen, color='pink', alpha=1, align='center')

tick_locations = [value for value in x_axis]
plt.xticks(tick_locations, ['Capomulin', 'Ceftamin', 'Infubinol', 'Ketapril', 'Naftisol', 'Placebo', 'Propriva', 'Ramicane', 'Stelasyn', 'Zoniferol'], rotation='vertical')


plt.title("Total Mice Treated")
plt.xlabel("Drug Regimen")
plt.ylabel("Number of Mice")

plt.tight_layout()
plt.show()
plt.savefig("Total Mice Bar Plot.png")
419/14:
# Generate a pie plot showing the distribution of female versus male mice using pandas
gender = pymaceuticals_clean.groupby(["Mouse ID","Sex"])
gender
419/15:
gender_df = pd.DataFrame(gender.size())
gender_df
419/16:
gender_data = pd.DataFrame(gender_df.groupby(["Sex"]).count())
gender_data.columns = ["Total Count"]
gender_data
419/17:
gender_data["Gender by Percentage"] = round(100*(gender_data["Total Count"]/gender_data["Total Count"].sum()), 2)
gender_data
419/18:
labels = ["Female", "Male"]
colors = ["pink", "lightblue"]
explode = (0.1, 0)
419/19:
plot = gender_data.plot.pie(y='Total Count', explode=explode, labels=labels, colors=colors, autopct="%1.1f%%", shadow=True, startangle=140)
plt.show()
plt.savefig("Mice_count_piechart.png")
419/20:
# Calculate the final tumor volume of each mouse across four of the treatment regimens:  
# Capomulin, Ramicane, Infubinol, and Ceftamin
regimens_df = pymaceuticals_clean.loc[(pymaceuticals_clean["Drug Regimen"] == "Capomulin") | (pymaceuticals_clean["Drug Regimen"] == "Ramicane") | 
                                      (pymaceuticals_clean["Drug Regimen"] == "Infubinol") | (pymaceuticals_clean["Drug Regimen"] == "Ceftamin")]
regimens_df.head()
# Start by getting the last (greatest) timepoint for each mouse
regimens_df = regimens_df.groupby('Mouse ID').max()['Timepoint']
regimens_df.head()

# Merge this group df with the original dataframe to get the tumor volume at the last timepoint
tumor_df = pd.merge(pymaceuticals_data, regimens_df, on=("Mouse ID","Timepoint"),how="right")
tumor_df
419/21:
# Put treatments into a list for for loop (and later for plot labels)
treatments = ['Capomulin', 'Ramicane', 'Infubinol', 'Ceftamin']

# Create empty list to fill with tumor vol data (for plotting)
tumor_vol = []

# Calculate the IQR and quantitatively determine if there are any potential outliers. 

quartiles = tumor_df.quantile([0.25,0.5,0.75])
lowerq = quartiles[0.25]
upperq = quartiles[0.75]
iqr = upperq-lowerq

print(f"The lower quartile is: {lowerq}")
print(f"The upper quartile is: {upperq}")
print(f"The interquartile range is: {iqr}")
print(f"The the median is: {quartiles[0.5]}")

    # Locate the rows which contain mice on each drug and get the tumor volumes
    
    
    # add subset 
    
    
    # Determine outliers using upper and lower bounds
419/22:
plot = gender_data.plot.pie(y='Total Count', explode=explode, labels=labels, colors=colors, autopct="%1.1f%%", shadow=True, startangle=140)
plt.show()
plt.savefig("Mice Male vs. Female Piechart.png")
419/23:
# Generate a bar plot showing the total number of mice for each treatment throughout the course of the study using pandas.
# tick locations horizontal?
plt.bar(x_axis, regimen, color='pink', alpha=1, align='center')

tick_locations = [value for value in x_axis]
plt.xticks(tick_locations, ['Capomulin', 'Ceftamin', 'Infubinol', 'Ketapril', 'Naftisol', 'Placebo', 'Propriva', 'Ramicane', 'Stelasyn', 'Zoniferol'], rotation='vertical')


plt.title("Total Mice Treated")
plt.xlabel("Drug Regimen")
plt.ylabel("Number of Mice")

plt.tight_layout()
plt.show()
plt.savefig("Total Mice Bar Plot.png")
419/24:
# Generate a bar plot showing the total number of mice for each treatment throughout the course of the study using pandas.
# tick locations horizontal?
plt.bar(x_axis, regimen, color='pink', alpha=1, align='center')

tick_locations = [value for value in x_axis]
plt.xticks(tick_locations, ['Capomulin', 'Ceftamin', 'Infubinol', 'Ketapril', 'Naftisol', 'Placebo', 'Propriva', 'Ramicane', 'Stelasyn', 'Zoniferol'], rotation='vertical')


plt.title("Total Mice Treated")
plt.xlabel("Drug Regimen")
plt.ylabel("Number of Mice")

plt.tight_layout()
plt.show()
plt.savefig("Total Mice Bar Plot.png")
419/25:
# Generate a bar plot showing the total number of mice for each treatment throughout the course of the study using pandas.
# tick locations horizontal?
plt.bar(x_axis, regimen, color='pink', alpha=1, align='center')

tick_locations = [value for value in x_axis]
plt.xticks(tick_locations, ['Capomulin', 'Ceftamin', 'Infubinol', 'Ketapril', 'Naftisol', 'Placebo', 'Propriva', 'Ramicane', 'Stelasyn', 'Zoniferol'], rotation='vertical')


plt.title("Total Mice Treated")
plt.xlabel("Drug Regimen")
plt.ylabel("Number of Mice")

plt.tight_layout()
plt.savefig("Total Mice Bar Plot.png")
plt.show()
419/26: x_axis = np.arange(len(regimen))
419/27:
# Generate a bar plot showing the total number of mice for each treatment throughout the course of the study using pandas.
# tick locations horizontal?
plt.bar(x_axis, regimen, color='pink', alpha=1, align='center')

tick_locations = [value for value in x_axis]
plt.xticks(tick_locations, ['Capomulin', 'Ceftamin', 'Infubinol', 'Ketapril', 'Naftisol', 'Placebo', 'Propriva', 'Ramicane', 'Stelasyn', 'Zoniferol'], rotation='vertical')


plt.title("Total Mice Treated")
plt.xlabel("Drug Regimen")
plt.ylabel("Number of Mice")

plt.tight_layout()
plt.savefig("Total Mice Bar Plot.png")
plt.show()
419/28:
# Generate a pie plot showing the distribution of female versus male mice using pandas
gender = pymaceuticals_clean.groupby(["Mouse ID","Sex"])
gender
419/29:
gender_df = pd.DataFrame(gender.size())
gender_df
420/1: %matplotlib notebook
420/2:
# Dependencies
import matplotlib.pyplot as plt
import pandas as pd
import scipy.stats as st
import numpy as np
420/3:
# Files
mouse_metadata_path = "data/Mouse_metadata.csv"
study_results_path = "data/Study_results.csv"
420/4:
# Read the mouse data and the study results
mouse_metadata = pd.read_csv(mouse_metadata_path)
study_results = pd.read_csv(study_results_path)

mouse_metadata.head()
study_results.head()
420/5:
# Combine the data into a single dataset
pymaceuticals_data = pd.merge(mouse_metadata, study_results, on=["Mouse ID", "Mouse ID"])

# Display the data table for preview
pymaceuticals_data
420/6:
 # Checking the number of mice.
number_of_mice = len(pymaceuticals_data["Mouse ID"])
number_of_mice
420/7:
# Getting the duplicate mice by ID number that shows up for Mouse ID and Timepoint. 
duplicate_mice = pymaceuticals_data.duplicated(subset=['Mouse ID', 'Timepoint'])
duplicate_mice
420/8:
# Create a clean DataFrame by dropping the duplicate mouse by its ID.
pymaceuticals_clean = pymaceuticals_data.drop_duplicates(subset=['Mouse ID', 'Timepoint'],keep='first',inplace=False)
pymaceuticals_clean
420/9:
 # Checking the number of mice in the clean DataFrame.
total_mice = len(pymaceuticals_clean["Mouse ID"])
total_mice
420/10:
# Generate a summary statistics table of mean, median, variance, standard deviation, and SEM of the tumor volume for each regimen

# This method is the most straighforward, creating multiple series and putting them all together at the end.
grouped = pymaceuticals_clean.groupby('Drug Regimen')
summary_table = round(grouped.agg(['mean','median','var','std','sem'])["Tumor Volume (mm3)"], 2)
summary_table
420/11:
regimen = pymaceuticals_clean.groupby(['Drug Regimen']).count()['Mouse ID']
regimen
420/12: x_axis = np.arange(len(regimen))
420/13:
# Generate a bar plot showing the total number of mice for each treatment throughout the course of the study using pandas.
# tick locations horizontal?
plt.bar(x_axis, regimen, color='pink', alpha=1, align='center')

tick_locations = [value for value in x_axis]
plt.xticks(tick_locations, ['Capomulin', 'Ceftamin', 'Infubinol', 'Ketapril', 'Naftisol', 'Placebo', 'Propriva', 'Ramicane', 'Stelasyn', 'Zoniferol'], rotation='vertical')


plt.title("Total Mice Treated")
plt.xlabel("Drug Regimen")
plt.ylabel("Number of Mice")

plt.tight_layout()
plt.savefig("Total Mice Bar Plot.png")
plt.show()
420/14:
# Generate a pie plot showing the distribution of female versus male mice using pandas
gender = pymaceuticals_clean.groupby(["Mouse ID","Sex"])
gender
420/15:
gender_df = pd.DataFrame(gender.size())
gender_df
420/16:
gender_data = pd.DataFrame(gender_df.groupby(["Sex"]).count())
gender_data.columns = ["Total Count"]
gender_data
420/17:
gender_data["Gender by Percentage"] = round(100*(gender_data["Total Count"]/gender_data["Total Count"].sum()), 2)
gender_data
420/18:
labels = ["Female", "Male"]
colors = ["pink", "lightblue"]
explode = (0.1, 0)
420/19:
plot = gender_data.plot.pie(y='Total Count', explode=explode, labels=labels, colors=colors, autopct="%1.1f%%", shadow=True, startangle=140)
plt.show()
plt.savefig("Mice Male vs. Female Piechart.png")
420/20:
# Calculate the final tumor volume of each mouse across four of the treatment regimens:  
# Capomulin, Ramicane, Infubinol, and Ceftamin
regimens_df = pymaceuticals_clean.loc[(pymaceuticals_clean["Drug Regimen"] == "Capomulin") | (pymaceuticals_clean["Drug Regimen"] == "Ramicane") | 
                                      (pymaceuticals_clean["Drug Regimen"] == "Infubinol") | (pymaceuticals_clean["Drug Regimen"] == "Ceftamin")]
regimens_df.head()
# Start by getting the last (greatest) timepoint for each mouse
regimens_df = regimens_df.groupby('Mouse ID').max()['Timepoint']
regimens_df.head()

# Merge this group df with the original dataframe to get the tumor volume at the last timepoint
tumor_df = pd.merge(pymaceuticals_data, regimens_df, on=("Mouse ID","Timepoint"),how="right")
tumor_df
420/21:
# Put treatments into a list for for loop (and later for plot labels)
treatments = ['Capomulin', 'Ramicane', 'Infubinol', 'Ceftamin']

# Create empty list to fill with tumor vol data (for plotting)
tumor_vol = []

# Calculate the IQR and quantitatively determine if there are any potential outliers. 

quartiles = tumor_df.quantile([0.25,0.5,0.75])
lowerq = quartiles[0.25]
upperq = quartiles[0.75]
iqr = upperq-lowerq

print(f"The lower quartile is: {lowerq}")
print(f"The upper quartile is: {upperq}")
print(f"The interquartile range is: {iqr}")
print(f"The the median is: {quartiles[0.5]}")

    # Locate the rows which contain mice on each drug and get the tumor volumes
    
    
    # add subset 
    
    
    # Determine outliers using upper and lower bounds
421/1:
# Your assignment is to get the last line to print without changing any
# of the code below. Instead, wrap each line that throws an error in a
# try/except block.

try:
    print("Infinity looks like + " + str(10 / 0) + ".")
except ZeroDivisionError:
    print("Uh oh! I cannot do that.")

try:
    print("I think her name was + " + name + "?")
except NameError:
    print("That name has not been defined!")
    
try:
    print("Your name is a nonsense number. Look: " + int("Gabriel"))
except ValueError:
    print("That is not number!")

print("You made it through the gauntlet--the message survived!")
423/1:
import json
import requests
import pandas as pd
423/2:
# List of character
search_characters = ['R2-D2', 'Darth Vader', 'Godzilla', 'Luke Skywalker', 'Frodo', \
              'Boba Fett', 'Iron Man', 'Jon Snow', 'Han Solo']

# Set url for API
url = 'https://swapi.dev/api/people/?search='

# Set empty lists to hold character's height and mass
height = []
mass = []

# Loop through each character and make API request
for character in search_characters:

    # Create search query, make request and store in json
    query = url + character
    response = requests.get(query)
    response_json = response.json
    print(resonse_json)
    
    # Try to grab the height and mass of characters if they are available in the Star Wars API
    
        
    # Handle exceptions for a character that is not available in the Star Wars API
423/3:
# List of character
search_characters = ['R2-D2', 'Darth Vader', 'Godzilla', 'Luke Skywalker', 'Frodo', \
              'Boba Fett', 'Iron Man', 'Jon Snow', 'Han Solo']

# Set url for API
url = 'https://swapi.dev/api/people/?search='

# Set empty lists to hold character's height and mass
height = []
mass = []

# Loop through each character and make API request
for character in search_characters:

    # Create search query, make request and store in json
    query = url + character
    response = requests.get(query)
    response_json = response.json
    print(resonse_json)
    
    # Try to grab the height and mass of characters if they are available in the Star Wars API
    try:
        height.append(response_json['results'][0]['height'])
        mass.append(response_json['results'][0]['mass'])
        starwars_characters.append(character)
        print(f"{character} found! Appending stats)
        
    # Handle exceptions for a character that is not available in the Star Wars API
    except:
423/4:
# List of character
search_characters = ['R2-D2', 'Darth Vader', 'Godzilla', 'Luke Skywalker', 'Frodo', \
              'Boba Fett', 'Iron Man', 'Jon Snow', 'Han Solo']

# Set url for API
url = 'https://swapi.dev/api/people/?search='

# Set empty lists to hold character's height and mass
height = []
mass = []

# Loop through each character and make API request
for character in search_characters:

    # Create search query, make request and store in json
    query = url + character
    response = requests.get(query)
    response_json = response.json
    print(resonse_json)
    
    # Try to grab the height and mass of characters if they are available in the Star Wars API
    try:
        height.append(response_json['results'][0]['height'])
        mass.append(response_json['results'][0]['mass'])
        starwars_characters.append(character)
        print(f"{character} found! Appending stats")
        
    # Handle exceptions for a character that is not available in the Star Wars API
    except:
423/5:
import json
import requests
import pandas as pd
423/6:
# List of character
search_characters = ['R2-D2', 'Darth Vader', 'Godzilla', 'Luke Skywalker', 'Frodo', \
              'Boba Fett', 'Iron Man', 'Jon Snow', 'Han Solo']

# Set url for API
url = 'https://swapi.dev/api/people/?search='

# Set empty lists to hold character's height and mass
height = []
mass = []

# Loop through each character and make API request
for character in search_characters:

    # Create search query, make request and store in json
    query = url + character
    response = requests.get(query)
    response_json = response.json
    print(resonse_json)
    
    # Try to grab the height and mass of characters if they are available in the Star Wars API
    try:
        height.append(response_json['results'][0]['height'])
        mass.append(response_json['results'][0]['mass'])
        starwars_characters.append(character)
        print(f"{character} found! Appending stats")
        
    # Handle exceptions for a character that is not available in the Star Wars API
    except:
423/7:
# List of character
search_characters = ['R2-D2', 'Darth Vader', 'Godzilla', 'Luke Skywalker', 'Frodo', \
              'Boba Fett', 'Iron Man', 'Jon Snow', 'Han Solo']

# Set url for API
url = 'https://swapi.dev/api/people/?search='

# Set empty lists to hold character's height and mass
height = []
mass = []

# Loop through each character and make API request
for character in search_characters:

    # Create search query, make request and store in json
    query = url + character
    response = requests.get(query)
    response_json = response.json
    print(resonse_json)
    
    # Try to grab the height and mass of characters if they are available in the Star Wars API
    try:
        height.append(response_json['results'][0]['height'])
        mass.append(response_json['results'][0]['mass'])
        starwars_characters.append(character)
        print(f"{character} found! Appending stats")
        
    # Handle exceptions for a character that is not available in the Star Wars API
    except:
        SyntaxError
        print("Character not found")
423/8:
# List of character
search_characters = ['R2-D2', 'Darth Vader', 'Godzilla', 'Luke Skywalker', 'Frodo', \
              'Boba Fett', 'Iron Man', 'Jon Snow', 'Han Solo']

# Set url for API
url = 'https://swapi.dev/api/people/?search='

# Set empty lists to hold character's height and mass
height = []
mass = []

# Loop through each character and make API request
for character in search_characters:

    # Create search query, make request and store in json
    query = url + character
    response = requests.get(query)
    response_json = response.json
    print(response_json)
    
    # Try to grab the height and mass of characters if they are available in the Star Wars API
    try:
        height.append(response_json['results'][0]['height'])
        mass.append(response_json['results'][0]['mass'])
        starwars_characters.append(character)
        print(f"{character} found! Appending stats")
        
    # Handle exceptions for a character that is not available in the Star Wars API
    except:
        SyntaxError
        print("Character not found")
423/9:
# List of character
search_characters = ['R2-D2', 'Darth Vader', 'Godzilla', 'Luke Skywalker', 'Frodo', \
              'Boba Fett', 'Iron Man', 'Jon Snow', 'Han Solo']

# Set url for API
url = 'https://swapi.dev/api/people/?search='

# Set empty lists to hold character's height and mass
height = []
mass = []

# Loop through each character and make API request
for character in search_characters:

    # Create search query, make request and store in json
    query = url + character
    response = requests.get(query)
    response_json = response.json
    print(response_json)
    
    # Try to grab the height and mass of characters if they are available in the Star Wars API
    try:
        height.append(response_json['results'][0]['height'])
        mass.append(response_json['results'][0]['mass'])
        print(f"{character} found! Appending stats")
        
    # Handle exceptions for a character that is not available in the Star Wars API
    except:
        SyntaxError
        print("Character not found")
423/10:
# Create DataFrame
df = pd.Dataframe({
    'name': search_characters,
    'height': height,
    'mass': mass
})
df.head()
423/11:
# Create DataFrame
df = pd.DataFrame({
    'name': search_characters,
    'height': height,
    'mass': mass
})
df.head()
423/12:
# List of character
search_characters = ['R2-D2', 'Darth Vader', 'Godzilla', 'Luke Skywalker', 'Frodo', \
              'Boba Fett', 'Iron Man', 'Jon Snow', 'Han Solo']

# Set url for API
url = 'https://swapi.dev/api/people/?search='

# Set empty lists to hold character's height and mass
height = []
mass = []

# Loop through each character and make API request
for character in search_characters:

    # Create search query, make request and store in json
    query = url + character
    response = requests.get(query).json

    # Try to grab the height and mass of characters if they are available in the Star Wars API
    try:
        height.append(response_json['results'][0]['height'])
        mass.append(response_json['results'][0]['mass'])
        print(f"{character} found! Appending stats")
        
    # Handle exceptions for a character that is not available in the Star Wars API
    except:
        SyntaxError
        print("Character not found")
423/13:
# Create DataFrame
df = pd.DataFrame({
    'name': search_characters,
    'height': height,
    'mass': mass
})
df.head()
423/14:
# List of character
search_characters = ['R2-D2', 'Darth Vader', 'Godzilla', 'Luke Skywalker', 'Frodo', \
              'Boba Fett', 'Iron Man', 'Jon Snow', 'Han Solo']

# Set url for API
url = 'https://swapi.dev/api/people/?search='

# Set empty lists to hold character's height and mass
height = []
mass = []

# Loop through each character and make API request
for character in search_characters:

    # Create search query, make request and store in json
    query = url + character
    response = requests.get(query).json

    # Try to grab the height and mass of characters if they are available in the Star Wars API
    try:
        height.append(response_json['results'][0]['height'])
    except IndexError:
        height.append(NaN
        print(f'height not found for {character}')
     
    try:
        mass.append(response_json['results'][0]['mass'])
    except IndexError:
        mass.append(NaN)
        print(f"mass not found for {character}")
        
    # Handle exceptions for a character that is not available in the Star Wars API
423/15:
# List of character
search_characters = ['R2-D2', 'Darth Vader', 'Godzilla', 'Luke Skywalker', 'Frodo', \
              'Boba Fett', 'Iron Man', 'Jon Snow', 'Han Solo']

# Set url for API
url = 'https://swapi.dev/api/people/?search='

# Set empty lists to hold character's height and mass
height = []
mass = []

# Loop through each character and make API request
for character in search_characters:

    # Create search query, make request and store in json
    query = f'{url}{character}'
    response = requests.get(query).json

    # Try to grab the height and mass of characters if they are available in the Star Wars API
    try:
        height.append(response_json['results'][0]['height'])
    except IndexError:
        height.append(NaN
        print(f'height not found for {character}')
     
    try:
        mass.append(response_json['results'][0]['mass'])
    except IndexError:
        mass.append(NaN)
        print(f"mass not found for {character}")
        
    # Handle exceptions for a character that is not available in the Star Wars API
423/16:
# Create DataFrame
df = pd.DataFrame({
    'name': search_characters,
    'height': height,
    'mass': mass
})
df.head()
423/17:
import json
import requests
import pandas as pd
423/18:
# List of character
search_characters = ['R2-D2', 'Darth Vader', 'Godzilla', 'Luke Skywalker', 'Frodo', \
              'Boba Fett', 'Iron Man', 'Jon Snow', 'Han Solo']

# Set url for API
url = 'https://swapi.dev/api/people/?search='

# Set empty lists to hold character's height and mass
height = []
mass = []

# Loop through each character and make API request
for character in search_characters:

    # Create search query, make request and store in json
    query = f'{url}{character}'
    response = requests.get(query).json

    # Try to grab the height and mass of characters if they are available in the Star Wars API
    try:
        height.append(response_json['results'][0]['height'])
    except IndexError:
        height.append(NaN
        print(f'height not found for {character}')
     
    try:
        mass.append(response_json['results'][0]['mass'])
    except IndexError:
        mass.append(NaN)
        print(f"mass not found for {character}")
        
    # Handle exceptions for a character that is not available in the Star Wars API
423/19:
# List of character
search_characters = ['R2-D2', 'Darth Vader', 'Godzilla', 'Luke Skywalker', 'Frodo', \
              'Boba Fett', 'Iron Man', 'Jon Snow', 'Han Solo']

# Set url for API
url = 'https://swapi.dev/api/people/?search='

# Set empty lists to hold character's height and mass
height = []
mass = []

# Loop through each character and make API request
for character in search_characters:

    # Create search query, make request and store in json
    query = f'{url}{character}'
    response = requests.get(query).json

    # Try to grab the height and mass of characters if they are available in the Star Wars API
    try:
        height.append(response_json['results'][0]['height'])
    except IndexError:
        height.append(NaN
        print(f"height not found for {character}")
     
    try:
        mass.append(response_json['results'][0]['mass'])
    except IndexError:
        mass.append(NaN)
        print(f"mass not found for {character}")
        
    # Handle exceptions for a character that is not available in the Star Wars API
423/20:
# List of character
search_characters = ['R2-D2', 'Darth Vader', 'Godzilla', 'Luke Skywalker', 'Frodo', \
              'Boba Fett', 'Iron Man', 'Jon Snow', 'Han Solo']

# Set url for API
url = 'https://swapi.dev/api/people/?search='

# Set empty lists to hold character's height and mass
height = []
mass = []

# Loop through each character and make API request
for character in search_characters:

    # Create search query, make request and store in json
    query = f'{url}{character}'
    response = requests.get(query).json

    # Try to grab the height and mass of characters if they are available in the Star Wars API
    try:
        height.append(response['results'][0]['height'])
    except IndexError:
        height.append(NaN
        print(f"height not found for {character}")
     
    try:
        mass.append(response['results'][0]['mass'])
    except IndexError:
        mass.append(NaN)
        print(f"mass not found for {character}")
        
    # Handle exceptions for a character that is not available in the Star Wars API
423/21:
# List of character
search_characters = ['R2-D2', 'Darth Vader', 'Godzilla', 'Luke Skywalker', 'Frodo', \
              'Boba Fett', 'Iron Man', 'Jon Snow', 'Han Solo']

# Set url for API
url = 'https://swapi.dev/api/people/?search='

# Set empty lists to hold character's height and mass
height = []
mass = []

# Loop through each character and make API request
for character in search_characters:

    # Create search query, make request and store in json
    query = f'{url}{character}'
    response = requests.get(query).json

    # Try to grab the height and mass of characters if they are available in the Star Wars API
    try:
        height.append(response['results'][0]['height'])
    except IndexError:
        height.append(NaN)
        print(f"height not found for {character}")
     
    try:
        mass.append(response['results'][0]['mass'])
    except IndexError:
        mass.append(NaN)
        print(f"mass not found for {character}")
        
    # Handle exceptions for a character that is not available in the Star Wars API
424/1:
import json
import requests
import pandas as pd
424/2:
# List of character
search_characters = ['R2-D2', 'Darth Vader', 'Godzilla', 'Luke Skywalker', 'Frodo', \
              'Boba Fett', 'Iron Man', 'Jon Snow', 'Han Solo']

# Set url for API
url = 'https://swapi.dev/api/people/?search='

# Set empty lists to hold characters height and mass
height = []
mass = []
starwars_characters = []

# Loop through each character
for character in search_characters:
    
    # Create search query, make request and store in json
    query = url + character
    response = requests.get(query)
    response_json = response.json()
    
    # Try to grab the height and mass of characters if they are available in the Star Wars API
    try:
        height.append(response_json['results'][0]['height'])
        mass.append(response_json['results'][0]['mass'])
        starwars_characters.append(character)
        print(f"{character} found! Appending stats")
        
    # Handle exceptions for a character that is not available in the Star Wars API
    except:
        # Append null values
        print("Character not found")
        pass
424/3:
# Create DataFrame
character_height = pd.DataFrame({
    'character': starwars_characters,
    'height': height,
    'mass': mass
})
character_height
425/1:
# Get the list of lending types the world bank has
lending_response = requests.get(f"{url}lendingTypes?format=json").json()
lending_response
425/2:
# Dependencies
import requests

url = "http://api.worldbank.org/v2/"
425/3:
# Get the list of lending types the world bank has
lending_response = requests.get(f"{url}lendingTypes?format=json").json()
lending_response
425/4:
# Get the list of lending types the world bank has
lending_response = requests.get(f"{url}lendingTypes?format=json").json()
lending_types = [lending_type["id"] for lending_type in lending_response[1]]
lending_types
425/5:
# Dependencies
import requests

url = "http://api.worldbank.org/v2/"
425/6:
# Get the list of lending types the world bank has
lending_response = requests.get(f"{url}lendingTypes?format=json").json()
lending_types = [lending_type["id"] for lending_type in lending_response[1]]
lending_types
425/7:
# Next, determine how many countries fall into each lending type.
# Hint: Look at the first element of the response array.
country_count_by_type = {}
for lending_type in lending_types:
    query = f"{url}countries?lendingType={lending_type}&format=json"
    response = requests.get(query).json()
    country_count_by_type[lending_type] = response[0]["total"]
425/8:
# Print the number of countries of each lending type
for key, value in country_count_by_items():
    print(f"The number of countries with lending type {key} is {value}.")
425/9:
# Print the number of countries of each lending type
for key, value in country_count_by_type():
    print(f"The number of countries with lending type {key} is {value}.")
425/10:
# Print the number of countries of each lending type
for key, value in country_count_by_type.items():
    print(f"The number of countries with lending type {key} is {value}.")
420/22:
# Put treatments into a list for for loop (and later for plot labels)
treatments = ['Capomulin', 'Ramicane', 'Infubinol', 'Ceftamin']

# Create empty list to fill with tumor vol data (for plotting)
tumor_vol = []

# Calculate the IQR and quantitatively determine if there are any potential outliers. 

quartiles = [tumor_df("Tumor Volume(mm3)").quantile([0.25,0.5,0.75])]
lowerq = quartiles[0.25]
upperq = quartiles[0.75]
iqr = upperq-lowerq

print(f"The lower quartile is: {lowerq}")
print(f"The upper quartile is: {upperq}")
print(f"The interquartile range is: {iqr}")
print(f"The the median is: {quartiles[0.5]}")

    # Locate the rows which contain mice on each drug and get the tumor volumes
    
    
    # add subset 
    
    
    # Determine outliers using upper and lower bounds
420/23:
# Put treatments into a list for for loop (and later for plot labels)
treatments = ['Capomulin', 'Ramicane', 'Infubinol', 'Ceftamin']

# Create empty list to fill with tumor vol data (for plotting)
tumor_vol = []

# Calculate the IQR and quantitatively determine if there are any potential outliers. 

quartiles = [tumor_df("Tumor Volume (mm3)").quantile([0.25,0.5,0.75])]
lowerq = quartiles[0.25]
upperq = quartiles[0.75]
iqr = upperq-lowerq

print(f"The lower quartile is: {lowerq}")
print(f"The upper quartile is: {upperq}")
print(f"The interquartile range is: {iqr}")
print(f"The the median is: {quartiles[0.5]}")

    # Locate the rows which contain mice on each drug and get the tumor volumes
    
    
    # add subset 
    
    
    # Determine outliers using upper and lower bounds
420/24:
# Put treatments into a list for for loop (and later for plot labels)
treatments = ['Capomulin', 'Ramicane', 'Infubinol', 'Ceftamin']

# Create empty list to fill with tumor vol data (for plotting)
tumor_vol = []

# Calculate the IQR and quantitatively determine if there are any potential outliers. 

quartiles = tumor_df.groupby("Tumor Volume (mm3)").quantile([0.25,0.5,0.75])
lowerq = quartiles[0.25]
upperq = quartiles[0.75]
iqr = upperq-lowerq

print(f"The lower quartile is: {lowerq}")
print(f"The upper quartile is: {upperq}")
print(f"The interquartile range is: {iqr}")
print(f"The the median is: {quartiles[0.5]}")

    # Locate the rows which contain mice on each drug and get the tumor volumes
    
    
    # add subset 
    
    
    # Determine outliers using upper and lower bounds
420/25:
# Put treatments into a list for for loop (and later for plot labels)
treatments = ['Capomulin', 'Ramicane', 'Infubinol', 'Ceftamin']

# Create empty list to fill with tumor vol data (for plotting)
tumor_vol = []

# Calculate the IQR and quantitatively determine if there are any potential outliers. 

quartiles = tumor_df.groupby("Tumor Volume (mm3)").quantile([.25,.5,.75])
lowerq = quartiles[.25]
upperq = quartiles[.75]
iqr = upperq-lowerq

print(f"The lower quartile is: {lowerq}")
print(f"The upper quartile is: {upperq}")
print(f"The interquartile range is: {iqr}")
print(f"The the median is: {quartiles[0.5]}")

    # Locate the rows which contain mice on each drug and get the tumor volumes
    
    
    # add subset 
    
    
    # Determine outliers using upper and lower bounds
427/1:
# Put treatments into a list for for loop (and later for plot labels)
treatments = ['Capomulin', 'Ramicane', 'Infubinol', 'Ceftamin']

# Create empty list to fill with tumor vol data (for plotting)
tumor_vol = []

# Calculate the IQR and quantitatively determine if there are any potential outliers. 
for treatments in tumor_vol:
    quartiles = tumor_df.groupby("Tumor Volume (mm3)").quantile([.25,.5,.75])
    lowerq = quartiles[.25]
    upperq = quartiles[.75]
    iqr = upperq-lowerq

print(f"The lower quartile is: {lowerq}")
print(f"The upper quartile is: {upperq}")
print(f"The interquartile range is: {iqr}")
print(f"The the median is: {quartiles[0.5]}")

    # Locate the rows which contain mice on each drug and get the tumor volumes
    
    
    # add subset 
    
    
    # Determine outliers using upper and lower bounds
427/2:
# Put treatments into a list for for loop (and later for plot labels)
treatments = ['Capomulin', 'Ramicane', 'Infubinol', 'Ceftamin']

# Create empty list to fill with tumor vol data (for plotting)
tumor_vol = []

# Calculate the IQR and quantitatively determine if there are any potential outliers. 
for treatments in tumor_vol:
    quartiles = tumor_df.groupby("Tumor Volume (mm3)").quantile([.25,.5,.75])
    lowerq = quartiles[.25]
    upperq = quartiles[.75]
    iqr = upperq-lowerq

print(f"The lower quartile is: {lowerq}")
print(f"The upper quartile is: {upperq}")
print(f"The interquartile range is: {iqr}")
print(f"The the median is: {quartiles[0.5]}")

    # Locate the rows which contain mice on each drug and get the tumor volumes
    
    
    # add subset 
    
    
    # Determine outliers using upper and lower bounds
427/3:
# Put treatments into a list for for loop (and later for plot labels)
treatments = ['Capomulin', 'Ramicane', 'Infubinol', 'Ceftamin']

# Create empty list to fill with tumor vol data (for plotting)
tumor_vol = []

# Calculate the IQR and quantitatively determine if there are any potential outliers. 
for treatment in treatments:
    quartiles = tumor_df.groupby("Tumor Volume (mm3)").quantile([.25,.5,.75])
    lowerq = quartiles[.25]
    upperq = quartiles[.75]
    iqr = upperq-lowerq

print(f"The lower quartile is: {lowerq}")
print(f"The upper quartile is: {upperq}")
print(f"The interquartile range is: {iqr}")
print(f"The the median is: {quartiles[0.5]}")

    # Locate the rows which contain mice on each drug and get the tumor volumes
    
    
    # add subset 
    
    
    # Determine outliers using upper and lower bounds
428/1: %matplotlib notebook
428/2:
# Dependencies
import matplotlib.pyplot as plt
import pandas as pd
import scipy.stats as st
import numpy as np
428/3:
# Files
mouse_metadata_path = "data/Mouse_metadata.csv"
study_results_path = "data/Study_results.csv"
428/4:
# Read the mouse data and the study results
mouse_metadata = pd.read_csv(mouse_metadata_path)
study_results = pd.read_csv(study_results_path)

mouse_metadata.head()
study_results.head()
428/5:
# Combine the data into a single dataset
pymaceuticals_data = pd.merge(mouse_metadata, study_results, on=["Mouse ID", "Mouse ID"])

# Display the data table for preview
pymaceuticals_data
428/6:
 # Checking the number of mice.
number_of_mice = len(pymaceuticals_data["Mouse ID"])
number_of_mice
428/7:
# Getting the duplicate mice by ID number that shows up for Mouse ID and Timepoint. 
duplicate_mice = pymaceuticals_data.duplicated(subset=['Mouse ID', 'Timepoint'])
duplicate_mice
428/8:
# Create a clean DataFrame by dropping the duplicate mouse by its ID.
pymaceuticals_clean = pymaceuticals_data.drop_duplicates(subset=['Mouse ID', 'Timepoint'],keep='first',inplace=False)
pymaceuticals_clean
428/9:
 # Checking the number of mice in the clean DataFrame.
total_mice = len(pymaceuticals_clean["Mouse ID"])
total_mice
428/10:
# Generate a summary statistics table of mean, median, variance, standard deviation, and SEM of the tumor volume for each regimen

# This method is the most straighforward, creating multiple series and putting them all together at the end.
grouped = pymaceuticals_clean.groupby('Drug Regimen')
summary_table = round(grouped.agg(['mean','median','var','std','sem'])["Tumor Volume (mm3)"], 2)
summary_table
428/11:
regimen = pymaceuticals_clean.groupby(['Drug Regimen']).count()['Mouse ID']
regimen
428/12: x_axis = np.arange(len(regimen))
428/13:
# Generate a bar plot showing the total number of mice for each treatment throughout the course of the study using pandas.
# tick locations horizontal?
plt.bar(x_axis, regimen, color='pink', alpha=1, align='center')

tick_locations = [value for value in x_axis]
plt.xticks(tick_locations, ['Capomulin', 'Ceftamin', 'Infubinol', 'Ketapril', 'Naftisol', 'Placebo', 'Propriva', 'Ramicane', 'Stelasyn', 'Zoniferol'], rotation='vertical')


plt.title("Total Mice Treated")
plt.xlabel("Drug Regimen")
plt.ylabel("Number of Mice")

plt.tight_layout()
plt.savefig("Total Mice Bar Plot.png")
plt.show()
428/14:
# Generate a pie plot showing the distribution of female versus male mice using pandas
gender = pymaceuticals_clean.groupby(["Mouse ID","Sex"])
gender
428/15:
gender_df = pd.DataFrame(gender.size())
gender_df
428/16:
gender_data = pd.DataFrame(gender_df.groupby(["Sex"]).count())
gender_data.columns = ["Total Count"]
gender_data
428/17:
gender_data["Gender by Percentage"] = round(100*(gender_data["Total Count"]/gender_data["Total Count"].sum()), 2)
gender_data
428/18:
labels = ["Female", "Male"]
colors = ["pink", "lightblue"]
explode = (0.1, 0)
428/19:
plot = gender_data.plot.pie(y='Total Count', explode=explode, labels=labels, colors=colors, autopct="%1.1f%%", shadow=True, startangle=140)
plt.show()
plt.savefig("Mice Male vs. Female Piechart.png")
428/20:
# Calculate the final tumor volume of each mouse across four of the treatment regimens:  
# Capomulin, Ramicane, Infubinol, and Ceftamin
regimens_df = pymaceuticals_clean.loc[(pymaceuticals_clean["Drug Regimen"] == "Capomulin") | (pymaceuticals_clean["Drug Regimen"] == "Ramicane") | 
                                      (pymaceuticals_clean["Drug Regimen"] == "Infubinol") | (pymaceuticals_clean["Drug Regimen"] == "Ceftamin")]
regimens_df.head()
# Start by getting the last (greatest) timepoint for each mouse
regimens_df = regimens_df.groupby('Mouse ID').max()['Timepoint']
regimens_df.head()

# Merge this group df with the original dataframe to get the tumor volume at the last timepoint
tumor_df = pd.merge(pymaceuticals_data, regimens_df, on=("Mouse ID","Timepoint"),how="right")
tumor_df
428/21:
# Put treatments into a list for for loop (and later for plot labels)
treatments = ['Capomulin', 'Ramicane', 'Infubinol', 'Ceftamin']

# Create empty list to fill with tumor vol data (for plotting)
tumor_vol = []

# Calculate the IQR and quantitatively determine if there are any potential outliers. 
for treatment in treatments:
    quartiles = tumor_df.groupby("Tumor Volume (mm3)").quantile([.25,.5,.75])
    lowerq = quartiles[.25]
    upperq = quartiles[.75]
    iqr = upperq-lowerq

print(f"The lower quartile is: {lowerq}")
print(f"The upper quartile is: {upperq}")
print(f"The interquartile range is: {iqr}")
print(f"The the median is: {quartiles[0.5]}")

    # Locate the rows which contain mice on each drug and get the tumor volumes
    
    
    # add subset 
    
    
    # Determine outliers using upper and lower bounds
428/22: treatments
428/23:
quartiles = tumor_df.groupby("Tumor Volume (mm3)").quantile([.25,.5,.75])
quartiles
428/24:
lowerq = quartiles[0.25]
lowerq
428/25:
lowerq = quartiles[0.5]
lowerq
428/26:
lowerq = quartiles.groupby("Tumor Volume(mm3)"),[0.25]
lowerq
428/27:
lowerq = quartiles[0.25]
lowerq
428/28:
lowerq = quartiles([0.25])
lowerq
428/29:
lowerq = quartiles.tumor_df([0.25])
lowerq
428/30:
lowerq = quartiles([0.25])
lowerq
428/31:
lowerq = quartiles[0.25]
lowerq
428/32:
# Put treatments into a list for for loop (and later for plot labels)
treatments = ['Capomulin', 'Ramicane', 'Infubinol', 'Ceftamin']

# Create empty list to fill with tumor vol data (for plotting)
tumor_vol = []

# Calculate the IQR and quantitatively determine if there are any potential outliers. 
quartiles = regimen_df.groupby("Tumor Volume (mm3)").quantile([.25,.5,.75])
lowerq = quartiles.groupby("Tumor Volume(mm3)"),[0.25]
upperq = quartiles[0.75]
iqr = upperq-lowerq

#print(f"The lower quartile is: {lowerq}")
#print(f"The upper quartile is: {upperq}")
#print(f"The interquartile range is: {iqr}")
#print(f"The the median is: {quartiles[0.5]}")

    # Locate the rows which contain mice on each drug and get the tumor volumes
    
    
    # add subset 
    
    
    # Determine outliers using upper and lower bounds
429/1: %matplotlib notebook
429/2:
# Dependencies
import matplotlib.pyplot as plt
import pandas as pd
import scipy.stats as st
import numpy as np
429/3:
# Files
mouse_metadata_path = "data/Mouse_metadata.csv"
study_results_path = "data/Study_results.csv"
429/4:
# Read the mouse data and the study results
mouse_metadata = pd.read_csv(mouse_metadata_path)
study_results = pd.read_csv(study_results_path)

mouse_metadata.head()
study_results.head()
429/5:
# Combine the data into a single dataset
pymaceuticals_data = pd.merge(mouse_metadata, study_results, on=["Mouse ID", "Mouse ID"])

# Display the data table for preview
pymaceuticals_data
429/6:
 # Checking the number of mice.
number_of_mice = len(pymaceuticals_data["Mouse ID"])
number_of_mice
429/7:
# Getting the duplicate mice by ID number that shows up for Mouse ID and Timepoint. 
duplicate_mice = pymaceuticals_data.duplicated(subset=['Mouse ID', 'Timepoint'])
duplicate_mice
429/8:
# Create a clean DataFrame by dropping the duplicate mouse by its ID.
pymaceuticals_clean = pymaceuticals_data.drop_duplicates(subset=['Mouse ID', 'Timepoint'],keep='first',inplace=False)
pymaceuticals_clean
429/9:
 # Checking the number of mice in the clean DataFrame.
total_mice = len(pymaceuticals_clean["Mouse ID"])
total_mice
429/10:
# Generate a summary statistics table of mean, median, variance, standard deviation, and SEM of the tumor volume for each regimen

# This method is the most straighforward, creating multiple series and putting them all together at the end.
grouped = pymaceuticals_clean.groupby('Drug Regimen')
summary_table = round(grouped.agg(['mean','median','var','std','sem'])["Tumor Volume (mm3)"], 2)
summary_table
429/11:
regimen = pymaceuticals_clean.groupby(['Drug Regimen']).count()['Mouse ID']
regimen
429/12: x_axis = np.arange(len(regimen))
429/13:
# Generate a bar plot showing the total number of mice for each treatment throughout the course of the study using pandas.
# tick locations horizontal?
plt.bar(x_axis, regimen, color='pink', alpha=1, align='center')

tick_locations = [value for value in x_axis]
plt.xticks(tick_locations, ['Capomulin', 'Ceftamin', 'Infubinol', 'Ketapril', 'Naftisol', 'Placebo', 'Propriva', 'Ramicane', 'Stelasyn', 'Zoniferol'], rotation='vertical')


plt.title("Total Mice Treated")
plt.xlabel("Drug Regimen")
plt.ylabel("Number of Mice")

plt.tight_layout()
plt.savefig("Total Mice Bar Plot.png")
plt.show()
429/14:
# Generate a pie plot showing the distribution of female versus male mice using pandas
gender = pymaceuticals_clean.groupby(["Mouse ID","Sex"])
gender
429/15:
gender_df = pd.DataFrame(gender.size())
gender_df
429/16:
gender_data = pd.DataFrame(gender_df.groupby(["Sex"]).count())
gender_data.columns = ["Total Count"]
gender_data
429/17:
gender_data["Gender by Percentage"] = round(100*(gender_data["Total Count"]/gender_data["Total Count"].sum()), 2)
gender_data
429/18:
labels = ["Female", "Male"]
colors = ["pink", "lightblue"]
explode = (0.1, 0)
429/19:
plot = gender_data.plot.pie(y='Total Count', explode=explode, labels=labels, colors=colors, autopct="%1.1f%%", shadow=True, startangle=140)
plt.show()
plt.savefig("Mice Male vs. Female Piechart.png")
429/20:
# Calculate the final tumor volume of each mouse across four of the treatment regimens:  
# Capomulin, Ramicane, Infubinol, and Ceftamin
regimens_df = pymaceuticals_clean.loc[(pymaceuticals_clean["Drug Regimen"] == "Capomulin") | (pymaceuticals_clean["Drug Regimen"] == "Ramicane") | 
                                      (pymaceuticals_clean["Drug Regimen"] == "Infubinol") | (pymaceuticals_clean["Drug Regimen"] == "Ceftamin")]
regimens_df.head()
# Start by getting the last (greatest) timepoint for each mouse
regimens_df = regimens_df.groupby('Mouse ID').max()['Timepoint']
regimens_df.head()

# Merge this group df with the original dataframe to get the tumor volume at the last timepoint
tumor_df = pd.merge(pymaceuticals_data, regimens_df, on=("Mouse ID","Timepoint"),how="right")
tumor_df
429/21:
# Put treatments into a list for for loop (and later for plot labels)
treatments = ['Capomulin', 'Ramicane', 'Infubinol', 'Ceftamin']

# Create empty list to fill with tumor vol data (for plotting)
tumor_vol = []

# Calculate the IQR and quantitatively determine if there are any potential outliers. 
quartiles = regimen_df.groupby("Tumor Volume (mm3)").quantile([.25,.5,.75])
lowerq = quartiles.groupby("Tumor Volume(mm3)"),[0.25]
upperq = quartiles[0.75]
iqr = upperq-lowerq

#print(f"The lower quartile is: {lowerq}")
#print(f"The upper quartile is: {upperq}")
#print(f"The interquartile range is: {iqr}")
#print(f"The the median is: {quartiles[0.5]}")

    # Locate the rows which contain mice on each drug and get the tumor volumes
    
    
    # add subset 
    
    
    # Determine outliers using upper and lower bounds
429/22:
# Put treatments into a list for for loop (and later for plot labels)
treatments = ['Capomulin', 'Ramicane', 'Infubinol', 'Ceftamin']

# Create empty list to fill with tumor vol data (for plotting)
tumor_vol = []

# Calculate the IQR and quantitatively determine if there are any potential outliers. 
quartiles = regimens_df.groupby("Tumor Volume (mm3)").quantile([.25,.5,.75])
lowerq = quartiles.groupby("Tumor Volume(mm3)"),[0.25]
upperq = quartiles[0.75]
iqr = upperq-lowerq

#print(f"The lower quartile is: {lowerq}")
#print(f"The upper quartile is: {upperq}")
#print(f"The interquartile range is: {iqr}")
#print(f"The the median is: {quartiles[0.5]}")

    # Locate the rows which contain mice on each drug and get the tumor volumes
    
    
    # add subset 
    
    
    # Determine outliers using upper and lower bounds
429/23:
# Put treatments into a list for for loop (and later for plot labels)
treatments = ['Capomulin', 'Ramicane', 'Infubinol', 'Ceftamin']

# Create empty list to fill with tumor vol data (for plotting)
tumor_vol = []

# Calculate the IQR and quantitatively determine if there are any potential outliers. 
quartiles = tumor_df.groupby("Tumor Volume (mm3)").quantile([.25,.5,.75])
lowerq = quartiles.groupby("Tumor Volume(mm3)"),[0.25]
upperq = quartiles[0.75]
iqr = upperq-lowerq

#print(f"The lower quartile is: {lowerq}")
#print(f"The upper quartile is: {upperq}")
#print(f"The interquartile range is: {iqr}")
#print(f"The the median is: {quartiles[0.5]}")

    # Locate the rows which contain mice on each drug and get the tumor volumes
    
    
    # add subset 
    
    
    # Determine outliers using upper and lower bounds
429/24: treatments
429/25:
quartiles = tumor_df.groupby("Tumor Volume (mm3)").quantile([.25,.5,.75])
quartiles
429/26:
lowerq = quartiles[0.25]
lowerq
429/27: %matplotlib notebook
429/28:
# Dependencies
import matplotlib.pyplot as plt
import pandas as pd
import scipy.stats as st
import numpy as np
429/29:
# Files
mouse_metadata_path = "data/Mouse_metadata.csv"
study_results_path = "data/Study_results.csv"
429/30:
# Read the mouse data and the study results
mouse_metadata = pd.read_csv(mouse_metadata_path)
study_results = pd.read_csv(study_results_path)

mouse_metadata.head()
study_results.head()
429/31:
# Combine the data into a single dataset
pymaceuticals_data = pd.merge(mouse_metadata, study_results, on=["Mouse ID", "Mouse ID"])

# Display the data table for preview
pymaceuticals_data
429/32:
 # Checking the number of mice.
number_of_mice = len(pymaceuticals_data["Mouse ID"])
number_of_mice
429/33:
# Getting the duplicate mice by ID number that shows up for Mouse ID and Timepoint. 
duplicate_mice = pymaceuticals_data.duplicated(subset=['Mouse ID', 'Timepoint'])
duplicate_mice
429/34:
# Create a clean DataFrame by dropping the duplicate mouse by its ID.
pymaceuticals_clean = pymaceuticals_data.drop_duplicates(subset=['Mouse ID', 'Timepoint'],keep='first',inplace=False)
pymaceuticals_clean
429/35:
 # Checking the number of mice in the clean DataFrame.
total_mice = len(pymaceuticals_clean["Mouse ID"])
total_mice
429/36:
# Generate a summary statistics table of mean, median, variance, standard deviation, and SEM of the tumor volume for each regimen

# This method is the most straighforward, creating multiple series and putting them all together at the end.
grouped = pymaceuticals_clean.groupby('Drug Regimen')
summary_table = round(grouped.agg(['mean','median','var','std','sem'])["Tumor Volume (mm3)"], 2)
summary_table
429/37:
regimen = pymaceuticals_clean.groupby(['Drug Regimen']).count()['Mouse ID']
regimen
429/38: x_axis = np.arange(len(regimen))
429/39:
# Generate a bar plot showing the total number of mice for each treatment throughout the course of the study using pandas.
# tick locations horizontal?
plt.bar(x_axis, regimen, color='pink', alpha=1, align='center')

tick_locations = [value for value in x_axis]
plt.xticks(tick_locations, ['Capomulin', 'Ceftamin', 'Infubinol', 'Ketapril', 'Naftisol', 'Placebo', 'Propriva', 'Ramicane', 'Stelasyn', 'Zoniferol'], rotation='vertical')


plt.title("Total Mice Treated")
plt.xlabel("Drug Regimen")
plt.ylabel("Number of Mice")

plt.tight_layout()
plt.savefig("Total Mice Bar Plot.png")
plt.show()
429/40:
# Generate a pie plot showing the distribution of female versus male mice using pandas
gender = pymaceuticals_clean.groupby(["Mouse ID","Sex"])
gender
429/41:
gender_df = pd.DataFrame(gender.size())
gender_df
429/42:
gender_data = pd.DataFrame(gender_df.groupby(["Sex"]).count())
gender_data.columns = ["Total Count"]
gender_data
429/43:
gender_data["Gender by Percentage"] = round(100*(gender_data["Total Count"]/gender_data["Total Count"].sum()), 2)
gender_data
429/44:
labels = ["Female", "Male"]
colors = ["pink", "lightblue"]
explode = (0.1, 0)
429/45:
plot = gender_data.plot.pie(y='Total Count', explode=explode, labels=labels, colors=colors, autopct="%1.1f%%", shadow=True, startangle=140)
plt.show()
plt.savefig("Mice Male vs. Female Piechart.png")
429/46:
# Calculate the final tumor volume of each mouse across four of the treatment regimens:  
# Capomulin, Ramicane, Infubinol, and Ceftamin
regimens_df = pymaceuticals_clean.loc[(pymaceuticals_clean["Drug Regimen"] == "Capomulin") | (pymaceuticals_clean["Drug Regimen"] == "Ramicane") | 
                                      (pymaceuticals_clean["Drug Regimen"] == "Infubinol") | (pymaceuticals_clean["Drug Regimen"] == "Ceftamin")]
regimens_df = regimens_df[["Mouse ID", "Drug Regimen", "Tumor Volume (mm3)"]]
regimens_df = regimens_df.groupby(["Mouse ID", "Drug Regimen"])
regimens_df = regimens_df["Tumor Volume (mm3)"].sum().to_frame()
regimenPlot = regimens_df["Tumor Volume (mm3)"]


# Merge this group df with the original dataframe to get the tumor volume at the last timepoint
tumor_df = pd.merge(pymaceuticals_data, regimens_df, on=("Mouse ID","Timepoint"),how="right")
tumor_df
429/47:
# Put treatments into a list for for loop (and later for plot labels)
treatments = ['Capomulin', 'Ramicane', 'Infubinol', 'Ceftamin']

# Create empty list to fill with tumor vol data (for plotting)
tumor_vol = []

# Calculate the IQR and quantitatively determine if there are any potential outliers. 
quartiles = tumor_df.groupby("Tumor Volume (mm3)").quantile([.25,.5,.75])
lowerq = quartiles[0.25]
upperq = quartiles[0.75]
iqr = upperq-lowerq

#print(f"The lower quartile is: {lowerq}")
#print(f"The upper quartile is: {upperq}")
#print(f"The interquartile range is: {iqr}")
#print(f"The the median is: {quartiles[0.5]}")

    # Locate the rows which contain mice on each drug and get the tumor volumes
    
    
    # add subset 
    
    
    # Determine outliers using upper and lower bounds
429/48: treatments
429/49:
quartiles = tumor_df.groupby("Tumor Volume (mm3)").quantile([.25,.5,.75])
quartiles
429/50:
lowerq = quartiles[0.25]
lowerq
429/51:
# Calculate the final tumor volume of each mouse across four of the treatment regimens:  
# Capomulin, Ramicane, Infubinol, and Ceftamin
regimens_df = pymaceuticals_clean.loc[(pymaceuticals_clean["Drug Regimen"] == "Capomulin") | (pymaceuticals_clean["Drug Regimen"] == "Ramicane") | 
                                      (pymaceuticals_clean["Drug Regimen"] == "Infubinol") | (pymaceuticals_clean["Drug Regimen"] == "Ceftamin")]
regimens_df = regimens_df[["Mouse ID", "Drug Regimen", "Tumor Volume (mm3)"]]
regimens_df = regimens_df.groupby(["Mouse ID", "Drug Regimen"])
regimens_df = regimens_df["Tumor Volume (mm3)"].sum().to_frame()
regimenPlot = regimens_df["Tumor Volume (mm3)"]


# Merge this group df with the original dataframe to get the tumor volume at the last timepoint
tumor_df = pd.merge(pymaceuticals_data, regimens_df, on=("Mouse ID","Timepoint"),how="right")
tumor_df
430/1: %matplotlib notebook
430/2:
# Dependencies
import matplotlib.pyplot as plt
import pandas as pd
import scipy.stats as st
import numpy as np
430/3:
# Files
mouse_metadata_path = "data/Mouse_metadata.csv"
study_results_path = "data/Study_results.csv"
430/4:
# Read the mouse data and the study results
mouse_metadata = pd.read_csv(mouse_metadata_path)
study_results = pd.read_csv(study_results_path)

mouse_metadata.head()
study_results.head()
430/5:
# Combine the data into a single dataset
pymaceuticals_data = pd.merge(mouse_metadata, study_results, on=["Mouse ID", "Mouse ID"])

# Display the data table for preview
pymaceuticals_data
430/6:
 # Checking the number of mice.
number_of_mice = len(pymaceuticals_data["Mouse ID"])
number_of_mice
430/7:
# Getting the duplicate mice by ID number that shows up for Mouse ID and Timepoint. 
duplicate_mice = pymaceuticals_data.duplicated(subset=['Mouse ID', 'Timepoint'])
duplicate_mice
430/8:
# Create a clean DataFrame by dropping the duplicate mouse by its ID.
pymaceuticals_clean = pymaceuticals_data.drop_duplicates(subset=['Mouse ID', 'Timepoint'],keep='first',inplace=False)
pymaceuticals_clean
430/9:
 # Checking the number of mice in the clean DataFrame.
total_mice = len(pymaceuticals_clean["Mouse ID"])
total_mice
430/10:
# Generate a summary statistics table of mean, median, variance, standard deviation, and SEM of the tumor volume for each regimen

# This method is the most straighforward, creating multiple series and putting them all together at the end.
grouped = pymaceuticals_clean.groupby('Drug Regimen')
summary_table = round(grouped.agg(['mean','median','var','std','sem'])["Tumor Volume (mm3)"], 2)
summary_table
430/11:
regimen = pymaceuticals_clean.groupby(['Drug Regimen']).count()['Mouse ID']
regimen
430/12: x_axis = np.arange(len(regimen))
430/13:
# Generate a bar plot showing the total number of mice for each treatment throughout the course of the study using pandas.
# tick locations horizontal?
plt.bar(x_axis, regimen, color='pink', alpha=1, align='center')

tick_locations = [value for value in x_axis]
plt.xticks(tick_locations, ['Capomulin', 'Ceftamin', 'Infubinol', 'Ketapril', 'Naftisol', 'Placebo', 'Propriva', 'Ramicane', 'Stelasyn', 'Zoniferol'], rotation='vertical')


plt.title("Total Mice Treated")
plt.xlabel("Drug Regimen")
plt.ylabel("Number of Mice")

plt.tight_layout()
plt.savefig("Total Mice Bar Plot.png")
plt.show()
430/14:
# Generate a pie plot showing the distribution of female versus male mice using pandas
gender = pymaceuticals_clean.groupby(["Mouse ID","Sex"])
gender
430/15:
gender_df = pd.DataFrame(gender.size())
gender_df
430/16:
gender_data = pd.DataFrame(gender_df.groupby(["Sex"]).count())
gender_data.columns = ["Total Count"]
gender_data
430/17:
gender_data["Gender by Percentage"] = round(100*(gender_data["Total Count"]/gender_data["Total Count"].sum()), 2)
gender_data
430/18:
labels = ["Female", "Male"]
colors = ["pink", "lightblue"]
explode = (0.1, 0)
430/19:
plot = gender_data.plot.pie(y='Total Count', explode=explode, labels=labels, colors=colors, autopct="%1.1f%%", shadow=True, startangle=140)
plt.show()
plt.savefig("Mice Male vs. Female Piechart.png")
430/20:
# Calculate the final tumor volume of each mouse across four of the treatment regimens:  
# Capomulin, Ramicane, Infubinol, and Ceftamin
regimens_df = pymaceuticals_clean.loc[(pymaceuticals_clean["Drug Regimen"] == "Capomulin") | (pymaceuticals_clean["Drug Regimen"] == "Ramicane") | 
                                      (pymaceuticals_clean["Drug Regimen"] == "Infubinol") | (pymaceuticals_clean["Drug Regimen"] == "Ceftamin")]
regimens_df = regimens_df[["Mouse ID", "Drug Regimen", "Tumor Volume (mm3)"]]
regimens_df = regimens_df.groupby(["Mouse ID", "Drug Regimen"])
regimens_df = regimens_df["Tumor Volume (mm3)"].sum().to_frame()
regimenPlot = regimens_df["Tumor Volume (mm3)"]


# Merge this group df with the original dataframe to get the tumor volume at the last timepoint
tumor_df = pd.merge(pymaceuticals_data, regimens_df, on=("Mouse ID","Timepoint"),how="right")
tumor_df
430/21:
# Calculate the final tumor volume of each mouse across four of the treatment regimens:  
# Capomulin, Ramicane, Infubinol, and Ceftamin
regimens_df = pymaceuticals_clean.loc[(pymaceuticals_clean["Drug Regimen"] == "Capomulin") | (pymaceuticals_clean["Drug Regimen"] == "Ramicane") | 
                                      (pymaceuticals_clean["Drug Regimen"] == "Infubinol") | (pymaceuticals_clean["Drug Regimen"] == "Ceftamin")]

# Start by getting the last (greatest) timepoint for each mouse
regimens_df = regimens_df.groupby('Mouse ID').max()['Timepoint']
regimens_df.head()

# Merge this group df with the original dataframe to get the tumor volume at the last timepoint
tumor_df = pd.merge(pymaceuticals_data, regimens_df, on=("Mouse ID","Timepoint"),how="right")
tumor_df
430/22:
# Put treatments into a list for for loop (and later for plot labels)
treatments = ['Capomulin', 'Ramicane', 'Infubinol', 'Ceftamin']

# Create empty list to fill with tumor vol data (for plotting)
tumor_vol = []

# Calculate the IQR and quantitatively determine if there are any potential outliers. 
quartiles = tumor_df.groupby("Tumor Volume (mm3)").quantile([.25,.5,.75])
lowerq = quartiles[0.25]
upperq = quartiles[0.75]
iqr = upperq-lowerq

#print(f"The lower quartile is: {lowerq}")
#print(f"The upper quartile is: {upperq}")
#print(f"The interquartile range is: {iqr}")
#print(f"The the median is: {quartiles[0.5]}")

    # Locate the rows which contain mice on each drug and get the tumor volumes
    
    
    # add subset 
    
    
    # Determine outliers using upper and lower bounds
430/23: treatments
430/24:
quartiles = tumor_df.groupby("Tumor Volume (mm3)").quantile([.25,.5,.75])
quartiles
430/25:
lowerq = quartiles[0.25]
lowerq
430/26:
lowerq = quartiles([0.25])
lowerq
430/27:
quartiles = tumor_df.groupby("Drug Regimen").quantile([.25,.5,.75])
quartiles
430/28:
# Put treatments into a list for for loop (and later for plot labels)
treatments = ['Capomulin', 'Ramicane', 'Infubinol', 'Ceftamin']

# Create empty list to fill with tumor vol data (for plotting)
tumor_vol = []

# Calculate the IQR and quantitatively determine if there are any potential outliers. 
quartiles = tumor_df.groupby("Drug Regimen").quantile([.25,.5,.75])
lowerq = quartiles[0.25]
upperq = quartiles[0.75]
iqr = upperq-lowerq

#print(f"The lower quartile is: {lowerq}")
#print(f"The upper quartile is: {upperq}")
#print(f"The interquartile range is: {iqr}")
#print(f"The the median is: {quartiles[0.5]}")

    # Locate the rows which contain mice on each drug and get the tumor volumes
    
    
    # add subset 
    
    
    # Determine outliers using upper and lower bounds
430/29:
quartiles = tumor_df.groupby("Drug Regimen").quantile([.25,.5,.75])
quartiles["Capomulin"]
430/30:
quartiles = tumor_df.groupby("Drug Regimen").quantile([.25,.5,.75])
quartiles = tumor_df.groupby["Capomulin"].quantile([.25])
quartiles
430/31:
quartiles = tumor_df.groupby("Drug Regimen").quantile([.25,.5,.75])
quartiles
430/32: quartiles.reset_index()
430/33: quartile.loc["Capomulin"]
430/34: quartiles.loc["Capomulin"]
430/35:
quartiles = tumor_df.groupby("Drug Regimen").quantile([.25,.5,.75]).reset_index()
quartiles
430/36:
cap_tumor = quartiles['Tumor Volume(mm3)'].values[0]
cap_tumor
430/37:
cap_tumor = quartiles['Tumor Volume (mm3)'].values[0]
cap_tumor
430/38:
cap_tumorq1 = quartiles['Tumor Volume (mm3)'].values[0]
cap_tumorq1
430/39:
cap_tumor_lowerq = quartiles['Tumor Volume (mm3)'].values[0]
cap_tumor_upperq = quartiles['Tumor Volume (mm3)'].values[2]
cap_iqr = cap_tumor_upperq - cap_tumor_lowerq

ceft_tumor_lowerq = quartiles['Tumor Volume (mm3)'].values[3]
ceft_tumor_upperq = quartiles['Tumor Volume (mm3)'].values[5]
ceft_iqr = ceft_tumor_upperq - ceft_tumor_lowerq

inf_tumor_lowerq = quartiles['Tumor Volume (mm3)'].values[6]
inf_tumor_upperq = quartiles['Tumor Volume (mm3)'].values[8]
inf_iqr = inf_tumor_upperq - inf_tumor_lowerq

rami_tumor_lowerq = quartiles['Tumor Volume (mm3)'].values[9]
rami_tumor_upperq = quartiles['Tumor Volume (mm3)'].values[11]
rami_iqr = rami_tumor_upperq - rami_tumor_lowerq
430/40:
# Calculate the IQR and quantitatively determine if there are any potential outliers. 
cap_tumor_lowerq = quartiles['Tumor Volume (mm3)'].values[0]
cap_tumor_lowerq = quartiles['Tumor Volume (mm3)'].values[1]
cap_tumor_upperq = quartiles['Tumor Volume (mm3)'].values[2]
cap_iqr = cap_tumor_upperq - cap_tumor_lowerq

ceft_tumor_lowerq = quartiles['Tumor Volume (mm3)'].values[3]
ceft_tumor_lowerq = quartiles['Tumor Volume (mm3)'].values[4]
ceft_tumor_upperq = quartiles['Tumor Volume (mm3)'].values[5]
ceft_iqr = ceft_tumor_upperq - ceft_tumor_lowerq

inf_tumor_lowerq = quartiles['Tumor Volume (mm3)'].values[6]
inf_tumor_lowerq = quartiles['Tumor Volume (mm3)'].values[7]
inf_tumor_upperq = quartiles['Tumor Volume (mm3)'].values[8]
inf_iqr = inf_tumor_upperq - inf_tumor_lowerq

rami_tumor_lowerq = quartiles['Tumor Volume (mm3)'].values[9]
rami_tumor_lowerq = quartiles['Tumor Volume (mm3)'].values[10]
rami_tumor_upperq = quartiles['Tumor Volume (mm3)'].values[11]
rami_iqr = rami_tumor_upperq - rami_tumor_lowerq

print(f"The lower quartile for Capomulin is: {cap_tumor_lowerq}")
print(f"The lower quartile for Capomulin is: {cap_tumor_lowerq}")
print(f"The lower quartile for Capomulin is: {cap_tumor_lowerq}")
#print(f"The upper quartile is: {upperq}")
#print(f"The interquartile range is: {iqr}")
#print(f"The the median is: {quartiles[0.5]}")
430/41:
# Put treatments into a list for for loop (and later for plot labels)
treatments = ['Capomulin', 'Ramicane', 'Infubinol', 'Ceftamin']

# Create empty list to fill with tumor vol data (for plotting)
tumor_vol = []

for drug in treatments:
    tumor_vol = tumor_df.loc[tumor_df['Drug Regimen']==drug, 'Tumor Volume (mm3)']

    # Locate the rows which contain mice on each drug and get the tumor volumes
    
    
    # add subset 
    
    
    # Determine outliers using upper and lower bounds
430/42:
# Put treatments into a list for for loop (and later for plot labels)
treatments = ['Capomulin', 'Ramicane', 'Infubinol', 'Ceftamin']

# Create empty list to fill with tumor vol data (for plotting)
tumor_vol = []

for drug in treatments:
    tumor_vol = tumor_df.loc[tumor_df['Drug Regimen']==drug, 'Tumor Volume (mm3)']
    print(drug)
    print(tumor_vol)

    # Locate the rows which contain mice on each drug and get the tumor volumes
    
    
    # add subset 
    
    
    # Determine outliers using upper and lower bounds
430/43:
# Put treatments into a list for for loop (and later for plot labels)
treatments = ['Capomulin', 'Ramicane', 'Infubinol', 'Ceftamin']

# Create empty list to fill with tumor vol data (for plotting)
tumor_vol = []

for drug in treatments:
    tumor_vol = tumor_df.loc[tumor_df['Drug Regimen']==drug, 'Tumor Volume (mm3)']
    print(drug)
    quantiles = tumor_vol.quartile([.25, .5, .75])
    quantiles

    # Locate the rows which contain mice on each drug and get the tumor volumes
    
    
    # add subset 
    
    
    # Determine outliers using upper and lower bounds
430/44:
# Put treatments into a list for for loop (and later for plot labels)
treatments = ['Capomulin', 'Ramicane', 'Infubinol', 'Ceftamin']

# Create empty list to fill with tumor vol data (for plotting)
tumor_vol = []

for drug in treatments:
    tumor_vol = tumor_df.loc[tumor_df['Drug Regimen']==drug, 'Tumor Volume (mm3)']
    print(drug)
    quantiles = tumor_vol.quantile([.25, .5, .75])
    quantiles

    # Locate the rows which contain mice on each drug and get the tumor volumes
    
    
    # add subset 
    
    
    # Determine outliers using upper and lower bounds
430/45:
# Put treatments into a list for for loop (and later for plot labels)
treatments = ['Capomulin', 'Ramicane', 'Infubinol', 'Ceftamin']

# Create empty list to fill with tumor vol data (for plotting)
tumor_vol = []

for drug in treatments:
    tumor_vol = tumor_df.loc[tumor_df['Drug Regimen']==drug, 'Tumor Volume (mm3)']
    print(drug)
    quantiles = tumor_vol.quantile([.25, .5, .75])
    quantiles[.25]

    # Locate the rows which contain mice on each drug and get the tumor volumes
    
    
    # add subset 
    
    
    # Determine outliers using upper and lower bounds
430/46:
# Put treatments into a list for for loop (and later for plot labels)
treatments = ['Capomulin', 'Ramicane', 'Infubinol', 'Ceftamin']

# Create empty list to fill with tumor vol data (for plotting)
tumor_vol = []

for drug in treatments:
    tumor_vol = tumor_df.loc[tumor_df['Drug Regimen']==drug, 'Tumor Volume (mm3)']
    print(drug)
    quantiles = tumor_vol.quantile([.25, .5, .75])
    print(quantiles[.25])

    # Locate the rows which contain mice on each drug and get the tumor volumes
    
    
    # add subset 
    
    
    # Determine outliers using upper and lower bounds
430/47:
# Put treatments into a list for for loop (and later for plot labels)
treatments = ['Capomulin', 'Ramicane', 'Infubinol', 'Ceftamin']

# Create empty list to fill with tumor vol data (for plotting)
tumor_vol = []

for drug in treatments:
    tumor_vol = tumor_df.loc[tumor_df['Drug Regimen']==drug, 'Tumor Volume (mm3)']
    print(drug)
    quantiles = tumor_vol.quantile([.25, .5, .75])
    lowerq = quantiles[0.25]
    upperq = quantiles[0.75]
    iqr = upperq - lowerq
    
    print(f"The lower quartile of is: {lowerq}")
    print(f"The upper quartile of is: {upperq}")
    print(f"The interquartile range is: {iqr}")
    print(f"The the median is: {quartiles[0.5]}")

    # Locate the rows which contain mice on each drug and get the tumor volumes
    
    
    # add subset 
    
    
    # Determine outliers using upper and lower bounds
430/48:
# Put treatments into a list for for loop (and later for plot labels)
treatments = ['Capomulin', 'Ramicane', 'Infubinol', 'Ceftamin']

# Create empty list to fill with tumor vol data (for plotting)
tumor_vol = []

for drug in treatments:
    tumor_vol = tumor_df.loc[tumor_df['Drug Regimen']==drug, 'Tumor Volume (mm3)']
    print(drug)
    quantiles = tumor_vol.quantile([.25, .5, .75])
    lowerq = quantiles[0.25]
    upperq = quantiles[0.75]
    iqr = upperq - lowerq
    
    lower_bound = lowerq - (1.5*iqr)
    upper_bound = upperq + (1.5*iqr)
    
    print(f"The lower quartile of is: {lowerq}")
    print(f"The upper quartile of is: {upperq}")
    print(f"The interquartile range is: {iqr}")
    print(f"The the median is: {quartiles[0.5]}")

    # Locate the rows which contain mice on each drug and get the tumor volumes
    
    
    # add subset 
    
    
    # Determine outliers using upper and lower bounds
431/1: %matplotlib notebook
431/2:
# Dependencies
import matplotlib.pyplot as plt
import pandas as pd
import scipy.stats as st
import numpy as np
431/3:
# Files
mouse_metadata_path = "data/Mouse_metadata.csv"
study_results_path = "data/Study_results.csv"
431/4:
# Read the mouse data and the study results
mouse_metadata = pd.read_csv(mouse_metadata_path)
study_results = pd.read_csv(study_results_path)

mouse_metadata.head()
study_results.head()
431/5:
# Combine the data into a single dataset
pymaceuticals_data = pd.merge(mouse_metadata, study_results, on=["Mouse ID", "Mouse ID"])

# Display the data table for preview
pymaceuticals_data
431/6:
 # Checking the number of mice.
number_of_mice = len(pymaceuticals_data["Mouse ID"])
number_of_mice
431/7:
# Getting the duplicate mice by ID number that shows up for Mouse ID and Timepoint. 
duplicate_mice = pymaceuticals_data.duplicated(subset=['Mouse ID', 'Timepoint'])
duplicate_mice
431/8:
# Create a clean DataFrame by dropping the duplicate mouse by its ID.
pymaceuticals_clean = pymaceuticals_data.drop_duplicates(subset=['Mouse ID', 'Timepoint'],keep='first',inplace=False)
pymaceuticals_clean
431/9:
 # Checking the number of mice in the clean DataFrame.
total_mice = len(pymaceuticals_clean["Mouse ID"])
total_mice
431/10:
# Generate a summary statistics table of mean, median, variance, standard deviation, and SEM of the tumor volume for each regimen

# This method is the most straighforward, creating multiple series and putting them all together at the end.
grouped = pymaceuticals_clean.groupby('Drug Regimen')
summary_table = round(grouped.agg(['mean','median','var','std','sem'])["Tumor Volume (mm3)"], 2)
summary_table
431/11:
regimen = pymaceuticals_clean.groupby(['Drug Regimen']).count()['Mouse ID']
regimen
431/12: x_axis = np.arange(len(regimen))
431/13:
# Generate a bar plot showing the total number of mice for each treatment throughout the course of the study using pandas.
# tick locations horizontal?
plt.bar(x_axis, regimen, color='pink', alpha=1, align='center')

tick_locations = [value for value in x_axis]
plt.xticks(tick_locations, ['Capomulin', 'Ceftamin', 'Infubinol', 'Ketapril', 'Naftisol', 'Placebo', 'Propriva', 'Ramicane', 'Stelasyn', 'Zoniferol'], rotation='vertical')


plt.title("Total Mice Treated")
plt.xlabel("Drug Regimen")
plt.ylabel("Number of Mice")

plt.tight_layout()
plt.savefig("Total Mice Bar Plot.png")
plt.show()
431/14:
# Generate a pie plot showing the distribution of female versus male mice using pandas
gender = pymaceuticals_clean.groupby(["Mouse ID","Sex"])
gender
431/15:
gender_df = pd.DataFrame(gender.size())
gender_df
431/16:
gender_data = pd.DataFrame(gender_df.groupby(["Sex"]).count())
gender_data.columns = ["Total Count"]
gender_data
431/17:
gender_data["Gender by Percentage"] = round(100*(gender_data["Total Count"]/gender_data["Total Count"].sum()), 2)
gender_data
431/18:
labels = ["Female", "Male"]
colors = ["pink", "lightblue"]
explode = (0.1, 0)
431/19:
plot = gender_data.plot.pie(y='Total Count', explode=explode, labels=labels, colors=colors, autopct="%1.1f%%", shadow=True, startangle=140)
plt.show()
plt.savefig("Mice Male vs. Female Piechart.png")
431/20:
# Calculate the final tumor volume of each mouse across four of the treatment regimens:  
# Capomulin, Ramicane, Infubinol, and Ceftamin
regimens_df = pymaceuticals_clean.loc[(pymaceuticals_clean["Drug Regimen"] == "Capomulin") | (pymaceuticals_clean["Drug Regimen"] == "Ramicane") | 
                                      (pymaceuticals_clean["Drug Regimen"] == "Infubinol") | (pymaceuticals_clean["Drug Regimen"] == "Ceftamin")]

# Start by getting the last (greatest) timepoint for each mouse
regimens_df = regimens_df.groupby('Mouse ID').max()['Timepoint']
regimens_df.head()

# Merge this group df with the original dataframe to get the tumor volume at the last timepoint
#Come back and round
tumor_df = pd.merge(pymaceuticals_data, regimens_df, on=("Mouse ID","Timepoint"),how="right")
tumor_df
431/21:
# Put treatments into a list for for loop (and later for plot labels)
treatments = ['Capomulin', 'Ramicane', 'Infubinol', 'Ceftamin']

# Create empty list to fill with tumor vol data (for plotting)
tumor_vol = []

# Calculate the IQR and quantitatively determine if there are any potential outliers. 
for drug in treatments:
    tumor_vol = tumor_df.loc[tumor_df['Drug Regimen']==drug, 'Tumor Volume (mm3)']
    print(drug)
    quantiles = tumor_vol.quantile([.25, .5, .75])
    lowerq = quantiles[0.25]
    upperq = quantiles[0.75]
    iqr = upperq - lowerq
    
    lower_bound = lowerq - (1.5*iqr)
    upper_bound = upperq + (1.5*iqr)
    
    print(f"The lower quartile of is: {lowerq}")
    print(f"The upper quartile of is: {upperq}")
    print(f"The interquartile range is: {iqr}")
    print(f"The the median is: {quartiles[0.5]}")
    
    lower_bound = lowerq - (1.5*iqr)
    upper_bound = upperq + (1.5*iqr)

    # Locate the rows which contain mice on each drug and get the tumor volumes
    
    
    # add subset 
    
    
    # Determine outliers using upper and lower bounds
431/22:
# Put treatments into a list for for loop (and later for plot labels)
treatments = ['Capomulin', 'Ramicane', 'Infubinol', 'Ceftamin']

# Create empty list to fill with tumor vol data (for plotting)
tumor_vol = []

# Calculate the IQR and quantitatively determine if there are any potential outliers. 
for drug in treatments:
    tumor_vol = tumor_df.loc[tumor_df['Drug Regimen']==drug, 'Tumor Volume (mm3)']
    print(drug)
    quantiles = tumor_vol.quantile([.25, .5, .75])
    lowerq = quantiles[0.25]
    upperq = quantiles[0.75]
    iqr = upperq - lowerq
    
    lower_bound = lowerq - (1.5*iqr)
    upper_bound = upperq + (1.5*iqr)
    
    print(f"The lower quartile of is: {lowerq}")
    print(f"The upper quartile of is: {upperq}")
    print(f"The interquartile range is: {iqr}")
    print(f"The the median is: {quantiles[0.5]}")
    
    lower_bound = lowerq - (1.5*iqr)
    upper_bound = upperq + (1.5*iqr)

    # Locate the rows which contain mice on each drug and get the tumor volumes
    
    
    # add subset 
    
    
    # Determine outliers using upper and lower bounds
431/23:
# Put treatments into a list for for loop (and later for plot labels)
treatments = ['Capomulin', 'Ramicane', 'Infubinol', 'Ceftamin']

# Create empty list to fill with tumor vol data (for plotting)
tumor_vol = []

# Calculate the IQR and quantitatively determine if there are any potential outliers. 
for drug in treatments:
    tumor_vol = tumor_df.loc[tumor_df['Drug Regimen']==drug, 'Tumor Volume (mm3)']
    print(drug)
    quantiles = tumor_vol.quantile([.25, .5, .75])
    lowerq = quantiles[0.25]
    upperq = quantiles[0.75]
    iqr = upperq - lowerq
    
    lower_bound = lowerq - (1.5*iqr)
    upper_bound = upperq + (1.5*iqr)
    
    print(f"The lower quartile of is: {lowerq}")
    print(f"The upper quartile of is: {upperq}")
    print(f"The interquartile range is: {iqr}")
    print(f"The the median is: {quantiles[0.5]}")
    
    print(f"Values below {lower_bound} could be outliers.")
    print(f"Values above {upper_bound} could be outliers.")

    # Locate the rows which contain mice on each drug and get the tumor volumes
    
    
    # add subset 
    
    
    # Determine outliers using upper and lower bounds
431/24: tumor_vol
431/25: tumor_vol.loc[tumor_vol<lower_bound]
431/26:
tumor_vol.loc[(tumor_vol<lower_bound) |
              (tumor_vol>upper_bound)
             ]
tumor_vol
431/27:
tumor_vol.loc[(tumor_vol<lower_bound) |
              (tumor_vol>upper_bound)
             ]
431/28:
print(tumor_vol.loc[(tumor_vol<lower_bound) |
              (tumor_vol>upper_bound)
             ])
431/29:
# Put treatments into a list for for loop (and later for plot labels)
treatments = ['Capomulin', 'Ramicane', 'Infubinol', 'Ceftamin']

# Create empty list to fill with tumor vol data (for plotting)
tumor_vol = []

# Calculate the IQR and quantitatively determine if there are any potential outliers. 
for drug in treatments:
    tumor_vol = tumor_df.loc[tumor_df['Drug Regimen']==drug, 'Tumor Volume (mm3)']
    print(drug)
    quantiles = tumor_vol.quantile([.25, .5, .75])
    lowerq = quantiles[0.25]
    upperq = quantiles[0.75]
    iqr = upperq - lowerq
    
    lower_bound = lowerq - (1.5*iqr)
    upper_bound = upperq + (1.5*iqr)
    
    print(f"The lower quartile of is: {lowerq}")
    print(f"The upper quartile of is: {upperq}")
    print(f"The interquartile range is: {iqr}")
    print(f"The the median is: {quantiles[0.5]}")
    
    print(f"Values below {lower_bound} could be outliers.")
    print(f"Values above {upper_bound} could be outliers.")
    
    print(tumor_vol.loc[(tumor_vol<lower_bound) |
              (tumor_vol>upper_bound)
             ])

    # Locate the rows which contain mice on each drug and get the tumor volumes
    
    
    # add subset 
    
    
    # Determine outliers using upper and lower bounds
431/30:
# Put treatments into a list for for loop (and later for plot labels)
treatments = ['Capomulin', 'Ramicane', 'Infubinol', 'Ceftamin']

# Create empty list to fill with tumor vol data (for plotting)
tumor_vol = []

# Calculate the IQR and quantitatively determine if there are any potential outliers. 
for drug in treatments:
    tumor_vol = tumor_df.loc[tumor_df['Drug Regimen']==drug, 'Tumor Volume (mm3)']
    print(drug)
    quantiles = tumor_vol.quantile([.25, .5, .75])
    lowerq = quantiles[0.25]
    upperq = quantiles[0.75]
    iqr = upperq - lowerq
    
    lower_bound = lowerq - (1.5*iqr)
    upper_bound = upperq + (1.5*iqr)
    
    print(f"The lower quartile of is: {lowerq}")
    print(f"The upper quartile of is: {upperq}")
    print(f"The interquartile range is: {iqr}")
    print(f"The the median is: {quantiles[0.5]}")
    
    print(f"Values below {lower_bound} could be outliers.")
    print(f"Values above {upper_bound} could be outliers.")
    
    print(tumor_vol.loc[(tumor_vol<lower_bound) |
              (tumor_vol>upper_bound)
             ])
    print()
    print()
    print()

    # Locate the rows which contain mice on each drug and get the tumor volumes
    
    
    # add subset 
    
    
    # Determine outliers using upper and lower bounds
431/31:
# Put treatments into a list for for loop (and later for plot labels)
treatments = ['Capomulin', 'Ramicane', 'Infubinol', 'Ceftamin']

# Create empty list to fill with tumor vol data (for plotting)
tumor_vol = []

# Calculate the IQR and quantitatively determine if there are any potential outliers. 
for drug in treatments:
    tumor_vol = tumor_df.loc[tumor_df['Drug Regimen']==drug, 'Tumor Volume (mm3)']
    print(drug)
    quantiles = tumor_vol.quantile([.25, .5, .75])
    lowerq = quantiles[0.25]
    upperq = quantiles[0.75]
    iqr = upperq - lowerq
    
    lower_bound = lowerq - (1.5*iqr)
    upper_bound = upperq + (1.5*iqr)
    
    print(f"The lower quartile of is: {lowerq}")
    print(f"The upper quartile of is: {upperq}")
    print(f"The interquartile range is: {iqr}")
    print(f"The the median is: {quantiles[0.5]}")
    
    print(f"Values below {lower_bound} could be outliers.")
    print(f"Values above {upper_bound} could be outliers.")
    
    print(f"Outliers are: {tumor_vol.loc[(tumor_vol<lower_bound) |
              (tumor_vol>upper_bound)
             ]})
    print()
    print()
    print()

    # Locate the rows which contain mice on each drug and get the tumor volumes
    
    
    # add subset 
    
    
    # Determine outliers using upper and lower bounds
431/32:
# Put treatments into a list for for loop (and later for plot labels)
treatments = ['Capomulin', 'Ramicane', 'Infubinol', 'Ceftamin']

# Create empty list to fill with tumor vol data (for plotting)
tumor_vol = []

# Calculate the IQR and quantitatively determine if there are any potential outliers. 
for drug in treatments:
    tumor_vol = tumor_df.loc[tumor_df['Drug Regimen']==drug, 'Tumor Volume (mm3)']
    print(drug)
    quantiles = tumor_vol.quantile([.25, .5, .75])
    lowerq = quantiles[0.25]
    upperq = quantiles[0.75]
    iqr = upperq - lowerq
    
    lower_bound = lowerq - (1.5*iqr)
    upper_bound = upperq + (1.5*iqr)
    
    print(f"The lower quartile of is: {lowerq}")
    print(f"The upper quartile of is: {upperq}")
    print(f"The interquartile range is: {iqr}")
    print(f"The the median is: {quantiles[0.5]}")
    
    print(f"Values below {lower_bound} could be outliers.")
    print(f"Values above {upper_bound} could be outliers.")
    
    print(f"Outliers are: {tumor_vol.loc[(tumor_vol<lower_bound) |
              (tumor_vol>upper_bound)
             ]}")
    print()
    print()
    print()

    # Locate the rows which contain mice on each drug and get the tumor volumes
    
    
    # add subset 
    
    
    # Determine outliers using upper and lower bounds
431/33:
# Put treatments into a list for for loop (and later for plot labels)
treatments = ['Capomulin', 'Ramicane', 'Infubinol', 'Ceftamin']

# Create empty list to fill with tumor vol data (for plotting)
tumor_vol = []

# Calculate the IQR and quantitatively determine if there are any potential outliers. 
for drug in treatments:
    tumor_vol = tumor_df.loc[tumor_df['Drug Regimen']==drug, 'Tumor Volume (mm3)']
    print(drug)
    quantiles = tumor_vol.quantile([.25, .5, .75])
    lowerq = quantiles[0.25]
    upperq = quantiles[0.75]
    iqr = upperq - lowerq
    
    lower_bound = lowerq - (1.5*iqr)
    upper_bound = upperq + (1.5*iqr)
    
    print(f"The lower quartile of is: {lowerq}")
    print(f"The upper quartile of is: {upperq}")
    print(f"The interquartile range is: {iqr}")
    print(f"The the median is: {quantiles[0.5]}")
    
    print(f"Values below {lower_bound} could be outliers.")
    print(f"Values above {upper_bound} could be outliers.")
    
    print(f"Outliers are: {tumor_vol.loc[(tumor_vol<lower_bound) | (tumor_vol>upper_bound)]}")
    print()
    print()
    print()

    # Locate the rows which contain mice on each drug and get the tumor volumes
    
    
    # add subset 
    
    
    # Determine outliers using upper and lower bounds
431/34:
# Put treatments into a list for for loop (and later for plot labels)
treatments = ['Capomulin', 'Ramicane', 'Infubinol', 'Ceftamin']

# Create empty list to fill with tumor vol data (for plotting)
tumor_vol_data = []

# Calculate the IQR and quantitatively determine if there are any potential outliers. 
# Locate the rows which contain mice on each drug and get the tumor volumes 
    # add subset 
    # Determine outliers using upper and lower bounds
    
    
for drug in treatments:
    tumor_vol = tumor_df.loc[tumor_df['Drug Regimen']==drug, 'Tumor Volume (mm3)']
    print(drug)
    quantiles = tumor_vol.quantile([.25, .5, .75])
    lowerq = quantiles[0.25]
    upperq = quantiles[0.75]
    iqr = upperq - lowerq
    
    lower_bound = lowerq - (1.5*iqr)
    upper_bound = upperq + (1.5*iqr)
    
    print(f"The lower quartile of is: {lowerq}")
    print(f"The upper quartile of is: {upperq}")
    print(f"The interquartile range is: {iqr}")
    print(f"The the median is: {quantiles[0.5]}")
    
    print(f"Values below {lower_bound} could be outliers.")
    print(f"Values above {upper_bound} could be outliers.")
    
    print(f"Outliers are: {tumor_vol.loc[(tumor_vol<lower_bound) | (tumor_vol>upper_bound)]}")
    print()
    print()
    print()
431/35:
# Put treatments into a list for for loop (and later for plot labels)
treatments = ['Capomulin', 'Ramicane', 'Infubinol', 'Ceftamin']

# Create empty list to fill with tumor vol data (for plotting)
tumor_vol_data = []

# Calculate the IQR and quantitatively determine if there are any potential outliers. 
# Locate the rows which contain mice on each drug and get the tumor volumes 
    # add subset 
    # Determine outliers using upper and lower bounds
    
    
for drug in treatments:
    tumor_vol = tumor_df.loc[tumor_df['Drug Regimen']==drug, 'Tumor Volume (mm3)']
    tumor_vol_data.append(tumor_vol)
    print(drug)
    quantiles = tumor_vol.quantile([.25, .5, .75])
    lowerq = quantiles[0.25]
    upperq = quantiles[0.75]
    iqr = upperq - lowerq
    
    lower_bound = lowerq - (1.5*iqr)
    upper_bound = upperq + (1.5*iqr)
    
    print(f"The lower quartile of is: {lowerq}")
    print(f"The upper quartile of is: {upperq}")
    print(f"The interquartile range is: {iqr}")
    print(f"The the median is: {quantiles[0.5]}")
    
    print(f"Values below {lower_bound} could be outliers.")
    print(f"Values above {upper_bound} could be outliers.")
    
    print(f"Outliers are: {tumor_vol.loc[(tumor_vol<lower_bound) | (tumor_vol>upper_bound)]}")
    print()
    print()
    print()
431/36:
# Generate a box plot of the final tumor volume of each mouse across four regimens of interest
tumor_vol_data
431/37:
# Generate a box plot of the final tumor volume of each mouse across four regimens of interest
plt.boxplot(tumor_vol_data)
431/38:
# Generate a box plot of the final tumor volume of each mouse across four regimens of interest
plt.boxplot(tumor_vol_data)
plt.show()
431/39:
# Generate a box plot of the final tumor volume of each mouse across four regimens of interest
plt.boxplot(tumor_vol_data)
plt.show()
432/1: %matplotlib notebook
432/2:
# Dependencies
import matplotlib.pyplot as plt
import pandas as pd
import scipy.stats as st
import numpy as np
432/3:
# Files
mouse_metadata_path = "data/Mouse_metadata.csv"
study_results_path = "data/Study_results.csv"
432/4:
# Read the mouse data and the study results
mouse_metadata = pd.read_csv(mouse_metadata_path)
study_results = pd.read_csv(study_results_path)

mouse_metadata.head()
study_results.head()
432/5:
# Combine the data into a single dataset
pymaceuticals_data = pd.merge(mouse_metadata, study_results, on=["Mouse ID", "Mouse ID"])

# Display the data table for preview
pymaceuticals_data
432/6:
 # Checking the number of mice.
number_of_mice = len(pymaceuticals_data["Mouse ID"])
number_of_mice
432/7:
# Getting the duplicate mice by ID number that shows up for Mouse ID and Timepoint. 
duplicate_mice = pymaceuticals_data.duplicated(subset=['Mouse ID', 'Timepoint'])
duplicate_mice
432/8:
# Create a clean DataFrame by dropping the duplicate mouse by its ID.
pymaceuticals_clean = pymaceuticals_data.drop_duplicates(subset=['Mouse ID', 'Timepoint'],keep='first',inplace=False)
pymaceuticals_clean
432/9:
 # Checking the number of mice in the clean DataFrame.
total_mice = len(pymaceuticals_clean["Mouse ID"])
total_mice
432/10:
# Generate a summary statistics table of mean, median, variance, standard deviation, and SEM of the tumor volume for each regimen

# This method is the most straighforward, creating multiple series and putting them all together at the end.
grouped = pymaceuticals_clean.groupby('Drug Regimen')
summary_table = round(grouped.agg(['mean','median','var','std','sem'])["Tumor Volume (mm3)"], 2)
summary_table
432/11:
regimen = pymaceuticals_clean.groupby(['Drug Regimen']).count()['Mouse ID']
regimen
432/12: x_axis = np.arange(len(regimen))
432/13:
# Generate a bar plot showing the total number of mice for each treatment throughout the course of the study using pandas.
# tick locations horizontal?
plt.bar(x_axis, regimen, color='pink', alpha=1, align='center')

tick_locations = [value for value in x_axis]
plt.xticks(tick_locations, ['Capomulin', 'Ceftamin', 'Infubinol', 'Ketapril', 'Naftisol', 'Placebo', 'Propriva', 'Ramicane', 'Stelasyn', 'Zoniferol'], rotation='vertical')


plt.title("Total Mice Treated")
plt.xlabel("Drug Regimen")
plt.ylabel("Number of Mice")

plt.tight_layout()
plt.savefig("Total Mice Bar Plot.png")
plt.show()
432/14:
# Generate a pie plot showing the distribution of female versus male mice using pandas
gender = pymaceuticals_clean.groupby(["Mouse ID","Sex"])
gender
432/15:
gender_df = pd.DataFrame(gender.size())
gender_df
432/16:
gender_data = pd.DataFrame(gender_df.groupby(["Sex"]).count())
gender_data.columns = ["Total Count"]
gender_data
432/17:
gender_data["Gender by Percentage"] = round(100*(gender_data["Total Count"]/gender_data["Total Count"].sum()), 2)
gender_data
432/18:
labels = ["Female", "Male"]
colors = ["pink", "lightblue"]
explode = (0.1, 0)
432/19:
plot = gender_data.plot.pie(y='Total Count', explode=explode, labels=labels, colors=colors, autopct="%1.1f%%", shadow=True, startangle=140)
plt.show()
plt.savefig("Mice Male vs. Female Piechart.png")
432/20:
# Calculate the final tumor volume of each mouse across four of the treatment regimens:  
# Capomulin, Ramicane, Infubinol, and Ceftamin
regimens_df = pymaceuticals_clean.loc[(pymaceuticals_clean["Drug Regimen"] == "Capomulin") | (pymaceuticals_clean["Drug Regimen"] == "Ramicane") | 
                                      (pymaceuticals_clean["Drug Regimen"] == "Infubinol") | (pymaceuticals_clean["Drug Regimen"] == "Ceftamin")]

# Start by getting the last (greatest) timepoint for each mouse
regimens_df = regimens_df.groupby('Mouse ID').max()['Timepoint']
regimens_df.head()

# Merge this group df with the original dataframe to get the tumor volume at the last timepoint
#Come back and round
tumor_df = pd.merge(pymaceuticals_data, regimens_df, on=("Mouse ID","Timepoint"),how="right")
tumor_df
432/21:
# Put treatments into a list for for loop (and later for plot labels)
treatments = ['Capomulin', 'Ramicane', 'Infubinol', 'Ceftamin']

# Create empty list to fill with tumor vol data (for plotting)
tumor_vol_data = []

# Calculate the IQR and quantitatively determine if there are any potential outliers. 
# Locate the rows which contain mice on each drug and get the tumor volumes 
    # add subset 
    # Determine outliers using upper and lower bounds
    
    
for drug in treatments:
    tumor_vol = tumor_df.loc[tumor_df['Drug Regimen']==drug, 'Tumor Volume (mm3)']
    tumor_vol_data.append(tumor_vol)
    print(drug)
    quantiles = tumor_vol.quantile([.25, .5, .75])
    lowerq = quantiles[0.25]
    upperq = quantiles[0.75]
    iqr = upperq - lowerq
    
    lower_bound = lowerq - (1.5*iqr)
    upper_bound = upperq + (1.5*iqr)
    
    print(f"The lower quartile of is: {lowerq}")
    print(f"The upper quartile of is: {upperq}")
    print(f"The interquartile range is: {iqr}")
    print(f"The the median is: {quantiles[0.5]}")
    
    print(f"Values below {lower_bound} could be outliers.")
    print(f"Values above {upper_bound} could be outliers.")
    
    print(f"Outliers are: {tumor_vol.loc[(tumor_vol<lower_bound) | (tumor_vol>upper_bound)]}")
    print()
    print()
    print()
432/22:
# Generate a box plot of the final tumor volume of each mouse across four regimens of interest
plt.boxplot(tumor_vol_data)
plt.show()
432/23:  # Generate a line plot of time point versus tumor volume for a mouse treated with Capomulin
432/24: # Generate a scatter plot of mouse weight versus average tumor volume for the Capomulin regimen
432/25:
# Calculate the correlation coefficient and linear regression model 
# for mouse weight and average tumor volume for the Capomulin regimen
432/26:
# Generate a box plot of the final tumor volume of each mouse across four regimens of interest
plt.boxplot(tumor_vol_data)
plt.show()
432/27:
# Generate a box plot of the final tumor volume of each mouse across four regimens of interest
plot = plt.boxplot(tumor_vol_data)
plt.show()
432/28:
# Generate a box plot of the final tumor volume of each mouse across four regimens of interest
plot = plot.boxplot(tumor_vol_data)
plt.show()
432/29:
# Generate a box plot of the final tumor volume of each mouse across four regimens of interest
fig1, ax1 = plt.subplots()
ax1.set_title('Final Tumor Volume of Each Mice')
ax1.set_ylabel('Tumor Vol')
ax1.boxplot(tumor_vol_data)
plt.show()
432/30:
# Generate a box plot of the final tumor volume of each mouse across four regimens of interest
fig1, ax1 = plt.subplots()
ax1.set_title('Final Tumor Volume of Each Mice')
ax1.set_ylabel('Tumor Vol')
ax1.boxplot(tumor_vol_data, labels=treatments)
plt.show()
432/31:
# Generate a box plot of the final tumor volume of each mouse across four regimens of interest
fig1, ax1 = plt.subplots()
ax1.set_title('Final Tumor Volume of Each Mouse')
ax1.set_ylabel('Tumor Vol')
ax1.boxplot(tumor_vol_data, labels=treatments)
plt.show()
plt.savefig("Final Tumor Volume of Each Mouse.png")
432/32:
 # Generate a line plot of time point versus tumor volume for a mouse treated with Capomulin
mouse_data = tumor_df[(tumor_df['Drug Regimen'] == 'Capomulin') & 
                      (tumor_df['Mouse ID'] == 'y793')]
mouse_data = tumor_df.groupby(['Drug Regimen', 'Timepoint'])
mouse_data = tumor_df['Tumor Volume (mm3)'].mean()
mouse_data
432/33:
 # Generate a line plot of time point versus tumor volume for a mouse treated with Capomulin
mouse_data = tumor_df[(tumor_df['Drug Regimen'] == 'Capomulin') & 
                      (tumor_df['Mouse ID'] == 'y793')]
mouse_data = tumor_df.groupby(['Drug Regimen', 'Timepoint'])
mouse_data_df = tumor_df['Tumor Volume (mm3)'].mean().to_frame()
mouse_data_dfplot = mouse_data_df['Tumor Volume (mm3)']

timepoints = [0,5,10,15,20,25,30,35,40,45,50]
plt.show()
432/34:
 # Generate a line plot of time point versus tumor volume for a mouse treated with Capomulin
mouse_data = tumor_df[(tumor_df['Drug Regimen'] == 'Capomulin') & 
                      (tumor_df['Mouse ID'] == 'y793')]
mouse_data = tumor_df.groupby(['Drug Regimen', 'Timepoint'])
mouse_data_df = tumor_df['Tumor Volume (mm3)'].mean().to_frame()
mouse_data_dfplot = mouse_data_df['Tumor Volume (mm3)']

timepoints = [0,5,10,15,20,25,30,35,40,45,50]
plt.figure()
432/35:
 # Generate a line plot of time point versus tumor volume for a mouse treated with Capomulin
mouse_data = tumor_df[(tumor_df['Drug Regimen'] == 'Capomulin') & 
                      (tumor_df['Mouse ID'] == 'y793')]
mouse_data = tumor_df.groupby(['Drug Regimen', 'Timepoint'])
mouse_data_df = pd.DataFrame(mouse_data)
mouse_data_df

timepoints = [0,5,10,15,20,25,30,35,40,45,50]
plt.figure()
432/36:
 # Generate a line plot of time point versus tumor volume for a mouse treated with Capomulin
mouse_data = tumor_df[(tumor_df['Drug Regimen'] == 'Capomulin') & 
                      (tumor_df['Mouse ID'] == 'y793')]
mouse_data = tumor_df.groupby(['Drug Regimen', 'Timepoint'])
mouse_data_df = pd.DataFrame(mouse_data)
mouse_data_df

timepoints = [0,5,10,15,20,25,30,35,40,45,50]
plot.scatter(timepoints, mouse_data_df, marker="o", facecolors='blue', edgecolors='black')
432/37:
 # Generate a line plot of time point versus tumor volume for a mouse treated with Capomulin
mouse_data = tumor_df[(tumor_df['Drug Regimen'] == 'Capomulin') & 
                      (tumor_df['Mouse ID'] == 'y793')]
mouse_data = tumor_df.groupby(['Drug Regimen', 'Timepoint'])
mouse_data_df = pd.DataFrame(mouse_data)
mouse_data_df

timepoints = [0,5,10,15,20,25,30,35,40,45,50]
plt.scatter(timepoints, mouse_data_df, marker="o", facecolors='blue', edgecolors='black')
432/38:
 # Generate a line plot of time point versus tumor volume for a mouse treated with Capomulin
mouse_data = tumor_df[(tumor_df['Drug Regimen'] == 'Capomulin') & 
                      (tumor_df['Mouse ID'] == 'y793')]
mouse_data = tumor_df.groupby(['Drug Regimen', 'Timepoint'])
mouse_data_df = pd.DataFrame(mouse_data)
mouse_data_df

timepoints = [0,5,10,15,20,25,30,35,40,45,50]
plt.plot(timepoints, mouse_data_df, marker="o", facecolors='blue', edgecolors='black')
432/39:
 # Generate a line plot of time point versus tumor volume for a mouse treated with Capomulin
mouse_data = tumor_df[(tumor_df['Drug Regimen'] == 'Capomulin') & 
                      (tumor_df['Mouse ID'] == 'y793')]
mouse_data = tumor_df.groupby(['Drug Regimen', 'Timepoint'])
mouse_data_df = pd.DataFrame(mouse_data)
mouse_data_df

timepoints = [0,5,10,15,20,25,30,35,40,45,50]
plt.plot(timepoints, mouse_data_df, 
                        color="green")
432/40:
 # Generate a line plot of time point versus tumor volume for a mouse treated with Capomulin
mouse_data = tumor_df[(tumor_df['Drug Regimen'] == 'Capomulin') & 
                      (tumor_df['Mouse ID'] == 'y793')]
mouse_data = tumor_df.groupby(['Drug Regimen', 'Timepoint'])
mouse_data_df = pd.DataFrame(mouse_data)
mouse_data_df

timepoints = [0,5,10,15,20,25,30,35,40,45,50]
plt.plot(timepoints, mouse_data_df, color="green")
432/41:
 # Generate a line plot of time point versus tumor volume for a mouse treated with Capomulin
# mouse_data = tumor_df[(tumor_df['Drug Regimen'] == 'Capomulin') & 
#                       (tumor_df['Mouse ID'] == 'y793')]
# mouse_data = tumor_df.groupby(['Drug Regimen', 'Timepoint'])
# mouse_data_df = pd.DataFrame(mouse_data)
# mouse_data_df

timepoints = [0,5,10,15,20,25,30,35,40,45,50]
plt.plot(timepoints, tumor_df.loc[(tumor_df['Drug Regimen']=='Capomulin') & (tumor_df['Mouse ID'] == 'y793')], color="green")
432/42:
 # Generate a line plot of time point versus tumor volume for a mouse treated with Capomulin

timepoints = [0,5,10,15,20,25,30,35,40,45,50]
tumor.df.plot(timepoints, tumor_df.loc[(tumor_df['Drug Regimen']=='Capomulin') & (tumor_df['Mouse ID'] == 'y793')], color="green")
432/43:
 # Generate a line plot of time point versus tumor volume for a mouse treated with Capomulin

timepoints = [0,5,10,15,20,25,30,35,40,45,50]
tumor_df.plot(timepoints, tumor_df.loc[(tumor_df['Drug Regimen']=='Capomulin') & (tumor_df['Mouse ID'] == 'y793')], color="green")
432/44:
 # Generate a line plot of time point versus tumor volume for a mouse treated with Capomulin

timepoints = [0,5,10,15,20,25,30,35,40,45,50]
tumor_df.plot(kind="line", timepoints, tumor_df.loc[(tumor_df['Drug Regimen']=='Capomulin') & (tumor_df['Mouse ID'] == 'y793')], color="green")
432/45:
 # Generate a line plot of time point versus tumor volume for a mouse treated with Capomulin

timepoints = [0,5,10,15,20,25,30,35,40,45,50]
tumor_df.plot(kind="line")
              #, timepoints, tumor_df.loc[(tumor_df['Drug Regimen']=='Capomulin') & (tumor_df['Mouse ID'] == 'y793')], color="green")
432/46:
 # Generate a line plot of time point versus tumor volume for a mouse treated with Capomulin

timepoints = [0,5,10,15,20,25,30,35,40,45,50]
tumor_df.plot.line(timepoints, tumor_df.loc[(tumor_df['Drug Regimen']=='Capomulin') & (tumor_df['Mouse ID'] == 'y793')], color="green" )
              #, timepoints, tumor_df.loc[(tumor_df['Drug Regimen']=='Capomulin') & (tumor_df['Mouse ID'] == 'y793')], color="green")
432/47:
 # Generate a line plot of time point versus tumor volume for a mouse treated with Capomulin

timepoints = [0,5,10,15,20,25,30,35,40,45,50]
tumor_df.plot(kind="line")
              #, timepoints, tumor_df.loc[(tumor_df['Drug Regimen']=='Capomulin') & (tumor_df['Mouse ID'] == 'y793')], color="green")
432/48:
 # Generate a line plot of time point versus tumor volume for a mouse treated with Capomulin
vol_data = tumor_df[(tumor_df["Drug Regimen"] == "Capomulin") & (combineData["Mouse ID"] == "y793")]
vol_data = tumor_df.groupby(["Drug Regimen", "Timepoint"])
vol_data_df = tumor_df["Tumor Volume (mm3)"].mean().to_frame()
tumor_index_df = vol_data_df.unstack(0)
tumor_line_df = tumor_index_df["Tumor Volume (mm3)"]

timepoints = [0,5,10,15,20,25,30,35,40,45,50]

tumor_df.plot(kind="line", timepoints, tumorPlotDF["Capomulin"], marker= "o", color="green", label="Total Tumor Volume")
432/49:
 # Generate a line plot of time point versus tumor volume for a mouse treated with Capomulin
vol_data = tumor_df[(tumor_df["Drug Regimen"] == "Capomulin") & (combineData["Mouse ID"] == "y793")]
vol_data = tumor_df.groupby(["Drug Regimen", "Timepoint"])
vol_data_df = tumor_df["Tumor Volume (mm3)"].mean().to_frame()
tumor_index_df = vol_data_df.unstack(0)
tumor_line_df = tumor_index_df["Tumor Volume (mm3)"]

timepoints = [0,5,10,15,20,25,30,35,40,45,50]

tumor_df.plot(kind="line", timepoints, tumor_line_df["Capomulin"], marker= "o", color="green", label="Total Tumor Volume")
432/50:
 # Generate a line plot of time point versus tumor volume for a mouse treated with Capomulin
vol_data = tumor_df[(tumor_df["Drug Regimen"] == "Capomulin") & (combineData["Mouse ID"] == "y793")]
vol_data = tumor_df.groupby(["Drug Regimen", "Timepoint"])
vol_data_df = tumor_df["Tumor Volume (mm3)"].mean().to_frame()
tumor_index_df = vol_data_df.unstack(0)
tumor_line_df = tumor_index_df["Tumor Volume (mm3)"]

timepoints = [0,5,10,15,20,25,30,35,40,45,50]

tumor_line_df.plot.line(timepoints, tumor_line_df["Capomulin"], marker= "o", color="green", label="Total Tumor Volume")
432/51:
 # Generate a line plot of time point versus tumor volume for a mouse treated with Capomulin
vol_data = tumor_df[(tumor_df["Drug Regimen"] == "Capomulin") & (tumor_df["Mouse ID"] == "y793")]
vol_data = tumor_df.groupby(["Drug Regimen", "Timepoint"])
vol_data_df = tumor_df["Tumor Volume (mm3)"].mean().to_frame()
tumor_index_df = vol_data_df.unstack(0)
tumor_line_df = tumor_index_df["Tumor Volume (mm3)"]

timepoints = [0,5,10,15,20,25,30,35,40,45,50]

tumor_line_df.plot.line(timepoints, tumor_line_df["Capomulin"], marker= "o", color="green", label="Total Tumor Volume")
432/52:
 # Generate a line plot of time point versus tumor volume for a mouse treated with Capomulin
vol_data = tumor_df[(tumor_df["Drug Regimen"] == "Capomulin") & (tumor_df["Mouse ID"] == "y793")]
vol_data = tumor_df.groupby(["Drug Regimen", "Timepoint"])
vol_data_df = tumor_df["Tumor Volume (mm3)"].mean()
tumor_index_df = vol_data_df.unstack(0)
tumor_line_df = tumor_index_df["Tumor Volume (mm3)"]

timepoints = [0,5,10,15,20,25,30,35,40,45,50]

tumor_line_df.plot.line(timepoints, tumor_line_df["Capomulin"], marker= "o", color="green", label="Total Tumor Volume")
432/53:
 # Generate a line plot of time point versus tumor volume for a mouse treated with Capomulin
vol_data = tumor_df[(tumor_df["Drug Regimen"] == "Capomulin") & (tumor_df["Mouse ID"] == "y793")]
vol_data = tumor_df.groupby(["Drug Regimen", "Timepoint"])
vol_data_df = tumor_df["Tumor Volume (mm3)"].mean()
tumor_index_df = vol_data_df[0]
tumor_line_df = tumor_index_df["Tumor Volume (mm3)"]

timepoints = [0,5,10,15,20,25,30,35,40,45,50]

tumor_line_df.plot.line(timepoints, tumor_line_df["Capomulin"], marker= "o", color="green", label="Total Tumor Volume")
432/54:
 # Generate a line plot of time point versus tumor volume for a mouse treated with Capomulin
vol_data = tumor_df[(tumor_df["Drug Regimen"] == "Capomulin") & (tumor_df["Mouse ID"] == "y793")]
vol_data = tumor_df.groupby(["Drug Regimen", "Timepoint"])
vol_data_df = tumor_df["Tumor Volume (mm3)"].mean().to_frame()
tumor_index_df = vol_data_df.unstack(0)
tumor_line_df = tumor_index_df["Tumor Volume (mm3)"]

timepoints = [0,5,10,15,20,25,30,35,40,45,50]

tumor_line_df.plot.line(timepoints, tumor_line_df["Capomulin"], marker= "o", color="green", label="Total Tumor Volume")
436/1:
# Create code to answer each of the following questions.
# Hint: You will need multiple target URLs and multiple API requests.

# Dependencies
import requests
import json

# Retrieve Google API key from config.py
from config import gkey
436/2:
# 1. What are the geocoordinates (latitude/longitude) of Seattle, Washington?
target_city = "Seattle, Washington"

params = {"address": target_city, "key": gkey}
436/3:
# 1. What are the geocoordinates (latitude/longitude) of Seattle, Washington?
target_city = "Seattle, Washington"

params = {"address": target_city, "key": gkey}

base_url = "https://maps.googleapis.com/maps/api/geocode/json"

print("Drill #1: The Geocoordinates of Seattle, WA")
436/4:
# 1. What are the geocoordinates (latitude/longitude) of Seattle, Washington?
target_city = "Seattle, Washington"

params = {"address": target_city, "key": gkey}

base_url = "https://maps.googleapis.com/maps/api/geocode/json"

print("Drill #1: The Geocoordinates of Seattle, WA")

response = requests.get(base_url, params=params)

seattle_geo = response.json()

lat = seattle_geo["results"][0]["geometry"]["location"]["lat"]
lng = seattle_geo["results"][0]["geometry"]["location"]["lng"]

print(f"{target_city}: {lat}, {lng}")
436/5:
# 2. What are the geocoordinates (latitude/longitude) of The White House?
target_city = "The White House"
params["address"] = target_city

print("Drill #2: The Geocoordinates of the White House")

response = requests.get(base_url, params=params)

dc_geo = response.json()

lat = dc_geo["results"][0]["geometry"]["location"]["lat"]
lng = dc_geo["results"][0]["geometry"]["location"]["lng"]

print(f"{target_city}: {lat}, {lng}")
436/6:
# 3. Find the name and address of a bike store in Seattle, Washington.
#    Hint: See https://developers.google.com/places/web-service/supported_types

target_type = "bicycle_store"
seattle_coords = "47.6062095,-122.3320708"
radius = 8000

params = {
    "location": seattle_coords,
    "types": target_type,
    "radius": radius,
    "key": gkey
}

base_url = "https://maps.googleapis.com/maps/api/place/nearbysearch/json"

print("Drill #3: A Bike Store in Seattle, WA")

response = requests.get(base_url, params)

seattle_bikes = response.json()

print(seattle_bikes["results"][0]["name"])
print(seattle_bikes["results"][0]["vicinity"])
437/1:
# 3. Find the name and address of a bike store in Seattle, Washington.
#    Hint: See https://developers.google.com/places/web-service/supported_types
target_type = "bicycle_store"
seattle_coords = "47.6062095,-122.3320708"
radius = 8000

# rewrite params dict

params = {
    "location": seattle_coords,
    "types": target_type,
    "radius": radius,
    "key": gkey
}

# Build URL using the Google Maps API
base_url = "https://maps.googleapis.com/maps/api/place/nearbysearch/json"

print("Drill #3: A Bike Store in Seattle, WA")

# Run request
response = requests.get(base_url, params)

# print the response URL, avoid doing for public GitHub repos in order to avoid exposing key
# print(response.url)

seattle_bikes = response.json()

# Print the JSON (pretty printed)
# print(json.dumps(seattle_bikes, indent=4, sort_keys=True))

# Print the name and address of the first bike shop to appear
print(seattle_bikes["results"][0]["name"])
print(seattle_bikes["results"][0]["vicinity"])
437/2:
# Create code to answer each of the following questions.
# Hint: You will need multiple target URLs and multiple API requests.

# Dependencies
import requests
import json

# Google API Key
from config import gkey
437/3:
# 1. What are the geocoordinates (latitude and longitude) of Seattle,
# Washington?
target_city = "Seattle, Washington"

params = {"address": target_city, "key": gkey}

# Build URL using the Google Maps API
base_url = "https://maps.googleapis.com/maps/api/geocode/json"

print("Drill #1: The Geocoordinates of Seattle, WA")

# Run request
response = requests.get(base_url, params=params)

# print the response URL, avoid doing for public GitHub repos in order to avoid exposing key
# print(response.url)

# Convert to JSON
seattle_geo = response.json()

# Extract lat/lng
lat = seattle_geo["results"][0]["geometry"]["location"]["lat"]
lng = seattle_geo["results"][0]["geometry"]["location"]["lng"]

# Print results
print(f"{target_city}: {lat}, {lng}")
437/4:
# 3. Find the name and address of a bike store in Seattle, Washington.
#    Hint: See https://developers.google.com/places/web-service/supported_types
target_type = "bicycle_store"
seattle_coords = "47.6062095,-122.3320708"
radius = 8000

# rewrite params dict

params = {
    "location": seattle_coords,
    "types": target_type,
    "radius": radius,
    "key": gkey
}

# Build URL using the Google Maps API
base_url = "https://maps.googleapis.com/maps/api/place/nearbysearch/json"

print("Drill #3: A Bike Store in Seattle, WA")

# Run request
response = requests.get(base_url, params)

# print the response URL, avoid doing for public GitHub repos in order to avoid exposing key
# print(response.url)

seattle_bikes = response.json()

# Print the JSON (pretty printed)
# print(json.dumps(seattle_bikes, indent=4, sort_keys=True))

# Print the name and address of the first bike shop to appear
print(seattle_bikes["results"][0]["name"])
print(seattle_bikes["results"][0]["vicinity"])
436/7: seattle_bikes
436/8: seattle_bikes.url
436/9: response.url
436/10:
# 3. Find the name and address of a bike store in Seattle, Washington.
#    Hint: See https://developers.google.com/places/web-service/supported_types

target_type = "bicycle_store"
seattle_coords = "47.6062095,-122.3320708"
radius = 8000

params = {
    "location": seattle_coords,
    "types": target_type,
    "radius": radius,
    "key": gkey
}

base_url = "https://maps.googleapis.com/maps/api/place/nearbysearch/json"

print("Drill #3: A Bike Store in Seattle, WA")

response = requests.get(base_url, params)

seattle_bikes = response.json()

print(seattle_bikes["results"][0]["name"])
print(seattle_bikes["results"][0]["vicinity"])
438/1:
# Dependencies
import pandas as pd
import numpy as np
import requests
import json

# Import API key
from config import gkey
438/2:
# Import cities file into the cities_pd DataFrame
cities_pd = pd.DataFrame()
cities_pd.head()
438/3:
# Import cities file into the cities_pd DataFrame
cities_pd = pd.DataFrame()
cities_pd.head()
438/4:
# Import cities file into the cities_pd DataFrame
cities_pd = pd.read_csv("../Resources/cities.csv")
cities_pd.head()
438/5:
# Add columns for lat, lng, airport name, airport address, airport rating.
# Hint: Be sure to specify "" as the initial value for each column.
cities_pd["Lat"] = ""
cities_pd["Lng"] = ""
cities_pd["Airport Name"] = ""
cities_pd["Airport Address"] = ""
cities_pd["Airport Rating"] = ""
cities_pd.head()
438/6:
# Loop through the cities_pd and get the lat/long for each city
# Hint: `requests.get(target_url).json()`
# Hint: `for index, row in cities_pd.iterrows():`
# Hint: `cities_pd.loc`

params = {"key": gkey}

for index, row in cities_pd.iterrows():
    base_url = "https://maps.googleapis.com/maps/api/geocode/json"

    city = row['City']
    state = row['State']

    # update address key value
    params['address'] = f"{city},{state}"

    # make request
    cities_lat_lng = requests.get(base_url, params=params)
    
    # print the cities_lat_lng url, avoid doing for public github repos in order to avoid exposing key
    # print(cities_lat_lng.url)
    
    # convert to json
    cities_lat_lng = cities_lat_lng.json()

    cities_pd.loc[index, "Lat"] = cities_lat_lng["results"][0]["geometry"]["location"]["lat"]
    cities_pd.loc[index, "Lng"] = cities_lat_lng["results"][0]["geometry"]["location"]["lng"]

# Visualize to confirm lat lng appear
cities_pd.head()
438/7:
# Add columns for lat, lng, airport name, airport address, airport rating.
# Hint: Be sure to specify "" as the initial value for each column.
cities_pd["Lat"] = ""
cities_pd["Lng"] = ""
cities_pd["Airport Name"] = ""
cities_pd["Airport Address"] = ""
cities_pd["Airport Rating"] = ""
cities_pd.head()
438/8:
# Loop through the cities_pd and get the lat/long for each city
# Hint: `requests.get(target_url).json()`
# Hint: `for index, row in cities_pd.iterrows():`
# Hint: `cities_pd.loc`

params = {"key": gkey}

for index, row in cities_pd.iterrows():
    base_url = "https://maps.googleapis.com/maps/api/geocode/json"

    city = row['City']
    state = row['State']

    # update address key value
    params['address'] = f"{city},{state}"

    # make request
    cities_lat_lng = requests.get(base_url, params=params)
    
    # print the cities_lat_lng url, avoid doing for public github repos in order to avoid exposing key
    # print(cities_lat_lng.url)
    
    # convert to json
    cities_lat_lng = cities_lat_lng.json()

    cities_pd.loc[index, "Lat"] = cities_lat_lng["results"][0]["geometry"]["location"]["lat"]
    cities_pd.loc[index, "Lng"] = cities_lat_lng["results"][0]["geometry"]["location"]["lng"]

# Visualize to confirm lat lng appear
cities_pd.head()
440/1:
# Dependencies
import pandas as pd
import numpy as np
import requests
import json

# Import API key
from config import gkey
440/2:
# Import cities file into the cities_pd DataFrame
cities_pd = pd.read_csv("../Resources/cities.csv")
cities_pd.head()
440/3:
# Add columns for lat, lng, airport name, airport address, airport rating.
# Hint: Be sure to specify "" as the initial value for each column.
cities_pd["Lat"] = ""
cities_pd["Lng"] = ""
cities_pd["Airport Name"] = ""
cities_pd["Airport Address"] = ""
cities_pd["Airport Rating"] = ""
cities_pd.head()
440/4:
# Loop through the cities_pd and get the lat/long for each city
# Hint: `requests.get(target_url).json()`
# Hint: `for index, row in cities_pd.iterrows():`
# Hint: `cities_pd.loc`

params = {"key": gkey}

for index, row in cities_pd.iterrows():
    base_url = "https://maps.googleapis.com/maps/api/geocode/json"

    city = row['City']
    state = row['State']

    # update address key value
    params['address'] = f"{city},{state}"

    # make request
    cities_lat_lng = requests.get(base_url, params=params)
    
    # print the cities_lat_lng url, avoid doing for public github repos in order to avoid exposing key
    # print(cities_lat_lng.url)
    
    # convert to json
    cities_lat_lng = cities_lat_lng.json()

    cities_pd.loc[index, "Lat"] = cities_lat_lng["results"][0]["geometry"]["location"]["lat"]
    cities_pd.loc[index, "Lng"] = cities_lat_lng["results"][0]["geometry"]["location"]["lng"]

# Visualize to confirm lat lng appear
cities_pd.head()
440/5:
# params dictionary to update each iteration
params = {
    "radius": 50000,
    "types": "airport",
    "keyword": "international airport",
    "key": gkey
}

# Use the lat/lng we recovered to identify airports
for index, row in cities_pd.iterrows():
    # get lat, lng from df
    lat = row["Lat"]
    lng = row["Lng"]

    # change location each iteration while leaving original params in place
    params["location"] = f"{lat},{lng}"

    # Use the search term: "International Airport" and our lat/lng
    base_url = "https://maps.googleapis.com/maps/api/place/nearbysearch/json"

    # make request and print url
    name_address = requests.get(base_url, params=params)
    
#     print the name_address url, avoid doing for public github repos in order to avoid exposing key
#     print(name_address.url)

    # convert to json
    name_address = name_address.json()
    # print(json.dumps(name_address, indent=4, sort_keys=True))

    # Since some data may be missing we incorporate a try-except to skip any that are missing a data point.
    try:
        cities_pd.loc[index, "Airport Name"] = name_address["results"][0]["name"]
        cities_pd.loc[index, "Airport Address"] = name_address["results"][0]["vicinity"]
        cities_pd.loc[index, "Airport Rating"] = name_address["results"][0]["rating"]
    except (KeyError, IndexError):
        print("Missing field/result... skipping.")
440/6:
# Save Data to csv


# Visualize to confirm airport data appears
441/1:
import gmaps
import pandas as pd
from config import gkey

# Configure gmaps
gmaps.configure(api_key=gkey)
441/2:
import gmaps
import pandas as pd
from config import gkey

# Configure gmaps
gmaps.configure(api_key=gkey)
441/3:
# Create aiport dataframe
airport_df = pd.read_csv('../Resources/Airport_Output.csv')
airport_df.dropna(inplace = True)
airport_df.head()
441/4:
# Plot Heatmap
fig = gmaps.figure()

heat_layer = gmaps.heatmap_layer(locations, weights=rating, dissipating=False, max_intensity=10, point_radius=1)

fig.add_layer(heat_layer)

fig
441/5:
# Store latitude and longitude in locations
locations = airport_df[["Lat", "Lng"]]

# Filla NaN values and convert to float
rating = airport_df["Airport Rating"].astype(float)
441/6:
# Plot Heatmap
fig = gmaps.figure()

heat_layer = gmaps.heatmap_layer(locations, weights=rating, dissipating=False, max_intensity=10, point_radius=1)

fig.add_layer(heat_layer)

fig
443/1:
import gmaps
import pandas as pd
from config import gkey

# Configure gmaps
gmaps.configure(api_key=gkey)
443/2:
# Create aiport dataframe
airport_df = pd.read_csv('../Resources/Airport_Output.csv')
airport_df.dropna(inplace = True)
airport_df.head()
443/3:
# Store latitude and longitude in locations
locations = airport_df[["Lat", "Lng"]]

# Filla NaN values and convert to float
rating = airport_df["Airport Rating"].astype(float)
443/4:
# Plot Heatmap
fig = gmaps.figure()

heat_layer = gmaps.heatmap_layer(locations, weights=rating, dissipating=False, max_intensity=10, point_radius=1)

fig.add_layer(heat_layer)

fig
443/5:
# Plot Heatmap
fig = gmaps.figure()

heat_layer = gmaps.heatmap_layer(locations, weights=rating, dissipating=False, max_intensity=10, point_radius=1)

fig.add_layer(heat_layer)

fig
443/6:
import gmaps
import pandas as pd
from config import gkey

# Configure gmaps
gmaps.configure(api_key=gkey)
443/7:
# Create aiport dataframe
airport_df = pd.read_csv('../Resources/Airport_Output.csv')
airport_df.dropna(inplace = True)
airport_df.head()
443/8:
# Store latitude and longitude in locations
locations = airport_df[["Lat", "Lng"]]

# Filla NaN values and convert to float
rating = airport_df["Airport Rating"].astype(float)
443/9:
# Plot Heatmap
fig = gmaps.figure()

heat_layer = gmaps.heatmap_layer(locations, weights=rating, dissipating=False, max_intensity=10, point_radius=1)

fig.add_layer(heat_layer)

fig
443/10:
import gmaps
import pandas as pd
from config import gkey

# Configure gmaps
gmaps.configure(api_key=gkey)
443/11:
# Create aiport dataframe
airport_df = pd.read_csv('../Resources/Airport_Output.csv')
airport_df.dropna(inplace = True)
airport_df.head()
443/12:
# Store latitude and longitude in locations
locations = airport_df[["Lat", "Lng"]]

# Filla NaN values and convert to float
rating = airport_df["Airport Rating"].astype(float)
443/13:
# Plot Heatmap
fig = gmaps.figure()

heat_layer = gmaps.heatmap_layer(locations, weights=rating, dissipating=False, max_intensity=10, point_radius=1)

fig.add_layer(heat_layer)

fig
443/14:
# Plot Heatmap
fig = gmaps.figure()

heat_layer = gmaps.heatmap_layer(locations, weights=rating, dissipating=False, max_intensity=10, point_radius=1)

fig.add_layer(heat_layer)

fig
443/15:
import gmaps
import pandas as pd
from config import gkey

# Configure gmaps
gmaps.configure(api_key=gkey)
443/16:
# Create aiport dataframe
airport_df = pd.read_csv('../Resources/Airport_Output.csv')
airport_df.dropna(inplace = True)
airport_df.head()
443/17:
# Store latitude and longitude in locations
locations = airport_df[["Lat", "Lng"]]

# Filla NaN values and convert to float
rating = airport_df["Airport Rating"].astype(float)
443/18:
# Plot Heatmap
fig = gmaps.figure()

heat_layer = gmaps.heatmap_layer(locations, weights=rating, dissipating=False, max_intensity=10, point_radius=1)

fig.add_layer(heat_layer)

fig
444/1:
import gmaps
import pandas as pd

# Google developer API key
from config import gkey

# Configure gmaps
gmaps.configure(api_key=gkey)
443/19:
# Plot Heatmap
fig = gmaps.figure()

heat_layer = gmaps.heatmap_layer(locations, weights=rating, dissipating=False, max_intensity=10, point_radius=1)

fig.add_layer(heat_layer)

fig
443/20:
# Plot Heatmap
fig = gmaps.figure()

heat_layer = gmaps.heatmap_layer(locations, weights=rating, dissipating=False, max_intensity=10, point_radius=1)

fig.add_layer(heat_layer)

fig
443/21:
# Plot Heatmap
fig = gmaps.figure()

heat_layer = gmaps.heatmap_layer(locations, weights=rating, dissipating=False, max_intensity=10, point_radius=1)

fig.add_layer(heat_layer)

fig
443/22: heat_layer
443/23:
# Plot Heatmap
fig = gmaps.figure()

heat_layer = gmaps.heatmap_layer(locations, weights=rating, dissipating=False, max_intensity=10, point_radius=1)

fig.add_layer(heat_layer)

fig.show()
443/24:
# Plot Heatmap
fig = gmaps.figure()

heat_layer = gmaps.heatmap_layer(locations, weights=rating, dissipating=False, max_intensity=10, point_radius=1)

fig.add_layer(heat_layer)

fig
443/25:
fig = gmaps.figure(map_type="HYBRID")

# Create heat layer
heat_layer = gmaps.heatmap_layer(locations, weights=rating, 
                                 dissipating=False, max_intensity=10,
                                 point_radius=1)

fig.add_layer(heat_layer)
fig
443/26:
# Plot Heatmap
fig = gmaps.figure()

heat_layer = gmaps.heatmap_layer(locations, weights=rating, dissipating=False, max_intensity=10, point_radius=1)

fig.add_layer(heat_layer)

fig
443/27:
fig = gmaps.figure(map_type="HYBRID")

# Create heat layer
heat_layer = gmaps.heatmap_layer(locations, weights=rating, 
                                 dissipating=False, max_intensity=10,
                                 point_radius=1)

fig.add_layer(heat_layer)
fig
443/28:
import gmaps
import pandas as pd
from config import gkey

# Configure gmaps
gmaps.configure(api_key=gkey)
443/29:
# Create aiport dataframe
airport_df = pd.read_csv('../Resources/Airport_Output.csv')
airport_df.dropna(inplace = True)
airport_df.head()
443/30:
# Store latitude and longitude in locations
locations = airport_df[["Lat", "Lng"]]

# Filla NaN values and convert to float
rating = airport_df["Airport Rating"].astype(float)
443/31:
# Plot Heatmap
fig = gmaps.figure()

heat_layer = gmaps.heatmap_layer(locations, weights=rating, dissipating=False, max_intensity=10, point_radius=1)

fig.add_layer(heat_layer)

fig
443/32:
fig = gmaps.figure(map_type="HYBRID")

# Create heat layer
heat_layer = gmaps.heatmap_layer(locations, weights=rating, 
                                 dissipating=False, max_intensity=10,
                                 point_radius=1)

fig.add_layer(heat_layer)
fig
443/33:
import gmaps
import pandas as pd
from config import gkey

# Configure gmaps
gmaps.configure(api_key=gkey)
443/34:
# Create aiport dataframe
airport_df = pd.read_csv('../Resources/Airport_Output.csv')
airport_df.dropna(inplace = True)
airport_df.head()
443/35:
# Store latitude and longitude in locations
locations = airport_df[["Lat", "Lng"]]

# Filla NaN values and convert to float
rating = airport_df["Airport Rating"].astype(float)
443/36:
# Plot Heatmap
fig = gmaps.figure()

heat_layer = gmaps.heatmap_layer(locations, weights=rating, dissipating=False, max_intensity=10, point_radius=1)

fig.add_layer(heat_layer)

fig
443/37:
fig = gmaps.figure(map_type="HYBRID")

# Create heat layer
heat_layer = gmaps.heatmap_layer(locations, weights=rating, 
                                 dissipating=False, max_intensity=10,
                                 point_radius=1)

fig.add_layer(heat_layer)
fig
445/1:
import gmaps
import pandas as pd
from config import gkey

# Configure gmaps
gmaps.configure(api_key=gkey)
445/2:
# Create aiport dataframe
airport_df = pd.read_csv('../Resources/Airport_Output.csv')
airport_df.dropna(inplace = True)
airport_df.head()
445/3:
# Store latitude and longitude in locations
locations = airport_df[["Lat", "Lng"]]

# Filla NaN values and convert to float
rating = airport_df["Airport Rating"].astype(float)
445/4:
# Plot Heatmap
fig = gmaps.figure()

heat_layer = gmaps.heatmap_layer(locations, weights=rating, dissipating=False, max_intensity=10, point_radius=1)

fig.add_layer(heat_layer)

fig
445/5:
fig = gmaps.figure(map_type="HYBRID")

# Create heat layer
heat_layer = gmaps.heatmap_layer(locations, weights=rating, 
                                 dissipating=False, max_intensity=10,
                                 point_radius=1)

fig.add_layer(heat_layer)
fig
445/6:
import gmaps
import pandas as pd
from config import gkey

# Configure gmaps
gmaps.configure(api_key=gkey)
445/7:
# Create aiport dataframe
airport_df = pd.read_csv('../Resources/Airport_Output.csv')
airport_df.dropna(inplace = True)
airport_df.head()
445/8:
# Store latitude and longitude in locations
locations = airport_df[["Lat", "Lng"]]

# Filla NaN values and convert to float
rating = airport_df["Airport Rating"].astype(float)
445/9:
# Plot Heatmap
fig = gmaps.figure()

heat_layer = gmaps.heatmap_layer(locations, weights=rating, dissipating=False, max_intensity=10, point_radius=1)

fig.add_layer(heat_layer)

fig
445/10:
fig = gmaps.figure(map_type="HYBRID")

# Create heat layer
heat_layer = gmaps.heatmap_layer(locations, weights=rating, 
                                 dissipating=False, max_intensity=10,
                                 point_radius=1)

fig.add_layer(heat_layer)
fig
446/1:
import gmaps
import pandas as pd
from config import gkey

# Configure gmaps
gmaps.configure(api_key=gkey)
446/2:
# Create aiport dataframe
airport_df = pd.read_csv('../Resources/Airport_Output.csv')
airport_df.dropna(inplace = True)
airport_df.head()
446/3:
# Store latitude and longitude in locations
locations = airport_df[["Lat", "Lng"]]

# Filla NaN values and convert to float
rating = airport_df["Airport Rating"].astype(float)
446/4:
# Plot Heatmap
fig = gmaps.figure()

heat_layer = gmaps.heatmap_layer(locations, weights=rating, dissipating=False, max_intensity=10, point_radius=1)

fig.add_layer(heat_layer)

fig
446/5:
fig = gmaps.figure(map_type="HYBRID")

# Create heat layer
heat_layer = gmaps.heatmap_layer(locations, weights=rating, 
                                 dissipating=False, max_intensity=10,
                                 point_radius=1)

fig.add_layer(heat_layer)
fig
446/6:
import gmaps
import pandas as pd
from config import gkey

# Configure gmaps
gmaps.configure(api_key=gkey)
446/7:
# Create aiport dataframe
airport_df = pd.read_csv('../Resources/Airport_Output.csv')
airport_df.dropna(inplace = True)
airport_df.head()
446/8:
# Store latitude and longitude in locations
locations = airport_df[["Lat", "Lng"]]

# Filla NaN values and convert to float
rating = airport_df["Airport Rating"].astype(float)
446/9:
# Plot Heatmap
fig = gmaps.figure()

heat_layer = gmaps.heatmap_layer(locations, weights=rating, dissipating=False, max_intensity=10, point_radius=1)

fig.add_layer(heat_layer)

fig
446/10:
fig = gmaps.figure(map_type="HYBRID")

# Create heat layer
heat_layer = gmaps.heatmap_layer(locations, weights=rating, 
                                 dissipating=False, max_intensity=10,
                                 point_radius=1)

fig.add_layer(heat_layer)
fig
447/1:
import gmaps
import pandas as pd
from config import gkey

# Configure gmaps
gmaps.configure(api_key=gkey)
447/2:
# Create aiport dataframe
airport_df = pd.read_csv('../Resources/Airport_Output.csv')
airport_df.dropna(inplace = True)
airport_df.head()
447/3:
# Store latitude and longitude in locations
locations = airport_df[["Lat", "Lng"]]

# Filla NaN values and convert to float
rating = airport_df["Airport Rating"].astype(float)
447/4:
# Plot Heatmap
fig = gmaps.figure()

heat_layer = gmaps.heatmap_layer(locations, weights=rating, dissipating=False, max_intensity=10, point_radius=1)

fig.add_layer(heat_layer)

fig
447/5:
fig = gmaps.figure(map_type="HYBRID")

# Create heat layer
heat_layer = gmaps.heatmap_layer(locations, weights=rating, 
                                 dissipating=False, max_intensity=10,
                                 point_radius=1)

fig.add_layer(heat_layer)
fig
448/1:
import gmaps
import pandas as pd
from config import gkey

# Configure gmaps
gmaps.configure(api_key=gkey)
448/2:
# Create aiport dataframe
airport_df = pd.read_csv('../Resources/Airport_Output.csv')
airport_df.dropna(inplace = True)
airport_df.head()
449/1:
import gmaps
import pandas as pd
from config import gkey

# Configure gmaps
gmaps.configure(api_key=gkey)
449/2:
# Create aiport dataframe
airport_df = pd.read_csv('../Resources/Airport_Output.csv')
airport_df.dropna(inplace = True)
airport_df.head()
449/3:
# Store latitude and longitude in locations
locations = airport_df[["Lat", "Lng"]]

# Filla NaN values and convert to float
rating = airport_df["Airport Rating"].astype(float)
449/4:
# Plot Heatmap
fig = gmaps.figure()

heat_layer = gmaps.heatmap_layer(locations, weights=rating, dissipating=False, max_intensity=10, point_radius=1)

fig.add_layer(heat_layer)

fig
449/5:
fig = gmaps.figure(map_type="HYBRID")

# Create heat layer
heat_layer = gmaps.heatmap_layer(locations, weights=rating, 
                                 dissipating=False, max_intensity=10,
                                 point_radius=1)

fig.add_layer(heat_layer)
fig
451/1:
import gmaps
import pandas as pd
from config import gkey

# Configure gmaps
gmaps.configure(api_key=gkey)
451/2:
# Create aiport dataframe
airport_df = pd.read_csv('../Resources/Airport_Output.csv')
airport_df.dropna(inplace = True)
airport_df.head()
451/3:
# Store latitude and longitude in locations
locations = airport_df[["Lat", "Lng"]]

# Filla NaN values and convert to float
rating = airport_df["Airport Rating"].astype(float)
451/4:
# Plot Heatmap
fig = gmaps.figure()

heat_layer = gmaps.heatmap_layer(locations, weights=rating, dissipating=False, max_intensity=10, point_radius=1)

fig.add_layer(heat_layer)

fig
451/5:
fig = gmaps.figure(map_type="HYBRID")

# Create heat layer
heat_layer = gmaps.heatmap_layer(locations, weights=rating, 
                                 dissipating=False, max_intensity=10,
                                 point_radius=1)

fig.add_layer(heat_layer)
fig
452/1:
import gmaps
import pandas as pd
from config import gkey

# Configure gmaps
gmaps.configure(api_key=gkey)
452/2:
# Create aiport dataframe
airport_df = pd.read_csv('../Resources/Airport_Output.csv')
airport_df.dropna(inplace = True)
airport_df.head()
452/3:
# Store latitude and longitude in locations
locations = airport_df[["Lat", "Lng"]]

# Filla NaN values and convert to float
rating = airport_df["Airport Rating"].astype(float)
452/4:
# Plot Heatmap
fig = gmaps.figure()

heat_layer = gmaps.heatmap_layer(locations, weights=rating, dissipating=False, max_intensity=10, point_radius=1)

fig.add_layer(heat_layer)

fig
452/5:
fig = gmaps.figure(map_type="HYBRID")

# Create heat layer
heat_layer = gmaps.heatmap_layer(locations, weights=rating, 
                                 dissipating=False, max_intensity=10,
                                 point_radius=1)

fig.add_layer(heat_layer)
fig
454/1:
import gmaps
import pandas as pd
from config import gkey

# Configure gmaps
gmaps.configure(api_key=gkey)
456/1:
import gmaps
import pandas as pd
from config import gkey

# Configure gmaps
gmaps.configure(api_key=gkey)
456/2:
# Create aiport dataframe
airport_df = pd.read_csv('../Resources/Airport_Output.csv')
airport_df.dropna(inplace = True)
airport_df.head()
456/3:
# Store latitude and longitude in locations
locations = airport_df[["Lat", "Lng"]]

# Filla NaN values and convert to float
rating = airport_df["Airport Rating"].astype(float)
456/4:
# Plot Heatmap
fig = gmaps.figure()

heat_layer = gmaps.heatmap_layer(locations, weights=rating, dissipating=False, max_intensity=10, point_radius=1)

fig.add_layer(heat_layer)

fig
456/5:
fig = gmaps.figure(map_type="HYBRID")

# Create heat layer
heat_layer = gmaps.heatmap_layer(locations, weights=rating, 
                                 dissipating=False, max_intensity=10,
                                 point_radius=1)

fig.add_layer(heat_layer)
fig
457/1:
import gmaps
import pandas as pd

# Google developer API key
from config import gkey

# Configure gmaps
gmaps.configure(api_key=gkey)
457/2:
# Create aiport dataframe
airport_df = pd.read_csv('../Resources/Airport_Output.csv')
airport_df.dropna(inplace = True) 
airport_df.head()
457/3:
# Store latitude and longitude in locations
locations = airport_df[["Lat", "Lng"]]

# Fill NaN values and convert to float
rating = airport_df["Airport Rating"].astype(float)
457/4:
# Plot Heatmap
fig = gmaps.figure()

# Create heat layer
heat_layer = gmaps.heatmap_layer(locations, weights=rating, 
                                 dissipating=False, max_intensity=10,
                                 point_radius=1)


# Add layer
fig.add_layer(heat_layer)

# Display figure
fig
458/1:
import gmaps
import pandas as pd

# Google developer API key
from config import gkey

# Configure gmaps
gmaps.configure(api_key=gkey)
458/2:
# Create aiport dataframe
airport_df = pd.read_csv('../Resources/Airport_Output.csv')
airport_df.dropna(inplace = True) 
airport_df.head()
458/3:
# Store latitude and longitude in locations
locations = airport_df[["Lat", "Lng"]]

# Fill NaN values and convert to float
rating = airport_df["Airport Rating"].astype(float)
458/4:
# Plot Heatmap
fig = gmaps.figure()

# Create heat layer
heat_layer = gmaps.heatmap_layer(locations, weights=rating, 
                                 dissipating=False, max_intensity=10,
                                 point_radius=1)


# Add layer
fig.add_layer(heat_layer)

# Display figure
fig
460/1:
import gmaps
import pandas as pd
from config import gkey

# Configure gmaps
gmaps.configure(api_key=gkey)
460/2:
# Create aiport dataframe
airport_df = pd.read_csv('../Resources/Airport_Output.csv')
airport_df.dropna(inplace = True)
airport_df.head()
460/3:
# Store latitude and longitude in locations
locations = airport_df[["Lat", "Lng"]]

# Filla NaN values and convert to float
rating = airport_df["Airport Rating"].astype(float)
460/4:
# Plot Heatmap
fig = gmaps.figure()

heat_layer = gmaps.heatmap_layer(locations, weights=rating, dissipating=False, max_intensity=10, point_radius=1)

fig.add_layer(heat_layer)

fig
460/5:
fig = gmaps.figure(map_type="HYBRID")

# Create heat layer
heat_layer = gmaps.heatmap_layer(locations, weights=rating, 
                                 dissipating=False, max_intensity=10,
                                 point_radius=1)

fig.add_layer(heat_layer)
fig
461/1:
import gmaps
import pandas as pd
from config import gkey

# Configure gmaps
gmaps.configure(api_key=gkey)
461/2:
# Create aiport dataframe
airport_df = pd.read_csv('../Resources/Airport_Output.csv')
airport_df.dropna(inplace = True)
airport_df.head()
461/3:
# Store latitude and longitude in locations
locations = airport_df[["Lat", "Lng"]]

# Filla NaN values and convert to float
rating = airport_df["Airport Rating"].astype(float)
461/4:
# Plot Heatmap
fig = gmaps.figure()

heat_layer = gmaps.heatmap_layer(locations, weights=rating, dissipating=False, max_intensity=10, point_radius=1)

fig.add_layer(heat_layer)

fig
461/5:
fig = gmaps.figure(map_type="HYBRID")

# Create heat layer
heat_layer = gmaps.heatmap_layer(locations, weights=rating, 
                                 dissipating=False, max_intensity=10,
                                 point_radius=1)

fig.add_layer(heat_layer)
fig
463/1:
import gmaps
import pandas as pd
from config import gkey

# Configure gmaps
gmaps.configure(api_key=gkey)
463/2:
# Create aiport dataframe
airport_df = pd.read_csv('../Resources/Airport_Output.csv')
airport_df.dropna(inplace = True)
airport_df.head()
463/3:
# Store latitude and longitude in locations
locations = airport_df[["Lat", "Lng"]]

# Filla NaN values and convert to float
rating = airport_df["Airport Rating"].astype(float)
463/4:
# Plot Heatmap
fig = gmaps.figure()

heat_layer = gmaps.heatmap_layer(locations, weights=rating, dissipating=False, max_intensity=10, point_radius=1)

fig.add_layer(heat_layer)

fig
463/5:
fig = gmaps.figure(map_type="HYBRID")

# Create heat layer
heat_layer = gmaps.heatmap_layer(locations, weights=rating, 
                                 dissipating=False, max_intensity=10,
                                 point_radius=1)

fig.add_layer(heat_layer)
fig
464/1:
# Dependencies
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import requests
from census import Census
import gmaps

# Census & gmaps API Keys
from config import (api_key, gkey)
c = Census(api_key, year=2013)

# Configure gmaps
gmaps.configure(api_key=gkey)
464/2:
# Run Census Search to retrieve data on all zip codes (2013 ACS5 Census)
# See: https://github.com/CommerceDataService/census-wrapper for library documentation
# See: https://gist.github.com/afhaque/60558290d6efd892351c4b64e5c01e9b for labels
census_data = c.acs5.get(("NAME", "B19013_001E", "B01003_001E", "B01002_001E",
                          "B19301_001E",
                          "B17001_002E"), {'for': 'zip code tabulation area:*'})

# Convert to DataFrame
census_pd = pd.DataFrame(census_data)

# Column Reordering
census_pd = census_pd.rename(columns={"B01003_001E": "Population",
                                      "B01002_001E": "Median Age",
                                      "B19013_001E": "Household Income",
                                      "B19301_001E": "Per Capita Income",
                                      "B17001_002E": "Poverty Count",
                                      "NAME": "Name", "zip code tabulation area": "Zipcode"})

# Add in Poverty Rate (Poverty Count / Population)
census_pd["Poverty Rate"] = 100 * \
    census_pd["Poverty Count"].astype(
        int) / census_pd["Population"].astype(int)

# Final DataFrame
census_pd = census_pd[["Zipcode", "Population", "Median Age", "Household Income",
                       "Per Capita Income", "Poverty Count", "Poverty Rate"]]

# Visualize
print(len(census_pd))
census_pd.head()
464/3:
# Dependencies
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import requests
from census import Census
import gmaps

# Census & gmaps API Keys
from config import (api_key, gkey)
c = Census(api_key, year=2013)

# Configure gmaps
gmaps.configure(api_key=gkey)
464/4:
# Run Census Search to retrieve data on all zip codes (2013 ACS5 Census)
# See: https://github.com/CommerceDataService/census-wrapper for library documentation
# See: https://gist.github.com/afhaque/60558290d6efd892351c4b64e5c01e9b for labels
census_data = c.acs5.get(("NAME", "B19013_001E", "B01003_001E", "B01002_001E",
                          "B19301_001E",
                          "B17001_002E"), {'for': 'zip code tabulation area:*'})

# Convert to DataFrame
census_pd = pd.DataFrame(census_data)

# Column Reordering
census_pd = census_pd.rename(columns={"B01003_001E": "Population",
                                      "B01002_001E": "Median Age",
                                      "B19013_001E": "Household Income",
                                      "B19301_001E": "Per Capita Income",
                                      "B17001_002E": "Poverty Count",
                                      "NAME": "Name", "zip code tabulation area": "Zipcode"})

# Add in Poverty Rate (Poverty Count / Population)
census_pd["Poverty Rate"] = 100 * \
    census_pd["Poverty Count"].astype(
        int) / census_pd["Population"].astype(int)

# Final DataFrame
census_pd = census_pd[["Zipcode", "Population", "Median Age", "Household Income",
                       "Per Capita Income", "Poverty Count", "Poverty Rate"]]

# Visualize
print(len(census_pd))
census_pd.head()
464/5:
# Save as a csv
# Note to avoid any issues later, use encoding="utf-8"
census_pd.to_csv("census_data.csv", encoding="utf-8", index=False)
464/6:
# Dependencies
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import requests
from census import Census
import gmaps

# Census & gmaps API Keys
from config import (api_key, gkey)
c = Census(api_key, year=2013)

# Configure gmaps
gmaps.configure(api_key=gkey)
464/7:
# Run Census Search to retrieve data on all zip codes (2013 ACS5 Census)
# See: https://github.com/CommerceDataService/census-wrapper for library documentation
# See: https://gist.github.com/afhaque/60558290d6efd892351c4b64e5c01e9b for labels
census_data = c.acs5.get(("NAME", "B19013_001E", "B01003_001E", "B01002_001E",
                          "B19301_001E",
                          "B17001_002E"), {'for': 'zip code tabulation area:*'})

# Convert to DataFrame
census_pd = pd.DataFrame(census_data)

# Column Reordering
census_pd = census_pd.rename(columns={"B01003_001E": "Population",
                                      "B01002_001E": "Median Age",
                                      "B19013_001E": "Household Income",
                                      "B19301_001E": "Per Capita Income",
                                      "B17001_002E": "Poverty Count",
                                      "NAME": "Name", "zip code tabulation area": "Zipcode"})

# Add in Poverty Rate (Poverty Count / Population)
census_pd["Poverty Rate"] = 100 * \
    census_pd["Poverty Count"].astype(
        int) / census_pd["Population"].astype(int)

# Final DataFrame
census_pd = census_pd[["State", "Name", "Population", "Median Age", "Household Income",
                       "Per Capita Income", "Poverty Count", "Poverty Rate"]]

# Visualize
print(len(census_pd))
census_pd.head()
465/1:
# Dependencies
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import requests
from census import Census
import gmaps

# Census & gmaps API Keys
from config import (api_key, gkey)
c = Census(api_key, year=2013)

# Configure gmaps
gmaps.configure(api_key=gkey)
465/2:
# Run Census Search to retrieve data on all states
# Note the addition of "B23025_005E" for unemployment count
census_data = c.acs5.get(("NAME", "B19013_001E", "B01003_001E", "B01002_001E",
                          "B19301_001E",
                          "B17001_002E",
                          "B23025_005E"), {'for': 'state:*'})

# Convert to DataFrame
census_pd = pd.DataFrame(census_data)

# Column Reordering
census_pd = census_pd.rename(columns={"B01003_001E": "Population",
                                      "B01002_001E": "Median Age",
                                      "B19013_001E": "Household Income",
                                      "B19301_001E": "Per Capita Income",
                                      "B17001_002E": "Poverty Count",
                                      "B23025_005E": "Unemployment Count",
                                      "NAME": "Name", "state": "State"})

# Add in Poverty Rate (Poverty Count / Population)
census_pd["Poverty Rate"] = 100 * \
    census_pd["Poverty Count"].astype(
        int) / census_pd["Population"].astype(int)

# Add in Employment Rate (Employment Count / Population)
census_pd["Unemployment Rate"] = 100 * \
    census_pd["Unemployment Count"].astype(
        int) / census_pd["Population"].astype(int)

# Final DataFrame
census_pd = census_pd[["State", "Name", "Population", "Median Age", "Household Income",
                       "Per Capita Income", "Poverty Count", "Poverty Rate", "Unemployment Rate"]]

census_pd.head()
464/8:
# Run Census Search to retrieve data on all zip codes (2013 ACS5 Census)

census_data = c.acs5.get(("NAME", "B19013_001E", "B01003_001E", "B01002_001E",
                          "B19301_001E",
                          "B17001_002E",
                          "B23025_005E"), {'for': 'state:*'})

# Convert to DataFrame
census_pd = pd.DataFrame(census_data)

# Column Reordering
census_pd = census_pd.rename(columns={"B01003_001E": "Population",
                                      "B01002_001E": "Median Age",
                                      "B19013_001E": "Household Income",
                                      "B19301_001E": "Per Capita Income",
                                      "B17001_002E": "Poverty Count",
                                      "B23025_005E": "Unemployment Count",
                                      "NAME": "Name", "state": "State"})

# Add in Poverty Rate (Poverty Count / Population)
census_pd["Poverty Rate"] = 100 * \
    census_pd["Poverty Count"].astype(
        int) / census_pd["Population"].astype(int)

# Add in Employment Rate (Employment Count / Population)
census_pd["Unemployment Rate"] = 100 * \
    census_pd["Unemployment Count"].astype(
        int) / census_pd["Population"].astype(int)

# Final DataFrame
census_pd = census_pd[["State", "Name", "Population", "Median Age", "Household Income",
                       "Per Capita Income", "Poverty Count", "Poverty Rate", "Unemployment Rate"]]

census_pd.head()
464/9:
# Save as a csv
# Note to avoid any issues later, use encoding="utf-8"
census_pd.to_csv("census_data.csv", encoding="utf-8", index=False)
469/1: gender_df.head()
469/2:
# Generate a pie plot showing the distribution of female versus male mice using pandas
gender = pymaceuticals_clean.groupby(["Mouse ID","Sex"])
gender
469/3:
gender_df = pd.DataFrame(gender.size())
gender_df
469/4:
gender_data = pd.DataFrame(gender_df.groupby(["Sex"]).count())
gender_data.columns = ["Total Count"]
gender_data
469/5:
gender_data["Gender by Percentage"] = round(100*(gender_data["Total Count"]/gender_data["Total Count"].sum()), 2)
gender_data
469/6:
labels = ["Female", "Male"]
colors = ["pink", "lightblue"]
explode = (0.1, 0)
469/7:
plot = gender_data.plot.pie(y='Total Count', explode=explode, labels=labels, colors=colors, autopct="%1.1f%%", shadow=True, startangle=140)
plt.show()
plt.savefig("Mice Male vs. Female Piechart.png")
470/1: %matplotlib notebook
470/2:
# Dependencies
import matplotlib.pyplot as plt
import pandas as pd
import scipy.stats as st
import numpy as np
470/3:
# Files
mouse_metadata_path = "data/Mouse_metadata.csv"
study_results_path = "data/Study_results.csv"
470/4:
# Read the mouse data and the study results
mouse_metadata = pd.read_csv(mouse_metadata_path)
study_results = pd.read_csv(study_results_path)

mouse_metadata.head()
study_results.head()
470/5:
# Combine the data into a single dataset
pymaceuticals_data = pd.merge(mouse_metadata, study_results, on=["Mouse ID", "Mouse ID"])

# Display the data table for preview
pymaceuticals_data
470/6:
 # Checking the number of mice.
number_of_mice = len(pymaceuticals_data["Mouse ID"])
number_of_mice
470/7:
# Getting the duplicate mice by ID number that shows up for Mouse ID and Timepoint. 
duplicate_mice = pymaceuticals_data.duplicated(subset=['Mouse ID', 'Timepoint'])
duplicate_mice
470/8:
# Create a clean DataFrame by dropping the duplicate mouse by its ID.
pymaceuticals_clean = pymaceuticals_data.drop_duplicates(subset=['Mouse ID', 'Timepoint'],keep='first',inplace=False)
pymaceuticals_clean
470/9:
 # Checking the number of mice in the clean DataFrame.
total_mice = len(pymaceuticals_clean["Mouse ID"])
total_mice
470/10:
# Generate a summary statistics table of mean, median, variance, standard deviation, and SEM of the tumor volume for each regimen

# This method is the most straighforward, creating multiple series and putting them all together at the end.
grouped = pymaceuticals_clean.groupby('Drug Regimen')
summary_table = round(grouped.agg(['mean','median','var','std','sem'])["Tumor Volume (mm3)"], 2)
summary_table
470/11:
regimen = pymaceuticals_clean.groupby(['Drug Regimen']).count()['Mouse ID']
regimen
470/12: x_axis = np.arange(len(regimen))
470/13:
# Generate a bar plot showing the total number of mice for each treatment throughout the course of the study using pandas.
# tick locations horizontal?
plt.bar(x_axis, regimen, color='pink', alpha=1, align='center')

tick_locations = [value for value in x_axis]
plt.xticks(tick_locations, ['Capomulin', 'Ceftamin', 'Infubinol', 'Ketapril', 'Naftisol', 'Placebo', 'Propriva', 'Ramicane', 'Stelasyn', 'Zoniferol'], rotation='vertical')


plt.title("Total Mice Treated")
plt.xlabel("Drug Regimen")
plt.ylabel("Number of Mice")

plt.tight_layout()
plt.savefig("Total Mice Bar Plot.png")
plt.show()
470/14:
# Generate a pie plot showing the distribution of female versus male mice using pandas
gender = pymaceuticals_clean.groupby(["Mouse ID","Sex"])
gender
470/15:
gender_df = pd.DataFrame(gender.size())
gender_df
470/16:
gender_data = pd.DataFrame(gender_df.groupby(["Sex"]).count())
gender_data.columns = ["Total Count"]
gender_data
470/17:
gender_data["Gender by Percentage"] = round(100*(gender_data["Total Count"]/gender_data["Total Count"].sum()), 2)
gender_data
470/18:
labels = ["Female", "Male"]
colors = ["pink", "lightblue"]
explode = (0.1, 0)
470/19:
plot = gender_data.plot.pie(y='Total Count', explode=explode, labels=labels, colors=colors, autopct="%1.1f%%", shadow=True, startangle=140)
plt.show()
plt.savefig("Mice Male vs. Female Piechart.png")
470/20: gender_df.head()
470/21:
# Calculate the final tumor volume of each mouse across four of the treatment regimens:  
# Capomulin, Ramicane, Infubinol, and Ceftamin
regimens_df = pymaceuticals_clean.loc[(pymaceuticals_clean["Drug Regimen"] == "Capomulin") | (pymaceuticals_clean["Drug Regimen"] == "Ramicane") | 
                                      (pymaceuticals_clean["Drug Regimen"] == "Infubinol") | (pymaceuticals_clean["Drug Regimen"] == "Ceftamin")]

# Start by getting the last (greatest) timepoint for each mouse
regimens_df = regimens_df.groupby('Mouse ID').max()['Timepoint']
regimens_df.head()

# Merge this group df with the original dataframe to get the tumor volume at the last timepoint
#Come back and round
tumor_df = pd.merge(pymaceuticals_data, regimens_df, on=("Mouse ID","Timepoint"),how="right")
tumor_df
470/22:
# Put treatments into a list for for loop (and later for plot labels)
treatments = ['Capomulin', 'Ramicane', 'Infubinol', 'Ceftamin']

# Create empty list to fill with tumor vol data (for plotting)
tumor_vol_data = []

# Calculate the IQR and quantitatively determine if there are any potential outliers. 
# Locate the rows which contain mice on each drug and get the tumor volumes 
    # add subset 
    # Determine outliers using upper and lower bounds
    
    
for drug in treatments:
    tumor_vol = tumor_df.loc[tumor_df['Drug Regimen']==drug, 'Tumor Volume (mm3)']
    tumor_vol_data.append(tumor_vol)
    print(drug)
    quantiles = tumor_vol.quantile([.25, .5, .75])
    lowerq = quantiles[0.25]
    upperq = quantiles[0.75]
    iqr = upperq - lowerq
    
    lower_bound = lowerq - (1.5*iqr)
    upper_bound = upperq + (1.5*iqr)
    
    print(f"The lower quartile of is: {lowerq}")
    print(f"The upper quartile of is: {upperq}")
    print(f"The interquartile range is: {iqr}")
    print(f"The the median is: {quantiles[0.5]}")
    
    print(f"Values below {lower_bound} could be outliers.")
    print(f"Values above {upper_bound} could be outliers.")
    
    print(f"Outliers are: {tumor_vol.loc[(tumor_vol<lower_bound) | (tumor_vol>upper_bound)]}")
    print()
    print()
    print()
470/23:
# Generate a box plot of the final tumor volume of each mouse across four regimens of interest
fig1, ax1 = plt.subplots()
ax1.set_title('Final Tumor Volume of Each Mouse')
ax1.set_ylabel('Tumor Vol')
ax1.boxplot(tumor_vol_data, labels=treatments)
plt.show()
plt.savefig("Final Tumor Volume of Each Mouse - Box Plots.png")
470/24:
 # Generate a line plot of time point versus tumor volume for a mouse treated with Capomulin
vol_data = tumor_df[(tumor_df["Drug Regimen"] == "Capomulin") & (tumor_df["Mouse ID"] == "y793")]
vol_data = tumor_df.groupby(["Drug Regimen", "Timepoint"])
tumor_index_df = vol_data_df.unstack(0)
tumor_line_df = tumor_index_df["Tumor Volume (mm3)"]

timepoints = [0,5,10,15,20,25,30,35,40,45,50]

tumor_line_df.plot.line(timepoints, tumor_line_df["Capomulin"], marker= "o", color="green", label="Total Tumor Volume")
470/25: gender_data.head()
470/26:
plt.figure(figsize=(10,6))
ax1 = plt.subplot(121, aspect='equal')
gender_data.plot(kind='pie', y = "Count", ax=ax1, autopct='%1.1f%%', 
startangle=90, shadow=False, labels=gender_df['Sex'], legend = False, fontsize=16)
470/27:
plt.figure(figsize=(10,6))
ax1 = plt.subplot(121, aspect='equal')
gender_data.plot(kind='pie', y = "Total Count", ax=ax1, autopct='%1.1f%%', 
startangle=90, shadow=False, labels=gender_df['Sex'], legend = False, fontsize=16)
470/28:
plt.figure(figsize=(10,6))
ax1 = plt.subplot(121, aspect='equal')
gender_data.plot(kind='pie', y = "Total Count", ax=ax1, autopct='%1.1f%%', 
startangle=90, shadow=False, labels=gender_df['Sex'], legend = False)
470/29:
plt.figure(figsize=(10,6))
ax1 = plt.subplot(121, aspect='equal')
gender_data.plot(kind='pie', y = "Total Count", ax=ax1, autopct='%1.1f%%', 
startangle=90, shadow=False, labels=gender_df['Gender by Percentage'], legend = False)
470/30:
plt.figure(figsize=(10,6))
ax1 = plt.subplot(121, aspect='equal')
gender_data.plot(kind='pie', y = "Total Count", ax=ax1, autopct='%1.1f%%', 
startangle=90, shadow=False, labels=gender_df['Sex'], legend = False)
470/31:
gender_dataframe = pd.DataFrame(gender_data.groupby(["Sex"]).count()).reset_index()
gender_df.head()
470/32:
gender_dataframe = pd.DataFrame(gender_data.groupby(["Sex"]).count()).reset_index()
gender_dataframe.head()
470/33:
gender_dataframe = pd.DataFrame(pymaceuticals_clean.groupby(["Sex"]).count()).reset_index()
gender_dataframe.head()
470/34:
gender_dataframe = gender_dataframe[["Sex","Mouse ID"]]
gender_dataframe = gender_dataframe.rename(columns={"Mouse ID": "Count"})
gender_dataframe.head()
470/35:
plt.figure(figsize=(10,6))
ax1 = plt.subplot(121, aspect='equal')
gender_dataframe.plot(kind='pie', y = "Total Count", ax=ax1, autopct='%1.1f%%', 
startangle=90, shadow=False, labels=gender_df['Sex'], legend = False)
470/36:
plt.figure(figsize=(10,6))
ax1 = plt.subplot(121, aspect='equal')
gender_dataframe.plot(kind='pie', y = "Total Count", ax=ax1, autopct='%1.1f%%', 
startangle=90, shadow=False, labels=gender_dataframe['Sex'], legend = False)
470/37:
gender_dataframe = gender_dataframe[["Sex","Mouse ID"]]
gender_dataframe = gender_dataframe.rename(columns={"Mouse ID": "Total Count"})
gender_dataframe.head()
470/38:
plt.figure(figsize=(10,6))
ax1 = plt.subplot(121, aspect='equal')
gender_dataframe.plot(kind='pie', y = "Total Count", ax=ax1, autopct='%1.1f%%', 
startangle=90, shadow=False, labels=gender_dataframe['Sex'], legend = False)
470/39:
gender_dataframe = pd.DataFrame(pymaceuticals_clean.groupby(["Sex"]).count()).reset_index()
gender_dataframe.head()
470/40:
gender_dataframe = gender_dataframe[["Sex","Mouse ID"]]
gender_dataframe = gender_dataframe.rename(columns={"Mouse ID": "Total Count"})
gender_dataframe.head()
470/41:
plt.figure(figsize=(10,6))
ax1 = plt.subplot(121, aspect='equal')
gender_dataframe.plot(kind='pie', y = "Total Count", ax=ax1, autopct='%1.1f%%', 
startangle=90, shadow=False, labels=gender_dataframe['Sex'], legend = False)
470/42:
plt.figure(figsize=(10,6))
ax1 = plt.subplot(121, aspect='equal')
gender_dataframe.plot(kind='pie', y = "Total Count", ax=ax1, autopct='%1.1f%%', 
startangle=90, shadow=False, labels=gender_dataframe['Sex'], legend = False)
plt.savefigure("Male Vs. Female Pie Chart Method 2".png)
470/43:
plt.figure(figsize=(10,6))
ax1 = plt.subplot(121, aspect='equal')
gender_dataframe.plot(kind='pie', y = "Total Count", ax=ax1, autopct='%1.1f%%', 
startangle=90, shadow=False, labels=gender_dataframe['Sex'], legend = False)
plt.savfig("Male Vs. Female Pie Chart Method 2".png)
470/44:
plt.figure(figsize=(10,6))
ax1 = plt.subplot(121, aspect='equal')
gender_dataframe.plot(kind='pie', y = "Total Count", ax=ax1, autopct='%1.1f%%', 
startangle=90, shadow=False, labels=gender_dataframe['Sex'], legend = False)
plt.savefigure("Male Vs. Female Pie Chart Method 2.png")
470/45:
plt.figure(figsize=(10,6))
ax1 = plt.subplot(121, aspect='equal')
gender_dataframe.plot(kind='pie', y = "Total Count", ax=ax1, autopct='%1.1f%%', 
startangle=90, shadow=False, labels=gender_dataframe['Sex'], legend = False)
470/46:
# Generate a bar plot showing the total number of mice for each treatment throughout the course of the study using pandas.
# tick locations horizontal?
plt.bar(x_axis, regimen, color='pink', alpha=1, align='center')

tick_locations = [value for value in x_axis]
plt.xticks(tick_locations, ['Capomulin', 'Ceftamin', 'Infubinol', 'Ketapril', 'Naftisol', 'Placebo', 'Propriva', 'Ramicane', 'Stelasyn', 'Zoniferol'], rotation='vertical')


plt.title("Total Mice Treated")
plt.xlabel("Drug Regimen")
plt.ylabel("Number of Mice")

plt.tight_layout()
plt.savefig("Total Mice Bar Plot.png")
plt.show()
470/47: x_axis = np.arange(len(regimen))
470/48:
# Generate a bar plot showing the total number of mice for each treatment throughout the course of the study using pandas.
# tick locations horizontal?
plt.bar(x_axis, regimen, color='pink', alpha=1, align='center')

tick_locations = [value for value in x_axis]
plt.xticks(tick_locations, ['Capomulin', 'Ceftamin', 'Infubinol', 'Ketapril', 'Naftisol', 'Placebo', 'Propriva', 'Ramicane', 'Stelasyn', 'Zoniferol'], rotation='vertical')


plt.title("Total Mice Treated")
plt.xlabel("Drug Regimen")
plt.ylabel("Number of Mice")

plt.tight_layout()
plt.savefig("Total Mice Bar Plot.png")
plt.show()
470/49:
# Generate a pie plot showing the distribution of female versus male mice using pandas
gender = pymaceuticals_clean.groupby(["Mouse ID","Sex"])
gender
470/50:
gender_df = pd.DataFrame(gender.size())
gender_df
470/51:
gender_data = pd.DataFrame(gender_df.groupby(["Sex"]).count())
gender_data.columns = ["Total Count"]
gender_data
470/52:
gender_data["Gender by Percentage"] = round(100*(gender_data["Total Count"]/gender_data["Total Count"].sum()), 2)
gender_data
470/53:
labels = ["Female", "Male"]
colors = ["pink", "lightblue"]
explode = (0.1, 0)
470/54:
plot = gender_data.plot.pie(y='Total Count', explode=explode, labels=labels, colors=colors, autopct="%1.1f%%", shadow=True, startangle=140)
plt.show()
plt.savefig("Mice Male vs. Female Piechart.png")
470/55:
# Generate a bar plot showing the total number of mice for each treatment throughout the course of the study using pandas.
# tick locations horizontal?
plt.bar(x_axis, regimen, color='pink', alpha=1, align='center')

tick_locations = [value for value in x_axis]
plt.xticks(tick_locations, ['Capomulin', 'Ceftamin', 'Infubinol', 'Ketapril', 'Naftisol', 'Placebo', 'Propriva', 'Ramicane', 'Stelasyn', 'Zoniferol'], rotation='vertical')


plt.title("Total Mice Treated")
plt.xlabel("Drug Regimen")
plt.ylabel("Number of Mice")

plt.tight_layout()
plt.savefig("Total Mice Bar Plot.png")
plt.show()
471/1: %matplotlib notebook
471/2:
# Dependencies
import matplotlib.pyplot as plt
import pandas as pd
import scipy.stats as st
import numpy as np
471/3:
# Files
mouse_metadata_path = "data/Mouse_metadata.csv"
study_results_path = "data/Study_results.csv"
471/4:
# Read the mouse data and the study results
mouse_metadata = pd.read_csv(mouse_metadata_path)
study_results = pd.read_csv(study_results_path)

mouse_metadata.head()
study_results.head()
471/5:
# Combine the data into a single dataset
pymaceuticals_data = pd.merge(mouse_metadata, study_results, on=["Mouse ID", "Mouse ID"])

# Display the data table for preview
pymaceuticals_data
471/6:
 # Checking the number of mice.
number_of_mice = len(pymaceuticals_data["Mouse ID"])
number_of_mice
471/7:
# Getting the duplicate mice by ID number that shows up for Mouse ID and Timepoint. 
duplicate_mice = pymaceuticals_data.duplicated(subset=['Mouse ID', 'Timepoint'])
duplicate_mice
471/8:
# Create a clean DataFrame by dropping the duplicate mouse by its ID.
pymaceuticals_clean = pymaceuticals_data.drop_duplicates(subset=['Mouse ID', 'Timepoint'],keep='first',inplace=False)
pymaceuticals_clean
471/9:
 # Checking the number of mice in the clean DataFrame.
total_mice = len(pymaceuticals_clean["Mouse ID"])
total_mice
471/10:
# Generate a summary statistics table of mean, median, variance, standard deviation, and SEM of the tumor volume for each regimen

# This method is the most straighforward, creating multiple series and putting them all together at the end.
grouped = pymaceuticals_clean.groupby('Drug Regimen')
summary_table = round(grouped.agg(['mean','median','var','std','sem'])["Tumor Volume (mm3)"], 2)
summary_table
471/11:
regimen = pymaceuticals_clean.groupby(['Drug Regimen']).count()['Mouse ID']
regimen
471/12: x_axis = np.arange(len(regimen))
471/13:
# Generate a bar plot showing the total number of mice for each treatment throughout the course of the study using pandas.
# tick locations horizontal?
plt.bar(x_axis, regimen, color='pink', alpha=1, align='center')

tick_locations = [value for value in x_axis]
plt.xticks(tick_locations, ['Capomulin', 'Ceftamin', 'Infubinol', 'Ketapril', 'Naftisol', 'Placebo', 'Propriva', 'Ramicane', 'Stelasyn', 'Zoniferol'], rotation='vertical')


plt.title("Total Mice Treated")
plt.xlabel("Drug Regimen")
plt.ylabel("Number of Mice")

plt.tight_layout()
plt.savefig("Total Mice Bar Plot.png")
plt.show()
471/14:
# Generate a pie plot showing the distribution of female versus male mice using pandas
gender = pymaceuticals_clean.groupby(["Mouse ID","Sex"])
gender
471/15:
gender_df = pd.DataFrame(gender.size())
gender_df
471/16:
gender_data = pd.DataFrame(gender_df.groupby(["Sex"]).count())
gender_data.columns = ["Total Count"]
gender_data
471/17:
gender_data["Gender by Percentage"] = round(100*(gender_data["Total Count"]/gender_data["Total Count"].sum()), 2)
gender_data
471/18:
labels = ["Female", "Male"]
colors = ["pink", "lightblue"]
explode = (0.1, 0)
471/19:
plot = gender_data.plot.pie(y='Total Count', explode=explode, labels=labels, colors=colors, autopct="%1.1f%%", shadow=True, startangle=140)
plt.show()
plt.savefig("Mice Male vs. Female Piechart.png")
471/20:
gender_dataframe = pd.DataFrame(pymaceuticals_clean.groupby(["Sex"]).count()).reset_index()
gender_dataframe.head()
471/21:
gender_dataframe = gender_dataframe[["Sex","Mouse ID"]]
gender_dataframe = gender_dataframe.rename(columns={"Mouse ID": "Total Count"})
gender_dataframe.head()
471/22:
plt.figure(figsize=(10,6))
ax1 = plt.subplot(121, aspect='equal')
gender_dataframe.plot(kind='pie', y = "Total Count", ax=ax1, autopct='%1.1f%%', 
startangle=90, shadow=False, labels=gender_dataframe['Sex'], legend = False)
471/23:
# Calculate the final tumor volume of each mouse across four of the treatment regimens:  
# Capomulin, Ramicane, Infubinol, and Ceftamin
regimens_df = pymaceuticals_clean.loc[(pymaceuticals_clean["Drug Regimen"] == "Capomulin") | (pymaceuticals_clean["Drug Regimen"] == "Ramicane") | 
                                      (pymaceuticals_clean["Drug Regimen"] == "Infubinol") | (pymaceuticals_clean["Drug Regimen"] == "Ceftamin")]

# Start by getting the last (greatest) timepoint for each mouse
regimens_df = regimens_df.groupby('Mouse ID').max()['Timepoint']
regimens_df.head()

# Merge this group df with the original dataframe to get the tumor volume at the last timepoint
#Come back and round
tumor_df = pd.merge(pymaceuticals_data, regimens_df, on=("Mouse ID","Timepoint"),how="right")
tumor_df
471/24:
# Put treatments into a list for for loop (and later for plot labels)
treatments = ['Capomulin', 'Ramicane', 'Infubinol', 'Ceftamin']

# Create empty list to fill with tumor vol data (for plotting)
tumor_vol_data = []

# Calculate the IQR and quantitatively determine if there are any potential outliers. 
# Locate the rows which contain mice on each drug and get the tumor volumes 
    # add subset 
    # Determine outliers using upper and lower bounds
    
    
for drug in treatments:
    tumor_vol = tumor_df.loc[tumor_df['Drug Regimen']==drug, 'Tumor Volume (mm3)']
    tumor_vol_data.append(tumor_vol)
    print(drug)
    quantiles = tumor_vol.quantile([.25, .5, .75])
    lowerq = quantiles[0.25]
    upperq = quantiles[0.75]
    iqr = upperq - lowerq
    
    lower_bound = lowerq - (1.5*iqr)
    upper_bound = upperq + (1.5*iqr)
    
    print(f"The lower quartile of is: {lowerq}")
    print(f"The upper quartile of is: {upperq}")
    print(f"The interquartile range is: {iqr}")
    print(f"The the median is: {quantiles[0.5]}")
    
    print(f"Values below {lower_bound} could be outliers.")
    print(f"Values above {upper_bound} could be outliers.")
    
    print(f"Outliers are: {tumor_vol.loc[(tumor_vol<lower_bound) | (tumor_vol>upper_bound)]}")
    print()
    print()
    print()
471/25:
# Generate a box plot of the final tumor volume of each mouse across four regimens of interest
fig1, ax1 = plt.subplots()
ax1.set_title('Final Tumor Volume of Each Mouse')
ax1.set_ylabel('Tumor Vol')
ax1.boxplot(tumor_vol_data, labels=treatments)
plt.show()
plt.savefig("Final Tumor Volume of Each Mouse - Box Plots.png")
471/26:
 # Generate a line plot of time point versus tumor volume for a mouse treated with Capomulin
vol_data = tumor_df[(tumor_df["Drug Regimen"] == "Capomulin") & (tumor_df["Mouse ID"] == "y793")]
vol_data = tumor_df.groupby(["Drug Regimen", "Timepoint"])
tumor_index_df = vol_data_df.unstack(0)
tumor_line_df = tumor_index_df["Tumor Volume (mm3)"]

timepoints = [0,5,10,15,20,25,30,35,40,45,50]

tumor_line_df.plot.line(timepoints, tumor_line_df["Capomulin"], marker= "o", color="green", label="Total Tumor Volume")
471/27:
 # Generate a line plot of time point versus tumor volume for a mouse treated with Capomulin
vol_data = tumor_df.loc[tumor_df["Drug Regimen"] == "Capomulin"]
vol_data = vol_data.reset_index()
vol_data.head()

# timepoints = [0,5,10,15,20,25,30,35,40,45,50]

# tumor_line_df.plot.line(timepoints, tumor_line_df["Capomulin"], marker= "o", color="green", label="Total Tumor Volume")
471/28:
 # Generate a line plot of time point versus tumor volume for a mouse treated with Capomulin
vol_data = tumor_df.loc[tumor_df["Drug Regimen"] == "Capomulin"]
vol_data = vol_data.reset_index()

mouse_df = vol_data.loc[vol_data["Mouse ID"] == "g288"]
mouse_df

# timepoints = [0,5,10,15,20,25,30,35,40,45,50]

# tumor_line_df.plot.line(timepoints, tumor_line_df["Capomulin"], marker= "o", color="green", label="Total Tumor Volume")
471/29:
#Locate the time point and tumor volume
mouse_df = mouse_df.loc[:, ["Timepoint", "Tumor Volume (mm3)"]]
mouse_df = mouse_df.reset_index(drop=True)

# Generate line plot
mouse_df.set_index('Timepoint').plot(figsize=(10, 8), linewidth=3, color='blue')
472/1: %matplotlib notebook
472/2:
# Dependencies
import matplotlib.pyplot as plt
import pandas as pd
import scipy.stats as st
import numpy as np
472/3:
# Files
mouse_metadata_path = "data/Mouse_metadata.csv"
study_results_path = "data/Study_results.csv"
472/4:
# Read the mouse data and the study results
mouse_metadata = pd.read_csv(mouse_metadata_path)
study_results = pd.read_csv(study_results_path)

mouse_metadata.head()
study_results.head()
472/5:
# Combine the data into a single dataset
pymaceuticals_data = pd.merge(mouse_metadata, study_results, on=["Mouse ID", "Mouse ID"])

# Display the data table for preview
pymaceuticals_data
472/6:
 # Checking the number of mice.
number_of_mice = len(pymaceuticals_data["Mouse ID"])
number_of_mice
472/7:
# Getting the duplicate mice by ID number that shows up for Mouse ID and Timepoint. 
duplicate_mice = pymaceuticals_data.duplicated(subset=['Mouse ID', 'Timepoint'])
duplicate_mice
472/8:
# Create a clean DataFrame by dropping the duplicate mouse by its ID.
pymaceuticals_clean = pymaceuticals_data.drop_duplicates(subset=['Mouse ID', 'Timepoint'],keep='first',inplace=False)
pymaceuticals_clean
472/9:
 # Checking the number of mice in the clean DataFrame.
total_mice = len(pymaceuticals_clean["Mouse ID"])
total_mice
472/10:
# Generate a summary statistics table of mean, median, variance, standard deviation, and SEM of the tumor volume for each regimen

# This method is the most straighforward, creating multiple series and putting them all together at the end.
grouped = pymaceuticals_clean.groupby('Drug Regimen')
summary_table = round(grouped.agg(['mean','median','var','std','sem'])["Tumor Volume (mm3)"], 2)
summary_table
472/11:
regimen = pymaceuticals_clean.groupby(['Drug Regimen']).count()['Mouse ID']
regimen
472/12: x_axis = np.arange(len(regimen))
472/13:
# Generate a bar plot showing the total number of mice for each treatment throughout the course of the study using pandas.
# tick locations horizontal?
plt.bar(x_axis, regimen, color='pink', alpha=1, align='center')

tick_locations = [value for value in x_axis]
plt.xticks(tick_locations, ['Capomulin', 'Ceftamin', 'Infubinol', 'Ketapril', 'Naftisol', 'Placebo', 'Propriva', 'Ramicane', 'Stelasyn', 'Zoniferol'], rotation='vertical')


plt.title("Total Mice Treated")
plt.xlabel("Drug Regimen")
plt.ylabel("Number of Mice")

plt.tight_layout()
plt.savefig("Total Mice Bar Plot.png")
plt.show()
472/14:
# Generate a pie plot showing the distribution of female versus male mice using pandas
gender = pymaceuticals_clean.groupby(["Mouse ID","Sex"])
gender
472/15:
gender_df = pd.DataFrame(gender.size())
gender_df
472/16:
gender_data = pd.DataFrame(gender_df.groupby(["Sex"]).count())
gender_data.columns = ["Total Count"]
gender_data
472/17:
gender_data["Gender by Percentage"] = round(100*(gender_data["Total Count"]/gender_data["Total Count"].sum()), 2)
gender_data
472/18:
labels = ["Female", "Male"]
colors = ["pink", "lightblue"]
explode = (0.1, 0)
472/19:
plot = gender_data.plot.pie(y='Total Count', explode=explode, labels=labels, colors=colors, autopct="%1.1f%%", shadow=True, startangle=140)
plt.show()
plt.savefig("Mice Male vs. Female Piechart.png")
472/20:
gender_dataframe = pd.DataFrame(pymaceuticals_clean.groupby(["Sex"]).count()).reset_index()
gender_dataframe.head()
472/21:
gender_dataframe = gender_dataframe[["Sex","Mouse ID"]]
gender_dataframe = gender_dataframe.rename(columns={"Mouse ID": "Total Count"})
gender_dataframe.head()
472/22:
plt.figure(figsize=(10,6))
ax1 = plt.subplot(121, aspect='equal')
gender_dataframe.plot(kind='pie', y = "Total Count", ax=ax1, autopct='%1.1f%%', 
startangle=90, shadow=False, labels=gender_dataframe['Sex'], legend = False)
472/23:
# Calculate the final tumor volume of each mouse across four of the treatment regimens:  
# Capomulin, Ramicane, Infubinol, and Ceftamin
regimens_df = pymaceuticals_clean.loc[(pymaceuticals_clean["Drug Regimen"] == "Capomulin") | (pymaceuticals_clean["Drug Regimen"] == "Ramicane") | 
                                      (pymaceuticals_clean["Drug Regimen"] == "Infubinol") | (pymaceuticals_clean["Drug Regimen"] == "Ceftamin")]

# Start by getting the last (greatest) timepoint for each mouse
regimens_df = regimens_df.groupby('Mouse ID').max()['Timepoint']
regimens_df.head()

# Merge this group df with the original dataframe to get the tumor volume at the last timepoint
#Come back and round
tumor_df = pd.merge(pymaceuticals_data, regimens_df, on=("Mouse ID","Timepoint"),how="right")
tumor_df
472/24:
# Put treatments into a list for for loop (and later for plot labels)
treatments = ['Capomulin', 'Ramicane', 'Infubinol', 'Ceftamin']

# Create empty list to fill with tumor vol data (for plotting)
tumor_vol_data = []

# Calculate the IQR and quantitatively determine if there are any potential outliers. 
# Locate the rows which contain mice on each drug and get the tumor volumes 
    # add subset 
    # Determine outliers using upper and lower bounds
    
    
for drug in treatments:
    tumor_vol = tumor_df.loc[tumor_df['Drug Regimen']==drug, 'Tumor Volume (mm3)']
    tumor_vol_data.append(tumor_vol)
    print(drug)
    quantiles = tumor_vol.quantile([.25, .5, .75])
    lowerq = quantiles[0.25]
    upperq = quantiles[0.75]
    iqr = upperq - lowerq
    
    lower_bound = lowerq - (1.5*iqr)
    upper_bound = upperq + (1.5*iqr)
    
    print(f"The lower quartile of is: {lowerq}")
    print(f"The upper quartile of is: {upperq}")
    print(f"The interquartile range is: {iqr}")
    print(f"The the median is: {quantiles[0.5]}")
    
    print(f"Values below {lower_bound} could be outliers.")
    print(f"Values above {upper_bound} could be outliers.")
    
    print(f"Outliers are: {tumor_vol.loc[(tumor_vol<lower_bound) | (tumor_vol>upper_bound)]}")
    print()
    print()
    print()
472/25:
# Generate a box plot of the final tumor volume of each mouse across four regimens of interest
fig1, ax1 = plt.subplots()
ax1.set_title('Final Tumor Volume of Each Mouse')
ax1.set_ylabel('Tumor Vol')
ax1.boxplot(tumor_vol_data, labels=treatments)
plt.show()
plt.savefig("Final Tumor Volume of Each Mouse - Box Plots.png")
472/26:
 # Generate a line plot of time point versus tumor volume for a mouse treated with Capomulin
    # Locate Capomulin and Mouse ID and put into new DF
vol_data = tumor_df.loc[tumor_df["Drug Regimen"] == "Capomulin"]
vol_data = vol_data.reset_index()

mouse_df = vol_data.loc[vol_data["Mouse ID"] == "g288"]
mouse_df
472/27:
#Locate the time point and tumor volume
mouse_df = mouse_df.loc[:, ["Timepoint", "Tumor Volume (mm3)"]]
mouse_df = mouse_df.reset_index(drop=True)

# Generate line plot
mouse_df.set_index('Timepoint').plot(figsize=(10, 8), linewidth=3, color='blue')
472/28: # Generate a scatter plot of mouse weight versus average tumor volume for the Capomulin regimen
472/29:
# Calculate the correlation coefficient and linear regression model 
# for mouse weight and average tumor volume for the Capomulin regimen
472/30:
#Locate the time point and tumor volume
mouse_df = mouse_df.loc[:, ["Timepoint", "Tumor Volume (mm3)"]]
mouse_df = mouse_df.reset_index(drop=True)

# Generate line plot
mouse_df.set_index('Timepoint').plot(figsize=(10, 8), linewidth=3, color='blue')
plt.savefig("Time Point VS. Tumor Volume - Line Plot.png")
472/31:
# Generate a scatter plot of mouse weight versus average tumor volume for the Capomulin regimen
vol_data.head()
472/32:
# Generate a scatter plot of mouse weight versus average tumor volume for the Capomulin regimen
vol_data_reduced = vol_data.loc[:, ["Mouse ID", "Weight (g)", "Tumor Volume (mm3)"]]
472/33:
# Generate a scatter plot of mouse weight versus average tumor volume for the Capomulin regimen
vol_data_reduced = vol_data.loc[:, ["Mouse ID", "Weight (g)", "Tumor Volume (mm3)"]]
vol_data_reduced
472/34:
# Generate a scatter plot of mouse weight versus average tumor volume for the Capomulin regimen
vol_data_reduced = vol_data.loc[:, ["Mouse ID", "Weight (g)", "Tumor Volume (mm3)"]]
vol_data_reduced

# Calculate average tumor volume for capomulin
avg_tumor_cap = pd.DataFrame(vol_data_reduced.groupby(["Mouse ID", "Weight (g)"])["Tumor Volume (mm3)"].mean()).reset_index()
avg_tumor_cap.head()
472/35:
# Generate a scatter plot of mouse weight versus average tumor volume for the Capomulin regimen
vol_data_reduced = vol_data.loc[:, ["Mouse ID", "Weight (g)", "Tumor Volume (mm3)"]]
vol_data_reduced

# Calculate average tumor volume for capomulin
avg_tumor_cap = pd.DataFrame(vol_data_reduced.groupby(["Mouse ID", "Weight (g)"])["Tumor Volume (mm3)"].mean()).reset_index()
avg_tumor_cap.head()

avg_tumor_cap = avg_tumor_cap.rename(columns={"Tumor Volume (mm3)": "Average Volume"})
472/36:
# Generate a scatter plot of mouse weight versus average tumor volume for the Capomulin regimen
vol_data_reduced = vol_data.loc[:, ["Mouse ID", "Weight (g)", "Tumor Volume (mm3)"]]
vol_data_reduced

# Calculate average tumor volume for capomulin
avg_tumor_cap = pd.DataFrame(vol_data_reduced.groupby(["Mouse ID", "Weight (g)"])["Tumor Volume (mm3)"].mean()).reset_index()
avg_tumor_cap = avg_tumor_cap.rename(columns={"Tumor Volume (mm3)": "Average Volume"})
avg_tumor_cap.head()
472/37:
# Generate a scatter plot of mouse weight versus average tumor volume for the Capomulin regimen
vol_data_reduced = vol_data.loc[:, ["Mouse ID", "Weight (g)", "Tumor Volume (mm3)"]]
vol_data_reduced

# Calculate average tumor volume for capomulin
avg_tumor_cap = pd.DataFrame(vol_data_reduced.groupby(["Mouse ID", "Weight (g)"])["Tumor Volume (mm3)"].mean()).reset_index()
avg_tumor_cap = avg_tumor_cap.rename(columns={"Tumor Volume (mm3)": "Average Volume"})
avg_tumor_cap = avg_tumor_cap.set_index('Mouse ID')
avg_tumor_cap.head()
472/38:
# Plot the scatter plot
avg_tumor_cap.plot(kind="scatter", x="Weight (g)", y="Average Volume",figsize=(4,4),
              title="Mouse Weight VS. Average Tumor Volume (Capomulin Regimen)", marker="o")
plt.show()
472/39:
# Plot the scatter plot
avg_tumor_cap.plot(kind="scatter", x="Weight (g)", y="Average Volume",figsize=(4,4),
              title="Mouse Weight VS. Average Tumor Volume (Capomulin Regimen)", marker="o")
plt.show()
plt.savefig("Mouse Weight VS. Average Tumor Volume (Capomulin Regimen) - Scatter Plot.png")
472/40:
# Calculate the correlation coefficient and linear regression model 
# for mouse weight and average tumor volume for the Capomulin regimen
weight = avg_vol_cap.iloc[:,0]
tumor_vol = avg_vol_cap.iloc[:,1]
correlation = st.pearsonr(mouse_weight,avg_tumor_volume)
print({round(correlation[0],2)})
472/41:
# Calculate the correlation coefficient and linear regression model 
# for mouse weight and average tumor volume for the Capomulin regimen
weight = avg_vol_cap.iloc[:,0]
tumor_vol = avg_vol_cap.iloc[:,1]
correlation = st.pearsonr(weight,tumor_vol)
print({round(correlation[0],2)})
472/42:
# Calculate the correlation coefficient and linear regression model 
# for mouse weight and average tumor volume for the Capomulin regimen
weight = avg_tumor_cap.iloc[:,0]
tumor_vol = avg_tumor_cap.iloc[:,1]
correlation = st.pearsonr(weight,tumor_vol)
print({round(correlation[0],2)})
472/43:
# Calculate the correlation coefficient and linear regression model 
# for mouse weight and average tumor volume for the Capomulin regimen
weight = avg_tumor_cap.iloc[:,0]
tumor_vol = avg_tumor_cap.iloc[:,1]
correlation = st.pearsonr(weight,tumor_vol)
print({correlation[0]})
472/44:
# Calculate the correlation coefficient and linear regression model 
# for mouse weight and average tumor volume for the Capomulin regimen
weight = avg_tumor_cap.iloc[:,0]
tumor_vol = avg_tumor_cap.iloc[:,1]
correlation = st.pearsonr(weight,tumor_vol)
print(f"The Capomulin regimen has a strong positive correlation coefficient of:{correlation[0]}")
472/45:
# Calculate the correlation coefficient and linear regression model 
# for mouse weight and average tumor volume for the Capomulin regimen
weight = avg_tumor_cap.iloc[:,0]
tumor_vol = avg_tumor_cap.iloc[:,1]
correlation = st.pearsonr(weight,tumor_vol)
print(f"The Capomulin regimen has a strong positive correlation coefficient of: {correlation[0]}")
472/46:
# Calculate the correlation coefficient and linear regression model 
# for mouse weight and average tumor volume for the Capomulin regimen
weight = avg_tumor_cap.iloc[:,0]
tumor_vol = avg_tumor_cap.iloc[:,1]
correlation = st.pearsonr(weight,tumor_vol)
print(f"The Capomulin regimen has a strong positive correlation coefficient of: {correlation[0]}")
472/47:
x_values = avg_tumor_cap['Weight (g)']
y_values = avg_tumor_cap['Average Volume']
(slope, intercept, rvalue, pvalue, stderr) = linregress(x_values, y_values)
regress_values = x_values * slope + intercept
line_eq = "y = " + str(round(slope,2)) + "x + " + str(round(intercept,2))
plt.scatter(x_values,y_values)
plt.plot(x_values,regress_values,"r-")
plt.annotate(line_eq,(6,10),fontsize=15,color="red")
plt.xlabel('Mouse Weight')
plt.ylabel('Average Tumor Volume')
plt.show()
472/48:
x_values = avg_tumor_cap['Weight (g)']
y_values = avg_tumor_cap['Average Volume']
(slope, intercept, rvalue, pvalue, stderr) = lineregress(x_values, y_values)
regress_values = x_values * slope + intercept
line_eq = "y = " + str(round(slope,2)) + "x + " + str(round(intercept,2))
plt.scatter(x_values,y_values)
plt.plot(x_values,regress_values,"r-")
plt.annotate(line_eq,(6,10),fontsize=15,color="red")
plt.xlabel('Mouse Weight')
plt.ylabel('Average Tumor Volume')
plt.show()
472/49:
x_values = avg_tumor_cap['Weight (g)']
y_values = avg_tumor_cap['Average Volume']
(slope, intercept, rvalue, pvalue, stderr) = stats.linregress(x_values, y_values)
regress_values = x_values * slope + intercept
line_eq = "y = " + str(round(slope,2)) + "x + " + str(round(intercept,2))
plt.scatter(x_values,y_values)
plt.plot(x_values,regress_values,"r-")
plt.annotate(line_eq,(6,10),fontsize=15,color="red")
plt.xlabel('Mouse Weight')
plt.ylabel('Average Tumor Volume')
plt.show()
472/50:
x_values = avg_tumor_cap['Weight (g)']
y_values = avg_tumor_cap['Average Volume']
(slope, intercept, rvalue, pvalue, stderr) = st.linregress(x_values, y_values)
regress_values = x_values * slope + intercept
line_eq = "y = " + str(round(slope,2)) + "x + " + str(round(intercept,2))
plt.scatter(x_values,y_values)
plt.plot(x_values,regress_values,"r-")
plt.annotate(line_eq,(6,10),fontsize=15,color="red")
plt.xlabel('Mouse Weight')
plt.ylabel('Average Tumor Volume')
plt.show()
472/51:
x_values = avg_tumor_cap['Weight (g)']
y_values = avg_tumor_cap['Average Volume']
(slope, intercept, rvalue, pvalue, stderr) = st.linregress(x_values, y_values)
regress_values = x_values * slope + intercept
line_eq = "y = " + str(round(slope,2)) + "x + " + str(round(intercept,2))
plt.scatter(x_values,y_values)
plt.plot(x_values,regress_values,"r-")
plt.annotate(line_eq,(6,10),fontsize=15,color="red")
plt.xlabel('Mouse Weight')
plt.ylabel('Average Tumor Volume')
plt.show()
476/1:
import citipy
import random
from config import api_key

url = "https://pypi.python.org/pypi/citipy"
query_url = f"{url}appid={api_key}&units={units}&q="
476/2:
import citipy
import random
import matplotlib.pyplot as plt
import requests
from scipy import stats
import pandas as pd
from config import api_key

url = "https://pypi.python.org/pypi/citipy"
query_url = f"{url}appid={api_key}&units={units}&q="
476/3:
import citipy
import random
# import matplotlib.pyplot as plt
# import requests
# from scipy import stats
# import pandas as pd
# from config import api_key

url = "https://pypi.python.org/pypi/citipy"
query_url = f"{url}appid={api_key}&units={units}&q="
476/4:
import citipy
import random
import requests
import pandas as pd
from config import api_key

url = "https://pypi.python.org/pypi/citipy"
query_url = f"{url}appid={api_key}&units={units}&q="
476/5:
import pandas as pd
import json
import requests
import matplotlib.pyplot as plt
import numpy as np
import random
from config import api_key
from citipy import citipy

url = "https://pypi.python.org/pypi/citipy"
query_url = f"{url}appid={api_key}&units={units}&q="
476/6:
import pandas as pd
import json
import requests
import matplotlib.pyplot as plt
import numpy as np
import random
from config import api_key
import citipy

url = "https://pypi.python.org/pypi/citipy"
query_url = f"{url}appid={api_key}&units={units}&q="
476/7:
import pandas as pd
import json
import requests
import matplotlib.pyplot as plt
import numpy as np
import random
from config import api_key
import citipy

url = "https://pypi.python.org/pypi/citipy"
query_url = f"{url}appid={api_key}&units={units}&q="
479/1:
import pandas as pd
import json
import requests
import matplotlib.pyplot as plt
import numpy as np
import random
from config import api_key
import citipy

url = "https://pypi.python.org/pypi/citipy"
query_url = f"{url}appid={api_key}&units={units}&q="
479/2:
import pandas as pd
import json
import requests
import matplotlib.pyplot as plt
import numpy as np
import random
from config.py import api_key
import citipy

url = "https://pypi.python.org/pypi/citipy"
query_url = f"{url}appid={api_key}&units={units}&q="
479/3:
import pandas as pd
import json
import requests
import matplotlib.pyplot as plt
import numpy as np
import random
from config import api_key
import citipy

url = "https://pypi.python.org/pypi/citipy"
query_url = f"{url}appid={api_key}&units={units}&q="
480/1:
import pandas as pd
import json
import requests
import matplotlib.pyplot as plt
import numpy as np
import random
from config import api_key
import citipy

url = "https://pypi.python.org/pypi/citipy"
query_url = f"{url}appid={api_key}&units={units}&q="
480/2:
import pandas as pd
import json
import requests
import matplotlib.pyplot as plt
import numpy as np
import random
from config.py import api_key
import citipy

url = "https://pypi.python.org/pypi/citipy"
query_url = f"{url}appid={api_key}&units={units}&q="
480/3:
import pandas as pd
import json
import requests
import matplotlib.pyplot as plt
import numpy as np
import random
from config import api_key
import citipy

url = "https://pypi.python.org/pypi/citipy"
query_url = f"{url}appid={api_key}&units={units}&q="
481/1:
import pandas as pd
import json
import requests
import matplotlib.pyplot as plt
import numpy as np
import random
from config import api_key
import citipy

url = "https://pypi.python.org/pypi/citipy"
query_url = f"{url}appid={api_key}&units={units}&q="
481/2:
import pandas as pd
import json
import requests
import matplotlib.pyplot as plt
import numpy as np
import random
import config
# from config import api_key
import citipy

url = "https://pypi.python.org/pypi/citipy"
query_url = f"{url}appid={api_key}&units={units}&q="
481/3:
import pandas as pd
import json
import requests
import matplotlib.pyplot as plt
import numpy as np
import random
import config
# from config import api_key
import citipy
dir(config)

# url = "https://pypi.python.org/pypi/citipy"
# query_url = f"{url}appid={api_key}&units={units}&q="
481/4:
import pandas as pd
import json
import requests
import matplotlib.pyplot as plt
import numpy as np
import random
import config
from config import weather_api_key
import citipy
dir(config)

# url = "https://pypi.python.org/pypi/citipy"
# query_url = f"{url}appid={api_key}&units={units}&q="
481/5:
import pandas as pd
import json
import requests
import matplotlib.pyplot as plt
import numpy as np
import random
import config
from config import weather_api_key
import citipy

url = "https://pypi.python.org/pypi/citipy"
query_url = f"{url}appid={api_key}&units={units}&q="
481/6:
import pandas as pd
import json
import requests
import matplotlib.pyplot as plt
import numpy as np
import random
import config
from config import weather_api_key
import citipy

url = "https://pypi.python.org/pypi/citipy"
query_url = f"{url}appid={weather_api_key}&units={units}&q="
481/7:
import pandas as pd
import json
import requests
import matplotlib.pyplot as plt
import numpy as np
import random
import config
from config import weather_api_key
import citipy

url = "https://pypi.python.org/pypi/citipy"
units = "metric"
query_url = f"{url}appid={weather_api_key}&units={units}&q="
482/1:
cities = []
lat = []

lats = np.random(low=-90.00, high=90.00, size=1500)
lngs = np.random(low=-180.00, high=180.00, size=1500)
lat = zip(lats, lngs)

for lat in lat:
    city = citipy.nearest_city(lat[0], lat[1]).city_name
    
    if city not in cities:
        cities.append(city)
        
len(cities)

# To slice the list in for loop do for value in x[0:3]
492/1:
import pandas as pd
import json
import requests
import matplotlib.pyplot as plt
import numpy as np
import random
import config
from config import weather_api_key
import citipy

url = "https://pypi.python.org/pypi/citipy"
units = "metric"
query_url = f"{url}appid={weather_api_key}&units={units}&q="
492/2:
cities = []
lat = []

lats = np.random(low=-90.00, high=90.00, size=1500)
lngs = np.random(low=-180.00, high=180.00, size=1500)
lat = zip(lats, lngs)

for lat in lat:
    city = citipy.nearest_city(lat[0], lat[1]).city_name
    
    if city not in cities:
        cities.append(city)
        
len(cities)

# To slice the list in for loop do for value in x[0:3]
492/3:
cities = []
lat = []

lats = np.random(low=-90.00, high=90.00, size=1500)
lngs = np.random(low=-180.00, high=180.00, size=1500)
lat = zip(lats, lngs)

for lat in lat:
    city = citipy.nearest_city(lat[0], lat[1]).city_name
    
    if city not in cities:
        cities.append(city)

# To slice the list in for loop do for value in x[0:3]
492/4:
cities = []
lat = []

lats = np.random(low=-90, high=90, size=1500)
lngs = np.random(low=-180, high=180, size=1500)
lat = zip(lats, lngs)

for lat in lat:
    city = citipy.nearest_city(lat[0], lat[1]).city_name
    
    if city not in cities:
        cities.append(city)

# To slice the list in for loop do for value in x[0:3]
492/5:
cities = []
latitude_longitude = []

lat = np.random(low=-90, high=90, size=1500)
lng = np.random(low=-180, high=180, size=1500)
latitude_longitude = zip(lat, lng)

for latitude_longitude in latitude_longitude:
    city = citipy.nearest_city(lat[0], lat[1]).city_name
    
    if city not in cities:
        cities.append(city)

# To slice the list in for loop do for value in x[0:3]
492/6: %pip install random
493/1:
import pandas as pd
import json
import requests
import matplotlib.pyplot as plt
import numpy as np
import random
import config
from config import weather_api_key
import citipy

url = "https://pypi.python.org/pypi/citipy"
units = "metric"
query_url = f"{url}appid={weather_api_key}&units={units}&q="
493/2: %pip install random
493/3:
cities = []
latitude_longitude = []

lat = np.random(low=-90, high=90, size=1500)
lng = np.random(low=-180, high=180, size=1500)
latitude_longitude = zip(lat, lng)

for latitude_longitude in latitude_longitude:
    city = citipy.nearest_city(lat[0], lat[1]).city_name
    
    if city not in cities:
        cities.append(city)

# To slice the list in for loop do for value in x[0:3]
493/4:
import pandas as pd
import json
import requests
import matplotlib.pyplot as plt
import numpy as np
import random
import config
from config import weather_api_key
import citipy

url = "https://pypi.python.org/pypi/citipy"
units = "metric"
query_url = f"{url}appid={weather_api_key}&units={units}&q="
493/5:
cities = []
latitude_longitude = []

lat = np.random(low=-90, high=90, size=1500)
lng = np.random(low=-180, high=180, size=1500)
latitude_longitude = zip(lat, lng)

for latitude_longitude in latitude_longitude:
    city = citipy.nearest_city(lat[0], lat[1]).city_name
    
    if city not in cities:
        cities.append(city)

# To slice the list in for loop do for value in x[0:3]
493/6:
cities = []
latitude_longitude = []

cities = []

while len(cities)<600:
    x = (random.choice(range(-90,90))) # randomly generate numbers in range -90 & 90 for latitude
    y = (random.choice(range(-180,180))) # randomly generagte numbers in range -180 & 180 for longitude
    city = citipy.nearest_city(x,y).city_name # look up city to match the lat & long randomly generated
    if city not in cities:
        cities.append(city)

# lat = np.random(low=-90, high=90, size=1500)
# lng = np.random(low=-180, high=180, size=1500)
# latitude_longitude = zip(lat, lng)

# for latitude_longitude in latitude_longitude:
#     city = citipy.nearest_city(lat[0], lat[1]).city_name
    
#     if city not in cities:
#         cities.append(city)

# To slice the list in for loop do for value in x[0:3]
494/1:
import pandas as pd
import json
import requests
import matplotlib.pyplot as plt
import numpy as np
import random
import config
from config import weather_api_key
import citipy

url = "https://pypi.python.org/pypi/citipy"
units = "metric"
query_url = f"{url}appid={weather_api_key}&units={units}&q="
494/2:
cities = []
latitude_longitude = []

cities = []

while len(cities)<600:
    x = (random.choice(range(-90,90))) # randomly generate numbers in range -90 & 90 for latitude
    y = (random.choice(range(-180,180))) # randomly generagte numbers in range -180 & 180 for longitude
    city = citipy.nearest_city(x,y).city_name # look up city to match the lat & long randomly generated
    if city not in cities:
        cities.append(city)

# lat = np.random(low=-90, high=90, size=1500)
# lng = np.random(low=-180, high=180, size=1500)
# latitude_longitude = zip(lat, lng)

# for latitude_longitude in latitude_longitude:
#     city = citipy.nearest_city(lat[0], lat[1]).city_name
    
#     if city not in cities:
#         cities.append(city)

# To slice the list in for loop do for value in x[0:3]
494/3: lat
494/4:
import pandas as pd
import json
import requests
import matplotlib.pyplot as plt
import numpy as np
import random
import config
from config import weather_api_key
import citipy

url = "https://pypi.python.org/pypi/citipy"
units = "metric"
query_url = f"{url}appid={weather_api_key}&units={units}&q="
494/5:
cities = []
latitude_longitude = []

lat = np.random(low=-90, high=90, size=1500)
lng = np.random(low=-180, high=180, size=1500)
latitude_longitude = zip(lat, lng)

# for latitude_longitude in latitude_longitude:
#     city = citipy.nearest_city(lat[0], lat[1]).city_name
    
#     if city not in cities:
#         cities.append(city)

# To slice the list in for loop do for value in x[0:3]
494/6: lat
496/1:
import pandas as pd
import json
import requests
import matplotlib.pyplot as plt
import numpy as np
import random
import config
from config import weather_api_key
import citipy

url = "https://pypi.python.org/pypi/citipy"
units = "metric"
query_url = f"{url}appid={weather_api_key}&units={units}&q="
496/2:
cities = []
latitude_longitude = []

lat = np.random(low=-90, high=90, size=1500)
lng = np.random(low=-180, high=180, size=1500)
latitude_longitude = zip(lat, lng)

# for latitude_longitude in latitude_longitude:
#     city = citipy.nearest_city(lat[0], lat[1]).city_name
    
#     if city not in cities:
#         cities.append(city)

# To slice the list in for loop do for value in x[0:3]
496/3:
cities = []
latitude_longitude = []

lat = np.random(low=-90, high=90, size=1500)
# lng = np.random(low=-180, high=180, size=1500)
# latitude_longitude = zip(lat, lng)

# for latitude_longitude in latitude_longitude:
#     city = citipy.nearest_city(lat[0], lat[1]).city_name
    
#     if city not in cities:
#         cities.append(city)

# To slice the list in for loop do for value in x[0:3]
496/4: lat
496/5:
import pandas as pd
import json
import requests
import matplotlib.pyplot as plt
import numpy as np
import random
import config
from config import weather_api_key
import citipy

# url = "https://pypi.python.org/pypi/citipy"
# units = "metric"
# query_url = f"{url}appid={weather_api_key}&units={units}&q="
496/6:
cities = []
latitude_longitude = []

lat = np.random(low=-90, high=90, size=1500)
# lng = np.random(low=-180, high=180, size=1500)
# latitude_longitude = zip(lat, lng)

# for latitude_longitude in latitude_longitude:
#     city = citipy.nearest_city(lat[0], lat[1]).city_name
    
#     if city not in cities:
#         cities.append(city)

# To slice the list in for loop do for value in x[0:3]
496/7: lat
496/8:
import pandas as pd
import json
import requests
import matplotlib.pyplot as plt
import numpy as np
import random
import math
import config
from config import weather_api_key
import citipy

# url = "https://pypi.python.org/pypi/citipy"
# units = "metric"
# query_url = f"{url}appid={weather_api_key}&units={units}&q="
496/9:
cities = []
latitude_longitude = []

lat = np.random(low=-90, high=90, size=1500)
# lng = np.random(low=-180, high=180, size=1500)
# latitude_longitude = zip(lat, lng)

# for latitude_longitude in latitude_longitude:
#     city = citipy.nearest_city(lat[0], lat[1]).city_name
    
#     if city not in cities:
#         cities.append(city)

# To slice the list in for loop do for value in x[0:3]
496/10:
import pandas as pd
import json
import requests
import matplotlib.pyplot as plt
import numpy as np
import random
import math
import config
from config import weather_api_key
import citipy

# url = "https://pypi.python.org/pypi/citipy"
# units = "metric"
# query_url = f"{url}appid={weather_api_key}&units={units}&q="
496/11:
cities = []
latitude_longitude = []

lat = np.random(low=-90, high=90, size=1500)
# lng = np.random(low=-180, high=180, size=1500)
# latitude_longitude = zip(lat, lng)

# for latitude_longitude in latitude_longitude:
#     city = citipy.nearest_city(lat[0], lat[1]).city_name
    
#     if city not in cities:
#         cities.append(city)

# To slice the list in for loop do for value in x[0:3]
496/12:
import pandas as pd
import json
import requests
import matplotlib.pyplot as plt
import numpy as np
import random
import math
import config
from config import weather_api_key
import citipy

# url = "https://pypi.python.org/pypi/citipy"
# units = "metric"
# query_url = f"{url}appid={weather_api_key}&units={units}&q="
496/13:
cities = []
latitude_longitude = []

lat = np.random(low=-90, high=90, size=1500)
# lng = np.random(low=-180, high=180, size=1500)
# latitude_longitude = zip(lat, lng)

# for latitude_longitude in latitude_longitude:
#     city = citipy.nearest_city(lat[0], lat[1]).city_name
    
#     if city not in cities:
#         cities.append(city)

# To slice the list in for loop do for value in x[0:3]
496/14: lat
496/15:
import pandas as pd
import json
import requests
import matplotlib.pyplot as plt
import numpy as np
import random
import os
import math
import config
from config import weather_api_key
import citipy

# url = "https://pypi.python.org/pypi/citipy"
# units = "metric"
# query_url = f"{url}appid={weather_api_key}&units={units}&q="
496/16:
cities = []
latitude_longitude = []

lat = np.random(low=-90, high=90, size=1500)
# lng = np.random(low=-180, high=180, size=1500)
# latitude_longitude = zip(lat, lng)

# for latitude_longitude in latitude_longitude:
#     city = citipy.nearest_city(lat[0], lat[1]).city_name
    
#     if city not in cities:
#         cities.append(city)

# To slice the list in for loop do for value in x[0:3]
496/17: lat
497/1:
import pandas as pd
import json
import requests
import matplotlib.pyplot as plt
import numpy as np
import random
import os
import math
import config
from config import weather_api_key
import citipy

# url = "https://pypi.python.org/pypi/citipy"
# units = "metric"
# query_url = f"{url}appid={weather_api_key}&units={units}&q="
497/2:
cities = []
latitude_longitude = []

lat = np.random(low=-90, high=90, size=1500)
# lng = np.random(low=-180, high=180, size=1500)
# latitude_longitude = zip(lat, lng)

# for latitude_longitude in latitude_longitude:
#     city = citipy.nearest_city(lat[0], lat[1]).city_name
    
#     if city not in cities:
#         cities.append(city)

# To slice the list in for loop do for value in x[0:3]
497/3:
cities = []
latitude_longitude = []

lat = np.random(low=-90, high=90, size=1500)
lng = np.random(low=-180, high=180, size=1500)
latitude_longitude = zip(lat, lng)

for latitude_longitude in latitude_longitude:
     city = citipy.nearest_city(lat[0], lat[1]).city_name
        
        if city not in cities:
        cities.append(city)

# To slice the list in for loop do for value in x[0:3]
497/4:
cities = []
latitude_longitude = []

lat = np.random(low=-90, high=90, size=1500)
lng = np.random(low=-180, high=180, size=1500)
latitude_longitude = zip(lat, lng)

for latitude_longitude in latitude_longitude:
    city = citipy.nearest_city(lat[0], lat[1]).city_name
    
    if city not in cities:
    cities.append(city)

# To slice the list in for loop do for value in x[0:3]
497/5: lat
497/6:
import pandas as pd
import json
import requests
import matplotlib.pyplot as plt
import numpy as np
import config
from config import weather_api_key
import citipy

# url = "https://pypi.python.org/pypi/citipy"
# units = "metric"
# query_url = f"{url}appid={weather_api_key}&units={units}&q="
497/7:
cities = []
latitude_longitude = []

lat = np.random(low=-90, high=90, size=1500)
lng = np.random(low=-180, high=180, size=1500)
latitude_longitude = zip(lat, lng)

for latitude_longitude in latitude_longitude:
    city = citipy.nearest_city(lat[0], lat[1]).city_name
    
    if city not in cities:
    cities.append(city)

# To slice the list in for loop do for value in x[0:3]
497/8: lat
497/9:
cities = []
latitude_longitude = []

lat = np.random(low=-90, high=90, size=1500)
lng = np.random(low=-180, high=180, size=1500)
latitude_longitude = zip(lat, lng)

for latitude_longitude in latitude_longitude:
    city = citipy.nearest_city(lat[0], lat[1]).city_name
    
    if city not in cities:
        cities.append(city)

# To slice the list in for loop do for value in x[0:3]
497/10:
import pandas as pd
import json
import requests
import matplotlib.pyplot as plt
import numpy as np
import config
from config import weather_api_key
import citipy

# url = "https://pypi.python.org/pypi/citipy"
# units = "metric"
# query_url = f"{url}appid={weather_api_key}&units={units}&q="
497/11:
cities = []
latitude_longitude = []

lat = np.random(low=-90, high=90, size=1500)
lng = np.random(low=-180, high=180, size=1500)
latitude_longitude = zip(lat, lng)

for latitude_longitude in latitude_longitude:
    city = citipy.nearest_city(lat[0], lat[1]).city_name
    
    if city not in cities:
        cities.append(city)

# To slice the list in for loop do for value in x[0:3]
497/12: lat
497/13:
cities = []
latitude_longitude = []

lat = np.random(low=-90, high=90, size=1500)
lng = np.random(low=-180, high=180, size=1500)
latitude_longitude = zip(lat, lng)

for latitude_longitude in latitude_longitude:
    city = citipy.nearest_city(lat[0], lat[1]).city_name
    
    if city not in cities:
        cities.append(city)

# To slice the list in for loop do for value in x[0:3]
497/14:
cities = []
latitude_longitude = []

lat = np.random(low=-90, high=90, size=1500)
lng = np.random(low=-180, high=180, size=1500)
latitude_longitude = zip(lat, lng)

for latitude_longitude in latitude_longitude:
    city = citipy.nearest_city(lat[0], lat[1]).city_name
    
    if city not in cities:
        cities.append(city)
        
        
len(cities)

# To slice the list in for loop do for value in x[0:3]
497/15:
import pandas as pd
import json
import requests
import matplotlib.pyplot as plt
import numpy as np
import config
from config import weather_api_key
import citipy

url = "https://pypi.python.org/pypi/citipy"
units = "metric"
query_url = f"{url}appid={weather_api_key}&units={units}&q="
497/16:
cities = []
latitude_longitude = []

coords = np.random.rand(10000, 2) * 2
latitude_longitude = zip(coords)

for latitude_longitude in latitude_longitude:
    city = citipy.nearest_city(coords[0], coords[1]).city_name
    
    if city not in cities:
        cities.append(city)
        
        
len(cities)

# To slice the list in for loop do for value in x[0:3]
497/17:
cities = []
latitude_longitude = []

coords = np.random.rand(10000, 2) * 2
latitude_longitude = zip(coords)

for latitude_longitude in latitude_longitude:
    city = citip(coords[0], coords[1]).city_name
    
    if city not in cities:
        cities.append(city)
        
        
len(cities)

# To slice the list in for loop do for value in x[0:3]
497/18:
cities = []
latitude_longitude = []

coords = np.random.rand(10000, 2) * 2
latitude_longitude = zip(coords)

for latitude_longitude in latitude_longitude:
    city = citipy(coords[0], coords[1]).city_name
    
    if city not in cities:
        cities.append(city)
        
        
len(cities)

# To slice the list in for loop do for value in x[0:3]
498/1:
import pandas as pd
import json
import requests
import matplotlib.pyplot as plt
import numpy as np
import config
import os
from config import weather_api_key
import citipy

url = "https://pypi.python.org/pypi/citipy"
units = "metric"
query_url = f"{url}appid={weather_api_key}&units={units}&q="
498/2:
cities = []
latitude_longitude = []

coords_lat = np.random.uniform(low=-90, high=90, size=1500)
coords_lng = np.random.uniform(low=-180, high=180, size=1500)
latitude_longitude = zip(coords_lat, coords_lng)

for latitude_longitude in latitude_longitude:
    city = citipy.nearest_city(latitude_longitude[0], latitude_longitude[1]).city_name
    
    if city not in cities:
        cities.append(city)
        
        
len(cities)

# To slice the list in for loop do for value in x[0:3]
498/3:
import pandas as pd
import json
import requests
import matplotlib.pyplot as plt
import numpy as np
import config
import os
from config import weather_api_key
import citipy

url = "https://pypi.python.org/pypi/citipy"
units = "metric"
query_url = f"{url}appid={weather_api_key}&units={units}&q="
498/4:
cities = []
latitude_longitude = []

coords_lat = np.random.uniform(low=-90, high=90, size=1500)
coords_lng = np.random.uniform(low=-180, high=180, size=1500)


for latitude_longitude in latitude_longitude:
    city = citipy.nearest_city(coords_lat,coords_lng).city_name
    
    if city not in cities:
        cities.append(city)
        
        
len(cities)

# To slice the list in for loop do for value in x[0:3]
498/5:
cities = []
latitude_longitude = []

coords_lat = np.random.uniform(low=-90, high=90, size=1500)
coords_lng = np.random.uniform(low=-180, high=180, size=1500)


for latitude_longitude in latitude_longitude:
    city = citipy.nearest_city(coords_lat[0],coords_lng[1]).city_name
    
    if city not in cities:
        cities.append(city)
        
        
len(cities)

# To slice the list in for loop do for value in x[0:3]
498/6:
cities = []
latitude_longitude = []

coords_lat = np.random.uniform(low=-90, high=90, size=1500)
coords_lng = np.random.uniform(low=-180, high=180, size=1500)


for latitude_longitude in latitude_longitude:
    city = citipy.nearest_city(coords_lat[0],coords_lng[1]).city_name
    
    if city not in cities:
        cities.append(city)
        
        
len(cities)

# To slice the list in for loop do for value in x[0:3]
498/7: lat
498/8:
import pandas as pd
import json
import requests
import matplotlib.pyplot as plt
import numpy as np
import config
import os
from config import weather_api_key
import citipy

url = "https://pypi.python.org/pypi/citipy"
units = "metric"
query_url = f"{url}appid={weather_api_key}&units={units}&q="
498/9:
cities = []
latitude_longitude = []

coords_lat = np.random.uniform(low=-90, high=90, size=1500)
coords_lng = np.random.uniform(low=-180, high=180, size=1500)


for latitude_longitude in latitude_longitude:
    city = citipy.nearest_city(coords_lat[0],coords_lng[1]).city_name
    
    if city not in cities:
        cities.append(city)
        
        
len(cities)

# To slice the list in for loop do for value in x[0:3]
498/10:
cities = []
latitude_longitude = []

coords_lat = np.random.uniform(low=-90, high=90, size=1500)
coords_lng = np.random.uniform(low=-180, high=180, size=1500)


for latitude_longitude in latitude_longitude:
    city = citipy.nearest_city(coords_lat[0],coords_lng[1]).city_name
    
    if city not in cities:
        cities.append(city)
        
        
len(cities)

# To slice the list in for loop do for value in x[0:3]
498/11:
cities = []

while len(cities)<600:
    lat = (random.choice(range(-90,90))) # randomly generate numbers in range -90 & 90 for latitude
    lng = (random.choice(range(-180,180))) # randomly generagte numbers in range -180 & 180 for longitude
    city = citipy.nearest_city(lat,lng).city_name # look up city to match the lat & long randomly generated
    if city not in cities:
        cities.append(city)
        
# coords_lat = np.random.uniform(low=-90, high=90, size=1500)
# coords_lng = np.random.uniform(low=-180, high=180, size=1500)


# for latitude_longitude in latitude_longitude:
#     city = citipy.nearest_city(coords_lat[0],coords_lng[1]).city_name
    
#     if city not in cities:
#         cities.append(city)
        
        
len(cities)

# To slice the list in for loop do for value in x[0:3]
498/12:
import pandas as pd
import json
import requests
import matplotlib.pyplot as plt
import numpy as np
import config
import os
from config import weather_api_key
import citipy

url = "https://pypi.python.org/pypi/citipy"
units = "metric"
query_url = f"{url}appid={weather_api_key}&units={units}&q="
498/13:
import random
cities = []

while len(cities)<600:
    lat = (random.choice(range(-90,90))) # randomly generate numbers in range -90 & 90 for latitude
    lng = (random.choice(range(-180,180))) # randomly generagte numbers in range -180 & 180 for longitude
    city = citipy.nearest_city(lat,lng).city_name # look up city to match the lat & long randomly generated
    if city not in cities:
        cities.append(city)
        
# coords_lat = np.random.uniform(low=-90, high=90, size=1500)
# coords_lng = np.random.uniform(low=-180, high=180, size=1500)


# for latitude_longitude in latitude_longitude:
#     city = citipy.nearest_city(coords_lat[0],coords_lng[1]).city_name
    
#     if city not in cities:
#         cities.append(city)
        
        
len(cities)

# To slice the list in for loop do for value in x[0:3]
499/1:
import pandas as pd
import json
import requests
import matplotlib.pyplot as plt
import numpy as np
import config
import os
from config import weather_api_key
import citipy

url = "https://pypi.python.org/pypi/citipy"
units = "metric"
query_url = f"{url}appid={weather_api_key}&units={units}&q="
499/2:
import random
cities = []

while len(cities)<600:
    lat = (random.choice(range(-90,90))) # randomly generate numbers in range -90 & 90 for latitude
    lng = (random.choice(range(-180,180))) # randomly generagte numbers in range -180 & 180 for longitude
    city = citipy.nearest_city(lat,lng).city_name # look up city to match the lat & long randomly generated
    if city not in cities:
        cities.append(city)
        
# coords_lat = np.random.uniform(low=-90, high=90, size=1500)
# coords_lng = np.random.uniform(low=-180, high=180, size=1500)


# for latitude_longitude in latitude_longitude:
#     city = citipy.nearest_city(coords_lat[0],coords_lng[1]).city_name
    
#     if city not in cities:
#         cities.append(city)
        
        
len(cities)

# To slice the list in for loop do for value in x[0:3]
499/3:
import random
cities = []
        
coords_lat = np.random.uniform(low=-90, high=90, size=1500)
coords_lng = np.random.uniform(low=-180, high=180, size=1500)

for latitude_longitude in latitude_longitude:
    city = citipy.nearest_city(coords_lat[0],coords_lng[1]).city_name
    
    if city not in cities:
        cities.append(city)
        
        
len(cities)

# To slice the list in for loop do for value in x[0:3]
499/4:
import random
cities = []
latitude_longitude = []
        
coords_lat = np.random.uniform(low=-90, high=90, size=1500)
coords_lng = np.random.uniform(low=-180, high=180, size=1500)

for latitude_longitude in latitude_longitude:
    city = citipy.nearest_city(coords_lat[0],coords_lng[1]).city_name
    
    if city not in cities:
        cities.append(city)
        
        
len(cities)

# To slice the list in for loop do for value in x[0:3]
499/5:
import random
cities = []
latitude_longitude = []
        
coords_lat = np.random.uniform(low=-90, high=90, size=1500)
coords_lng = np.random.uniform(low=-180, high=180, size=1500)
latitude_longitude = zim(coords_lat, coords_lng)

for latitude_longitude in latitude_longitude:
    city = citipy.nearest_city(coords_lat[0],coords_lng[1]).city_name
    
    if city not in cities:
        cities.append(city)
        
        
len(cities)

# To slice the list in for loop do for value in x[0:3]
499/6:
import random
cities = []
latitude_longitude = []
        
coords_lat = np.random.uniform(low=-90, high=90, size=1500)
coords_lng = np.random.uniform(low=-180, high=180, size=1500)
latitude_longitude = zip(coords_lat, coords_lng)

for latitude_longitude in latitude_longitude:
    city = citipy.nearest_city(coords_lat[0],coords_lng[1]).city_name
    
    if city not in cities:
        cities.append(city)
        
        
len(cities)

# To slice the list in for loop do for value in x[0:3]
499/7:
import random
cities = []
latitude_longitude = []
        
coords_lat = np.random.uniform(low=-90, high=90, size=1500)
coords_lng = np.random.uniform(low=-180, high=180, size=1500)
latitude_longitude = zip(coords_lat, coords_lng)

for latitude_longitude in latitude_longitude:
    city = citipy.nearest_city(latitude_longitude[0],latitude_longitude[1]).city_name
    
    if city not in cities:
        cities.append(city)
        
        
len(cities)

# To slice the list in for loop do for value in x[0:3]
499/8:
from scipy import integrate

cities = []
latitude_longitude = []
        
coords_lat = np.random.uniform(low=-90, high=90, size=1500)
coords_lng = np.random.uniform(low=-180, high=180, size=1500)
latitude_longitude = zip(coords_lat, coords_lng)

for latitude_longitude in latitude_longitude:
    city = citipy.nearest_city(latitude_longitude[0],latitude_longitude[1]).city_name
    
    if city not in cities:
        cities.append(city)
        
        
len(cities)

# To slice the list in for loop do for value in x[0:3]
499/9:
import pandas as pd
import json
import requests
import matplotlib.pyplot as plt
import numpy as np
import config
import os
from config import weather_api_key
import citipy

url = "https://pypi.python.org/pypi/citipy"
units = "metric"
query_url = f"{url}appid={weather_api_key}&units={units}&q="
499/10:
from scipy import integrate

cities = []
latitude_longitude = []
        
coords_lat = np.random.uniform(low=-90, high=90, size=1500)
coords_lng = np.random.uniform(low=-180, high=180, size=1500)
latitude_longitude = zip(coords_lat, coords_lng)

for latitude_longitude in latitude_longitude:
    city = citipy.nearest_city(latitude_longitude[0],latitude_longitude[1]).city_name
    
    if city not in cities:
        cities.append(city)
        
        
len(cities)

# To slice the list in for loop do for value in x[0:3]
500/1:
import pandas as pd
import json
import requests
import matplotlib.pyplot as plt
import numpy as np
import config
import os
from config import weather_api_key
import citipy

url = "https://pypi.python.org/pypi/citipy"
units = "metric"
query_url = f"{url}appid={weather_api_key}&units={units}&q="
500/2:
from scipy import integrate

cities = []
latitude_longitude = []
        
coords_lat = np.random.uniform(low=-90, high=90, size=1500)
coords_lng = np.random.uniform(low=-180, high=180, size=1500)
latitude_longitude = zip(coords_lat, coords_lng)

for latitude_longitude in latitude_longitude:
    city = citipy.nearest_city(latitude_longitude[0],latitude_longitude[1]).city_name
    
    if city not in cities:
        cities.append(city)
        
        
len(cities)

# To slice the list in for loop do for value in x[0:3]
500/3:
import pandas as pd
import json
import requests
import matplotlib.pyplot as plt
import numpy as np
import config
import os
from config import weather_api_key
import citipy

url = "https://pypi.python.org/pypi/citipy"
units = "metric"
query_url = f"{url}appid={weather_api_key}&units={units}&q="
500/4:
from scipy import integrate

cities = []
latitude_longitude = []

#Latitude & Longitude ranges
coords_lat_range = (-90, 90)
coords_lng_range = (-180, 180)
        
coords_lat = np.random.uniform(low=-90, high=90, size=1500)
coords_lng = np.random.uniform(low=-180, high=180, size=1500)
latitude_longitude = zip(coords_lat, coords_lng)

for latitude_longitude in latitude_longitude:
    city = citipy.nearest_city(latitude_longitude[0],latitude_longitude[1]).city_name
    
    if city not in cities:
        cities.append(city)
        
        
len(cities)

# To slice the list in for loop do for value in x[0:3]
502/1:
import pandas as pd
import json
import requests
import matplotlib.pyplot as plt
import numpy as np
import config
import os
from config import weather_api_key
import citipy

url = "https://pypi.python.org/pypi/citipy"
units = "metric"
query_url = f"{url}appid={weather_api_key}&units={units}&q="
502/2:
from scipy import integrate

cities = []
latitude_longitude = []

#Latitude & Longitude ranges
coords_lat_range = (-90, 90)
coords_lng_range = (-180, 180)
        
coords_lat = np.random.uniform(low=-90, high=90, size=1500)
coords_lng = np.random.uniform(low=-180, high=180, size=1500)
latitude_longitude = zip(coords_lat, coords_lng)

for latitude_longitude in latitude_longitude:
    city = citipy.nearest_city(latitude_longitude[0],latitude_longitude[1]).city_name
    
    if city not in cities:
        cities.append(city)
        
        
len(cities)

# To slice the list in for loop do for value in x[0:3]
502/3:
import pandas as pd
import json
import requests
import matplotlib.pyplot as plt
import numpy as np
import config
import os
from config import weather_api_key
import citipy

url = "https://pypi.python.org/pypi/citipy"
units = "metric"
query_url = f"{url}appid={weather_api_key}&units={units}&q="
502/4: dir(citipy)
502/5: dir(citipy)
503/1:
import pandas as pd
import json
import requests
import matplotlib.pyplot as plt
import numpy as np
import config
import os
from config import weather_api_key
import citipy

url = "https://pypi.python.org/pypi/citipy"
units = "metric"
query_url = f"{url}appid={weather_api_key}&units={units}&q="
503/2: dir(citipy)
504/1:
import pandas as pd
import json
import requests
import matplotlib.pyplot as plt
import numpy as np
import config
import os
from config import weather_api_key
from citipy import citipy

url = "https://pypi.python.org/pypi/citipy"
units = "metric"
query_url = f"{url}appid={weather_api_key}&units={units}&q="
504/2: dir(citipy)
504/3:
from scipy import integrate

cities = []
latitude_longitude = []

#Latitude & Longitude ranges
coords_lat_range = (-90, 90)
coords_lng_range = (-180, 180)
        
coords_lat = np.random.uniform(low=-90, high=90, size=1500)
coords_lng = np.random.uniform(low=-180, high=180, size=1500)
latitude_longitude = zip(coords_lat, coords_lng)

for latitude_longitude in latitude_longitude:
    city = citipy.nearest_city(latitude_longitude[0],latitude_longitude[1]).city_name
    
    if city not in cities:
        cities.append(city)
        
        
len(cities)

# To slice the list in for loop do for value in x[0:3]
504/4: #  Get the indices of cities that have humidity over 100%.
504/5:
# Make a new DataFrame equal to the city data to drop all humidity outliers by index.
# Passing "inplace=False" will make a copy of the city_data DataFrame, which we call "clean_city_data".
504/6: dir(citipy)
504/7: dir(citipy)
504/8:
import pandas as pd
import json
import requests
import matplotlib.pyplot as plt
import numpy as np
import config
import os
from config import weather_api_key
import citipy

url = "https://pypi.python.org/pypi/citipy"
units = "metric"
query_url = f"{url}appid={weather_api_key}&units={units}&q="
504/9: dir(citipy)
504/10:
import pandas as pd
import json
import requests
import matplotlib.pyplot as plt
import numpy as np
import config
import os
from config import weather_api_key
import citipy

url = "http://api.openweathermap.org/data/2.5/weather?"
units = "metric"
query_url = f"{url}appid={weather_api_key}&units={units}&q="
504/11: dir(citipy)
504/12:
from scipy import integrate

cities = []
latitude_longitude = []
#Latitude & Longitude ranges
coords_lat_range = (-90, 90)
coords_lng_range = (-180, 180)
        
coords_lat = np.random.uniform(low=-90, high=90, size=1500)
coords_lng = np.random.uniform(low=-180, high=180, size=1500)
latitude_longitude = zip(coords_lat, coords_lng)

for latitude_longitude in latitude_longitude:
    city = citipy.nearest_city(latitude_longitude[0],latitude_longitude[1]).city_name
    
    if city not in cities:
        cities.append(city)
        
        
# len(cities)

# # To slice the list in for loop do for value in x[0:3]
504/13:
import pandas as pd
import json
import requests
import matplotlib.pyplot as plt
import numpy as np
import config
import os
from config import weather_api_key
from citipy import citipy

url = "http://api.openweathermap.org/data/2.5/weather?"
units = "metric"
query_url = f"{url}appid={weather_api_key}&units={units}&q="
504/14:
from scipy import integrate

cities = []
latitude_longitude = []
#Latitude & Longitude ranges
coords_lat_range = (-90, 90)
coords_lng_range = (-180, 180)
        
coords_lat = np.random.uniform(low=-90, high=90, size=1500)
coords_lng = np.random.uniform(low=-180, high=180, size=1500)
latitude_longitude = zip(coords_lat, coords_lng)

for latitude_longitude in latitude_longitude:
    city = citipy.nearest_city(latitude_longitude[0],latitude_longitude[1]).city_name
    
    if city not in cities:
        cities.append(city)
        
        
len(cities)

# # To slice the list in for loop do for value in x[0:3]
504/15:
# set up lists to hold reponse info
lat = []
temp = []

# Loop through the list of cities and perform a request for data on each
for city in cities:
    response = requests.get(query_url + city).json()
    lat.append(response['coord']['lat'])
    temp.append(response['main']['temp'])
504/16:
# set up lists to hold reponse info
lat = []
temp = []

# Loop through the list of cities and perform a request for data on each
for city in cities:
    response = requests.get(query_url + city).json()
    lat.append(response['coord']['lat'])
    temp.append(response['main']['temp'])
    
print(f"The latitude information received is: {lat}")
print(f"The temperature information received is: {temp}")
504/17: response
504/18: requests.get(query_url + "Dallas").json()
504/19:
# set up lists to hold reponse info
city_data = []
lat = []
temp = []

# Loop through the list of cities and perform a request for data on each
for city in cities:
    response = requests.get(query_url + city).json()
    try:
        A={'lat': response[cfdsff] , 
           'long': response[asdsadsa],
          'temp': response[]}
        blah.append(A)
        lat.append(response['coord']['lat'])
        temp.append(response['main']['temp'])
        
 
    
    
print(f"The latitude information received is: {lat}")
print(f"The temperature information received is: {temp}")
504/20: b = requests.get(query_url + "Dallas").json()
504/21:
b = requests.get(query_url + "Dallas").json()
b
504/22:
b = requests.get(query_url + "Dallas").json()
b['coord']['long']
504/23:
b = requests.get(query_url + "Dallas").json()
b['coord']['long']
b
504/24:
b = requests.get(query_url + "Dallas").json()
b['coord']['long']
504/25: b
504/26:
b = requests.get(query_url + "Dallas").json()
b['coord']['lon']
504/27: b
504/28: cities
504/29:
# Testing cell
b = requests.get(query_url + "Miami").json()
b['coord']['lon']
504/30: b
504/31:
# set up lists to hold reponse info
city_data = []
lat = []
temp = []

# Loop through the list of cities and perform a request for data on each
for city in cities:
    response = requests.get(query_url + city).json()
    try:
        A={'lat': response[cfdsff] , 
           'long': response['coord']['long'],
          'temp': response['main']['temp']}
        blah.append(A)
        lat.append(response['coord']['lat'])
        temp.append(response['main']['temp'])
        
 
    
    
print(f"The latitude information received is: {lat}")
print(f"The temperature information received is: {temp}")
504/32:
# set up lists to hold reponse info
city_data = []

# Loop through the list of cities and perform a request for data on each
for city in cities:
    response = requests.get(query_url + city).json()
    try:
        A={'lat': response['coord']['lat'], 
           'long': response['coord']['long'],
          'temp': response['main']['temp']}
        city_data.append(A)
        
        
 
    
    
print(f"The latitude information received is: {lat}")
print(f"The temperature information received is: {temp}")
504/33:
# set up lists to hold reponse info
city_data = []

# Loop through the list of cities and perform a request for data on each
for city in cities:
    response = requests.get(query_url + city).json()
    try:
        A={'lat': response['coord']['lat'], 
           'long': response['coord']['long'],
          'temp': response['main']['temp']}
        
        city_data.append(A)
        
        
 
    
    
# print(f"The latitude information received is: {lat}")
# print(f"The temperature information received is: {temp}")
504/34:
# set up lists to hold reponse info
city_data = []
city_counter = 0
# Loop through the list of cities and perform a request for data on each
for city in cities:
    response = requests.get(query_url + city).json()
    city_counter = city_counter + 1
    try:
        A={'lat': response['coord']['lat'], 
           'long': response['coord']['long'],
          'temp': response['main']['temp']}
        
        city_data.append(A)
504/35:
# set up lists to hold reponse info
city_data = []
city_counter = 0

# Loop through the list of cities and perform a request for data on each
for city in cities:
    response = requests.get(query_url + city).json()
    city_counter = city_counter + 1
    try:
        A={'lat': response['coord']['lat'], 
           'long': response['coord']['long'],
          'temp': response['main']['temp']}
        
        city_data.append(A)
504/36:
# set up lists to hold reponse info
city_data = []
city_counter = 0

# Loop through the list of cities and perform a request for data on each
for city in cities:
    response = requests.get(query_url + city).json()
    city_counter = city_counter + 1
    try:
        A={'lat': response['coord']['lat'], 
           'long': response['coord']['long'],
          'temp': response['main']['temp']}
        
        city_data.append(A)
504/37:
# set up lists to hold reponse info
city_data = []
city_counter = 0

# Loop through the list of cities and perform a request for data on each
for city in cities:
    response = requests.get(query_url + city).json()
    city_counter = city_counter + 1
    try:
        A={'lat': response['coord']['lat'], 
           'long': response['coord']['long'],
          'temp': response['main']['temp']}
504/38:
for city in cities:
    response = requests.get(query_url + city).json()
    city_counter = city_counter + 1
    try:
        A={'lat': response['coord']['lat'], 
           'long': response['coord']['long'],
          'temp': response['main']['temp']}
504/39:
for city in cities:
    response = requests.get(query_url + city).json()
    city_counter = city_counter + 1
    try:
        A={'lat': response['coord']['lat'], 
           'long': response['coord']['long'],
          'temp': response['main']['temp']}
504/40:
# set up lists to hold response info
city_data = []
city_counter = 0

#Loop through the list of cities and perform a request for data on each

for city in cities:
    response = requests.get(query_url + city).json()
    city_counter = city_counter + 1
    
    try:
        A={'lat': response['coord']['lat'],
          'long': response['coord']['long'],
          'temp': response['main']['temp']}
        
        city_data.append
504/41:
# set up lists to hold response info
city_data = []
city_counter = 0

#Loop through the list of cities and perform a request for data on each

for city in cities:
    response = requests.get(query_url + city).json()
    city_counter = city_counter + 1
    
    try:
        A={'lat': response['coord']['lat'],
          'long': response['coord']['long'],
          'temp': response['main']['temp']}
        
        city_data.append
504/42:
# set up lists to hold response info
city_data = []
city_counter = 0

#Loop through the list of cities and perform a request for data on each

for city in cities:
    response = requests.get(query_url + city).json()
    city_counter = city_counter + 1
    
    try:
        A={'lat': response['coord']['lat'],
          'long': response['coord']['long'],
          'temp': response['main']['temp']}
        
        city_data.append()
504/43:
# set up lists to hold response info
city_data = []
city_counter = 0

#Loop through the list of cities and perform a request for data on each

for city in cities:
    response = requests.get(query_url + city).json()
    city_counter = city_counter + 1
    
    try:
        A={'lat': response['coord']['lat'],
          'long': response['coord']['long'],
          'temp': response['main']['temp']}
        
        city_data.append(A)
504/44:
# set up lists to hold response info
city_data = []
city_counter = 0

#Loop through the list of cities and perform a request for data on each

for city in cities:
    response = requests.get(query_url + city).json()
    city_counter = city_counter + 1
504/45:
# set up lists to hold response info
city_data = []
city_counter = 0

#Loop through the list of cities and perform a request for data on each

for city in cities:
    response = requests.get(query_url + city).json()
    city_counter = city_counter + 1
    try:
        A={'lat': response['coord']['lat'],
          'long': response['coord']['long'],
          'temp': response['main']['temp']}
        
        city_data.append(A)
        
    except:
        print(f'{city_counter} {city} not found')
504/46:
# set up lists to hold response info
city_data = []
city_counter = 0

#Loop through the list of cities and perform a request for data on each

for city in cities:
    response = requests.get(query_url + city).json()
    city_counter = city_counter + 1
    try:
        A={'lat': response['coord']['lat'],
          'long': response['coord']['long'],
          'temp': response['main']['temp']}
        
        city_data.append(A)
        
        print(f'{city_counter}{city}')
        
    except:
        print(f'{city_counter} {city} not found')
504/47:
# set up lists to hold response info
city_data = []
city_counter = 0

#Loop through the list of cities and perform a request for data on each

for city in cities:
    response = requests.get(query_url + city).json()
    city_counter = city_counter + 1
    try:
        A={'lat': response['coord']['lat'],
          'long': response['coord']['lon'],
          'temp': response['main']['temp']}
        
        city_data.append(A)
        
        print(f'{city_counter} {city}')
        
    except:
        print(f'{city_counter} {city} not found')
504/48:
weatherpy_data = pd.DataFrame(city_data)
weather_data
504/49:
weatherpy_data = pd.DataFrame(city_data)
weatherpy_data
504/50:
# set up lists to hold response info
city_data = []
city_counter = 0

#Loop through the list of cities and perform a request for data on each

for city in cities:
    response = requests.get(query_url + city).json()
    city_counter = city_counter + 1
    try:
        A={'lat': response['coord']['lat'],
          'long': response['coord']['lon'],
          'temp': response['main']['temp'],
          'city'}
        
        city_data.append(A)
        
        print(f'{city_counter} {city}')
        
    except:
        print(f'{city_counter} {city} not found')
504/51:
# set up lists to hold response info
city_data = []
city_counter = 0

#Loop through the list of cities and perform a request for data on each

for city in cities:
    response = requests.get(query_url + city).json()
    city_counter = city_counter + 1
    try:
        A={'lat': response['coord']['lat'],
          'long': response['coord']['lon'],
          'temp': response['main']['temp'],
          'city': city}
        
        city_data.append(A)
        
        print(f'{city_counter} {city}')
        
    except:
        print(f'{city_counter} {city} not found')
504/52:
weatherpy_data = pd.DataFrame(city_data)
weatherpy_data
504/53:
weatherpy_data = pd.DataFrame(city_data)
weatherpy_data.head()
507/1:
import pandas as pd
import json
import requests
import matplotlib.pyplot as plt
import numpy as np
import config
import os
from config import weather_api_key
from citipy import citipy

url = "http://api.openweathermap.org/data/2.5/weather?"
units = "metric"
query_url = f"{url}appid={weather_api_key}&units={units}&q={city}"
507/2:
import pandas as pd
import json
import requests
import matplotlib.pyplot as plt
import numpy as np
import config
import os
from api_keys import weather_api_key
from citipy import citipy

url = "http://api.openweathermap.org/data/2.5/weather?"
units = "metric"
query_url = f"{url}appid={weather_api_key}&units={units}&q={city}"
507/3:
import pandas as pd
import json
import requests
import matplotlib.pyplot as plt
import numpy as np
import config
import os
from config import weather_api_key
from citipy import citipy

url = "http://api.openweathermap.org/data/2.5/weather?"
units = "metric"
query_url = f"{url}appid={weather_api_key}&units={units}&q={city}"
507/4:
import pandas as pd
import json
import requests
import matplotlib.pyplot as plt
import numpy as np
import config
import os
from config import weather_api_key
from citipy import citipy
507/5:
from scipy import integrate

cities = []
latitude_longitude = []
#Latitude & Longitude ranges
coords_lat_range = (-90, 90)
coords_lng_range = (-180, 180)
        
coords_lat = np.random.uniform(low=-90, high=90, size=1500)
coords_lng = np.random.uniform(low=-180, high=180, size=1500)
latitude_longitude = zip(coords_lat, coords_lng)

for latitude_longitude in latitude_longitude:
    city = citipy.nearest_city(latitude_longitude[0],latitude_longitude[1]).city_name
    
    if city not in cities:
        cities.append(city)
        
        
len(cities)

# # To slice the list in for loop do for value in x[0:3]
507/6:
from scipy import integrate

url = "http://api.openweathermap.org/data/2.5/weather?"
units = "metric"
query_url = f"{url}appid={weather_api_key}&units={units}&q={city}"

cities = []
latitude_longitude = []
#Latitude & Longitude ranges
coords_lat_range = (-90, 90)
coords_lng_range = (-180, 180)
        
coords_lat = np.random.uniform(low=-90, high=90, size=1500)
coords_lng = np.random.uniform(low=-180, high=180, size=1500)
latitude_longitude = zip(coords_lat, coords_lng)

for latitude_longitude in latitude_longitude:
    city = citipy.nearest_city(latitude_longitude[0],latitude_longitude[1]).city_name
    
    if city not in cities:
        cities.append(city)
        
        
len(cities)

# # To slice the list in for loop do for value in x[0:3]
507/7:
# Testing cell
b = requests.get(query_url + "Miami").json()
b['coord']['lon']
507/8: b
507/9:
# Testing cell
b = requests.get(query_url + "Dallas").json()
b['coord']['lon']
507/10: b
507/11: b
507/12: cities
507/13:
# Testing cell
b = requests.get(query_url + "east london").json()
b
507/14:
# Testing cell
b = requests.get(query_url + city).json()
b
507/15:
# Testing cell
b = requests.get(query_url + "Dallas").json()
b
507/16: b
507/17:
# set up lists to hold response info
city_data = []
city_counter = 0

#Loop through the list of cities and perform a request for data on each

for city in cities:
    response = requests.get(query_url + city).json()
    city_counter = city_counter + 1
    try:
        A={'lat': response['coord']['lat'],
          'long': response['coord']['lon'],
          'temp': response['main']['temp'],
           'humidity': response['main']['humidity'],
           'cloudiness': response['main']['clouds'],
           'wind_speed': response['main']['wind']['speed']
          'city': city}
        
        city_data.append(A)
        
        print(f'{city_counter} {city}')
        
    except:
        print(f'{city_counter} {city} not found')
507/18:
# set up lists to hold response info
city_data = []
city_counter = 0

#Loop through the list of cities and perform a request for data on each

for city in cities:
    response = requests.get(query_url + city).json()
    city_counter = city_counter + 1
    try:
        A={'lat': response['coord']['lat'],
          'long': response['coord']['lon'],
          'temp': response['main']['temp'],
           'humidity': response['main']['humidity'],
           'wind_speed': response['main']['wind']['speed'],
           'cloudiness': response['weather']['clouds']
          'city': city}
        
        city_data.append(A)
        
        print(f'{city_counter} {city}')
        
    except:
        print(f'{city_counter} {city} not found')
507/19:
# set up lists to hold response info
city_data = []
city_counter = 0

#Loop through the list of cities and perform a request for data on each

for city in cities:
    response = requests.get(query_url + city).json()
    city_counter = city_counter + 1
    try:
        A={'lat': response['coord']['lat'],
          'long': response['coord']['lon'],
          'temp': response['main']['temp'],
           'humidity': response['main']['humidity'],
           'wind_speed': response['main']['wind']['speed'],
           'cloudiness': response['weather']['clouds'],
          'city': city}
        
        city_data.append(A)
        
        print(f'{city_counter} {city}')
        
    except:
        print(f'{city_counter} {city} not found')
507/20:
# set up lists to hold response info
city_data = []
city_counter = 0

#Loop through the list of cities and perform a request for data on each

for city in cities:
    response = requests.get(query_url + city).json()
    city_counter = city_counter + 1
    try:
        A={'lat': response['coord']['lat'],
          'long': response['coord']['lon'],
          'temp': response['main']['temp'],
           'humidity': response['main']['humidity'],
           'wind_speed': response['main']['wind']['speed'],
           'cloudiness': response['weather']['clouds'],
          'city': city}
        
        city_data.append(A)
        
        print(f'{city_counter} {city}')
        
    except:
        print(f'{city_counter} {city} not found')
508/1:
import pandas as pd
import json
import requests
import matplotlib.pyplot as plt
import numpy as np
import config
import os
from config import weather_api_key
from citipy import citipy

url = "http://api.openweathermap.org/data/2.5/weather?"
units = "metric"
query_url = f"{url}appid={weather_api_key}&units={units}&q={city}"
509/1:
import pandas as pd
import json
import requests
import matplotlib.pyplot as plt
import numpy as np
import config
import os
from config import weather_api_key
from citipy import citipy

url = "http://api.openweathermap.org/data/2.5/weather?"
units = "metric"
query_url = f"{url}appid={weather_api_key}&units={units}&q={city}"
509/2:
from scipy import integrate
url = "http://api.openweathermap.org/data/2.5/weather?"
units = "metric"
query_url = f"{url}appid={weather_api_key}&units={units}&q={city}"

cities = []
latitude_longitude = []
#Latitude & Longitude ranges
coords_lat_range = (-90, 90)
coords_lng_range = (-180, 180)
        
coords_lat = np.random.uniform(low=-90, high=90, size=1500)
coords_lng = np.random.uniform(low=-180, high=180, size=1500)
latitude_longitude = zip(coords_lat, coords_lng)

for latitude_longitude in latitude_longitude:
    city = citipy.nearest_city(latitude_longitude[0],latitude_longitude[1]).city_name
    
    if city not in cities:
        cities.append(city)
        
        
len(cities)

# # To slice the list in for loop do for value in x[0:3]
509/3:
import pandas as pd
import json
import requests
import matplotlib.pyplot as plt
import numpy as np
import config
import os
from config import weather_api_key
from citipy import citipy
509/4:
from scipy import integrate

cities = []
latitude_longitude = []
#Latitude & Longitude ranges
coords_lat_range = (-90, 90)
coords_lng_range = (-180, 180)
        
coords_lat = np.random.uniform(low=-90, high=90, size=1500)
coords_lng = np.random.uniform(low=-180, high=180, size=1500)
latitude_longitude = zip(coords_lat, coords_lng)

for latitude_longitude in latitude_longitude:
    city = citipy.nearest_city(latitude_longitude[0],latitude_longitude[1]).city_name
    
    if city not in cities:
        cities.append(city)
        
        
len(cities)

# # To slice the list in for loop do for value in x[0:3]
509/5:
# Testing cell
b = requests.get(query_url + "Miami").json()
b['coord']['lon']
509/6:
url = "http://api.openweathermap.org/data/2.5/weather?"
units = "metric"
query_url = f"{url}appid={weather_api_key}&units={units}&q={cities}"

# Testing cell
b = requests.get(query_url + "Miami").json()
b['coord']['lon']
509/7:
url = "http://api.openweathermap.org/data/2.5/weather?"
units = "metric"
query_url = f"{url}appid={weather_api_key}&units={units}&q={cities}"
509/8:
# Testing cell
b = requests.get(query_url + "Miami").json()
b['coord']['lon']
509/9: b
509/10:
# set up lists to hold response info
city_data = []
city_counter = 0

#Loop through the list of cities and perform a request for data on each

for city in cities:
    response = requests.get(query_url + city).json()
    city_counter = city_counter + 1
    try:
        A={'lat': response['coord']['lat'],
          'long': response['coord']['lon'],
          'temp': response['main']['temp'],
           'humidity': response['main']['humidity'],
           'wind_speed': response['main']['wind']['speed'],
           'cloudiness': response['weather']['clouds'],
          'city': city}
        
        city_data.append(A)
        
        print(f'{city_counter} {city}')
        
    except:
        print(f'{city_counter} {city} not found')
509/11:
url = "http://api.openweathermap.org/data/2.5/weather?"
units = "metric"
query_url = f"{url}appid={weather_api_key}&units={units}&q={cities}"
509/12:
# # Testing cell
# b = requests.get(query_url + "Miami").json()
# b['coord']['lon']
509/13: #b
509/14:
# set up lists to hold response info
city_data = []
city_counter = 0

#Loop through the list of cities and perform a request for data on each

for city in cities:
    response = requests.get(query_url + city).json()
    city_counter = city_counter + 1
    try:
        A={'lat': response['coord']['lat'],
          'long': response['coord']['lon'],
          'temp': response['main']['temp'],
           'humidity': response['main']['humidity'],
           'wind_speed': response['main']['wind']['speed'],
           'cloudiness': response['weather']['clouds'],
          'city': city}
        
        city_data.append(A)
        
        print(f'{city_counter} {city}')
        
    except:
        print(f'{city_counter} {city} not found')
509/15:
weatherpy_data = pd.DataFrame(city_data)
weatherpy_data.head()
509/16:
from scipy import integrate

url = "http://api.openweathermap.org/data/2.5/weather?"
units = "metric"
query_url = f"{url}appid={weather_api_key}&units={units}&q={city}"

cities = []
latitude_longitude = []
#Latitude & Longitude ranges
coords_lat_range = (-90, 90)
coords_lng_range = (-180, 180)
        
coords_lat = np.random.uniform(low=-90, high=90, size=1500)
coords_lng = np.random.uniform(low=-180, high=180, size=1500)
latitude_longitude = zip(coords_lat, coords_lng)

for latitude_longitude in latitude_longitude:
    city = citipy.nearest_city(latitude_longitude[0],latitude_longitude[1]).city_name
    
    if city not in cities:
        cities.append(city)
        
        
len(cities)

# # To slice the list in for loop do for value in x[0:3]
509/17: cities
509/18:
# # Testing cell
b = requests.get(query_url + "Miami").json()
b['coord']['lon']
509/19:
import pandas as pd
import json
import requests
import matplotlib.pyplot as plt
import numpy as np
import config
import os
from config import weather_api_key
from citipy import citipy

url = "http://api.openweathermap.org/data/2.5/weather?"
units = "metric"
query_url = f"{url}appid={weather_api_key}&units={units}&q={city}"
509/20:
from scipy import integrate

cities = []
latitude_longitude = []
#Latitude & Longitude ranges
coords_lat_range = (-90, 90)
coords_lng_range = (-180, 180)
        
coords_lat = np.random.uniform(low=-90, high=90, size=1500)
coords_lng = np.random.uniform(low=-180, high=180, size=1500)
latitude_longitude = zip(coords_lat, coords_lng)

for latitude_longitude in latitude_longitude:
    city = citipy.nearest_city(latitude_longitude[0],latitude_longitude[1]).city_name
    
    if city not in cities:
        cities.append(city)
        
        
len(cities)

# # To slice the list in for loop do for value in x[0:3]
509/21:
# Testing cell
b = requests.get(query_url + "Miami").json()
b['coord']['lon']
509/22:
import pandas as pd
import json
import requests
import matplotlib.pyplot as plt
import numpy as np
import config
import os
from config import weather_api_key
from citipy import citipy

url = "http://api.openweathermap.org/data/2.5/weather?"
units = "metric"
query_url = f"{url}appid={weather_api_key}&units={units}&q={city}"
509/23:
from scipy import integrate

cities = []
latitude_longitude = []
#Latitude & Longitude ranges
coords_lat_range = (-90, 90)
coords_lng_range = (-180, 180)
        
coords_lat = np.random.uniform(low=-90, high=90, size=1500)
coords_lng = np.random.uniform(low=-180, high=180, size=1500)
latitude_longitude = zip(coords_lat, coords_lng)

for latitude_longitude in latitude_longitude:
    city = citipy.nearest_city(latitude_longitude[0],latitude_longitude[1]).city_name
    
    if city not in cities:
        cities.append(city)
        
        
len(cities)

# # To slice the list in for loop do for value in x[0:3]
509/24:
# Testing cell
b = requests.get(query_url + "Miami").json()
b['coord']['lon']
509/25:
# Testing cell
b = requests.get(query_url + "Miami").json()
b
510/1:
import pandas as pd
import json
import requests
import matplotlib.pyplot as plt
import numpy as np
import config
import os
from config import weather_api_key
from citipy import citipy

url = "http://api.openweathermap.org/data/2.5/weather?"
units = "metric"
query_url = f"{url}appid={weather_api_key}&units={units}&q={city}"
510/2:
import pandas as pd
import json
import requests
import matplotlib.pyplot as plt
import numpy as np
import config
import os
from config import weather_api_key
from citipy import citipy

url = "http://api.openweathermap.org/data/2.5/weather?"
units = "metric"
query_url = f"{url}appid={weather_api_key}&units={units}"
510/3:
from scipy import integrate

cities = []
latitude_longitude = []
#Latitude & Longitude ranges
coords_lat_range = (-90, 90)
coords_lng_range = (-180, 180)
        
coords_lat = np.random.uniform(low=-90, high=90, size=1500)
coords_lng = np.random.uniform(low=-180, high=180, size=1500)
latitude_longitude = zip(coords_lat, coords_lng)

for latitude_longitude in latitude_longitude:
    city = citipy.nearest_city(latitude_longitude[0],latitude_longitude[1]).city_name
    
    if city not in cities:
        cities.append(city)
        
        
len(cities)

# # To slice the list in for loop do for value in x[0:3]
510/4:
# # Testing cell
b = requests.get(query_url + "Miami").json()
b['coord']['lon']
510/5:
# # Testing cell
b = requests.get(query_url + "Miami").json()
b
510/6:
# # Testing cell
b = requests.get(query_url + city).json()
b
510/7: cities
510/8:
# # Testing cell
b = requests.get(query_url + 'san lawrenz').json()
b
510/9:
# # Testing cell
b = requests.get(query_url + 'san lawrenz').json()
510/10: cities
510/11:
# set up lists to hold response info
city_data = []
city_counter = 0

#Loop through the list of cities and perform a request for data on each

for city in cities:
    response = requests.get(query_url + city).json()
    city_counter = city_counter + 1
    try:
        A={'lat': response['coord']['lat'],
          'long': response['coord']['lon'],
          'temp': response['main']['temp'],
           'humidity': response['main']['humidity'],
           'wind_speed': response['main']['wind']['speed'],
           'cloudiness': response['weather']['clouds'],
          'city': city}
        
        city_data.append(A)
        
        print(f'{city_counter} {city}')
        
    except:
        print(f'{city_counter} {city} not found')
510/12:
# set up lists to hold response info
city_data = []
city_counter = 0

#Loop through the list of cities and perform a request for data on each

for city in cities:
    time.sleep(1)
    response = requests.get(query_url + city).json()
    city_counter = city_counter + 1
    try:
        A={'lat': response['coord']['lat'],
          'long': response['coord']['lon'],
          'temp': response['main']['temp'],
           'humidity': response['main']['humidity'],
           'wind_speed': response['main']['wind']['speed'],
           'cloudiness': response['weather']['clouds'],
          'city': city}
        
        city_data.append(A)
        
        print(f'{city_counter} {city}')
        
    except:
        print(f'{city_counter} {city} not found')
510/13:
import pandas as pd
import json
import requests
import matplotlib.pyplot as plt
import numpy as np
import config
import os
import time
from config import weather_api_key
from citipy import citipy

url = "http://api.openweathermap.org/data/2.5/weather?"
units = "metric"
query_url = f"{url}appid={weather_api_key}&units={units}"
510/14:
from scipy import integrate

cities = []
latitude_longitude = []
#Latitude & Longitude ranges
coords_lat_range = (-90, 90)
coords_lng_range = (-180, 180)
        
coords_lat = np.random.uniform(low=-90, high=90, size=1500)
coords_lng = np.random.uniform(low=-180, high=180, size=1500)
latitude_longitude = zip(coords_lat, coords_lng)

for latitude_longitude in latitude_longitude:
    city = citipy.nearest_city(latitude_longitude[0],latitude_longitude[1]).city_name
    
    if city not in cities:
        cities.append(city)
        
        
len(cities)

# # To slice the list in for loop do for value in x[0:3]
510/15:
# # Testing cell
b = requests.get(query_url + 'san lawrenz').json()
510/16: cities
510/17:
# set up lists to hold response info
city_data = []
city_counter = 0

#Loop through the list of cities and perform a request for data on each

for city in cities:
    time.sleep(1)
    response = requests.get(query_url + city).json()
    city_counter = city_counter + 1
    try:
        A={'lat': response['coord']['lat'],
          'long': response['coord']['lon'],
          'temp': response['main']['temp'],
           'humidity': response['main']['humidity'],
           'wind_speed': response['main']['wind']['speed'],
           'cloudiness': response['weather']['clouds'],
          'city': city}
        
        city_data.append(A)
        
        print(f'{city_counter} {city}')
        
    except:
        print(f'{city_counter} {city} not found')
510/18:
# set up lists to hold response info
city_data = []
city_counter = 0

#Loop through the list of cities and perform a request for data on each

for city in cities:
    response = requests.get(query_url + city).json()
    city_counter = city_counter + 1
    try:
        city_data.append(city)
        clouds.append(response["clouds"]["all"])
        country.append(response["sys"]["country"])
        date.append(response["dt"])
        humidity.append(response["main"]["humidity"])
        lat.append(response["coord"]["lat"])
        lon.append(response["coord"]["lon"])
        temp_max.append(response["main"]["temp_max"])
        wind.append(response["wind"]["speed"])
                 
#         A={'lat': response['coord']['lat'],
#           'long': response['coord']['lon'],
#           'temp': response['main']['temp'],
#            'humidity': response['main']['humidity'],
#            'wind_speed': response['main']['wind']['speed'],
#            'cloudiness': response['weather']['clouds'],
#           'city': city}
        
        city_data.append(A)
        
    except:
    if response["message"] == "city not found":
        print(f'{city_counter} {city} not found')
510/19:
# set up lists to hold response info
city_data = []
city_counter = 0

#Loop through the list of cities and perform a request for data on each

for city in cities:
    response = requests.get(query_url + city).json()
    city_counter = city_counter + 1
    try:
        city_data.append(city)
        clouds.append(response["clouds"]["all"])
        country.append(response["sys"]["country"])
        date.append(response["dt"])
        humidity.append(response["main"]["humidity"])
        lat.append(response["coord"]["lat"])
        lon.append(response["coord"]["lon"])
        temp_max.append(response["main"]["temp_max"])
        wind.append(response["wind"]["speed"])
                 
#         A={'lat': response['coord']['lat'],
#           'long': response['coord']['lon'],
#           'temp': response['main']['temp'],
#            'humidity': response['main']['humidity'],
#            'wind_speed': response['main']['wind']['speed'],
#            'cloudiness': response['weather']['clouds'],
#           'city': city}
        
#         city_data.append(A)
        
    except:
    if response["message"] == "city not found":
        print(f'{city_counter} {city} not found')
510/20:
# set up lists to hold response info
city_data = []
city_counter = 0

#Loop through the list of cities and perform a request for data on each

for city in cities:
    response = requests.get(query_url + city).json()
    city_counter = city_counter + 1
    try:
        city_data.append(city)
        clouds.append(response["clouds"]["all"])
        country.append(response["sys"]["country"])
        date.append(response["dt"])
        humidity.append(response["main"]["humidity"])
        lat.append(response["coord"]["lat"])
        lon.append(response["coord"]["lon"])
        temp_max.append(response["main"]["temp_max"])
        wind.append(response["wind"]["speed"])
                 
#         A={'lat': response['coord']['lat'],
#           'long': response['coord']['lon'],
#           'temp': response['main']['temp'],
#            'humidity': response['main']['humidity'],
#            'wind_speed': response['main']['wind']['speed'],
#            'cloudiness': response['weather']['clouds'],
#           'city': city}
        
#         city_data.append(A)
        
    except:
        if response["message"] == "city not found":
        print(f'{city_counter} {city} not found')
511/1:
import pandas as pd
import json
import requests
import matplotlib.pyplot as plt
import numpy as np
import config
import os
import time
from config import weather_api_key
from citipy import citipy

url = "http://api.openweathermap.org/data/2.5/weather?"
units = "metric"
query_url = f"{url}appid={weather_api_key}&units={units}"
511/2:
from scipy import integrate

cities = []
latitude_longitude = []
#Latitude & Longitude ranges
coords_lat_range = (-90, 90)
coords_lng_range = (-180, 180)
        
coords_lat = np.random.uniform(low=-90, high=90, size=1500)
coords_lng = np.random.uniform(low=-180, high=180, size=1500)
latitude_longitude = zip(coords_lat, coords_lng)

for latitude_longitude in latitude_longitude:
    city = citipy.nearest_city(latitude_longitude[0],latitude_longitude[1]).city_name
    
    if city not in cities:
        cities.append(city)
        
        
len(cities)

# # To slice the list in for loop do for value in x[0:3]
511/3:
# # Testing cell
b = requests.get(query_url + 'san lawrenz').json()
511/4: cities
511/5:
# set up lists to hold response info
city_data = []
clouds = []
humidi

city_counter = 0

#Loop through the list of cities and perform a request for data on each

for city in cities:
    response = requests.get(query_url + city).json()
    city_counter = city_counter + 1
    try:
                 
        A={'lat': response['coord']['lat'],
          'long': response['coord']['lon'],
          'temp': response['main']['temp'],
           'humidity': response['main']['humidity'],
           'wind_speed': response['main']['wind']['speed'],
           'cloudiness': response['weather']['clouds'],
          'city': city}
        
        city_data.append(A)
        
    except:
        if response["message"] == "city not found":
        print(f'{city_counter} {city} not found')
511/6:
import pandas as pd
import json
import requests
import matplotlib.pyplot as plt
import numpy as np
import config
import os
import time
from config import weather_api_key
from citipy import citipy

url = "http://api.openweathermap.org/data/2.5/weather?"
units = "metric"
query_url = f"{url}appid={weather_api_key}&units={units}"
511/7:
from scipy import integrate

cities = []
latitude_longitude = []
#Latitude & Longitude ranges
coords_lat_range = (-90, 90)
coords_lng_range = (-180, 180)
        
coords_lat = np.random.uniform(low=-90, high=90, size=1500)
coords_lng = np.random.uniform(low=-180, high=180, size=1500)
latitude_longitude = zip(coords_lat, coords_lng)

for latitude_longitude in latitude_longitude:
    city = citipy.nearest_city(latitude_longitude[0],latitude_longitude[1]).city_name
    
    if city not in cities:
        cities.append(city)
        
        
len(cities)

# # To slice the list in for loop do for value in x[0:3]
511/8:
# # Testing cell
b = requests.get(query_url + 'san lawrenz').json()
b
511/9:
# set up lists to hold response info
city_data = []
clouds = []
humidi

city_counter = 0

#Loop through the list of cities and perform a request for data on each

for city in cities:
    response = requests.get(query_url + city).json()
    city_counter = city_counter + 1
    try:
                 
        A={'lat': response['coord']['lat'],
          'long': response['coord']['lon'],
          'temp': response['main']['temp'],
           'humidity': response['main']['humidity'],
           'wind_speed': response['main']['wind']['speed'],
           'cloudiness': response['weather']['clouds'],
          'city': city}
        
        city_data.append(A)
        
    except:
        if response["message"] == "city not found":
        print(f'{city_counter} {city} not found')
511/10:
# set up lists to hold response info
city_data = []
clouds = []
humidi

city_counter = 0

#Loop through the list of cities and perform a request for data on each

for city in cities:
    response = requests.get(query_url + city).json()
    city_counter = city_counter + 1
    
    try:
        A={'lat': response['coord']['lat'],
          'long': response['coord']['lon'],
          'temp': response['main']['temp'],
           'humidity': response['main']['humidity'],
           'wind_speed': response['main']['wind']['speed'],
           'cloudiness': response['weather']['clouds'],
          'city': city}
        
        city_data.append(A)
        
#     except:
#         if response["message"] = "city not found":
#         print(f'{city_counter} {city} not found')
511/11:
# set up lists to hold response info
city_data = []
clouds = []
humidi

city_counter = 0

#Loop through the list of cities and perform a request for data on each

for city in cities:
    response = requests.get(query_url + city).json()
    city_counter = city_counter + 1
    
    try:
        A={'lat': response['coord']['lat'],
          'long': response['coord']['lon'],
          'temp': response['main']['temp'],
           'humidity': response['main']['humidity'],
           'wind_speed': response['main']['wind']['speed'],
           'cloudiness': response['weather']['clouds'],
          'city': city}
        
        city_data.append(A)
    except:
        if response["message"] = "city not found":
        print(f'{city_counter} {city} not found')
511/12:
# set up lists to hold response info
city_data = []
clouds = []
humidi

city_counter = 0

#Loop through the list of cities and perform a request for data on each

for city in cities:
    response = requests.get(query_url + city).json()
    city_counter = city_counter + 1
    
    try:
        A={'lat': response['coord']['lat'],
          'long': response['coord']['lon'],
          'temp': response['main']['temp'],
           'humidity': response['main']['humidity'],
           'wind_speed': response['main']['wind']['speed'],
           'cloudiness': response['weather']['clouds'],
          'city': city}
        
        city_data.append(A)
    except:
        print(f'{city_counter} {city} not found')
511/13:
# set up lists to hold response info
city_data = []
clouds = []

city_counter = 0

#Loop through the list of cities and perform a request for data on each

for city in cities:
    response = requests.get(query_url + city).json()
    city_counter = city_counter + 1
    
    try:
        A={'lat': response['coord']['lat'],
          'long': response['coord']['lon'],
          'temp': response['main']['temp'],
           'humidity': response['main']['humidity'],
           'wind_speed': response['main']['wind']['speed'],
           'cloudiness': response['weather']['clouds'],
          'city': city}
        
        city_data.append(A)
    except:
        print(f'{city_counter} {city} not found')
511/14:
# set up lists to hold response info
city_data = []
clouds = []

city_counter = 0

#Loop through the list of cities and perform a request for data on each

for city in cities:
    response = requests.get(query_url + city).json()
    city_counter = city_counter + 1
    
    try:
        A={'lat': response['coord']['lat'],
          'long': response['coord']['lon'],
          'temp': response['main']['temp'],
           'humidity': response['main']['humidity'],
           'wind_speed': response['main']['wind']['speed'],
           'cloudiness': response['weather']['clouds'],
          'city': city}
        
        city_data.append(A)
        print(f'{city_counter} {city}')
        
    except:
        print(f'{city_counter} {city} not found')
511/15:
# set up lists to hold response info
city_data = []
clouds = []

city_counter = 0

#Loop through the list of cities and perform a request for data on each

for city in cities:
    response = requests.get(query_url + city).json()
    city_counter = city_counter + 1

    try:
        A={'lat': response['coord']['lat'],
          'long': response['coord']['lon'],
          'temp': response['main']['temp'],
          'city': city}
        
        city_data.append(A)
        
        print(f'{city_counter} {city}')
        
    except:
        print(f'{city_counter} {city} not found')
511/16:
# set up lists to hold response info
city_data = []
clouds = []

city_counter = 0

#Loop through the list of cities and perform a request for data on each

for city in cities:
    response = requests.get(query_url + city).json()
    city_counter = city_counter + 1

    try:
        weather={'lat': response['coord']['lat'],
          'long': response['coord']['lon'],
          'temp': response['main']['temp'],
          'city': city}
        
        city_data.append(weather)
        
        print(f'{city_counter} {city}')
        
    except:
        print(f'{city_counter} {city} not found')
511/17:
# set up lists to hold response info
city_data = []
clouds = []

city_counter = 0

#Loop through the list of cities and perform a request for data on each

for city in cities:
    response = requests.get(query_url + city).json()
    city_counter = city_counter + 1

    try:
        weather={'lat': response['coord']['lat'],
          'long': response['coord']['lon'],
          'temp': response['main']['temp'],
          'city': city}
        
        city_data.append(weather)
        
        print(f'{city_counter} {city}')
        
    except:
        print(f'{city} not found')
511/18:
# set up lists to hold response info
city_data = []
lat = []
lng = []
temp = []
cloudiness = []
humidity = []
wind_speed = []

city_counter = 0

#Loop through the list of cities and perform a request for data on each

for city in cities:
    response = requests.get(query_url + city).json()
    lat.append(response['coord']['lat'])
    lng.append(response['coord']['lon'])
#           'temp': response['main']['temp'],
#           'city': city}
    

        print(f'{city} not found')
511/19:
# set up lists to hold response info
city_data = []
lat = []
lng = []
temp = []
cloudiness = []
humidity = []
wind_speed = []

city_counter = 0

#Loop through the list of cities and perform a request for data on each

for city in cities:
    response = requests.get(query_url + city).json()
    lat.append(response['coord']['lat'])
    lng.append(response['coord']['lon'])
#           'temp': response['main']['temp'],
#           'city': city}

print(f'{city} not found')
511/20: city_data
511/21:
# set up lists to hold response info
city_data = []
city_counter = 0

#Loop through the list of cities and perform a request for data on each

for city in cities:
    response = requests.get(query_url + city).json()
    city_counter = city_counter + 1
    try:
        A={'lat': response['coord']['lat'],
          'long': response['coord']['lon'],
          'temp': response['main']['temp'],
          'city': city}
        
        city_data.append(A)
        
        print(f'{city_counter} {city}')
511/22:
# set up lists to hold response info
city_data = []
city_counter = 0

#Loop through the list of cities and perform a request for data on each

for city in cities:
    response = requests.get(query_url + city).json()
    city_counter = city_counter + 1
    try:
        A={'lat': response['coord']['lat'],
          'long': response['coord']['lon'],
          'temp': response['main']['temp'],
          'city': city}
        
        city_data.append(A)
        print(f'{city_counter} {city}')
    except:
511/23:
# set up lists to hold response info
city_data = []
city_counter = 0

#Loop through the list of cities and perform a request for data on each

for city in cities:
    response = requests.get(query_url + city).json()
    city_counter = city_counter + 1
    try:
        A={'lat': response['coord']['lat'],
          'long': response['coord']['lon'],
          'temp': response['main']['temp'],
          'city': city}
        
        city_data.append(A)
        print(f'{city_counter} {city}')
    except:
511/24:
from scipy import integrate

cities = []
latitude_longitude = []
#Latitude & Longitude ranges
coords_lat_range = (-90, 90)
coords_lng_range = (-180, 180)
        
coords_lat = np.random.uniform(low=-90, high=90, size=1500)
coords_lng = np.random.uniform(low=-180, high=180, size=1500)
latitude_longitude = zip(coords_lat, coords_lng)

for latitude_longitude in latitude_longitude:
    city = citipy.nearest_city(latitude_longitude[0],latitude_longitude[1]).city_name
    
    if city not in cities:
        cities.append(city)
        
        
len(cities)

# To slice the list in for loop do for value in x[0:3]
511/25: cities
511/26:
# set up lists to hold response info
city_data = []
city_counter = 0

#Loop through the list of cities and perform a request for data on each

for city in cities:
    response = requests.get(query_url + city).json()
    city_counter = city_counter + 1
        A={'lat': response['coord']['lat'],
          'long': response['coord']['lon'],
          'temp': response['main']['temp'],
          'city': city}
        
        city_data.append(A)
        print(f'{city_counter} {city}')
511/27:
# set up lists to hold response info
city_data = []
city_counter = 0

#Loop through the list of cities and perform a request for data on each

for city in cities:
    response = requests.get(query_url + city).json()
    city_counter = city_counter + 1
    A={'lat': response['coord']['lat'],
          'long': response['coord']['lon'],
          'temp': response['main']['temp'],
          'city': city}
    city_data.append(A)

    print(f'{city_counter} {city}')
511/28:
# testing
b = requests.get(query_url + "Miami").json()
b
511/29:
# testing
b = requests.get(query_url + "miami").json()
b
511/30:
import pandas as pd
import json
import requests
import matplotlib.pyplot as plt
import numpy as np
import config
import os
import time
from config import weather_api_key
from citipy import citipy

url = "https://openweathermap.org/api
units = "metric"
query_url = f"{url}appid={weather_api_key}&units={units}"
511/31:
from scipy import integrate

cities = []
latitude_longitude = []
#Latitude & Longitude ranges
coords_lat_range = (-90, 90)
coords_lng_range = (-180, 180)
        
coords_lat = np.random.uniform(low=-90, high=90, size=1500)
coords_lng = np.random.uniform(low=-180, high=180, size=1500)
latitude_longitude = zip(coords_lat, coords_lng)

for latitude_longitude in latitude_longitude:
    city = citipy.nearest_city(latitude_longitude[0],latitude_longitude[1]).city_name
    
    if city not in cities:
        cities.append(city)
        
        
len(cities)

# To slice the list in for loop do for value in x[0:3]
511/32:
# testing
b = requests.get(query_url + "miami").json()
b
511/33:
import pandas as pd
import json
import requests
import matplotlib.pyplot as plt
import numpy as np
import config
import os
import time
from config import weather_api_key
from citipy import citipy

url = "http://api.openweathermap.org/data/2.5/weather?"
units = "metric"
query_url = f"{url}appid={weather_api_key}&units={units}"
511/34:
from scipy import integrate

cities = []
latitude_longitude = []
#Latitude & Longitude ranges
coords_lat_range = (-90, 90)
coords_lng_range = (-180, 180)
        
coords_lat = np.random.uniform(low=-90, high=90, size=1500)
coords_lng = np.random.uniform(low=-180, high=180, size=1500)
latitude_longitude = zip(coords_lat, coords_lng)

for latitude_longitude in latitude_longitude:
    city = citipy.nearest_city(latitude_longitude[0],latitude_longitude[1]).city_name
    
    if city not in cities:
        cities.append(city)
        
        
len(cities)

# To slice the list in for loop do for value in x[0:3]
511/35:
# testing
b = requests.get(query_url + "miami").json()
b
514/1:
df = pd.read_csv("../Resources/sardines.csv")
df.head()
514/2:
import pandas as pd
import scipy.stats as stats
514/3:
df = pd.read_csv("../Resources/sardines.csv")
df.head()
514/4:
# Calculate the population mean for Sardine Vertebrae in Alaska
population1 = df[df["location"] == 6]
population1.vertebrae.mean()
514/5:
# Calculate the population mean for Sardine Vertebrae in Alaska
population1 = df[df["location"] == 1]
population1.vertebrae.mean()
514/6:
# Calculate the population mean for Sardine Vertebrae in San Diego
population2 = df[df["location"] == 6]
population2.vertebrae.mean()
514/7:
# Calculate Independent (Two Sample) t-test
stats.ttest_ind(population1.vertebrae, population2.vertebrae, equal_var=False)
518/1:
import warnings
warnings.filterwarnings('ignore')
518/2:
%matplotlib inline
from matplotlib import pyplot as plt
import numpy as np
import scipy.stats as stats
518/3:
# Generate some fake data to test with
def gendata(loc=0):
    population = stats.norm.rvs(size=1000, random_state=42)
    sample = stats.norm.rvs(loc=loc, size=200, random_state=42)

    # Scatter Plot of Data
    plt.subplot(2, 1, 1)
    plt.scatter(range(len(population)), population, label="population")
    plt.scatter(range(len(sample)), sample, label="sample")
    plt.legend()

    # Histogram Plot of Data
    plt.subplot(2, 1, 2)
    plt.hist(population, 20, density=True, alpha=0.7, label="population")
    plt.hist(sample, 20, density=True, alpha=0.7, label="sample")
    plt.axvline(population.mean(), color='k', linestyle='dashed', linewidth=1)
    plt.axvline(sample.mean(), color='k', linestyle='dashed', linewidth=1)
    plt.legend()  
    
    return population, sample
518/4:
# Generate some data and show the scatter plot
population, sample = gendata()
518/5: stats.ttest_1samp(sample, population.mean())
518/6:
# Generate data with a bigger difference in means
population, sample = gendata(loc=-2)
518/7: stats.ttest_1samp(sample, population.mean())
520/1: df.boxplot("pain")
520/2: df.boxplot("Pain", by="HairColour", figsize=(20, 10))
520/3:
df = pd.read_csv("../Resources/hair.csv")
df.head()
520/4: df.boxplot("Pain", by="HairColour", figsize=(20, 10))
521/1:
import warnings
warnings.filterwarnings('ignore')
521/2:
%matplotlib inline
import pandas as pd
import scipy.stats as stats
521/3:
df = pd.read_csv("../Resources/hair.csv")
df.head()
521/4: df.boxplot("Pain", by="HairColour", figsize=(20, 10))
521/5:
group1 = df[df["HairColour"] == "LightBlond"["Pain"]]
group2 = df[df["HairColour"] == "DarkBlond"["Pain"]]
group3 = df[df["HairColour"] == "LightBrunette"["Pain"]]
group4 = df[df["HairColour"] == "DarkBrunette"["Pain"]]
521/6:
group1 = df[df["HairColour"] == "LightBlond"]["Pain"]]
group2 = df[df["HairColour"] == "DarkBlond"]["Pain"]]
group3 = df[df["HairColour"] == "LightBrunette"]["Pain"]]
group4 = df[df["HairColour"] == "DarkBrunette"]["Pain"]]
523/1:
import warnings
warnings.filterwarnings('ignore')
523/2:
%matplotlib inline
import pandas as pd
import scipy.stats as stats
523/3:
df = pd.read_csv("../Resources/hair.csv")
df.head()
523/4:
# Create a boxplot to compare means
df.boxplot("Pain", by="HairColour", figsize=(20, 10))
523/5:
# Extract individual groups
group1 = df[df["HairColour"] == "LightBlond"]["Pain"]
group2 = df[df["HairColour"] == "DarkBlond"]["Pain"]
group3 = df[df["HairColour"] == "LightBrunette"]["Pain"]
group4 = df[df["HairColour"] == "DarkBrunette"]["Pain"]
523/6:
# Perform the ANOVA
stats.f_oneway(group1, group2, group3, group4)
524/1:
import warnings
warnings.filterwarnings('ignore')
524/2:
%matplotlib inline
import pandas as pd
import scipy.stats as stats
524/3:
df = pd.read_csv("../Resources/hair.csv")
df.head()
524/4:
# Create a boxplot to compare means
df.boxplot("Pain", by="HairColour", figsize=(20, 10))
524/5:
# Extract individual groups
group1 = df[df["HairColour"] == "LightBlond"]["Pain"]
group2 = df[df["HairColour"] == "DarkBlond"]["Pain"]
group3 = df[df["HairColour"] == "LightBrunette"]["Pain"]
group4 = df[df["HairColour"] == "DarkBrunette"]["Pain"]
524/6:
# Perform the ANOVA
stats.f_oneway(group1, group2, group3, group4)
524/7:
# Perform the ANOVA
stats.f_oneway(group2, group3, group4)
524/8:
# Perform the ANOVA
stats.f_oneway(group1, group3, group4)
524/9:
# Perform the ANOVA
stats.f_oneway(group1, group2, group4)
524/10:
# Perform the ANOVA
stats.f_oneway(group1, group2, group3)
525/1:
import pandas as pd
import json
import requests
import matplotlib.pyplot as plt
import numpy as np
import config
import os
import time
from config import weather_api_key
from citipy import citipy

url = "http://api.openweathermap.org/data/2.5/weather?"
units = "metric"
query_url = f"{url}appid={weather_api_key}&units={units}"
525/2:
import pandas as pd
import json
import requests
import matplotlib.pyplot as plt
import numpy as np
import config
import os
import time
from config import weather_api_key
from citipy import citipy

url = "http://api.openweathermap.org/data/2.5/weather"
units = "metric"
query_url = f"{url}appid={weather_api_key}&units={units}"
525/3:
from scipy import integrate

cities = []
latitude_longitude = []
#Latitude & Longitude ranges
coords_lat_range = (-90, 90)
coords_lng_range = (-180, 180)
        
coords_lat = np.random.uniform(low=-90, high=90, size=1500)
coords_lng = np.random.uniform(low=-180, high=180, size=1500)
latitude_longitude = zip(coords_lat, coords_lng)

for latitude_longitude in latitude_longitude:
    city = citipy.nearest_city(latitude_longitude[0],latitude_longitude[1]).city_name
    
    if city not in cities:
        cities.append(city)
        
        
len(cities)

# To slice the list in for loop do for value in x[0:3]
525/4:
# testing
b = requests.get(query_url + "miami").json()
b
525/5: weather_api_key
525/6:
# testing
b = requests.get(query_url + "miami").json()
b
525/7: weather_api_key
526/1:
import pandas as pd
import json
import requests
import matplotlib.pyplot as plt
import numpy as np
import config
import os
import time
from config import weather_api_key
from citipy import citipy

url = "http://api.openweathermap.org/data/2.5/weather"
units = "metric"
query_url = f"{url}appid={weather_api_key}&units={units}"
526/2:
import pandas as pd
import json
import requests
import matplotlib.pyplot as plt
import numpy as np
import config
import os
import time
from config import weather_api_key
from citipy import citipy

url = "http://api.openweathermap.org/data/2.5/weather"
units = "metric"
query_url = f"{url}appid={weather_api_key}&units={units}"
526/3:
import pandas as pd
import json
import requests
import matplotlib.pyplot as plt
import numpy as np
import config
import os
import time
from config import weather_api_key
from citipy import citipy

# url = "http://api.openweathermap.org/data/2.5/weather"
# units = "metric"
# query_url = f"{url}appid={weather_api_key}&units={units}"
527/1:
import pandas as pd
import json
import requests
import matplotlib.pyplot as plt
import numpy as np
import config
import os
import time
from config import weather_api_key
from citipy import citipy

url = "http://api.openweathermap.org/data/2.5/weather"
units = "metric"
query_url = f"{url}appid={weather_api_key}&units={units}"
527/2:
from scipy import integrate

cities = []
latitude_longitude = []
#Latitude & Longitude ranges
coords_lat_range = (-90, 90)
coords_lng_range = (-180, 180)
        
coords_lat = np.random.uniform(low=-90, high=90, size=1500)
coords_lng = np.random.uniform(low=-180, high=180, size=1500)
latitude_longitude = zip(coords_lat, coords_lng)

for latitude_longitude in latitude_longitude:
    city = citipy.nearest_city(latitude_longitude[0],latitude_longitude[1]).city_name
    
    if city not in cities:
        cities.append(city)
        
        
len(cities)

# To slice the list in for loop do for value in x[0:3]
527/3:
# testing
b = requests.get(query_url + "miami").json()
b
527/4:
import pandas as pd
import json
import requests
import matplotlib.pyplot as plt
import numpy as np
import config
import os
import time
from config import weather_api_key
from citipy import citipy

url = "http://api.openweathermap.org/data/2.5/weather"
units = "metric"
query_url = f"{url}appid={weather_api_key}&units={units}"
527/5:
from scipy import integrate

cities = []
latitude_longitude = []
#Latitude & Longitude ranges
coords_lat_range = (-90, 90)
coords_lng_range = (-180, 180)
        
coords_lat = np.random.uniform(low=-90, high=90, size=1500)
coords_lng = np.random.uniform(low=-180, high=180, size=1500)
latitude_longitude = zip(coords_lat, coords_lng)

for latitude_longitude in latitude_longitude:
    city = citipy.nearest_city(latitude_longitude[0],latitude_longitude[1]).city_name
    
    if city not in cities:
        cities.append(city)
        
        
len(cities)

# To slice the list in for loop do for value in x[0:3]
527/6:
# testing
b = requests.get(query_url + "miami").json()
b
527/7:
import pandas as pd
import json
import requests
import matplotlib.pyplot as plt
import numpy as np
import config
import os
import time
from config import weather_api_key
from citipy import citipy

url = "http://api.openweathermap.org"
units = "metric"
query_url = f"{url}appid={weather_api_key}&units={units}"
527/8:
from scipy import integrate

cities = []
latitude_longitude = []
#Latitude & Longitude ranges
coords_lat_range = (-90, 90)
coords_lng_range = (-180, 180)
        
coords_lat = np.random.uniform(low=-90, high=90, size=1500)
coords_lng = np.random.uniform(low=-180, high=180, size=1500)
latitude_longitude = zip(coords_lat, coords_lng)

for latitude_longitude in latitude_longitude:
    city = citipy.nearest_city(latitude_longitude[0],latitude_longitude[1]).city_name
    
    if city not in cities:
        cities.append(city)
        
        
len(cities)

# To slice the list in for loop do for value in x[0:3]
527/9:
# testing
b = requests.get(query_url + "miami").json()
b
527/10:
# testing
b = requests.get(query_url + "Miami").json()
b
527/11:
# testing
b = requests.get(query_url + "Miami").json()
b['coord']['lon']
528/1:
import pandas as pd
import json
import requests
import matplotlib.pyplot as plt
import numpy as np
import config
import os
import time
from config import weather_api_key
from citipy import citipy

url = "http://api.openweathermap.org"
units = "metric"
query_url = f"{url}appid={weather_api_key}&units={units}"
528/2:
from scipy import integrate

cities = []
latitude_longitude = []
#Latitude & Longitude ranges
coords_lat_range = (-90, 90)
coords_lng_range = (-180, 180)
        
coords_lat = np.random.uniform(low=-90, high=90, size=1500)
coords_lng = np.random.uniform(low=-180, high=180, size=1500)
latitude_longitude = zip(coords_lat, coords_lng)

for latitude_longitude in latitude_longitude:
    city = citipy.nearest_city(latitude_longitude[0],latitude_longitude[1]).city_name
    
    if city not in cities:
        cities.append(city)
        
        
len(cities)

# To slice the list in for loop do for value in x[0:3]
528/3:
# testing
b = requests.get(query_url + "Miami").json()
b['coord']['lon']
528/4: weatherpy_data.to_csv("WeatherPyData.csv", encoding='utf-8', index=False)
530/1:
import pandas as pd
import json
import requests
import matplotlib.pyplot as plt
import numpy as np
import config
import os
import time
from config import weather_api_key
from citipy import citipy

url = "http://api.openweathermap.org"
units = "metric"
query_url = f"{url}appid={weather_api_key}&units={units}"
530/2:
from scipy import integrate

cities = []
latitude_longitude = []
#Latitude & Longitude ranges
coords_lat_range = (-90, 90)
coords_lng_range = (-180, 180)
        
coords_lat = np.random.uniform(low=-90, high=90, size=1500)
coords_lng = np.random.uniform(low=-180, high=180, size=1500)
latitude_longitude = zip(coords_lat, coords_lng)

for latitude_longitude in latitude_longitude:
    city = citipy.nearest_city(latitude_longitude[0],latitude_longitude[1]).city_name
    
    if city not in cities:
        cities.append(city)
        
        
len(cities)

# To slice the list in for loop do for value in x[0:3]
530/3:
# testing
b = requests.get(query_url + "Miami").json()
b['coord']['lon']
531/1:
import pandas as pd
import json
import requests
import matplotlib.pyplot as plt
import numpy as np
import config
import os
import time
from config import weather_api_key
from citipy import citipy

url = "http://api.openweathermap.org"
units = "metric"
query_url = f"{url}appid={weather_api_key}&units={units}"
531/2:
from scipy import integrate

cities = []
latitude_longitude = []
#Latitude & Longitude ranges
coords_lat_range = (-90, 90)
coords_lng_range = (-180, 180)
        
coords_lat = np.random.uniform(low=-90, high=90, size=1500)
coords_lng = np.random.uniform(low=-180, high=180, size=1500)
latitude_longitude = zip(coords_lat, coords_lng)

for latitude_longitude in latitude_longitude:
    city = citipy.nearest_city(latitude_longitude[0],latitude_longitude[1]).city_name
    
    if city not in cities:
        cities.append(city)
        
        
len(cities)

# To slice the list in for loop do for value in x[0:3]
531/3:
# testing
b = requests.get(query_url + "Miami").json()
b['coord']['lon']
531/4:
from scipy import integrate

cities = []
latitude_longitude = []
#Latitude & Longitude ranges
coords_lat_range = (-90, 90)
coords_lng_range = (-180, 180)
        
coords_lat = np.random.uniform(low=-90, high=90, size=1500)
coords_lng = np.random.uniform(low=-180, high=180, size=1500)
latitude_longitude = zip(coords_lat, coords_lng)

for latitude_longitude in latitude_longitude:
    time.sleep()
    city = citipy.nearest_city(latitude_longitude[0],latitude_longitude[1]).city_name
    
    if city not in cities:
        cities.append(city)
        
        
len(cities)

# To slice the list in for loop do for value in x[0:3]
531/5:
from scipy import integrate

cities = []
latitude_longitude = []
#Latitude & Longitude ranges
coords_lat_range = (-90, 90)
coords_lng_range = (-180, 180)
        
coords_lat = np.random.uniform(low=-90, high=90, size=1500)
coords_lng = np.random.uniform(low=-180, high=180, size=1500)
latitude_longitude = zip(coords_lat, coords_lng)

for latitude_longitude in latitude_longitude:
    time.sleep(5)
    city = citipy.nearest_city(latitude_longitude[0],latitude_longitude[1]).city_name
    
    if city not in cities:
        cities.append(city)
        
        
len(cities)

# To slice the list in for loop do for value in x[0:3]
532/1:
import pandas as pd
import json
import requests
import matplotlib.pyplot as plt
import numpy as np
import config
import os
import time
from config import weather_api_key
from citipy import citipy

url = "http://api.openweathermap.org"
units = "metric"
query_url = f"{url}appid={weather_api_key}&units={units}"
532/2:
from scipy import integrate

cities = []
latitude_longitude = []
#Latitude & Longitude ranges
coords_lat_range = (-90, 90)
coords_lng_range = (-180, 180)
        
coords_lat = np.random.uniform(low=-90, high=90, size=1500)
coords_lng = np.random.uniform(low=-180, high=180, size=1500)
latitude_longitude = zip(coords_lat, coords_lng)

for latitude_longitude in latitude_longitude:
#     time.sleep(5)
    city = citipy.nearest_city(latitude_longitude[0],latitude_longitude[1]).city_name
    
    if city not in cities:
        cities.append(city)
        
        
len(cities)

# To slice the list in for loop do for value in x[0:3]
532/3:
# testing
b = requests.get(query_url + "Miami").json()
b['coord']['lon']
533/1:
import pandas as pd
import json
import requests
import matplotlib.pyplot as plt
import numpy as np
import config
import os
import time
from config import weather_api_key
from citipy import citipy

url = "http://api.openweathermap.org"
units = "metric"
query_url = f"{url}appid={weather_api_key}&units={units}"
533/2:
from scipy import integrate

cities = []
latitude_longitude = []
#Latitude & Longitude ranges
coords_lat_range = (-90, 90)
coords_lng_range = (-180, 180)
        
coords_lat = np.random.uniform(low=-90, high=90, size=1500)
coords_lng = np.random.uniform(low=-180, high=180, size=1500)
latitude_longitude = zip(coords_lat, coords_lng)

for latitude_longitude in latitude_longitude:
#     time.sleep(5)
    city = citipy.nearest_city(latitude_longitude[0],latitude_longitude[1]).city_name
    
    if city not in cities:
        cities.append(city)
        
        
len(cities)

# To slice the list in for loop do for value in x[0:3]
533/3:
# testing
b = requests.get(query_url + "Miami").json()
b['coord']['lon']
534/1:
import pandas as pd
import json
import requests
import matplotlib.pyplot as plt
import numpy as np
import config
import os
from time import sleep
from config import weather_api_key
from citipy import citipy

url = "http://api.openweathermap.org"
units = "metric"
query_url = f"{url}appid={weather_api_key}&units={units}"
534/2:
from scipy import integrate

cities = []
latitude_longitude = []
#Latitude & Longitude ranges
coords_lat_range = (-90, 90)
coords_lng_range = (-180, 180)
        
coords_lat = np.random.uniform(low=-90, high=90, size=1500)
coords_lng = np.random.uniform(low=-180, high=180, size=1500)
latitude_longitude = zip(coords_lat, coords_lng)

for latitude_longitude in latitude_longitude:
#     time.sleep(5)
    city = citipy.nearest_city(latitude_longitude[0],latitude_longitude[1]).city_name
    
    if city not in cities:
        cities.append(city)
        
        
len(cities)

# To slice the list in for loop do for value in x[0:3]
534/3:
# testing
b = requests.get(query_url + "Miami").json()
b['coord']['lon']
534/4:
# testing
b = requests.get(query_url + "Miami").json()
b['coord']['lon']
535/1:
import pandas as pd
import json
import requests
import matplotlib.pyplot as plt
import numpy as np
import config
import os
from time import sleep
from config import weather_api_key
from citipy import citipy

url = "http://api.openweathermap.org"
units = "metric"
query_url = f"{url}appid={weather_api_key}&units={units}"
535/2:
from scipy import integrate

cities = []
latitude_longitude = []
#Latitude & Longitude ranges
coords_lat_range = (-90, 90)
coords_lng_range = (-180, 180)
        
coords_lat = np.random.uniform(low=-90, high=90, size=1500)
coords_lng = np.random.uniform(low=-180, high=180, size=1500)
latitude_longitude = zip(coords_lat, coords_lng)

for latitude_longitude in latitude_longitude:
#     time.sleep(5)
    city = citipy.nearest_city(latitude_longitude[0],latitude_longitude[1]).city_name
    
    if city not in cities:
        cities.append(city)
        
        
len(cities)

# To slice the list in for loop do for value in x[0:3]
535/3:
# testing
b = requests.get(query_url + "Miami").json()
b['coord']['lon']
535/4:
import pandas as pd
import json
import requests
import matplotlib.pyplot as plt
import numpy as np
import config
import os
from time import sleep
from config import weather_api_key
from citipy import citipy

url = "http://api.openweathermap.org/"
units = "metric"
query_url = f"{url}appid={weather_api_key}&units={units}"
535/5:
from scipy import integrate

cities = []
latitude_longitude = []
#Latitude & Longitude ranges
coords_lat_range = (-90, 90)
coords_lng_range = (-180, 180)
        
coords_lat = np.random.uniform(low=-90, high=90, size=1500)
coords_lng = np.random.uniform(low=-180, high=180, size=1500)
latitude_longitude = zip(coords_lat, coords_lng)

for latitude_longitude in latitude_longitude:
#     time.sleep(5)
    city = citipy.nearest_city(latitude_longitude[0],latitude_longitude[1]).city_name
    
    if city not in cities:
        cities.append(city)
        
        
len(cities)

# To slice the list in for loop do for value in x[0:3]
535/6:
# testing
b = requests.get(query_url + "Miami").json()
b['coord']['lon']
535/7:
# testing
b = requests.get(query_url + "Miami").json()
b
535/8:
# testing
b = requests.get(query_url + "Evansville").json()
b
535/9:
# testing
import requests

b = return requests.get(query_url + "Miami").json()
b
535/10:
# testing
import requests

return requests.get(query_url + "Miami").json()
535/11:
# set up lists to hold response info
city_data = []
city_counter = 0

#Loop through the list of cities and perform a request for data on each

for city in cities:
    response = requests.get(query_url + city).json()
    city_counter = city_counter + 1
    A={'lat': response['coord']['lat'],
          'long': response['coord']['lon'],
          'temp': response['main']['temp'],
           'cloudiness': response['weather']['description'],
           'humidity': response['main']['humidity'],
           'wind_speed': response[]
          'city': city}
    city_data.append(A)

    print(f'{city_counter} {city}')
535/12: city
535/13: city_data
535/14:
# set up lists to hold response info
city_data = []
city_counter = 0

#Loop through the list of cities and perform a request for data on each

for city in cities:
    response = requests.get(query_url + city).json()
    city_counter = city_counter + 1
    A={'lat': response['coord']['lat'],
          'long': response['coord']['lon'],
          'temp': response['main']['temp'],
           'cloudiness': response['weather']['description'],
           'humidity': response['main']['humidity'],
           'wind_speed': response[]
          'city': city}
    city_data.append(A)

    print(f'{city_counter} {city}')
535/15: city_data
535/16:
# set up lists to hold response info
city_data = []
city_counter = 0

#Loop through the list of cities and perform a request for data on each

for city in cities:
    response = requests.get(query_url + city).json()
    city_counter = city_counter + 1
    A={'lat': response['coord']['lat'],
          'long': response['coord']['lon'],
          'temp': response['main']['temp'],
           'cloudiness': response['weather']['description'],
           'humidity': response['main']['humidity'],
           #'wind_speed': response[]
          'city': city}
    city_data.append(A)

    print(f'{city_counter} {city}')
535/17: city_data
535/18:
# set up lists to hold response info
city_data = []
city_counter = 0

#Loop through the list of cities and perform a request for data on each

for city in cities:
    response = requests.get(query_url + city).json()
    city_counter = city_counter + 1
    A={'lat': response['coord']['lat'],
          'long': response['coord']['lon'],
          'temp': response['main']['temp'],
           'cloudiness': response['weather']['description'],
           'humidity': response['main']['humidity'],
           #'wind_speed': response[]
          'city': city}
    city_data.append(A)

    print(f'{city_counter} {city}')
536/1:
#save file path to variable
US_retail_csv = "Datasets/US_Total_Rev_2018and2017"
536/2:
#Dependencies
import pandas as pd
import numpy as np
536/3:
#save file path to variable
US_retail_csv = "Datasets/US_Total_Rev_2018and2017"
536/4:
#Read with Pandas
US_retail_df = pd.read_csv(US_retail_csv)
US_retail_df.head()
536/5:
#save file path to variable
US_retail_csv = "../Datasets/US_Total_Rev_2018and2017"
536/6:
#Read with Pandas
US_retail_df = pd.read_csv(US_retail_csv)
US_retail_df.head()
536/7:
#save file path to variable
US_retail_csv = "Class_Projects/ecommerce_project_groupH/Datasets/US_Total_Rev_2018and2017"
536/8:
#Read with Pandas
US_retail_df = pd.read_csv(US_retail_csv)
US_retail_df.head()
536/9:
#save file path to variable
US_retail_csv = "Datasets/US_Total_Rev_2018and2017.csv"
536/10:
#Read with Pandas
US_retail_df = pd.read_csv(US_retail_csv)
US_retail_df.head()
536/11:
#save file path to variable
US_retailhistory_csv = "Datasets/Estimated Annual U.S. Retail Trade Sales Total and E-commerce1- 1998-2018.csv
"
536/12:
#Read with Pandas
US_retail_df = pd.read_csv(US_retail_csv)
US_retail_df.head()
536/13:
#Dependencies
import pandas as pd
import numpy as np
536/14:
#save file path to variable
US_retailhistory_csv = "Datasets/Estimated Annual U.S. Retail Trade Sales Total and E-commerce1- 1998-2018.csv
"
536/15:
#Read with Pandas
US_retail_df = pd.read_csv(US_retail_csv)
US_retail_df.head()
536/16:
#save file path to variable
US_retailhistory_csv = "Datasets/Estimated Annual U.S. Retail Trade Sales Total and E-commerce1- 1998-2018.csv"
536/17:
#Read with Pandas
US_retail_df = pd.read_csv(US_retail_csv)
US_retail_df.head()
536/18:
#save file path to variable
US_retailhistory_csv = "ecommerce_project_groupH/Datasets/US_Total_Rev_2018and2017.csv"
536/19:
#Read with Pandas
US_retail_df = pd.read_csv(US_retail_csv)
US_retail_df.head()
536/20:
#save file path to variable
US_retailhistory_csv = "Class_Projects/ecommerce_project_groupH/Datasets/US_Total_Rev_2018and2017.csv"
536/21:
#Read with Pandas
US_retail_df = pd.read_csv(US_retail_csv)
US_retail_df.head()
536/22:
#Dependencies
import pandas as pd
import numpy as np
536/23:
#save file path to variable
US_retailhistory_csv = "Class_Projects/ecommerce_project_groupH/Datasets/US_Total_Rev_2018and2017.csv"
536/24:
#Read with Pandas
US_retail_df = pd.read_csv(US_retail_csv)
US_retail_df.head()
539/1:
# Dependencies
import pandas as pd
import numpy as np
539/2:
# Save file patth to variable
retail_history_csv = "Datasets/EstUS_Retail_Trade_E-commercehistory.csv"
539/3:
# Dependencies
import pandas as pd
import numpy as np
539/4:
# Save file patth to variable
retail_history_csv = "Datasets/EstUS_Retail_Trade_E-commercehistory.csv"
539/5:
retail_history_df = pd.read_csv(retail_history_csv)
retail_history_df.head()
539/6:
retail_history_df = pd.read_csv(retail_history_csv)
US_totalrev_df = pd.read_csv()
retail_history_df.head()
539/7:
# Dependencies
import pandas as pd
import numpy as np
539/8:
# Save file path to variable
retail_history_csv = "Datasets/EstUS_Retail_Trade_E-commercehistory.csv"
US_totalrev_2018_2017_csv = "Datasets/US_Total_rrev_2018and2017.csv"
539/9:
retail_history_df = pd.read_csv(retail_history_csv)
retail_history_df.head()
539/10:
US_totalrev_df = pd.read_csv()
US_totalrev_df.head()
539/11:
US_totalrev_df = pd.read_csv(US_totalrev_2018_2017_csv)
US_totalrev_df.head()
539/12:
# Dependencies
import pandas as pd
import numpy as np
539/13:
# Save file path to variable
retail_history_csv = "Datasets/EstUS_Retail_Trade_E-commercehistory.csv"
US_totalrev_2018_2017_csv = "Datasets/US_Total_rev_2018and2017.csv"
539/14:
retail_history_df = pd.read_csv(retail_history_csv)
retail_history_df.head()
539/15:
US_totalrev_df = pd.read_csv(US_totalrev_2018_2017_csv)
US_totalrev_df.head()
539/16:
# Dependencies
import pandas as pd
import numpy as np
539/17:
# Save file path to variable
retail_history_csv = "Datasets/EstUS_Retail_Trade_E-commercehistory.csv"
US_totalrev_2018_2017_csv = "Datasets/US_Total_rev_2018and2017.csv"
539/18:
retail_history_df = pd.read_csv(retail_history_csv)
retail_history_df.head()
539/19:
# Dependencies
import pandas as pd
import numpy as np
539/20:
# Save file path to variable
retail_history_csv = "Datasets/EstUS_Retail_Trade_E-commercehistory.csv"
US_totalrev_2018_2017_csv = "Datasets/US_Total_rev_2018and2017.csv"
539/21:
retail_history_df = pd.read_csv(retail_history_csv)
retail_history_df.head()
539/22: # Clean up DF
539/23:
# Save file path to variable
retail_history_csv = "Datasets/EstUS_Retail_Trade_E-commercehistory.csv"
US_totalrev_2018_2017_csv = "Datasets/US_Total_rev_2018and2017.csv"
539/24:
# Read with pandas
retail_history_df = pd.read_csv(retail_history_csv)
retail_history_df.head()
539/25:
# Clean up DF
retail_history_df.rename(columns={"Unnamed:1": "Kind of Business",
                                 "2018": "2018 Total",
                                 "Unnamed 3": "2018 e-commerce",
                                   "2017r": "2017 Total",
                                 "Unnamed 5": "2017 e-commerce"
                                 })
retail_history_df.head()
539/26:
# Clean up DF
retail_history_df.rename(columns={"Unnamed:1": "Kind of Business",
                                 "2018": "2018 Total",
                                 "Unnamed 3": "2018 e-commerce",
                                   "2017r": "2017 Total",
                                 "Unnamed 5": "2017 e-commerce"
                                 })
retail_history_df.head()
539/27:
# Clean up DF
cleaned_retail_history = retail_history_df.rename(columns={"Unnamed:1": "Kind of Business",
                                 "2018": "2018 Total",
                                 "Unnamed 3": "2018 e-commerce",
                                   "2017r": "2017 Total",
                                 "Unnamed 5": "2017 e-commerce"
                                 })
cleaned_retail_history.head()
539/28:
# Clean up DF
cleaned_retail_history = retail_history_df.rename(columns={"Unnamed: 1": "Kind of Business",
                                 "2018": "2018 Total",
                                 "Unnamed 3": "2018 e-commerce",
                                   "2017r": "2017 Total",
                                 "Unnamed 5": "2017 e-commerce"
                                 })
cleaned_retail_history.head()
539/29:
# Clean up DF
cleaned_retail_history = retail_history_df.rename(columns={"Unnamed: 1": "Kind of Business",
                                 "2018": "2018 Total",
                                 "Unnamed: 3": "2018 e-commerce",
                                   "2017r": "2017 Total",
                                 "Unnamed: 5": "2017 e-commerce"
                                 })
cleaned_retail_history.head()
539/30:
# Clean up DF
retail_history_df = retail_history_df.rename(columns={"Unnamed: 1": "Kind of Business",
                                 "2018": "2018 Total",
                                 "Unnamed: 3": "2018 ecommerce",
                                 "2017r": "2017 Total",
                                 "Unnamed: 5": "2017 ecommerce",
                                 "2016r": "2016 Total",
                                 "Unnamed: 7": "2016 ecommerce",
                                 })
retail_history_df.head()
539/31:
# Clean up DF
retail_history_df = retail_history_df.rename(columns={"Unnamed: 1": "Kind of Business",
                                 "2018": "2018 Total",
                                 "Unnamed: 3": "2018 ecommerce",
                                 "2017r": "2017 Total",
                                 "Unnamed: 5": "2017 ecommerce",
                                 "2016r": "2016 Total",
                                 "Unnamed: 7": "2016 ecommerce",
                                 "2015r": "2015 Total",
                                 "Unnamed: 9": "2015 ecommerce"
                                
                                 })
retail_history_df
539/32:
# Clean up DF
retail_history_df = retail_history_df.rename(columns={"Unnamed: 1": "Kind of Business",
                                 "2018": "2018 Total",
                                 "Unnamed: 3": "2018 ecommerce",
                                 "2017r": "2017 Total",
                                 "Unnamed: 5": "2017 ecommerce",
                                 "2016r": "2016 Total",
                                 "Unnamed: 7": "2016 ecommerce",
                                 "2015r": "2015 Total",
                                 "Unnamed: 9": "2015 ecommerce",
                                 "2014r": "2014 Total",
                                 "Unnamed: 10": "2014 ecommerce"
                                 })
retail_history_df
539/33:
# Clean up DF
retail_history_df = retail_history_df.rename(columns={"Unnamed: 1": "Kind of Business",
                                 "2018": "2018 Total",
                                 "Unnamed: 3": "2018 ecommerce",
                                 "2017r": "2017 Total",
                                 "Unnamed: 5": "2017 ecommerce",
                                 "2016r": "2016 Total",
                                 "Unnamed: 7": "2016 ecommerce",
                                 "2015r": "2015 Total",
                                 "Unnamed: 9": "2015 ecommerce",
                                 "2014r": "2014 Total",
                                 "Unnamed: 10": "2014 ecommerce"
                                 })
retail_history_df.head()
539/34:
# Clean up DF
retail_history_df = retail_history_df.rename(columns={"Unnamed: 1": "Kind of Business",
                                 "2018": "2018 Total",
                                 "Unnamed: 3": "2018 ecommerce",
                                 "2017r": "2017 Total",
                                 "Unnamed: 5": "2017 ecommerce",
                                 "2016r": "2016 Total",
                                 "Unnamed: 7": "2016 ecommerce",
                                 "2015r": "2015 Total",
                                 "Unnamed: 9": "2015 ecommerce",
                                 "2014r": "2014 Total",
                                 "Unnamed: 10": "2014 ecommerce",
                                 "1998": "1998 Total",
                                 "Unnamed 43": "1998 ecommerce"
                                 })
retail_history_df.head()
539/35:
# Drop columns we do not need
retail_history_df = retail_history_df.drop(columns=['2013r', 'Unnamed: 13', 
                                                    '2012r', "Unnamed: 15", 
                                                    '2011r', 'Unnamed: 17', 
                                                    '2010', 'Unnamed: 19',
                                                    '2009', 'Unnamed: 21',
                                                    '2008', 'Unnamed: 23',
                                                    '2007', 'Unnamed: 25',
                                                    '2006', 'Unnamed: 27',
                                                    '2005', 'unnamed: 29'
                                                   
                                                   
                                                   
                                                   
                                                   
                                                   
                                                   ])
retail_history_df.head()
539/36:
# Drop columns we do not need
retail_history_df = retail_history_df.drop(columns=['2013r', 'Unnamed: 13', 
                                                    '2012r', "Unnamed: 15", 
                                                    '2011r', 'Unnamed: 17', 
                                                    '2010', 'Unnamed: 19',
                                                    '2009', 'Unnamed: 21',
                                                    '2008', 'Unnamed: 23',
                                                    '2007', 'Unnamed: 25',
                                                    '2006', 'Unnamed: 27',
                                                    '2005', 'Unnamed: 29'
                                                   
                                                   
                                                   
                                                   
                                                   
                                                   
                                                   ])
retail_history_df.head()
539/37:
# Drop columns we do not need
retail_history_df = retail_history_df.drop(columns=['2013r', 'Unnamed: 13', 
                                                    '2012r', "Unnamed: 15", 
                                                    '2011r', 'Unnamed: 17', 
                                                    '2010', 'Unnamed: 19',
                                                    '2009', 'Unnamed: 21',
                                                    '2008', 'Unnamed: 23',
                                                    '2007', 'Unnamed: 25',
                                                    '2006', 'Unnamed: 27',
                                                    '2005', 'Unnamed: 29',
                                                    '2004', 'Unnamed: 31',
                                                    '2003', 'Unnamed: 33',
                                                    '2002', 'Unnamed: 35',
                                                    '2001', 'Unnamed: 37',
                                                    '2000', 'Unnamed: 39',
                                                    '1999', 'Unnamed: 41',
                                                   
                                                   
                                                   
                                                   
                                                   
                                                   ])
retail_history_df.head()
539/38:
# Drop columns we do not need
retail_history_df = retail_history_df.drop(columns=['2013r', 'Unnamed: 13', 
                                                    '2012r', 'Unnamed: 15', 
                                                    '2011r', 'Unnamed: 17', 
                                                    '2010', 'Unnamed: 19',
                                                    '2009', 'Unnamed: 21',
                                                    '2008', 'Unnamed: 23',
                                                    '2007', 'Unnamed: 25',
                                                    '2006', 'Unnamed: 27',
                                                    '2005', 'Unnamed: 29',
                                                    '2004', 'Unnamed: 31',
                                                    '2003', 'Unnamed: 33',
                                                    '2002', 'Unnamed: 35',
                                                    '2001', 'Unnamed: 37',
                                                    '2000', 'Unnamed: 39',
                                                    '1999', 'Unnamed: 41',
                                                   
                                                   ])
retail_history_df.head()
539/39: columns
539/40: list(retail_history_df.columns.values)
539/41:
# Drop columns we do not need
retail_history_df = retail_history_df.drop(columns=['Unnamed: 11',
                                                    '2004', 
                                                    'Unnamed: 31',
                                                    '2003', 
                                                    'Unnamed: 33',
                                                    '2002',
                                                    'Unnamed: 35',
                                                    '2001',
                                                    'Unnamed: 37',
                                                    '2000',
                                                    'Unnamed: 39',
                                                    '1999',
                                                    'Unnamed: 41',
                                                   
                                                   ])
retail_history_df.head()
539/42:
# Drop columns we do not need
retail_history_df = retail_history_df.drop(columns=['Unnamed: 11',
                                                    '2004', 
                                                    'Unnamed: 31',
                                                    '2003', 
                                                    'Unnamed: 33',
                                                    '2002',
                                                    'Unnamed: 35',
                                                    '2001',
                                                    'Unnamed: 37',
                                                    '2000',
                                                    'Unnamed: 39',
                                                    '1999',
                                                    'Unnamed: 41',
                                                   
                                                   ])
retail_history_df
539/43: list(retail_history_df.columns.values)
539/44:
# Dependencies
import pandas as pd
import numpy as np
539/45:
# Save file path to variable
retail_history_csv = "Datasets/EstUS_Retail_Trade_E-commercehistory.csv"
US_totalrev_2018_2017_csv = "Datasets/US_Total_rev_2018and2017.csv"
539/46:
# Read with pandas
retail_history_df = pd.read_csv(retail_history_csv)
retail_history_df.head()
539/47:
# Clean up DF
retail_history_df = retail_history_df.rename(columns={"Unnamed: 1": "Kind of Business",
                                 "2018": "2018 Total",
                                 "Unnamed: 3": "2018 ecommerce",
                                 "2017r": "2017 Total",
                                 "Unnamed: 5": "2017 ecommerce",
                                 "2016r": "2016 Total",
                                 "Unnamed: 7": "2016 ecommerce",
                                 "2015r": "2015 Total",
                                 "Unnamed: 9": "2015 ecommerce",
                                 "2014r": "2014 Total",
                                 "Unnamed: 11": "2014 ecommerce",
                                 "1998": "1998 Total",
                                 "Unnamed 43": "1998 ecommerce"
                                 })
retail_history_df.head()
539/48: list(retail_history_df.columns.values)
539/49:
# Drop columns we do not need
retail_history_df = retail_history_df.drop(columns=['Unnamed: 11',
                                                    '2004', 
                                                    'Unnamed: 31',
                                                    '2003', 
                                                    'Unnamed: 33',
                                                    '2002',
                                                    'Unnamed: 35',
                                                    '2001',
                                                    'Unnamed: 37',
                                                    '2000',
                                                    'Unnamed: 39',
                                                    '1999',
                                                    'Unnamed: 41',
                                                   
                                                   ])
retail_history_df.head()
539/50:
# Drop columns we do not need
retail_history_df = retail_history_df.drop(columns=['2013r',
                                                    'Unnamed: 13',
                                                    '2012r',
                                                    'Unnamed: 15',
                                                    '2011r',
                                                    'Unnamed: 17',
                                                    '2010',
                                                     'Unnamed: 19',
                                                     '2009',
                                                     'Unnamed: 21',
                                                     '2008',
                                                     'Unnamed: 23',
                                                     '2007',
                                                     'Unnamed: 25',
                                                     '2006',
                                                     'Unnamed: 27',
                                                     '2005',
                                                     'Unnamed: 29',
                                                     '2004',
                                                     'Unnamed: 31',
                                                     '2003',
                                                     'Unnamed: 33',
                                                     '2002',
                                                     'Unnamed: 35',
                                                     '2001',
                                                     'Unnamed: 37',
                                                     '2000',
                                                     'Unnamed: 39',
                                                     '1999',
                                                     'Unnamed: 41',
                                                    '2004', 
                                                    'Unnamed: 31',
                                                    '2003', 
                                                    'Unnamed: 33',
                                                    '2002',
                                                    'Unnamed: 35',
                                                    '2001',
                                                    'Unnamed: 37',
                                                    '2000',
                                                    'Unnamed: 39',
                                                    '1999',
                                                    'Unnamed: 41',
                                                   
                                                   ])
retail_history_df.head()
539/51:
# Dependencies
import pandas as pd
import numpy as np
539/52:
# Save file path to variable
retail_history_csv = "Datasets/EstUS_Retail_Trade_E-commercehistory.csv"
US_totalrev_2018_2017_csv = "Datasets/US_Total_rev_2018and2017.csv"
539/53:
# Read with pandas
retail_history_df = pd.read_csv(retail_history_csv)
retail_history_df.head()
539/54:
# Clean up DF
retail_history_df = retail_history_df.rename(columns={"Unnamed: 1": "Kind of Business",
                                 "2018": "2018 Total",
                                 "Unnamed: 3": "2018 ecommerce",
                                 "2017r": "2017 Total",
                                 "Unnamed: 5": "2017 ecommerce",
                                 "2016r": "2016 Total",
                                 "Unnamed: 7": "2016 ecommerce",
                                 "2015r": "2015 Total",
                                 "Unnamed: 9": "2015 ecommerce",
                                 "2014r": "2014 Total",
                                 "Unnamed: 11": "2014 ecommerce",
                                 "1998": "1998 Total",
                                 "Unnamed 43": "1998 ecommerce"
                                 })
retail_history_df.head()
539/55: list(retail_history_df.columns.values)
539/56:
# Drop columns we do not need
retail_history_df = retail_history_df.drop(columns=['2013r',
                                                    'Unnamed: 13',
                                                    '2012r',
                                                    'Unnamed: 15',
                                                    '2011r',
                                                    'Unnamed: 17',
                                                    '2010',
                                                     'Unnamed: 19',
                                                     '2009',
                                                     'Unnamed: 21',
                                                     '2008',
                                                     'Unnamed: 23',
                                                     '2007',
                                                     'Unnamed: 25',
                                                     '2006',
                                                     'Unnamed: 27',
                                                     '2005',
                                                     'Unnamed: 29',
                                                     '2004',
                                                     'Unnamed: 31',
                                                     '2003',
                                                     'Unnamed: 33',
                                                     '2002',
                                                     'Unnamed: 35',
                                                     '2001',
                                                     'Unnamed: 37',
                                                     '2000',
                                                     'Unnamed: 39',
                                                     '1999',
                                                     'Unnamed: 41',
                                                    '2004', 
                                                    'Unnamed: 31',
                                                    '2003', 
                                                    'Unnamed: 33',
                                                    '2002',
                                                    'Unnamed: 35',
                                                    '2001',
                                                    'Unnamed: 37',
                                                    '2000',
                                                    'Unnamed: 39',
                                                    '1999',
                                                    'Unnamed: 41',
                                                   
                                                   ])
retail_history_df.head()
539/57:
# Clean up DF
retail_history_df = retail_history_df.rename(columns={"Unnamed: 1": "Kind of Business",
                                 "2018": "2018 Total",
                                 "Unnamed: 3": "2018 ecommerce",
                                 "2017r": "2017 Total",
                                 "Unnamed: 5": "2017 ecommerce",
                                 "2016r": "2016 Total",
                                 "Unnamed: 7": "2016 ecommerce",
                                 "2015r": "2015 Total",
                                 "Unnamed: 9": "2015 ecommerce",
                                 "2014r": "2014 Total",
                                 "Unnamed: 11": "2014 ecommerce",
                                 "1998": "1998 Total",
                                 "Unnamed: 43": "1998 ecommerce"
                                 })
retail_history_df.head()
539/58: list(retail_history_df.columns.values)
539/59:
# Drop columns we do not need
retail_history_df = retail_history_df.drop(columns=['2013r',
                                                    'Unnamed: 13',
                                                    '2012r',
                                                    'Unnamed: 15',
                                                    '2011r',
                                                    'Unnamed: 17',
                                                    '2010',
                                                     'Unnamed: 19',
                                                     '2009',
                                                     'Unnamed: 21',
                                                     '2008',
                                                     'Unnamed: 23',
                                                     '2007',
                                                     'Unnamed: 25',
                                                     '2006',
                                                     'Unnamed: 27',
                                                     '2005',
                                                     'Unnamed: 29',
                                                     '2004',
                                                     'Unnamed: 31',
                                                     '2003',
                                                     'Unnamed: 33',
                                                     '2002',
                                                     'Unnamed: 35',
                                                     '2001',
                                                     'Unnamed: 37',
                                                     '2000',
                                                     'Unnamed: 39',
                                                     '1999',
                                                     'Unnamed: 41',
                                                    '2004', 
                                                    'Unnamed: 31',
                                                    '2003', 
                                                    'Unnamed: 33',
                                                    '2002',
                                                    'Unnamed: 35',
                                                    '2001',
                                                    'Unnamed: 37',
                                                    '2000',
                                                    'Unnamed: 39',
                                                    '1999',
                                                    'Unnamed: 41',
                                                   
                                                   ])
retail_history_df.head()
539/60:
# Drop columns we do not need
retail_history_df = retail_history_df.drop(columns=['2013r',
                                                    'Unnamed: 13',
                                                    '2012r',
                                                    'Unnamed: 15',
                                                    '2011r',
                                                    'Unnamed: 17',
                                                    '2010',
                                                     'Unnamed: 19',
                                                     '2009',
                                                     'Unnamed: 21',
                                                     '2008',
                                                     'Unnamed: 23',
                                                     '2007',
                                                     'Unnamed: 25',
                                                     '2006',
                                                     'Unnamed: 27',
                                                     '2005',
                                                     'Unnamed: 29',
                                                     '2004',
                                                     'Unnamed: 31',
                                                     '2003',
                                                     'Unnamed: 33',
                                                     '2002',
                                                     'Unnamed: 35',
                                                     '2001',
                                                     'Unnamed: 37',
                                                     '2000',
                                                     'Unnamed: 39',
                                                     '1999',
                                                     'Unnamed: 41',
                                                    '2004', 
                                                    'Unnamed: 31',
                                                    '2003', 
                                                    'Unnamed: 33',
                                                    '2002',
                                                    'Unnamed: 35',
                                                    '2001',
                                                    'Unnamed: 37',
                                                    '2000',
                                                    'Unnamed: 39',
                                                    '1999',
                                                    'Unnamed: 41',
                                                   
                                                   ])
retail_history_df.head()
539/61:
# Dependencies
import pandas as pd
import numpy as np
539/62:
# Save file path to variable
retail_history_csv = "Datasets/EstUS_Retail_Trade_E-commercehistory.csv"
US_totalrev_2018_2017_csv = "Datasets/US_Total_rev_2018and2017.csv"
539/63:
# Read with pandas
retail_history_df = pd.read_csv(retail_history_csv)
retail_history_df.head()
539/64:
# Clean up DF
retail_history_df = retail_history_df.rename(columns={"Unnamed: 1": "Kind of Business",
                                 "2018": "2018 Total",
                                 "Unnamed: 3": "2018 ecommerce",
                                 "2017r": "2017 Total",
                                 "Unnamed: 5": "2017 ecommerce",
                                 "2016r": "2016 Total",
                                 "Unnamed: 7": "2016 ecommerce",
                                 "2015r": "2015 Total",
                                 "Unnamed: 9": "2015 ecommerce",
                                 "2014r": "2014 Total",
                                 "Unnamed: 11": "2014 ecommerce",
                                 "1998": "1998 Total",
                                 "Unnamed: 43": "1998 ecommerce"
                                 })
retail_history_df.head()
539/65:
# Drop columns we do not need
retail_history_df = retail_history_df.drop(columns=['2013r',
                                                    'Unnamed: 13',
                                                    '2012r',
                                                    'Unnamed: 15',
                                                    '2011r',
                                                    'Unnamed: 17',
                                                    '2010',
                                                     'Unnamed: 19',
                                                     '2009',
                                                     'Unnamed: 21',
                                                     '2008',
                                                     'Unnamed: 23',
                                                     '2007',
                                                     'Unnamed: 25',
                                                     '2006',
                                                     'Unnamed: 27',
                                                     '2005',
                                                     'Unnamed: 29',
                                                     '2004',
                                                     'Unnamed: 31',
                                                     '2003',
                                                     'Unnamed: 33',
                                                     '2002',
                                                     'Unnamed: 35',
                                                     '2001',
                                                     'Unnamed: 37',
                                                     '2000',
                                                     'Unnamed: 39',
                                                     '1999',
                                                     'Unnamed: 41',
                                                    '2004', 
                                                    'Unnamed: 31',
                                                    '2003', 
                                                    'Unnamed: 33',
                                                    '2002',
                                                    'Unnamed: 35',
                                                    '2001',
                                                    'Unnamed: 37',
                                                    '2000',
                                                    'Unnamed: 39',
                                                    '1999',
                                                    'Unnamed: 41',
                                                   
                                                   ])
retail_history_df.head()
539/66: retail_history_df
539/67: retail_history_df
541/1:
# Dependencies
import pandas as pd
import numpy as np
541/2:
# Save file path to variable
retail_history_csv = "Datasets/EstUS_Retail_Trade_E-commercehistory.csv"
US_totalrev_2018_2017_csv = "Datasets/US_Total_rev_2018and2017.csv"
541/3:
# Read with pandas
retail_history_df = pd.read_csv(retail_history_csv)
retail_history_df.head()
541/4:
# Clean up DF
retail_history_df = retail_history_df.rename(columns={"Unnamed: 1": "Kind of Business",
                                 "2018": "2018 Total",
                                 "Unnamed: 3": "2018 ecommerce",
                                 "2017r": "2017 Total",
                                 "Unnamed: 5": "2017 ecommerce",
                                 "2016r": "2016 Total",
                                 "Unnamed: 7": "2016 ecommerce",
                                 "2015r": "2015 Total",
                                 "Unnamed: 9": "2015 ecommerce",
                                 "2014r": "2014 Total",
                                 "Unnamed: 11": "2014 ecommerce",
                                 "1998": "1998 Total",
                                 "Unnamed: 43": "1998 ecommerce"
                                 })
retail_history_df.head()
541/5:
# Drop columns we do not need
retail_history_df = retail_history_df.drop(columns=['2013r',
                                                    'Unnamed: 13',
                                                    '2012r',
                                                    'Unnamed: 15',
                                                    '2011r',
                                                    'Unnamed: 17',
                                                    '2010',
                                                     'Unnamed: 19',
                                                     '2009',
                                                     'Unnamed: 21',
                                                     '2008',
                                                     'Unnamed: 23',
                                                     '2007',
                                                     'Unnamed: 25',
                                                     '2006',
                                                     'Unnamed: 27',
                                                     '2005',
                                                     'Unnamed: 29',
                                                     '2004',
                                                     'Unnamed: 31',
                                                     '2003',
                                                     'Unnamed: 33',
                                                     '2002',
                                                     'Unnamed: 35',
                                                     '2001',
                                                     'Unnamed: 37',
                                                     '2000',
                                                     'Unnamed: 39',
                                                     '1999',
                                                     'Unnamed: 41',
                                                    '2004', 
                                                    'Unnamed: 31',
                                                    '2003', 
                                                    'Unnamed: 33',
                                                    '2002',
                                                    'Unnamed: 35',
                                                    '2001',
                                                    'Unnamed: 37',
                                                    '2000',
                                                    'Unnamed: 39',
                                                    '1999',
                                                    'Unnamed: 41',
                                                   
                                                   ])
retail_history_df.head()
541/6: retail_history_df
541/7: list(retail_history_df.columns.values)
541/8:
US_totalrev_df = pd.read_csv(US_totalrev_2018_2017_csv)
US_totalrev_df.head()
541/9: # Rename columns
544/1:
# Dependencies
import pandas as pd
import numpy as np
544/2:
# Save file path to variable
retail_history_csv = "Datasets/EstUS_Retail_Trade_E-commercehistory.csv"
US_totalrev_2018_2017_csv = "Datasets/US_Total_rev_2018and2017.csv"
544/3:
# Read with pandas
retail_history_df = pd.read_csv(retail_history_csv)
retail_history_df.head()
544/4:
# Clean up DF
retail_history_df = retail_history_df.rename(columns={"Unnamed: 1": "Kind of Business",
                                 "2018": "2018 Total",
                                 "Unnamed: 3": "2018 Ecommerce Total",
                                 "2017r": "2017 Total",
                                 "Unnamed: 5": "2017 Ecommerce Total",
                                 "2016r": "2016 Total",
                                 "Unnamed: 7": "2016 Ecommerce Total",
                                 "2015r": "2015 Total",
                                 "Unnamed: 9": "2015 Ecommerce Total",
                                 "2014r": "2014 Total",
                                 "Unnamed: 11": "2014 Ecommerce Total",
                                 "1998": "1998 Total",
                                 "Unnamed: 43": "1998 Ecommerce Total"
                                 })
retail_history_df.head()
544/5:
# Drop columns we do not need
retail_history_df = retail_history_df.drop(columns=['2013r',
                                                    'Unnamed: 13',
                                                    '2012r',
                                                    'Unnamed: 15',
                                                    '2011r',
                                                    'Unnamed: 17',
                                                    '2010',
                                                     'Unnamed: 19',
                                                     '2009',
                                                     'Unnamed: 21',
                                                     '2008',
                                                     'Unnamed: 23',
                                                     '2007',
                                                     'Unnamed: 25',
                                                     '2006',
                                                     'Unnamed: 27',
                                                     '2005',
                                                     'Unnamed: 29',
                                                     '2004',
                                                     'Unnamed: 31',
                                                     '2003',
                                                     'Unnamed: 33',
                                                     '2002',
                                                     'Unnamed: 35',
                                                     '2001',
                                                     'Unnamed: 37',
                                                     '2000',
                                                     'Unnamed: 39',
                                                     '1999',
                                                     'Unnamed: 41',
                                                    '2004', 
                                                    'Unnamed: 31',
                                                    '2003', 
                                                    'Unnamed: 33',
                                                    '2002',
                                                    'Unnamed: 35',
                                                    '2001',
                                                    'Unnamed: 37',
                                                    '2000',
                                                    'Unnamed: 39',
                                                    '1999',
                                                    'Unnamed: 41',
                                                   
                                                   ])
retail_history_df.head()
544/6:
retail_history_df
# D - Denotes an estimate withheld to avoid disclosing data of individual companies; data are included in higher-level totals.
# S - Estimate does not meet publication standards because of high sampling variability (coefficient of variation is greater than 30%), poor response quality (total quantity response rate is less than 50%), or other concerns about the estimate's quality. Unpublished estimates derived from this table by subtraction are subject to these same limitations and should not be attributed to the U.S. Census Bureau. For a description of publication standards and the total quantity response rate, see https://www.census.gov/about/policies/quality/standards/standardf1.html.
544/7: list(retail_history_df.columns.values)
544/8:
US_totalrev_df = pd.read_csv(US_totalrev_2018_2017_csv)
US_totalrev_df.head()
544/9: # Rename columns
544/10:
# Drop rows that do not have ecommerce data
retail_history_df = retail_history_df.drop([7])
retail_history_df
544/11:
US_totalrev_df = pd.read_csv(US_totalrev_2018_2017_csv)
US_totalrev_df.head()
544/12:
US_totalrev_df = pd.read_csv(US_totalrev_2018_2017_csv)
US_totalrev_df.head()
544/13:
US_totalrev_df = pd.read_csv(US_totalrev_2018_2017_csv)
US_totalrev_df.head()
544/14:
US_totalrev_df = pd.read_csv(US_totalrev_2018_2017_csv)
US_totalrev_df.head()
544/15:
US_totalrev_df = pd.read_csv(US_totalrev_2018_2017_csv)
US_totalrev_df.head()
544/16:
US_totalrev_df = pd.read_csv(US_totalrev_2018_2017_csv)
US_totalrev_df.head()
544/17:
US_totalrev_df = pd.read_csv(US_totalrev_2018_2017_csv)
US_totalrev_df
544/18:
US_totalrev_df = pd.read_csv(US_totalrev_2018_2017_csv)
US_totalrev_df.head()
544/19:
US_totalrev_df = US_totalrev_df.drop(columns=['NaN', 'NAICS Code'])
US_totalrev_df.head()
544/20:
US_totalrev_df.resetindex()
US_totalrev_df.head()
544/21:
US_totalrev_df.reset_index()
US_totalrev_df.head()
544/22:
US_totalrev_df = US_totalrev_df.drop(columns=['NaN', 'NAICS Code'])
US_totalrev_df.head()
544/23:
US_totalrev_df = pd.read_csv(US_totalrev_2018_2017_csv)
US_totalrev_df.head()
544/24:
US_totalrev_df = US_totalrev_df.drop(columns=['Unnamed: 0', 'NAICS Code'])
US_totalrev_df.head()
544/25:
# Rename columns
US_totalrev_df = US_totalrev_df.rename(columns={"Description": "Description of Inudstries",
                                                "Total": "Value of Revenue: 2018 Total",
                                                "Revenue from Electronic Sources": "Value of Revenue: 2018 Ecommerce Total",
                                                "Total.1": "Value of Revenue: 2017 Total",
                                                "Revenue from Electronic Sources.1": "Value of Revenue: 2017 Ecommerce Total",
                                                "Total": "Year over Year % Change",
                                                "Revenue from Electronic Sources.2": "Year over Year % Change: Ecommerce",
                                                "2018": "Ecommerce Revenue as a % of Total Revenue, 2018",
                                                "2017": "Ecommerce Revenue as a % of Total Revenue, 2017",
                                                "2018.1": "% Distribution of Revenue, Ecommerce"
                                                 })
US_totalrev_df.head()
544/26:
# Drop Columns
US_totalrev_df = US_totalrev_df.drop(columns=['Unnamed: 0', 'NAICS Code'])
US_totalrev_df.head()
544/27:
US_totalrev_df = pd.read_csv(US_totalrev_2018_2017_csv)
US_totalrev_df.head()
544/28:
# Drop Columns
US_totalrev_df = US_totalrev_df.drop(columns=['Unnamed: 0', 'NAICS Code'])
US_totalrev_df.head()
544/29:
# Rename columns
US_totalrev_df = US_totalrev_df.rename(columns={"Description": "Description of Inudstries",
                                                "Total": "2018 Revenue Total",
                                                "Revenue from Electronic Sources": "2018 Ecommerce Total",
                                                "Total.2": "2017 Revenue Total",
                                                "Revenue from Electronic Sources.2": "2017 Ecommerce Total",
                                                "Total": "Year over Year % Change",
                                                "Revenue from Electronic Sources.3": "Year over Year % Change: Ecommerce",
                                                "2018": "Ecommerce Revenue as a % of Total Revenue, 2018",
                                                "2017": "Ecommerce Revenue as a % of Total Revenue, 2017",
                                                "2018(2)": "% Distribution of Revenue, Ecommerce 2018"
                                                 })
US_totalrev_df.head()
544/30:
# Rename columns
US_totalrev_df = US_totalrev_df.rename(columns={"Description": "Description of Inudstries",
                                                "Total": "2018 Revenue Total",
                                                "Revenue from Electronic Sources": "2018 Ecommerce Total",
                                                "Total.2": "2017 Revenue Total",
                                                "Revenue from Electronic Sources.2": "2017 Ecommerce Total",
                                                "Total.3": "Year over Year % Change",
                                                "Revenue from Electronic Sources.3": "Year over Year % Change: Ecommerce",
                                                "2018": "Ecommerce Revenue as a % of Total Revenue, 2018",
                                                "2017": "Ecommerce Revenue as a % of Total Revenue, 2017",
                                                "2018(2)": "Percent Distribution of Revenue, Ecommerce 2018"
                                                 })
US_totalrev_df.head()
544/31:
# Rename columns
US_totalrev_df = US_totalrev_df.rename(columns={"Description": "Description of Inudstries",
                                                "Total": "2018 Revenue Total",
                                                "Revenue from Electronic Sources": "2018 Ecommerce Total",
                                                "Total.2": "2017 Revenue Total",
                                                "Revenue from Electronic Sources.2": "2017 Ecommerce Total",
                                                "Total.3": "Year over Year % Change",
                                                "Revenue from Electronic Sources.3": "Year over Year % Change: Ecommerce",
                                                "2018": "Ecommerce Revenue as a % of Total Revenue, 2018",
                                                "2017": "Ecommerce Revenue as a % of Total Revenue, 2017",
                                                "2018(2)": "Percent Distribution of Revenue, Ecommerce 2018"
                                                 })
US_totalrev_df.head()
545/1:
# Dependencies
import pandas as pd
import numpy as np
545/2:
# Save file path to variable
retail_history_csv = "Datasets/EstUS_Retail_Trade_E-commercehistory.csv"
US_totalrev_2018_2017_csv = "Datasets/US_Total_rev_2018and2017.csv"
545/3:
# Read with pandas
retail_history_df = pd.read_csv(retail_history_csv)
retail_history_df.head()
545/4:
# Clean up DF
retail_history_df = retail_history_df.rename(columns={"Unnamed: 1": "Kind of Business",
                                 "2018": "2018 Total",
                                 "Unnamed: 3": "2018 Ecommerce Total",
                                 "2017r": "2017 Total",
                                 "Unnamed: 5": "2017 Ecommerce Total",
                                 "2016r": "2016 Total",
                                 "Unnamed: 7": "2016 Ecommerce Total",
                                 "2015r": "2015 Total",
                                 "Unnamed: 9": "2015 Ecommerce Total",
                                 "2014r": "2014 Total",
                                 "Unnamed: 11": "2014 Ecommerce Total",
                                 "1998": "1998 Total",
                                 "Unnamed: 43": "1998 Ecommerce Total"
                                 })
retail_history_df.head()
545/5:
# Drop columns we do not need
retail_history_df = retail_history_df.drop(columns=['2013r',
                                                    'Unnamed: 13',
                                                    '2012r',
                                                    'Unnamed: 15',
                                                    '2011r',
                                                    'Unnamed: 17',
                                                    '2010',
                                                     'Unnamed: 19',
                                                     '2009',
                                                     'Unnamed: 21',
                                                     '2008',
                                                     'Unnamed: 23',
                                                     '2007',
                                                     'Unnamed: 25',
                                                     '2006',
                                                     'Unnamed: 27',
                                                     '2005',
                                                     'Unnamed: 29',
                                                     '2004',
                                                     'Unnamed: 31',
                                                     '2003',
                                                     'Unnamed: 33',
                                                     '2002',
                                                     'Unnamed: 35',
                                                     '2001',
                                                     'Unnamed: 37',
                                                     '2000',
                                                     'Unnamed: 39',
                                                     '1999',
                                                     'Unnamed: 41',
                                                    '2004', 
                                                    'Unnamed: 31',
                                                    '2003', 
                                                    'Unnamed: 33',
                                                    '2002',
                                                    'Unnamed: 35',
                                                    '2001',
                                                    'Unnamed: 37',
                                                    '2000',
                                                    'Unnamed: 39',
                                                    '1999',
                                                    'Unnamed: 41',
                                                   
                                                   ])
retail_history_df.head()
545/6:
# Dependencies
import pandas as pd
import numpy as np
545/7:
# Save file path to variable
retail_history_csv = "Datasets/EstUS_Retail_Trade_E-commercehistory.csv"
US_totalrev_2018_2017_csv = "Datasets/US_Total_rev_2018and2017.csv"
545/8:
# Read with pandas
retail_history_df = pd.read_csv(retail_history_csv)
retail_history_df.head()
545/9:
# Dependencies
import pandas as pd
import numpy as np
545/10:
# Save file path to variable
retail_history_csv = "Datasets/EstUS_Retail_Trade_E-commercehistory.csv"
US_totalrev_2018_2017_csv = "Datasets/US_Total_rev_2018and2017.csv"
545/11:
# Read with pandas
retail_history_df = pd.read_csv(retail_history_csv)
retail_history_df.head()
545/12:
# Clean up DF
retail_history_df = retail_history_df.rename(columns={"Unnamed: 1": "Kind of Business",
                                 "2018": "2018 Total",
                                 "Unnamed: 3": "2018 Ecommerce Total",
                                 "2017r": "2017 Total",
                                 "Unnamed: 5": "2017 Ecommerce Total",
                                 "2016r": "2016 Total",
                                 "Unnamed: 7": "2016 Ecommerce Total",
                                 "2015r": "2015 Total",
                                 "Unnamed: 9": "2015 Ecommerce Total",
                                 "2014r": "2014 Total",
                                 "Unnamed: 11": "2014 Ecommerce Total",
                                 "1998": "1998 Total",
                                 "Unnamed: 43": "1998 Ecommerce Total"
                                 })
retail_history_df.head()
545/13:
# Dependencies
import pandas as pd
import numpy as np
545/14:
# Save file path to variable
retail_history_csv = "Datasets/EstUS_Retail_Trade_E-commercehistory.csv"
US_totalrev_2018_2017_csv = "Datasets/US_Total_rev_2018and2017.csv"
545/15:
# Read with pandas
retail_history_df = pd.read_csv(retail_history_csv)
retail_history_df.head()
545/16:
# Clean up DF
retail_history_df = retail_history_df.rename(columns={"Unnamed: 1": "Kind of Business",
                                 "2018": "2018 Total",
                                 "Unnamed: 3": "2018 Ecommerce Total",
                                 "2017r": "2017 Total",
                                 "Unnamed: 5": "2017 Ecommerce Total",
                                 "2016r": "2016 Total",
                                 "Unnamed: 7": "2016 Ecommerce Total",
                                 "2015r": "2015 Total",
                                 "Unnamed: 9": "2015 Ecommerce Total",
                                 "2014r": "2014 Total",
                                 "Unnamed: 11": "2014 Ecommerce Total",
                                 "1998": "1998 Total",
                                 "Unnamed: 43": "1998 Ecommerce Total"
                                 })
retail_history_df.head()
545/17:
# Drop columns we do not need
retail_history_df = retail_history_df.drop(columns=['2013r',
                                                    'Unnamed: 13',
                                                    '2012r',
                                                    'Unnamed: 15',
                                                    '2011r',
                                                    'Unnamed: 17',
                                                    '2010',
                                                     'Unnamed: 19',
                                                     '2009',
                                                     'Unnamed: 21',
                                                     '2008',
                                                     'Unnamed: 23',
                                                     '2007',
                                                     'Unnamed: 25',
                                                     '2006',
                                                     'Unnamed: 27',
                                                     '2005',
                                                     'Unnamed: 29',
                                                     '2004',
                                                     'Unnamed: 31',
                                                     '2003',
                                                     'Unnamed: 33',
                                                     '2002',
                                                     'Unnamed: 35',
                                                     '2001',
                                                     'Unnamed: 37',
                                                     '2000',
                                                     'Unnamed: 39',
                                                     '1999',
                                                     'Unnamed: 41',
                                                    '2004', 
                                                    'Unnamed: 31',
                                                    '2003', 
                                                    'Unnamed: 33',
                                                    '2002',
                                                    'Unnamed: 35',
                                                    '2001',
                                                    'Unnamed: 37',
                                                    '2000',
                                                    'Unnamed: 39',
                                                    '1999',
                                                    'Unnamed: 41',
                                                   
                                                   ])
retail_history_df.head()
545/18:
# Drop columns we do not need
retail_history_df = retail_history_df.drop(columns=['NAICS Code'
                                                    '2013r',
                                                    'Unnamed: 13',
                                                    '2012r',
                                                    'Unnamed: 15',
                                                    '2011r',
                                                    'Unnamed: 17',
                                                    '2010',
                                                     'Unnamed: 19',
                                                     '2009',
                                                     'Unnamed: 21',
                                                     '2008',
                                                     'Unnamed: 23',
                                                     '2007',
                                                     'Unnamed: 25',
                                                     '2006',
                                                     'Unnamed: 27',
                                                     '2005',
                                                     'Unnamed: 29',
                                                     '2004',
                                                     'Unnamed: 31',
                                                     '2003',
                                                     'Unnamed: 33',
                                                     '2002',
                                                     'Unnamed: 35',
                                                     '2001',
                                                     'Unnamed: 37',
                                                     '2000',
                                                     'Unnamed: 39',
                                                     '1999',
                                                     'Unnamed: 41',
                                                    '2004', 
                                                    'Unnamed: 31',
                                                    '2003', 
                                                    'Unnamed: 33',
                                                    '2002',
                                                    'Unnamed: 35',
                                                    '2001',
                                                    'Unnamed: 37',
                                                    '2000',
                                                    'Unnamed: 39',
                                                    '1999',
                                                    'Unnamed: 41',
                                                   
                                                   ])
retail_history_df.head()
545/19:
# Read with pandas
retail_history_df = pd.read_csv(retail_history_csv)
retail_history_df.head()
545/20:
# Clean up DF
retail_history_df = retail_history_df.rename(columns={"Unnamed: 1": "Kind of Business",
                                 "2018": "2018 Total",
                                 "Unnamed: 3": "2018 Ecommerce Total",
                                 "2017r": "2017 Total",
                                 "Unnamed: 5": "2017 Ecommerce Total",
                                 "2016r": "2016 Total",
                                 "Unnamed: 7": "2016 Ecommerce Total",
                                 "2015r": "2015 Total",
                                 "Unnamed: 9": "2015 Ecommerce Total",
                                 "2014r": "2014 Total",
                                 "Unnamed: 11": "2014 Ecommerce Total",
                                 "1998": "1998 Total",
                                 "Unnamed: 43": "1998 Ecommerce Total"
                                 })
retail_history_df.head()
545/21:
# Drop columns we do not need
retail_history_df = retail_history_df.drop(columns=['NAICS Code'
                                                    '2013r',
                                                    'Unnamed: 13',
                                                    '2012r',
                                                    'Unnamed: 15',
                                                    '2011r',
                                                    'Unnamed: 17',
                                                    '2010',
                                                     'Unnamed: 19',
                                                     '2009',
                                                     'Unnamed: 21',
                                                     '2008',
                                                     'Unnamed: 23',
                                                     '2007',
                                                     'Unnamed: 25',
                                                     '2006',
                                                     'Unnamed: 27',
                                                     '2005',
                                                     'Unnamed: 29',
                                                     '2004',
                                                     'Unnamed: 31',
                                                     '2003',
                                                     'Unnamed: 33',
                                                     '2002',
                                                     'Unnamed: 35',
                                                     '2001',
                                                     'Unnamed: 37',
                                                     '2000',
                                                     'Unnamed: 39',
                                                     '1999',
                                                     'Unnamed: 41',
                                                    '2004', 
                                                    'Unnamed: 31',
                                                    '2003', 
                                                    'Unnamed: 33',
                                                    '2002',
                                                    'Unnamed: 35',
                                                    '2001',
                                                    'Unnamed: 37',
                                                    '2000',
                                                    'Unnamed: 39',
                                                    '1999',
                                                    'Unnamed: 41',
                                                   
                                                   ])
retail_history_df.head()
545/22:
# Drop columns we do not need
retail_history_df = retail_history_df.drop(columns=['NAICS Code',
                                                    '2013r',
                                                    'Unnamed: 13',
                                                    '2012r',
                                                    'Unnamed: 15',
                                                    '2011r',
                                                    'Unnamed: 17',
                                                    '2010',
                                                     'Unnamed: 19',
                                                     '2009',
                                                     'Unnamed: 21',
                                                     '2008',
                                                     'Unnamed: 23',
                                                     '2007',
                                                     'Unnamed: 25',
                                                     '2006',
                                                     'Unnamed: 27',
                                                     '2005',
                                                     'Unnamed: 29',
                                                     '2004',
                                                     'Unnamed: 31',
                                                     '2003',
                                                     'Unnamed: 33',
                                                     '2002',
                                                     'Unnamed: 35',
                                                     '2001',
                                                     'Unnamed: 37',
                                                     '2000',
                                                     'Unnamed: 39',
                                                     '1999',
                                                     'Unnamed: 41',
                                                    '2004', 
                                                    'Unnamed: 31',
                                                    '2003', 
                                                    'Unnamed: 33',
                                                    '2002',
                                                    'Unnamed: 35',
                                                    '2001',
                                                    'Unnamed: 37',
                                                    '2000',
                                                    'Unnamed: 39',
                                                    '1999',
                                                    'Unnamed: 41',
                                                   
                                                   ])
retail_history_df.head()
545/23:
retail_history_df
# D - Denotes an estimate withheld to avoid disclosing data of individual companies; data are included in higher-level totals.
# S - Estimate does not meet publication standards because of high sampling variability (coefficient of variation is greater than 30%), poor response quality (total quantity response rate is less than 50%), or other concerns about the estimate's quality. Unpublished estimates derived from this table by subtraction are subject to these same limitations and should not be attributed to the U.S. Census Bureau. For a description of publication standards and the total quantity response rate, see https://www.census.gov/about/policies/quality/standards/standardf1.html.
545/24:
retail_history_df.head()
# D - Denotes an estimate withheld to avoid disclosing data of individual companies; data are included in higher-level totals.
# S - Estimate does not meet publication standards because of high sampling variability (coefficient of variation is greater than 30%), poor response quality (total quantity response rate is less than 50%), or other concerns about the estimate's quality. Unpublished estimates derived from this table by subtraction are subject to these same limitations and should not be attributed to the U.S. Census Bureau. For a description of publication standards and the total quantity response rate, see https://www.census.gov/about/policies/quality/standards/standardf1.html.
545/25:
# Drop rows that do not have ecommerce data
#[Estimates are shown in millions of dollars and are based on data from the Annual Retail Trade Survey. Estimates have been adjusted using final results of the 2012 Economic Census.] 
retail_history_df = retail_history_df.drop([0, 7])
retail_history_df
545/26:
# Drop rows that do not have ecommerce data
#[Estimates are shown in millions of dollars and are based on data from the Annual Retail Trade Survey. Estimates have been adjusted using final results of the 2012 Economic Census.] 
retail_history_df = retail_history_df.drop([0, 7])
retail_history_df
545/27:
retail_history_df.head()
# D - Denotes an estimate withheld to avoid disclosing data of individual companies; data are included in higher-level totals.
# S - Estimate does not meet publication standards because of high sampling variability (coefficient of variation is greater than 30%), poor response quality (total quantity response rate is less than 50%), or other concerns about the estimate's quality. Unpublished estimates derived from this table by subtraction are subject to these same limitations and should not be attributed to the U.S. Census Bureau. For a description of publication standards and the total quantity response rate, see https://www.census.gov/about/policies/quality/standards/standardf1.html.
545/28:
# Drop rows that do not have ecommerce data
#[Estimates are shown in millions of dollars and are based on data from the Annual Retail Trade Survey. Estimates have been adjusted using final results of the 2012 Economic Census.] 
retail_history_df = retail_history_df.drop([0, 7])
retail_history_df
546/1:
# Dependencies
import pandas as pd
import numpy as np
546/2:
# Save file path to variable
retail_history_csv = "Datasets/EstUS_Retail_Trade_E-commercehistory.csv"
US_totalrev_2018_2017_csv = "Datasets/US_Total_rev_2018and2017.csv"
546/3:
# Read with pandas
retail_history_df = pd.read_csv(retail_history_csv)
retail_history_df.head()
546/4:
# Clean up DF
retail_history_df = retail_history_df.rename(columns={"Unnamed: 1": "Kind of Business",
                                 "2018": "2018 Total",
                                 "Unnamed: 3": "2018 Ecommerce Total",
                                 "2017r": "2017 Total",
                                 "Unnamed: 5": "2017 Ecommerce Total",
                                 "2016r": "2016 Total",
                                 "Unnamed: 7": "2016 Ecommerce Total",
                                 "2015r": "2015 Total",
                                 "Unnamed: 9": "2015 Ecommerce Total",
                                 "2014r": "2014 Total",
                                 "Unnamed: 11": "2014 Ecommerce Total",
                                 "1998": "1998 Total",
                                 "Unnamed: 43": "1998 Ecommerce"
                                 })
retail_history_df.head()
546/5:
# Drop columns we do not need
retail_history_df = retail_history_df.drop(columns=['NAICS Code',
                                                    '2013r',
                                                    'Unnamed: 13',
                                                    '2012r',
                                                    'Unnamed: 15',
                                                    '2011r',
                                                    'Unnamed: 17',
                                                    '2010',
                                                     'Unnamed: 19',
                                                     '2009',
                                                     'Unnamed: 21',
                                                     '2008',
                                                     'Unnamed: 23',
                                                     '2007',
                                                     'Unnamed: 25',
                                                     '2006',
                                                     'Unnamed: 27',
                                                     '2005',
                                                     'Unnamed: 29',
                                                     '2004',
                                                     'Unnamed: 31',
                                                     '2003',
                                                     'Unnamed: 33',
                                                     '2002',
                                                     'Unnamed: 35',
                                                     '2001',
                                                     'Unnamed: 37',
                                                     '2000',
                                                     'Unnamed: 39',
                                                     '1999',
                                                     'Unnamed: 41',
                                                    '2004', 
                                                    'Unnamed: 31',
                                                    '2003', 
                                                    'Unnamed: 33',
                                                    '2002',
                                                    'Unnamed: 35',
                                                    '2001',
                                                    'Unnamed: 37',
                                                    '2000',
                                                    'Unnamed: 39',
                                                    '1999',
                                                    'Unnamed: 41',
                                                   
                                                   ])
retail_history_df.head()
546/6:
retail_history_df.head()
# D - Denotes an estimate withheld to avoid disclosing data of individual companies; data are included in higher-level totals.
# S - Estimate does not meet publication standards because of high sampling variability (coefficient of variation is greater than 30%), poor response quality (total quantity response rate is less than 50%), or other concerns about the estimate's quality. Unpublished estimates derived from this table by subtraction are subject to these same limitations and should not be attributed to the U.S. Census Bureau. For a description of publication standards and the total quantity response rate, see https://www.census.gov/about/policies/quality/standards/standardf1.html.
546/7:
# Drop rows that do not have ecommerce data
#[Estimates are shown in millions of dollars and are based on data from the Annual Retail Trade Survey. Estimates have been adjusted using final results of the 2012 Economic Census.] 
retail_history_df = retail_history_df.drop([0, 7])
retail_history_df
546/8:
US_totalrev_df = pd.read_csv(US_totalrev_2018_2017_csv)
US_totalrev_df.head()
546/9:
# Drop Columns
US_totalrev_df = US_totalrev_df.drop(columns=['Unnamed: 0', 'NAICS Code'])
US_totalrev_df.head()
546/10:
# Rename columns
US_totalrev_df = US_totalrev_df.rename(columns={"Description": "Description of Inudstries",
                                                "Total": "2018 Revenue Total",
                                                "Revenue from Electronic Sources": "2018 Ecommerce Total",
                                                "Total.2": "2017 Revenue Total",
                                                "Revenue from Electronic Sources.2": "2017 Ecommerce Total",
                                                "Total.3": "Year over Year % Change",
                                                "Revenue from Electronic Sources.3": "Year over Year % Change: Ecommerce",
                                                "2018": "Ecommerce Revenue as a % of Total Revenue, 2018",
                                                "2017": "Ecommerce Revenue as a % of Total Revenue, 2017",
                                                "2018(2)": "Percent Distribution of Revenue, Ecommerce 2018"
                                                 })
US_totalrev_df.head()
546/11:
# Drop rows that do not have ecommerce data
#[Estimates are shown in millions of dollars and are based on data from the Annual Retail Trade Survey. Estimates have been adjusted using final results of the 2012 Economic Census.] 
retail_history_df = retail_history_df.drop([0, 8])
retail_history_df
547/1:
# Dependencies
import pandas as pd
import numpy as np
547/2:
# Save file path to variable
retail_history_csv = "Datasets/EstUS_Retail_Trade_E-commercehistory.csv"
US_totalrev_2018_2017_csv = "Datasets/US_Total_rev_2018and2017.csv"
547/3:
# Read with pandas
retail_history_df = pd.read_csv(retail_history_csv)
retail_history_df.head()
547/4:
# Clean up DF
retail_history_df = retail_history_df.rename(columns={"Unnamed: 1": "Kind of Business",
                                 "2018": "2018 Total",
                                 "Unnamed: 3": "2018 Ecommerce Total",
                                 "2017r": "2017 Total",
                                 "Unnamed: 5": "2017 Ecommerce Total",
                                 "2016r": "2016 Total",
                                 "Unnamed: 7": "2016 Ecommerce Total",
                                 "2015r": "2015 Total",
                                 "Unnamed: 9": "2015 Ecommerce Total",
                                 "2014r": "2014 Total",
                                 "Unnamed: 11": "2014 Ecommerce Total",
                                 "1998": "1998 Total",
                                 "Unnamed: 43": "1998 Ecommerce"
                                 })
retail_history_df.head()
547/5:
# Drop columns we do not need
retail_history_df = retail_history_df.drop(columns=['NAICS Code',
                                                    '2013r',
                                                    'Unnamed: 13',
                                                    '2012r',
                                                    'Unnamed: 15',
                                                    '2011r',
                                                    'Unnamed: 17',
                                                    '2010',
                                                     'Unnamed: 19',
                                                     '2009',
                                                     'Unnamed: 21',
                                                     '2008',
                                                     'Unnamed: 23',
                                                     '2007',
                                                     'Unnamed: 25',
                                                     '2006',
                                                     'Unnamed: 27',
                                                     '2005',
                                                     'Unnamed: 29',
                                                     '2004',
                                                     'Unnamed: 31',
                                                     '2003',
                                                     'Unnamed: 33',
                                                     '2002',
                                                     'Unnamed: 35',
                                                     '2001',
                                                     'Unnamed: 37',
                                                     '2000',
                                                     'Unnamed: 39',
                                                     '1999',
                                                     'Unnamed: 41',
                                                    '2004', 
                                                    'Unnamed: 31',
                                                    '2003', 
                                                    'Unnamed: 33',
                                                    '2002',
                                                    'Unnamed: 35',
                                                    '2001',
                                                    'Unnamed: 37',
                                                    '2000',
                                                    'Unnamed: 39',
                                                    '1999',
                                                    'Unnamed: 41',
                                                   
                                                   ])
retail_history_df.head()
547/6:
retail_history_df.head()
# D - Denotes an estimate withheld to avoid disclosing data of individual companies; data are included in higher-level totals.
# S - Estimate does not meet publication standards because of high sampling variability (coefficient of variation is greater than 30%), poor response quality (total quantity response rate is less than 50%), or other concerns about the estimate's quality. Unpublished estimates derived from this table by subtraction are subject to these same limitations and should not be attributed to the U.S. Census Bureau. For a description of publication standards and the total quantity response rate, see https://www.census.gov/about/policies/quality/standards/standardf1.html.
547/7:
# Drop rows that do not have ecommerce data
#[Estimates are shown in millions of dollars and are based on data from the Annual Retail Trade Survey. Estimates have been adjusted using final results of the 2012 Economic Census.] 
retail_history_df = retail_history_df.drop([0, 8])
retail_history_df
547/8:
US_totalrev_df = pd.read_csv(US_totalrev_2018_2017_csv)
US_totalrev_df.head()
547/9:
# Drop Columns
US_totalrev_df = US_totalrev_df.drop(columns=['Unnamed: 0', 'NAICS Code'])
US_totalrev_df.head()
547/10:
# Rename columns
US_totalrev_df = US_totalrev_df.rename(columns={"Description": "Description of Inudstries",
                                                "Total": "2018 Revenue Total",
                                                "Revenue from Electronic Sources": "2018 Ecommerce Total",
                                                "Total.2": "2017 Revenue Total",
                                                "Revenue from Electronic Sources.2": "2017 Ecommerce Total",
                                                "Total.3": "Year over Year % Change",
                                                "Revenue from Electronic Sources.3": "Year over Year % Change: Ecommerce",
                                                "2018": "Ecommerce Revenue as a % of Total Revenue, 2018",
                                                "2017": "Ecommerce Revenue as a % of Total Revenue, 2017",
                                                "2018(2)": "Percent Distribution of Revenue, Ecommerce 2018"
                                                 })
US_totalrev_df.head()
547/11:
# Drop rows that do not have ecommerce data
#[Estimates are shown in millions of dollars and are based on data from the Annual Retail Trade Survey. Estimates have been adjusted using final results of the 2012 Economic Census.] 
retail_history_df = retail_history_df.drop([0, 8])
retail_history_df.reset_index()
548/1:
# Dependencies
import pandas as pd
import numpy as np
548/2:
# Save file path to variable
retail_history_csv = "Datasets/EstUS_Retail_Trade_E-commercehistory.csv"
US_totalrev_2018_2017_csv = "Datasets/US_Total_rev_2018and2017.csv"
548/3:
# Read with pandas
retail_history_df = pd.read_csv(retail_history_csv)
retail_history_df.head()
548/4:
# Clean up DF
retail_history_df = retail_history_df.rename(columns={"Unnamed: 1": "Kind of Business",
                                 "2018": "2018 Total",
                                 "Unnamed: 3": "2018 Ecommerce Total",
                                 "2017r": "2017 Total",
                                 "Unnamed: 5": "2017 Ecommerce Total",
                                 "2016r": "2016 Total",
                                 "Unnamed: 7": "2016 Ecommerce Total",
                                 "2015r": "2015 Total",
                                 "Unnamed: 9": "2015 Ecommerce Total",
                                 "2014r": "2014 Total",
                                 "Unnamed: 11": "2014 Ecommerce Total",
                                 "1998": "1998 Total",
                                 "Unnamed: 43": "1998 Ecommerce"
                                 })
retail_history_df.head()
548/5:
# Drop columns we do not need
retail_history_df = retail_history_df.drop(columns=['NAICS Code',
                                                    '2013r',
                                                    'Unnamed: 13',
                                                    '2012r',
                                                    'Unnamed: 15',
                                                    '2011r',
                                                    'Unnamed: 17',
                                                    '2010',
                                                     'Unnamed: 19',
                                                     '2009',
                                                     'Unnamed: 21',
                                                     '2008',
                                                     'Unnamed: 23',
                                                     '2007',
                                                     'Unnamed: 25',
                                                     '2006',
                                                     'Unnamed: 27',
                                                     '2005',
                                                     'Unnamed: 29',
                                                     '2004',
                                                     'Unnamed: 31',
                                                     '2003',
                                                     'Unnamed: 33',
                                                     '2002',
                                                     'Unnamed: 35',
                                                     '2001',
                                                     'Unnamed: 37',
                                                     '2000',
                                                     'Unnamed: 39',
                                                     '1999',
                                                     'Unnamed: 41',
                                                    '2004', 
                                                    'Unnamed: 31',
                                                    '2003', 
                                                    'Unnamed: 33',
                                                    '2002',
                                                    'Unnamed: 35',
                                                    '2001',
                                                    'Unnamed: 37',
                                                    '2000',
                                                    'Unnamed: 39',
                                                    '1999',
                                                    'Unnamed: 41',
                                                   
                                                   ])
retail_history_df.head()
548/6:
retail_history_df.head()
# D - Denotes an estimate withheld to avoid disclosing data of individual companies; data are included in higher-level totals.
# S - Estimate does not meet publication standards because of high sampling variability (coefficient of variation is greater than 30%), poor response quality (total quantity response rate is less than 50%), or other concerns about the estimate's quality. Unpublished estimates derived from this table by subtraction are subject to these same limitations and should not be attributed to the U.S. Census Bureau. For a description of publication standards and the total quantity response rate, see https://www.census.gov/about/policies/quality/standards/standardf1.html.
548/7:
# Drop rows that do not have ecommerce data
#[Estimates are shown in millions of dollars and are based on data from the Annual Retail Trade Survey. Estimates have been adjusted using final results of the 2012 Economic Census.] 
retail_history_df = retail_history_df.drop([0, 8])
retail_history_df
548/8:
US_totalrev_df = pd.read_csv(US_totalrev_2018_2017_csv)
US_totalrev_df.head()
548/9:
# Drop Columns
US_totalrev_df = US_totalrev_df.drop(columns=['Unnamed: 0', 'NAICS Code'])
US_totalrev_df.head()
548/10:
# Rename columns
US_totalrev_df = US_totalrev_df.rename(columns={"Description": "Description of Inudstries",
                                                "Total": "2018 Revenue Total",
                                                "Revenue from Electronic Sources": "2018 Ecommerce Total",
                                                "Total.2": "2017 Revenue Total",
                                                "Revenue from Electronic Sources.2": "2017 Ecommerce Total",
                                                "Total.3": "Year over Year % Change",
                                                "Revenue from Electronic Sources.3": "Year over Year % Change: Ecommerce",
                                                "2018": "Ecommerce Revenue as a % of Total Revenue, 2018",
                                                "2017": "Ecommerce Revenue as a % of Total Revenue, 2017",
                                                "2018(2)": "Percent Distribution of Revenue, Ecommerce 2018"
                                                 })
US_totalrev_df.head()
549/1:
# Dependencies
import pandas as pd
import numpy as np
549/2:
# Save file path to variable
retail_history_csv = "Datasets/EstUS_Retail_Trade_E-commercehistory.csv"
US_totalrev_2018_2017_csv = "Datasets/US_Total_rev_2018and2017.csv"
549/3:
# Read with pandas
retail_history_df = pd.read_csv(retail_history_csv)
retail_history_df.head()
549/4:
# Clean up DF
retail_history_df = retail_history_df.rename(columns={"Unnamed: 1": "Kind of Business",
                                 "2018": "2018 Total",
                                 "Unnamed: 3": "2018 Ecommerce Total",
                                 "2017r": "2017 Total",
                                 "Unnamed: 5": "2017 Ecommerce Total",
                                 "2016r": "2016 Total",
                                 "Unnamed: 7": "2016 Ecommerce Total",
                                 "2015r": "2015 Total",
                                 "Unnamed: 9": "2015 Ecommerce Total",
                                 "2014r": "2014 Total",
                                 "Unnamed: 11": "2014 Ecommerce Total",
                                 "1998": "1998 Total",
                                 "Unnamed: 43": "1998 Ecommerce"
                                 })
retail_history_df.head()
549/5:
# Drop columns we do not need
retail_history_df = retail_history_df.drop(columns=['NAICS Code',
                                                    '2013r',
                                                    'Unnamed: 13',
                                                    '2012r',
                                                    'Unnamed: 15',
                                                    '2011r',
                                                    'Unnamed: 17',
                                                    '2010',
                                                     'Unnamed: 19',
                                                     '2009',
                                                     'Unnamed: 21',
                                                     '2008',
                                                     'Unnamed: 23',
                                                     '2007',
                                                     'Unnamed: 25',
                                                     '2006',
                                                     'Unnamed: 27',
                                                     '2005',
                                                     'Unnamed: 29',
                                                     '2004',
                                                     'Unnamed: 31',
                                                     '2003',
                                                     'Unnamed: 33',
                                                     '2002',
                                                     'Unnamed: 35',
                                                     '2001',
                                                     'Unnamed: 37',
                                                     '2000',
                                                     'Unnamed: 39',
                                                     '1999',
                                                     'Unnamed: 41',
                                                    '2004', 
                                                    'Unnamed: 31',
                                                    '2003', 
                                                    'Unnamed: 33',
                                                    '2002',
                                                    'Unnamed: 35',
                                                    '2001',
                                                    'Unnamed: 37',
                                                    '2000',
                                                    'Unnamed: 39',
                                                    '1999',
                                                    'Unnamed: 41',
                                                   
                                                   ])
retail_history_df.head()
549/6:
retail_history_df.head()
# D - Denotes an estimate withheld to avoid disclosing data of individual companies; data are included in higher-level totals.
# S - Estimate does not meet publication standards because of high sampling variability (coefficient of variation is greater than 30%), poor response quality (total quantity response rate is less than 50%), or other concerns about the estimate's quality. Unpublished estimates derived from this table by subtraction are subject to these same limitations and should not be attributed to the U.S. Census Bureau. For a description of publication standards and the total quantity response rate, see https://www.census.gov/about/policies/quality/standards/standardf1.html.
549/7:
# Drop rows that do not have ecommerce data
#[Estimates are shown in millions of dollars and are based on data from the Annual Retail Trade Survey. Estimates have been adjusted using final results of the 2012 Economic Census.] 
retail_history_df = retail_history_df.drop([0, 8])
retail_history_df.reset_index(drop=True)
retail_history_df.head()
549/8:
US_totalrev_df = pd.read_csv(US_totalrev_2018_2017_csv)
US_totalrev_df.head()
549/9:
# Drop Columns
US_totalrev_df = US_totalrev_df.drop(columns=['Unnamed: 0', 'NAICS Code'])
US_totalrev_df.head()
549/10:
# Rename columns
US_totalrev_df = US_totalrev_df.rename(columns={"Description": "Description of Inudstries",
                                                "Total": "2018 Revenue Total",
                                                "Revenue from Electronic Sources": "2018 Ecommerce Total",
                                                "Total.2": "2017 Revenue Total",
                                                "Revenue from Electronic Sources.2": "2017 Ecommerce Total",
                                                "Total.3": "Year over Year % Change",
                                                "Revenue from Electronic Sources.3": "Year over Year % Change: Ecommerce",
                                                "2018": "Ecommerce Revenue as a % of Total Revenue, 2018",
                                                "2017": "Ecommerce Revenue as a % of Total Revenue, 2017",
                                                "2018(2)": "Percent Distribution of Revenue, Ecommerce 2018"
                                                 })
US_totalrev_df.head()
549/11:
# Drop rows that do not have ecommerce data
#[Estimates are shown in millions of dollars and are based on data from the Annual Retail Trade Survey. Estimates have been adjusted using final results of the 2012 Economic Census.] 
retail_history_df = retail_history_df.drop([0, 8])
retail_history_df.reset_index(drop=True)
retail_history_df
550/1:
# Dependencies
import pandas as pd
import numpy as np
550/2:
# Save file path to variable
retail_history_csv = "Datasets/EstUS_Retail_Trade_E-commercehistory.csv"
US_totalrev_2018_2017_csv = "Datasets/US_Total_rev_2018and2017.csv"
550/3:
# Read with pandas
retail_history_df = pd.read_csv(retail_history_csv)
retail_history_df.head()
550/4:
# Clean up DF
retail_history_df = retail_history_df.rename(columns={"Unnamed: 1": "Kind of Business",
                                 "2018": "2018 Total",
                                 "Unnamed: 3": "2018 Ecommerce Total",
                                 "2017r": "2017 Total",
                                 "Unnamed: 5": "2017 Ecommerce Total",
                                 "2016r": "2016 Total",
                                 "Unnamed: 7": "2016 Ecommerce Total",
                                 "2015r": "2015 Total",
                                 "Unnamed: 9": "2015 Ecommerce Total",
                                 "2014r": "2014 Total",
                                 "Unnamed: 11": "2014 Ecommerce Total",
                                 "1998": "1998 Total",
                                 "Unnamed: 43": "1998 Ecommerce"
                                 })
retail_history_df.head()
550/5:
# Drop columns we do not need
retail_history_df = retail_history_df.drop(columns=['NAICS Code',
                                                    '2013r',
                                                    'Unnamed: 13',
                                                    '2012r',
                                                    'Unnamed: 15',
                                                    '2011r',
                                                    'Unnamed: 17',
                                                    '2010',
                                                     'Unnamed: 19',
                                                     '2009',
                                                     'Unnamed: 21',
                                                     '2008',
                                                     'Unnamed: 23',
                                                     '2007',
                                                     'Unnamed: 25',
                                                     '2006',
                                                     'Unnamed: 27',
                                                     '2005',
                                                     'Unnamed: 29',
                                                     '2004',
                                                     'Unnamed: 31',
                                                     '2003',
                                                     'Unnamed: 33',
                                                     '2002',
                                                     'Unnamed: 35',
                                                     '2001',
                                                     'Unnamed: 37',
                                                     '2000',
                                                     'Unnamed: 39',
                                                     '1999',
                                                     'Unnamed: 41',
                                                    '2004', 
                                                    'Unnamed: 31',
                                                    '2003', 
                                                    'Unnamed: 33',
                                                    '2002',
                                                    'Unnamed: 35',
                                                    '2001',
                                                    'Unnamed: 37',
                                                    '2000',
                                                    'Unnamed: 39',
                                                    '1999',
                                                    'Unnamed: 41',
                                                   
                                                   ])
retail_history_df.head()
550/6:
retail_history_df.head()
# D - Denotes an estimate withheld to avoid disclosing data of individual companies; data are included in higher-level totals.
# S - Estimate does not meet publication standards because of high sampling variability (coefficient of variation is greater than 30%), poor response quality (total quantity response rate is less than 50%), or other concerns about the estimate's quality. Unpublished estimates derived from this table by subtraction are subject to these same limitations and should not be attributed to the U.S. Census Bureau. For a description of publication standards and the total quantity response rate, see https://www.census.gov/about/policies/quality/standards/standardf1.html.
550/7:
# Drop rows that do not have ecommerce data
#[Estimates are shown in millions of dollars and are based on data from the Annual Retail Trade Survey. Estimates have been adjusted using final results of the 2012 Economic Census.] 
retail_history_df = retail_history_df.drop([0, 8])
retail_history_df.reset_index(drop=True)
retail_history_df
550/8:
US_totalrev_df = pd.read_csv(US_totalrev_2018_2017_csv)
US_totalrev_df.head()
550/9:
# Drop Columns
US_totalrev_df = US_totalrev_df.drop(columns=['Unnamed: 0', 'NAICS Code'])
US_totalrev_df.head()
550/10:
# Rename columns
US_totalrev_df = US_totalrev_df.rename(columns={"Description": "Description of Inudstries",
                                                "Total": "2018 Revenue Total",
                                                "Revenue from Electronic Sources": "2018 Ecommerce Total",
                                                "Total.2": "2017 Revenue Total",
                                                "Revenue from Electronic Sources.2": "2017 Ecommerce Total",
                                                "Total.3": "Year over Year % Change",
                                                "Revenue from Electronic Sources.3": "Year over Year % Change: Ecommerce",
                                                "2018": "Ecommerce Revenue as a % of Total Revenue, 2018",
                                                "2017": "Ecommerce Revenue as a % of Total Revenue, 2017",
                                                "2018(2)": "Percent Distribution of Revenue, Ecommerce 2018"
                                                 })
US_totalrev_df.head()
550/11:
# Drop rows that do not have ecommerce data
#[Estimates are shown in millions of dollars and are based on data from the Annual Retail Trade Survey. Estimates have been adjusted using final results of the 2012 Economic Census.] 
retail_history_df = retail_history_df.drop([0, 8])
retail_history_df.reset_index(drop=True)
retail_history_df
550/12:
retail_history_df.head()
# D - Denotes an estimate withheld to avoid disclosing data of individual companies; data are included in higher-level totals.
# S - Estimate does not meet publication standards because of high sampling variability (coefficient of variation is greater than 30%), poor response quality (total quantity response rate is less than 50%), or other concerns about the estimate's quality. Unpublished estimates derived from this table by subtraction are subject to these same limitations and should not be attributed to the U.S. Census Bureau. For a description of publication standards and the total quantity response rate, see https://www.census.gov/about/policies/quality/standards/standardf1.html.
550/13:
# Drop rows that do not have ecommerce data
#[Estimates are shown in millions of dollars and are based on data from the Annual Retail Trade Survey. Estimates have been adjusted using final results of the 2012 Economic Census.] 
retail_history_df = retail_history_df.drop([0, 8])
retail_history_df.reset_index(drop=True)
retail_history_df
551/1:
# Dependencies
import pandas as pd
import numpy as np
551/2:
# Save file path to variable
retail_history_csv = "Datasets/EstUS_Retail_Trade_E-commercehistory.csv"
US_totalrev_2018_2017_csv = "Datasets/US_Total_rev_2018and2017.csv"
551/3:
# Read with pandas
retail_history_df = pd.read_csv(retail_history_csv)
retail_history_df.head()
551/4:
# Clean up DF
retail_history_df = retail_history_df.rename(columns={"Unnamed: 1": "Kind of Business",
                                 "2018": "2018 Total",
                                 "Unnamed: 3": "2018 Ecommerce Total",
                                 "2017r": "2017 Total",
                                 "Unnamed: 5": "2017 Ecommerce Total",
                                 "2016r": "2016 Total",
                                 "Unnamed: 7": "2016 Ecommerce Total",
                                 "2015r": "2015 Total",
                                 "Unnamed: 9": "2015 Ecommerce Total",
                                 "2014r": "2014 Total",
                                 "Unnamed: 11": "2014 Ecommerce Total",
                                 "1998": "1998 Total",
                                 "Unnamed: 43": "1998 Ecommerce"
                                 })
retail_history_df.head()
551/5:
# Drop columns we do not need
retail_history_df = retail_history_df.drop(columns=['NAICS Code',
                                                    '2013r',
                                                    'Unnamed: 13',
                                                    '2012r',
                                                    'Unnamed: 15',
                                                    '2011r',
                                                    'Unnamed: 17',
                                                    '2010',
                                                     'Unnamed: 19',
                                                     '2009',
                                                     'Unnamed: 21',
                                                     '2008',
                                                     'Unnamed: 23',
                                                     '2007',
                                                     'Unnamed: 25',
                                                     '2006',
                                                     'Unnamed: 27',
                                                     '2005',
                                                     'Unnamed: 29',
                                                     '2004',
                                                     'Unnamed: 31',
                                                     '2003',
                                                     'Unnamed: 33',
                                                     '2002',
                                                     'Unnamed: 35',
                                                     '2001',
                                                     'Unnamed: 37',
                                                     '2000',
                                                     'Unnamed: 39',
                                                     '1999',
                                                     'Unnamed: 41',
                                                    '2004', 
                                                    'Unnamed: 31',
                                                    '2003', 
                                                    'Unnamed: 33',
                                                    '2002',
                                                    'Unnamed: 35',
                                                    '2001',
                                                    'Unnamed: 37',
                                                    '2000',
                                                    'Unnamed: 39',
                                                    '1999',
                                                    'Unnamed: 41',
                                                   
                                                   ])
retail_history_df.head()
551/6:
retail_history_df.head()
# D - Denotes an estimate withheld to avoid disclosing data of individual companies; data are included in higher-level totals.
# S - Estimate does not meet publication standards because of high sampling variability (coefficient of variation is greater than 30%), poor response quality (total quantity response rate is less than 50%), or other concerns about the estimate's quality. Unpublished estimates derived from this table by subtraction are subject to these same limitations and should not be attributed to the U.S. Census Bureau. For a description of publication standards and the total quantity response rate, see https://www.census.gov/about/policies/quality/standards/standardf1.html.
551/7:
# Drop rows that do not have ecommerce data
#[Estimates are shown in millions of dollars and are based on data from the Annual Retail Trade Survey. Estimates have been adjusted using final results of the 2012 Economic Census.] 
retail_history_df = retail_history_df.drop([0, 8])
retail_history_df.reset_index(drop=True)
retail_history_df
551/8:
US_totalrev_df = pd.read_csv(US_totalrev_2018_2017_csv)
US_totalrev_df.head()
551/9:
# Drop Columns
US_totalrev_df = US_totalrev_df.drop(columns=['Unnamed: 0', 'NAICS Code'])
US_totalrev_df.head()
551/10:
# Rename columns
US_totalrev_df = US_totalrev_df.rename(columns={"Description": "Description of Inudstries",
                                                "Total": "2018 Revenue Total",
                                                "Revenue from Electronic Sources": "2018 Ecommerce Total",
                                                "Total.2": "2017 Revenue Total",
                                                "Revenue from Electronic Sources.2": "2017 Ecommerce Total",
                                                "Total.3": "Year over Year % Change",
                                                "Revenue from Electronic Sources.3": "Year over Year % Change: Ecommerce",
                                                "2018": "Ecommerce Revenue as a % of Total Revenue, 2018",
                                                "2017": "Ecommerce Revenue as a % of Total Revenue, 2017",
                                                "2018(2)": "Percent Distribution of Revenue, Ecommerce 2018"
                                                 })
US_totalrev_df.head()
551/11:
retail_history_df.reset_index(drop=True)
retail_history_df
551/12:
retail_history_df = retail_history_df.reset_index(drop=True)
retail_history_df
551/13:
# Rename columns
US_totalrev_df = US_totalrev_df.rename(columns={"Description": "Description of Inudstries",
                                                "Total": "2018 Total Revenue",
                                                "Revenue from Electronic Sources": "2018 Ecommerce Total",
                                                "Total.2": "2017 Total Revenue",
                                                "Revenue from Electronic Sources.2": "2017 Ecommerce Total",
                                                "Total.3": "Year over Year % Change",
                                                "Revenue from Electronic Sources.3": "Year over Year % Change: Ecommerce",
                                                "2018": "Ecommerce Revenue as a % of Total Revenue, 2018",
                                                "2017": "Ecommerce Revenue as a % of Total Revenue, 2017",
                                                "2018(2)": "Percent Distribution of Revenue, Ecommerce 2018"
                                                 })
US_totalrev_df.head()
552/1:
# Dependencies
import pandas as pd
import numpy as np
552/2:
# Save file path to variable
retail_history_csv = "Datasets/EstUS_Retail_Trade_E-commercehistory.csv"
US_totalrev_2018_2017_csv = "Datasets/US_Total_rev_2018and2017.csv"
552/3:
# Read with pandas
retail_history_df = pd.read_csv(retail_history_csv)
retail_history_df.head()
552/4:
# Clean up DF
retail_history_df = retail_history_df.rename(columns={"Unnamed: 1": "Kind of Business",
                                 "2018": "2018 Total",
                                 "Unnamed: 3": "2018 Ecommerce Total",
                                 "2017r": "2017 Total",
                                 "Unnamed: 5": "2017 Ecommerce Total",
                                 "2016r": "2016 Total",
                                 "Unnamed: 7": "2016 Ecommerce Total",
                                 "2015r": "2015 Total",
                                 "Unnamed: 9": "2015 Ecommerce Total",
                                 "2014r": "2014 Total",
                                 "Unnamed: 11": "2014 Ecommerce Total",
                                 "1998": "1998 Total",
                                 "Unnamed: 43": "1998 Ecommerce"
                                 })
retail_history_df.head()
552/5:
# Drop columns we do not need
retail_history_df = retail_history_df.drop(columns=['NAICS Code',
                                                    '2013r',
                                                    'Unnamed: 13',
                                                    '2012r',
                                                    'Unnamed: 15',
                                                    '2011r',
                                                    'Unnamed: 17',
                                                    '2010',
                                                     'Unnamed: 19',
                                                     '2009',
                                                     'Unnamed: 21',
                                                     '2008',
                                                     'Unnamed: 23',
                                                     '2007',
                                                     'Unnamed: 25',
                                                     '2006',
                                                     'Unnamed: 27',
                                                     '2005',
                                                     'Unnamed: 29',
                                                     '2004',
                                                     'Unnamed: 31',
                                                     '2003',
                                                     'Unnamed: 33',
                                                     '2002',
                                                     'Unnamed: 35',
                                                     '2001',
                                                     'Unnamed: 37',
                                                     '2000',
                                                     'Unnamed: 39',
                                                     '1999',
                                                     'Unnamed: 41',
                                                    '2004', 
                                                    'Unnamed: 31',
                                                    '2003', 
                                                    'Unnamed: 33',
                                                    '2002',
                                                    'Unnamed: 35',
                                                    '2001',
                                                    'Unnamed: 37',
                                                    '2000',
                                                    'Unnamed: 39',
                                                    '1999',
                                                    'Unnamed: 41',
                                                   
                                                   ])
retail_history_df.head()
552/6:
retail_history_df.head()
# D - Denotes an estimate withheld to avoid disclosing data of individual companies; data are included in higher-level totals.
# S - Estimate does not meet publication standards because of high sampling variability (coefficient of variation is greater than 30%), poor response quality (total quantity response rate is less than 50%), or other concerns about the estimate's quality. Unpublished estimates derived from this table by subtraction are subject to these same limitations and should not be attributed to the U.S. Census Bureau. For a description of publication standards and the total quantity response rate, see https://www.census.gov/about/policies/quality/standards/standardf1.html.
552/7:
# Drop rows that do not have ecommerce data
#[Estimates are shown in millions of dollars and are based on data from the Annual Retail Trade Survey. Estimates have been adjusted using final results of the 2012 Economic Census.] 
retail_history_df = retail_history_df.drop([0, 8])
552/8:
retail_history_df = retail_history_df.reset_index(drop=True)
retail_history_df
552/9:
US_totalrev_df = pd.read_csv(US_totalrev_2018_2017_csv)
US_totalrev_df.head()
552/10:
# Drop Columns
US_totalrev_df = US_totalrev_df.drop(columns=['Unnamed: 0', 'NAICS Code'])
US_totalrev_df.head()
552/11:
# Rename columns
US_totalrev_df = US_totalrev_df.rename(columns={"Description": "Description of Inudstries",
                                                "Total": "2018 Total Revenue",
                                                "Revenue from Electronic Sources": "2018 Ecommerce Total",
                                                "Total.2": "2017 Total Revenue",
                                                "Revenue from Electronic Sources.2": "2017 Ecommerce Total",
                                                "Total.3": "Year over Year % Change",
                                                "Revenue from Electronic Sources.3": "Year over Year % Change: Ecommerce",
                                                "2018": "Ecommerce Revenue as a % of Total Revenue, 2018",
                                                "2017": "Ecommerce Revenue as a % of Total Revenue, 2017",
                                                "2018(2)": "Percent Distribution of Revenue, Ecommerce 2018"
                                                 })
US_totalrev_df.head()
552/12:
US_totalrev_df = US_totalrev_df.drop(columns=["Unnamed: 12",
                                              "Unnamed: 13",
                                              "Unnamed: 14",
                                              "Unnamed: 15",
                                             ])
US_totalrev_df
552/13:
US_totalrev_df = US_totalrev_df.drop(columns=["Unnamed: 12",
                                              "Unnamed: 13",
                                              "Unnamed: 14",
                                              "Unnamed: 15",
                                             ])
US_totalrev_df.head()
553/1:
# Dependencies
import pandas as pd
import numpy as np
553/2:
# Save file path to variable
retail_history_csv = "Datasets/EstUS_Retail_Trade_E-commercehistory.csv"
US_totalrev_2018_2017_csv = "Datasets/US_Total_rev_2018and2017.csv"
553/3:
# Read with pandas
retail_history_df = pd.read_csv(retail_history_csv)
retail_history_df.head()
553/4:
# Clean up DF
retail_history_df = retail_history_df.rename(columns={"Unnamed: 1": "Kind of Business",
                                 "2018": "2018 Total",
                                 "Unnamed: 3": "2018 Ecommerce Total",
                                 "2017r": "2017 Total",
                                 "Unnamed: 5": "2017 Ecommerce Total",
                                 "2016r": "2016 Total",
                                 "Unnamed: 7": "2016 Ecommerce Total",
                                 "2015r": "2015 Total",
                                 "Unnamed: 9": "2015 Ecommerce Total",
                                 "2014r": "2014 Total",
                                 "Unnamed: 11": "2014 Ecommerce Total",
                                 "1998": "1998 Total",
                                 "Unnamed: 43": "1998 Ecommerce"
                                 })
retail_history_df.head()
553/5:
# Drop columns we do not need
retail_history_df = retail_history_df.drop(columns=['NAICS Code',
                                                    '2013r',
                                                    'Unnamed: 13',
                                                    '2012r',
                                                    'Unnamed: 15',
                                                    '2011r',
                                                    'Unnamed: 17',
                                                    '2010',
                                                     'Unnamed: 19',
                                                     '2009',
                                                     'Unnamed: 21',
                                                     '2008',
                                                     'Unnamed: 23',
                                                     '2007',
                                                     'Unnamed: 25',
                                                     '2006',
                                                     'Unnamed: 27',
                                                     '2005',
                                                     'Unnamed: 29',
                                                     '2004',
                                                     'Unnamed: 31',
                                                     '2003',
                                                     'Unnamed: 33',
                                                     '2002',
                                                     'Unnamed: 35',
                                                     '2001',
                                                     'Unnamed: 37',
                                                     '2000',
                                                     'Unnamed: 39',
                                                     '1999',
                                                     'Unnamed: 41',
                                                    '2004', 
                                                    'Unnamed: 31',
                                                    '2003', 
                                                    'Unnamed: 33',
                                                    '2002',
                                                    'Unnamed: 35',
                                                    '2001',
                                                    'Unnamed: 37',
                                                    '2000',
                                                    'Unnamed: 39',
                                                    '1999',
                                                    'Unnamed: 41',
                                                   
                                                   ])
retail_history_df.head()
553/6:
retail_history_df.head()
# D - Denotes an estimate withheld to avoid disclosing data of individual companies; data are included in higher-level totals.
# S - Estimate does not meet publication standards because of high sampling variability (coefficient of variation is greater than 30%), poor response quality (total quantity response rate is less than 50%), or other concerns about the estimate's quality. Unpublished estimates derived from this table by subtraction are subject to these same limitations and should not be attributed to the U.S. Census Bureau. For a description of publication standards and the total quantity response rate, see https://www.census.gov/about/policies/quality/standards/standardf1.html.
553/7:
# Drop rows that do not have ecommerce data
#[Estimates are shown in millions of dollars and are based on data from the Annual Retail Trade Survey. Estimates have been adjusted using final results of the 2012 Economic Census.] 
retail_history_df = retail_history_df.drop([0, 8])
553/8:
retail_history_df = retail_history_df.reset_index(drop=True)
retail_history_df
553/9:
US_totalrev_df = pd.read_csv(US_totalrev_2018_2017_csv)
US_totalrev_df.head()
553/10:
# Drop Columns
US_totalrev_df = US_totalrev_df.drop(columns=['Unnamed: 0', 'NAICS Code'])
US_totalrev_df.head()
553/11:
# Rename columns
US_totalrev_df = US_totalrev_df.rename(columns={"Description": "Description of Inudstries",
                                                "Total": "2018 Total Revenue",
                                                "Revenue from Electronic Sources": "2018 Ecommerce Total",
                                                "Total.2": "2017 Total Revenue",
                                                "Revenue from Electronic Sources.2": "2017 Ecommerce Total",
                                                "Total.3": "Year over Year % Change",
                                                "Revenue from Electronic Sources.3": "Year over Year % Change: Ecommerce",
                                                "2018": "Ecommerce Revenue as a % of Total Revenue, 2018",
                                                "2017": "Ecommerce Revenue as a % of Total Revenue, 2017",
                                                "2018(2)": "Percent Distribution of Revenue, Ecommerce 2018"
                                                 })
US_totalrev_df.head()
553/12:
US_totalrev_df = US_totalrev_df.drop(columns=["Unnamed: 12",
                                              "Unnamed: 13",
                                              "Unnamed: 14",
                                              "Unnamed: 15",
                                             ])
US_totalrev_df.head()
553/13:
US_totalrev_df = US_totalrev_df.style.apply(highlight_max, color='lightgreen', axis=None)
US_totalrev_df
553/14:
def highlight_max(data, color='green'):
    attr = 'background-color: {}'.format(color)
    if data.ndim == 1
        is_max = data == data.max()
        return [attr if v else '' for v in is_max]
    else:  # from .apply(axis=None)
        is_max = data == data.max().max()
        return pd.DataFrame(np.where(is_max, attr, ''),
                            index=data.index, columns=data.columns)
553/15:
def highlight_max(data, color='green'):
    attr = 'background-color: {}'.format(color)
    if data.ndim == 1: 
        is_max = data == data.max()
        return [attr if v else '' for v in is_max]
    else:  # from .apply(axis=None)
        is_max = data == data.max().max()
        return pd.DataFrame(np.where(is_max, attr, ''),
                            index=data.index, columns=data.columns)
553/16:
US_totalrev_df = US_totalrev_df.style.apply(highlight_max, color='lightgreen', axis=None)
US_totalrev_df
554/1:
# Dependencies
import pandas as pd
import numpy as np
554/2:
# Save file path to variable
retail_history_csv = "Datasets/EstUS_Retail_Trade_E-commercehistory.csv"
US_totalrev_2018_2017_csv = "Datasets/US_Total_rev_2018and2017.csv"
554/3:
# Read with pandas
retail_history_df = pd.read_csv(retail_history_csv)
retail_history_df.head()
554/4:
# Clean up DF
retail_history_df = retail_history_df.rename(columns={"Unnamed: 1": "Kind of Business",
                                 "2018": "2018 Total",
                                 "Unnamed: 3": "2018 Ecommerce Total",
                                 "2017r": "2017 Total",
                                 "Unnamed: 5": "2017 Ecommerce Total",
                                 "2016r": "2016 Total",
                                 "Unnamed: 7": "2016 Ecommerce Total",
                                 "2015r": "2015 Total",
                                 "Unnamed: 9": "2015 Ecommerce Total",
                                 "2014r": "2014 Total",
                                 "Unnamed: 11": "2014 Ecommerce Total",
                                 "1998": "1998 Total",
                                 "Unnamed: 43": "1998 Ecommerce"
                                 })
retail_history_df.head()
554/5:
# Drop columns we do not need
retail_history_df = retail_history_df.drop(columns=['NAICS Code',
                                                    '2013r',
                                                    'Unnamed: 13',
                                                    '2012r',
                                                    'Unnamed: 15',
                                                    '2011r',
                                                    'Unnamed: 17',
                                                    '2010',
                                                     'Unnamed: 19',
                                                     '2009',
                                                     'Unnamed: 21',
                                                     '2008',
                                                     'Unnamed: 23',
                                                     '2007',
                                                     'Unnamed: 25',
                                                     '2006',
                                                     'Unnamed: 27',
                                                     '2005',
                                                     'Unnamed: 29',
                                                     '2004',
                                                     'Unnamed: 31',
                                                     '2003',
                                                     'Unnamed: 33',
                                                     '2002',
                                                     'Unnamed: 35',
                                                     '2001',
                                                     'Unnamed: 37',
                                                     '2000',
                                                     'Unnamed: 39',
                                                     '1999',
                                                     'Unnamed: 41',
                                                    '2004', 
                                                    'Unnamed: 31',
                                                    '2003', 
                                                    'Unnamed: 33',
                                                    '2002',
                                                    'Unnamed: 35',
                                                    '2001',
                                                    'Unnamed: 37',
                                                    '2000',
                                                    'Unnamed: 39',
                                                    '1999',
                                                    'Unnamed: 41',
                                                   
                                                   ])
retail_history_df.head()
554/6:
retail_history_df.head()
# D - Denotes an estimate withheld to avoid disclosing data of individual companies; data are included in higher-level totals.
# S - Estimate does not meet publication standards because of high sampling variability (coefficient of variation is greater than 30%), poor response quality (total quantity response rate is less than 50%), or other concerns about the estimate's quality. Unpublished estimates derived from this table by subtraction are subject to these same limitations and should not be attributed to the U.S. Census Bureau. For a description of publication standards and the total quantity response rate, see https://www.census.gov/about/policies/quality/standards/standardf1.html.
554/7:
# Drop rows that do not have ecommerce data
#[Estimates are shown in millions of dollars and are based on data from the Annual Retail Trade Survey. Estimates have been adjusted using final results of the 2012 Economic Census.] 
retail_history_df = retail_history_df.drop([0, 8])
554/8:
retail_history_df = retail_history_df.reset_index(drop=True)
retail_history_df
554/9:
US_totalrev_df = pd.read_csv(US_totalrev_2018_2017_csv)
US_totalrev_df.head()
554/10:
# Drop Columns
US_totalrev_df = US_totalrev_df.drop(columns=['Unnamed: 0', 'NAICS Code'])
US_totalrev_df.head()
554/11:
# Rename columns
US_totalrev_df = US_totalrev_df.rename(columns={"Description": "Description of Inudstries",
                                                "Total": "2018 Total Revenue",
                                                "Revenue from Electronic Sources": "2018 Ecommerce Total",
                                                "Total.2": "2017 Total Revenue",
                                                "Revenue from Electronic Sources.2": "2017 Ecommerce Total",
                                                "Total.3": "Year over Year % Change",
                                                "Revenue from Electronic Sources.3": "Year over Year % Change: Ecommerce",
                                                "2018": "Ecommerce Revenue as a % of Total Revenue, 2018",
                                                "2017": "Ecommerce Revenue as a % of Total Revenue, 2017",
                                                "2018(2)": "Percent Distribution of Revenue, Ecommerce 2018"
                                                 })
US_totalrev_df.head()
554/12:
US_totalrev_df = US_totalrev_df.drop(columns=["Unnamed: 12",
                                              "Unnamed: 13",
                                              "Unnamed: 14",
                                              "Unnamed: 15",
                                             ])
US_totalrev_df.head()
554/13:
def highlight_max(data, color='green'):
    attr = 'background-color: {}'.format(color)
    if data.ndim == 1: 
        is_max = data == data.max()
        return [attr if v else '' for v in is_max]
    else:  # from .apply(axis=None)
        is_max = data == data.max().max()
        return pd.DataFrame(np.where(is_max, attr, ''),
                            index=data.index, columns=data.columns)
554/14:
US_totalrev_df = US_totalrev_df.style.apply(highlight_max, color='green', axis=None)
US_totalrev_df
554/15:
US_totalrev_df = US_totalrev_df.style.highlight_max(color='lightgreen', axis=None)
US_totalrev_df
554/16:
def highlight_max(s): 
    if s.dtype == np.object: 
        is_max = [False for _ in range(s.shape[0])] 
    else: 
        is_max = s == s.max() 
    return ['background: lightgreen' if cell else '' for cell in is_max] 
  
US_totalrev_df.style.apply(highlight_max)
US_totalrev_df
555/1:
# Dependencies
import pandas as pd
import numpy as np
import os
import webbrowser
import io
555/2:
# Save file path to variable
retail_history_csv = "Datasets/EstUS_Retail_Trade_E-commercehistory.csv"
US_totalrev_2018_2017_csv = "Datasets/US_Total_rev_2018and2017.csv"
555/3:
# Read with pandas
retail_history_df = pd.read_csv(retail_history_csv)
retail_history_df.head()
555/4:
# Clean up DF
retail_history_df = retail_history_df.rename(columns={"Unnamed: 1": "Kind of Business",
                                 "2018": "2018 Total",
                                 "Unnamed: 3": "2018 Ecommerce Total",
                                 "2017r": "2017 Total",
                                 "Unnamed: 5": "2017 Ecommerce Total",
                                 "2016r": "2016 Total",
                                 "Unnamed: 7": "2016 Ecommerce Total",
                                 "2015r": "2015 Total",
                                 "Unnamed: 9": "2015 Ecommerce Total",
                                 "2014r": "2014 Total",
                                 "Unnamed: 11": "2014 Ecommerce Total",
                                 "1998": "1998 Total",
                                 "Unnamed: 43": "1998 Ecommerce"
                                 })
retail_history_df.head()
555/5:
# Drop columns we do not need
retail_history_df = retail_history_df.drop(columns=['NAICS Code',
                                                    '2013r',
                                                    'Unnamed: 13',
                                                    '2012r',
                                                    'Unnamed: 15',
                                                    '2011r',
                                                    'Unnamed: 17',
                                                    '2010',
                                                     'Unnamed: 19',
                                                     '2009',
                                                     'Unnamed: 21',
                                                     '2008',
                                                     'Unnamed: 23',
                                                     '2007',
                                                     'Unnamed: 25',
                                                     '2006',
                                                     'Unnamed: 27',
                                                     '2005',
                                                     'Unnamed: 29',
                                                     '2004',
                                                     'Unnamed: 31',
                                                     '2003',
                                                     'Unnamed: 33',
                                                     '2002',
                                                     'Unnamed: 35',
                                                     '2001',
                                                     'Unnamed: 37',
                                                     '2000',
                                                     'Unnamed: 39',
                                                     '1999',
                                                     'Unnamed: 41',
                                                    '2004', 
                                                    'Unnamed: 31',
                                                    '2003', 
                                                    'Unnamed: 33',
                                                    '2002',
                                                    'Unnamed: 35',
                                                    '2001',
                                                    'Unnamed: 37',
                                                    '2000',
                                                    'Unnamed: 39',
                                                    '1999',
                                                    'Unnamed: 41',
                                                   
                                                   ])
retail_history_df.head()
556/1:
# Dependencies
import pandas as pd
import numpy as np
import os
import webbrowser
import io
556/2:
# Save file path to variable
retail_history_csv = "Datasets/EstUS_Retail_Trade_E-commercehistory.csv"
US_totalrev_2018_2017_csv = "Datasets/US_Total_rev_2018and2017.csv"
556/3:
# Read with pandas
retail_history_df = pd.read_csv(retail_history_csv)
retail_history_df.head()
556/4:
# Clean up DF
retail_history_df = retail_history_df.rename(columns={"Unnamed: 1": "Kind of Business",
                                 "2018": "2018 Total",
                                 "Unnamed: 3": "2018 Ecommerce Total",
                                 "2017r": "2017 Total",
                                 "Unnamed: 5": "2017 Ecommerce Total",
                                 "2016r": "2016 Total",
                                 "Unnamed: 7": "2016 Ecommerce Total",
                                 "2015r": "2015 Total",
                                 "Unnamed: 9": "2015 Ecommerce Total",
                                 "2014r": "2014 Total",
                                 "Unnamed: 11": "2014 Ecommerce Total",
                                 "1998": "1998 Total",
                                 "Unnamed: 43": "1998 Ecommerce"
                                 })
retail_history_df.head()
556/5:
# Drop columns we do not need
retail_history_df = retail_history_df.drop(columns=['NAICS Code',
                                                    '2013r',
                                                    'Unnamed: 13',
                                                    '2012r',
                                                    'Unnamed: 15',
                                                    '2011r',
                                                    'Unnamed: 17',
                                                    '2010',
                                                     'Unnamed: 19',
                                                     '2009',
                                                     'Unnamed: 21',
                                                     '2008',
                                                     'Unnamed: 23',
                                                     '2007',
                                                     'Unnamed: 25',
                                                     '2006',
                                                     'Unnamed: 27',
                                                     '2005',
                                                     'Unnamed: 29',
                                                     '2004',
                                                     'Unnamed: 31',
                                                     '2003',
                                                     'Unnamed: 33',
                                                     '2002',
                                                     'Unnamed: 35',
                                                     '2001',
                                                     'Unnamed: 37',
                                                     '2000',
                                                     'Unnamed: 39',
                                                     '1999',
                                                     'Unnamed: 41',
                                                    '2004', 
                                                    'Unnamed: 31',
                                                    '2003', 
                                                    'Unnamed: 33',
                                                    '2002',
                                                    'Unnamed: 35',
                                                    '2001',
                                                    'Unnamed: 37',
                                                    '2000',
                                                    'Unnamed: 39',
                                                    '1999',
                                                    'Unnamed: 41',
                                                   
                                                   ])
retail_history_df.head()
556/6:
retail_history_df.head()
# D - Denotes an estimate withheld to avoid disclosing data of individual companies; data are included in higher-level totals.
# S - Estimate does not meet publication standards because of high sampling variability (coefficient of variation is greater than 30%), poor response quality (total quantity response rate is less than 50%), or other concerns about the estimate's quality. Unpublished estimates derived from this table by subtraction are subject to these same limitations and should not be attributed to the U.S. Census Bureau. For a description of publication standards and the total quantity response rate, see https://www.census.gov/about/policies/quality/standards/standardf1.html.
556/7:
# Drop rows that do not have ecommerce data
#[Estimates are shown in millions of dollars and are based on data from the Annual Retail Trade Survey. Estimates have been adjusted using final results of the 2012 Economic Census.] 
retail_history_df = retail_history_df.drop([0, 8])
556/8:
retail_history_df = retail_history_df.reset_index(drop=True)
retail_history_df
556/9:
US_totalrev_df = pd.read_csv(US_totalrev_2018_2017_csv)
US_totalrev_df.head()
556/10:
# Drop Columns
US_totalrev_df = US_totalrev_df.drop(columns=['Unnamed: 0', 'NAICS Code'])
US_totalrev_df.head()
556/11:
# Rename columns
US_totalrev_df = US_totalrev_df.rename(columns={"Description": "Description of Inudstries",
                                                "Total": "2018 Total Revenue",
                                                "Revenue from Electronic Sources": "2018 Ecommerce Total",
                                                "Total.2": "2017 Total Revenue",
                                                "Revenue from Electronic Sources.2": "2017 Ecommerce Total",
                                                "Total.3": "Year over Year % Change",
                                                "Revenue from Electronic Sources.3": "Year over Year % Change: Ecommerce",
                                                "2018": "Ecommerce Revenue as a % of Total Revenue, 2018",
                                                "2017": "Ecommerce Revenue as a % of Total Revenue, 2017",
                                                "2018(2)": "Percent Distribution of Revenue, Ecommerce 2018"
                                                 })
US_totalrev_df.head()
556/12:
US_totalrev_df = US_totalrev_df.drop(columns=["Unnamed: 12",
                                              "Unnamed: 13",
                                              "Unnamed: 14",
                                              "Unnamed: 15",
                                             ])
US_totalrev_df.head()
556/13:
def highlight_max(s): 
    if s.dtype == np.object: 
        is_max = [False for _ in range(s.shape[0])] 
    else: 
        is_max = s == s.max() 
    return ['background: lightgreen' if cell else '' for cell in is_max] 
  
US_totalrev_df.style.apply(highlight_max)
US_totalrev_df
556/14:
def highlight_max(s): 
    if s.dtype == np.object: 
        is_max = [False for _ in range(s.shape[0])] 
    else: 
        is_max = s == s.max() 
    return ['background: lightgreen' if cell else '' for cell in is_max] 
  
US_totalrev_df.style.apply(highlight_max)
US_totalrev_df
556/15:
# Removing rows that do not have ecommerce data
US_totalrev_df = US_totalrev_df.drop([1, 6, 7, 14])
US_totalrev_df
556/16:
US_totalrev_df = US_totalrev_df.reset_index(drop=True)
US_totalrev_df
556/17:
# Removing rows that do not have ecommerce data
US_totalrev_df = US_totalrev_df.drop([1, 6, 7, 14])
556/18:
US_totalrev_df = US_totalrev_df.reset_index(drop=True)
US_totalrev_df
556/19:
format_dict = {'2018 Total Revenue':'${0:,.0f}', 
               '2018 Ecommerce Total':'${0:,.0f}', 
               '2017 Total Revenue':'${0:,.0f}', 
               '2017 Ecommerce Total':'${0:,.0f}', 
               'Year over Year % Change': '{:.2%}',
               'Year over Year % Change: Ecommerce': '{:.2%}',
               'Ecommerce Revenue as a % of Total Revenue, 2018': '{:.2%}',
               'Ecommerce Revenue as a % of Total Revenue, 2017': '{:.2%}',
               'Percent Distribution of Revenue, Ecommerce 2018': '{:.2%}'
              
              }
US_totalrev_df.style.format(format_dict)
US_totalev_df
556/20:
format_dict = {'2018 Total Revenue':'${0:,.0f}', 
               '2018 Ecommerce Total':'${0:,.0f}', 
               '2017 Total Revenue':'${0:,.0f}', 
               '2017 Ecommerce Total':'${0:,.0f}', 
               'Year over Year % Change': '{:.2%}',
               'Year over Year % Change: Ecommerce': '{:.2%}',
               'Ecommerce Revenue as a % of Total Revenue, 2018': '{:.2%}',
               'Ecommerce Revenue as a % of Total Revenue, 2017': '{:.2%}',
               'Percent Distribution of Revenue, Ecommerce 2018': '{:.2%}'
              
              }
US_totalev_df = US_totalrev_df.style.format(format_dict)
US_totalev_df
557/1:
# Dependencies
import pandas as pd
import numpy as np
import os
import webbrowser
import io
557/2:
# Save file path to variable
retail_history_csv = "Datasets/EstUS_Retail_Trade_E-commercehistory.csv"
US_totalrev_2018_2017_csv = "Datasets/US_Total_rev_2018and2017.csv"
557/3:
# Read with pandas
retail_history_df = pd.read_csv(retail_history_csv)
retail_history_df.head()
557/4:
# Clean up DF
retail_history_df = retail_history_df.rename(columns={"Unnamed: 1": "Kind of Business",
                                 "2018": "2018 Total",
                                 "Unnamed: 3": "2018 Ecommerce Total",
                                 "2017r": "2017 Total",
                                 "Unnamed: 5": "2017 Ecommerce Total",
                                 "2016r": "2016 Total",
                                 "Unnamed: 7": "2016 Ecommerce Total",
                                 "2015r": "2015 Total",
                                 "Unnamed: 9": "2015 Ecommerce Total",
                                 "2014r": "2014 Total",
                                 "Unnamed: 11": "2014 Ecommerce Total",
                                 "1998": "1998 Total",
                                 "Unnamed: 43": "1998 Ecommerce"
                                 })
retail_history_df.head()
557/5:
# Drop columns we do not need
retail_history_df = retail_history_df.drop(columns=['NAICS Code',
                                                    '2013r',
                                                    'Unnamed: 13',
                                                    '2012r',
                                                    'Unnamed: 15',
                                                    '2011r',
                                                    'Unnamed: 17',
                                                    '2010',
                                                     'Unnamed: 19',
                                                     '2009',
                                                     'Unnamed: 21',
                                                     '2008',
                                                     'Unnamed: 23',
                                                     '2007',
                                                     'Unnamed: 25',
                                                     '2006',
                                                     'Unnamed: 27',
                                                     '2005',
                                                     'Unnamed: 29',
                                                     '2004',
                                                     'Unnamed: 31',
                                                     '2003',
                                                     'Unnamed: 33',
                                                     '2002',
                                                     'Unnamed: 35',
                                                     '2001',
                                                     'Unnamed: 37',
                                                     '2000',
                                                     'Unnamed: 39',
                                                     '1999',
                                                     'Unnamed: 41',
                                                    '2004', 
                                                    'Unnamed: 31',
                                                    '2003', 
                                                    'Unnamed: 33',
                                                    '2002',
                                                    'Unnamed: 35',
                                                    '2001',
                                                    'Unnamed: 37',
                                                    '2000',
                                                    'Unnamed: 39',
                                                    '1999',
                                                    'Unnamed: 41',
                                                   
                                                   ])
retail_history_df.head()
557/6:
retail_history_df.head()
# D - Denotes an estimate withheld to avoid disclosing data of individual companies; data are included in higher-level totals.
# S - Estimate does not meet publication standards because of high sampling variability (coefficient of variation is greater than 30%), poor response quality (total quantity response rate is less than 50%), or other concerns about the estimate's quality. Unpublished estimates derived from this table by subtraction are subject to these same limitations and should not be attributed to the U.S. Census Bureau. For a description of publication standards and the total quantity response rate, see https://www.census.gov/about/policies/quality/standards/standardf1.html.
557/7:
# Drop rows that do not have ecommerce data
#[Estimates are shown in millions of dollars and are based on data from the Annual Retail Trade Survey. Estimates have been adjusted using final results of the 2012 Economic Census.] 
retail_history_df = retail_history_df.drop([0, 8])
557/8:
retail_history_df = retail_history_df.reset_index(drop=True)
retail_history_df
557/9:
US_totalrev_df = pd.read_csv(US_totalrev_2018_2017_csv)
US_totalrev_df.head()
557/10:
# Drop Columns
US_totalrev_df = US_totalrev_df.drop(columns=['Unnamed: 0', 'NAICS Code'])
US_totalrev_df.head()
557/11:
# Rename columns
US_totalrev_df = US_totalrev_df.rename(columns={"Description": "Description of Inudstries",
                                                "Total": "2018 Total Revenue",
                                                "Revenue from Electronic Sources": "2018 Ecommerce Total",
                                                "Total.2": "2017 Total Revenue",
                                                "Revenue from Electronic Sources.2": "2017 Ecommerce Total",
                                                "Total.3": "Year over Year % Change",
                                                "Revenue from Electronic Sources.3": "Year over Year % Change: Ecommerce",
                                                "2018": "Ecommerce Revenue as a % of Total Revenue, 2018",
                                                "2017": "Ecommerce Revenue as a % of Total Revenue, 2017",
                                                "2018(2)": "Percent Distribution of Revenue, Ecommerce 2018"
                                                 })
US_totalrev_df.head()
557/12:
US_totalrev_df = US_totalrev_df.drop(columns=["Unnamed: 12",
                                              "Unnamed: 13",
                                              "Unnamed: 14",
                                              "Unnamed: 15",
                                             ])
US_totalrev_df.head()
557/13:
# Removing rows that do not have ecommerce data
US_totalrev_df = US_totalrev_df.drop([1, 6, 7, 14])
557/14:
US_totalrev_df = US_totalrev_df.reset_index(drop=True)
US_totalrev_df
557/15:
format_dict = {'2018 Total Revenue':'${0:,.0f}', 
               '2018 Ecommerce Total':'${0:,.0f}', 
               '2017 Total Revenue':'${0:,.0f}', 
               '2017 Ecommerce Total':'${0:,.0f}', 
               'Year over Year % Change': '{:.2%}',
               'Year over Year % Change: Ecommerce': '{:.2%}',
               'Ecommerce Revenue as a % of Total Revenue, 2018': '{:.2%}',
               'Ecommerce Revenue as a % of Total Revenue, 2017': '{:.2%}',
               'Percent Distribution of Revenue, Ecommerce 2018': '{:.2%}'
              
              }
US_totalev_df = US_totalrev_df.style.format(format_dict)
US_totalev_df
557/16:
US_totalrev_df.style.format("{:.2%}", na_rep="-")
US_totalrev_df
557/17:
US_totalrev_df.style.\
    applymap(color_negative_red).\
    apply(highlight_max)
US_totalrev_df
557/18:
US_totalrev_df = US_totalrev_df.style.\
    applymap(color_negative_red).\
    apply(highlight_max)
US_totalrev_df
557/19:
def color_negative_red(val):
    color = 'red' if val < 0 else 'black'
    return 'color: %s' % color
557/20:
def highlight_max(s):
    is_max = s == s.max()
    return ['background-color: yellow' if v else '' for v in is_max]
557/21:
US_totalrev_df = US_totalrev_df.style.\
    applymap(color_negative_red).\
    apply(highlight_max)
US_totalrev_df
560/1:
# Dependencies
import pandas as pd
import numpy as np
import os
import webbrowser
import io
560/2:
# Save file path to variable
retail_history_csv = "Datasets/EstUS_Retail_Trade_E-commercehistory.csv"
US_totalrev_2018_2017_csv = "Datasets/US_Total_rev_2018and2017.csv"
560/3:
# Read with pandas
retail_history_df = pd.read_csv(retail_history_csv)
retail_history_df.head()
560/4:
# Clean up DF
retail_history_df = retail_history_df.rename(columns={"Unnamed: 1": "Kind of Business",
                                 "2018": "2018 Total",
                                 "Unnamed: 3": "2018 Ecommerce Total",
                                 "2017r": "2017 Total",
                                 "Unnamed: 5": "2017 Ecommerce Total",
                                 "2016r": "2016 Total",
                                 "Unnamed: 7": "2016 Ecommerce Total",
                                 "2015r": "2015 Total",
                                 "Unnamed: 9": "2015 Ecommerce Total",
                                 "2014r": "2014 Total",
                                 "Unnamed: 11": "2014 Ecommerce Total",
                                 "1998": "1998 Total",
                                 "Unnamed: 43": "1998 Ecommerce"
                                 })
retail_history_df.head()
560/5:
# Drop columns we do not need
retail_history_df = retail_history_df.drop(columns=['NAICS Code',
                                                    '2013r',
                                                    'Unnamed: 13',
                                                    '2012r',
                                                    'Unnamed: 15',
                                                    '2011r',
                                                    'Unnamed: 17',
                                                    '2010',
                                                     'Unnamed: 19',
                                                     '2009',
                                                     'Unnamed: 21',
                                                     '2008',
                                                     'Unnamed: 23',
                                                     '2007',
                                                     'Unnamed: 25',
                                                     '2006',
                                                     'Unnamed: 27',
                                                     '2005',
                                                     'Unnamed: 29',
                                                     '2004',
                                                     'Unnamed: 31',
                                                     '2003',
                                                     'Unnamed: 33',
                                                     '2002',
                                                     'Unnamed: 35',
                                                     '2001',
                                                     'Unnamed: 37',
                                                     '2000',
                                                     'Unnamed: 39',
                                                     '1999',
                                                     'Unnamed: 41',
                                                    '2004', 
                                                    'Unnamed: 31',
                                                    '2003', 
                                                    'Unnamed: 33',
                                                    '2002',
                                                    'Unnamed: 35',
                                                    '2001',
                                                    'Unnamed: 37',
                                                    '2000',
                                                    'Unnamed: 39',
                                                    '1999',
                                                    'Unnamed: 41',
                                                   
                                                   ])
retail_history_df.head()
560/6:
retail_history_df.head()
# D - Denotes an estimate withheld to avoid disclosing data of individual companies; data are included in higher-level totals.
# S - Estimate does not meet publication standards because of high sampling variability (coefficient of variation is greater than 30%), poor response quality (total quantity response rate is less than 50%), or other concerns about the estimate's quality. Unpublished estimates derived from this table by subtraction are subject to these same limitations and should not be attributed to the U.S. Census Bureau. For a description of publication standards and the total quantity response rate, see https://www.census.gov/about/policies/quality/standards/standardf1.html.
560/7:
# Drop rows that do not have ecommerce data
#[Estimates are shown in millions of dollars and are based on data from the Annual Retail Trade Survey. Estimates have been adjusted using final results of the 2012 Economic Census.] 
retail_history_df = retail_history_df.drop([0, 8])
560/8:
retail_history_df = retail_history_df.reset_index(drop=True)
retail_history_df
560/9:
US_totalrev_df = pd.read_csv(US_totalrev_2018_2017_csv)
US_totalrev_df.head()
560/10:
# Drop Columns
US_totalrev_df = US_totalrev_df.drop(columns=['Unnamed: 0', 'NAICS Code'])
US_totalrev_df.head()
560/11:
# Rename columns
US_totalrev_df = US_totalrev_df.rename(columns={"Description": "Description of Inudstries",
                                                "Total": "2018 Total Revenue",
                                                "Revenue from Electronic Sources": "2018 Ecommerce Total",
                                                "Total.2": "2017 Total Revenue",
                                                "Revenue from Electronic Sources.2": "2017 Ecommerce Total",
                                                "Total.3": "Year over Year % Change",
                                                "Revenue from Electronic Sources.3": "Year over Year % Change: Ecommerce",
                                                "2018": "Ecommerce Revenue as a % of Total Revenue, 2018",
                                                "2017": "Ecommerce Revenue as a % of Total Revenue, 2017",
                                                "2018(2)": "Percent Distribution of Revenue, Ecommerce 2018"
                                                 })
US_totalrev_df.head()
560/12:
US_totalrev_df = US_totalrev_df.drop(columns=["Unnamed: 12",
                                              "Unnamed: 13",
                                              "Unnamed: 14",
                                              "Unnamed: 15",
                                             ])
US_totalrev_df.head()
560/13:
# Removing rows that do not have ecommerce data
US_totalrev_df = US_totalrev_df.drop([1, 6, 7, 14])
560/14: US_totalrev_df = US_totalrev_df.reset_index(drop=True)
560/15:
US_totalrev_df = US_totalrevdf.fillna(0)
US_totalrev_df
560/16:
US_totalrev_df = US_totalrev_df.fillna(0)
US_totalrev_df
560/17:
US_totalrev_df = US_totalrev_df.replace('S': None)
US_totalrev_df
560/18:
US_totalrev_df = US_totalrev_df.replace({'S': None})
US_totalrev_df
560/19:
# Dependencies
import pandas as pd
import numpy as np
import os
import webbrowser
import io
560/20:
# Save file path to variable
retail_history_csv = "Datasets/EstUS_Retail_Trade_E-commercehistory.csv"
US_totalrev_2018_2017_csv = "Datasets/US_Retail_Trade_Sales_2018_2017.csv"
560/21:
# Read with pandas
retail_history_df = pd.read_csv(retail_history_csv)
retail_history_df.head()
560/22:
# Clean up DF
retail_history_df = retail_history_df.rename(columns={"Unnamed: 1": "Kind of Business",
                                 "2018": "2018 Total",
                                 "Unnamed: 3": "2018 Ecommerce Total",
                                 "2017r": "2017 Total",
                                 "Unnamed: 5": "2017 Ecommerce Total",
                                 "2016r": "2016 Total",
                                 "Unnamed: 7": "2016 Ecommerce Total",
                                 "2015r": "2015 Total",
                                 "Unnamed: 9": "2015 Ecommerce Total",
                                 "2014r": "2014 Total",
                                 "Unnamed: 11": "2014 Ecommerce Total",
                                 "1998": "1998 Total",
                                 "Unnamed: 43": "1998 Ecommerce"
                                 })
retail_history_df.head()
560/23:
# Drop columns we do not need
retail_history_df = retail_history_df.drop(columns=['NAICS Code',
                                                    '2013r',
                                                    'Unnamed: 13',
                                                    '2012r',
                                                    'Unnamed: 15',
                                                    '2011r',
                                                    'Unnamed: 17',
                                                    '2010',
                                                     'Unnamed: 19',
                                                     '2009',
                                                     'Unnamed: 21',
                                                     '2008',
                                                     'Unnamed: 23',
                                                     '2007',
                                                     'Unnamed: 25',
                                                     '2006',
                                                     'Unnamed: 27',
                                                     '2005',
                                                     'Unnamed: 29',
                                                     '2004',
                                                     'Unnamed: 31',
                                                     '2003',
                                                     'Unnamed: 33',
                                                     '2002',
                                                     'Unnamed: 35',
                                                     '2001',
                                                     'Unnamed: 37',
                                                     '2000',
                                                     'Unnamed: 39',
                                                     '1999',
                                                     'Unnamed: 41',
                                                    '2004', 
                                                    'Unnamed: 31',
                                                    '2003', 
                                                    'Unnamed: 33',
                                                    '2002',
                                                    'Unnamed: 35',
                                                    '2001',
                                                    'Unnamed: 37',
                                                    '2000',
                                                    'Unnamed: 39',
                                                    '1999',
                                                    'Unnamed: 41',
                                                   
                                                   ])
retail_history_df.head()
561/1:
# Dependencies
import pandas as pd
import numpy as np
import os
import webbrowser
import io
561/2:
# Save file path to variable
retail_history_csv = "Datasets/EstUS_Retail_Trade_E-commercehistory.csv"
US_totalrev_2018_2017_csv = "Datasets/US_Retail_Trade_Sales_2018_2017.csv"
561/3:
# Read with pandas
retail_history_df = pd.read_csv(retail_history_csv)
retail_history_df.head()
562/1:
# Dependencies
import pandas as pd
import numpy as np
import os
import webbrowser
import io
562/2:
# Save file path to variable
retail_history_csv = "Datasets/EstUS_Retail_Trade_E-commercehistory.csv"
US_totalrev_2018_2017_csv = "Datasets/US_Retail_Trade_Sales_2018_2017.csv"
562/3:
# Read with pandas
retail_history_df = pd.read_csv(retail_history_csv)
retail_history_df.head()
562/4:
# Clean up DF
retail_history_df = retail_history_df.rename(columns={"Unnamed: 1": "Kind of Business",
                                 "2018": "2018 Total",
                                 "Unnamed: 3": "2018 Ecommerce Total",
                                 "2017r": "2017 Total",
                                 "Unnamed: 5": "2017 Ecommerce Total",
                                 "2016r": "2016 Total",
                                 "Unnamed: 7": "2016 Ecommerce Total",
                                 "2015r": "2015 Total",
                                 "Unnamed: 9": "2015 Ecommerce Total",
                                 "2014r": "2014 Total",
                                 "Unnamed: 11": "2014 Ecommerce Total",
                                 "1998": "1998 Total",
                                 "Unnamed: 43": "1998 Ecommerce"
                                 })
retail_history_df.head()
562/5:
# Drop columns we do not need
retail_history_df = retail_history_df.drop(columns=['NAICS Code',
                                                    '2013r',
                                                    'Unnamed: 13',
                                                    '2012r',
                                                    'Unnamed: 15',
                                                    '2011r',
                                                    'Unnamed: 17',
                                                    '2010',
                                                     'Unnamed: 19',
                                                     '2009',
                                                     'Unnamed: 21',
                                                     '2008',
                                                     'Unnamed: 23',
                                                     '2007',
                                                     'Unnamed: 25',
                                                     '2006',
                                                     'Unnamed: 27',
                                                     '2005',
                                                     'Unnamed: 29',
                                                     '2004',
                                                     'Unnamed: 31',
                                                     '2003',
                                                     'Unnamed: 33',
                                                     '2002',
                                                     'Unnamed: 35',
                                                     '2001',
                                                     'Unnamed: 37',
                                                     '2000',
                                                     'Unnamed: 39',
                                                     '1999',
                                                     'Unnamed: 41',
                                                    '2004', 
                                                    'Unnamed: 31',
                                                    '2003', 
                                                    'Unnamed: 33',
                                                    '2002',
                                                    'Unnamed: 35',
                                                    '2001',
                                                    'Unnamed: 37',
                                                    '2000',
                                                    'Unnamed: 39',
                                                    '1999',
                                                    'Unnamed: 41',
                                                   
                                                   ])
retail_history_df.head()
562/6:
retail_history_df.head()
# D - Denotes an estimate withheld to avoid disclosing data of individual companies; data are included in higher-level totals.
# S - Estimate does not meet publication standards because of high sampling variability (coefficient of variation is greater than 30%), poor response quality (total quantity response rate is less than 50%), or other concerns about the estimate's quality. Unpublished estimates derived from this table by subtraction are subject to these same limitations and should not be attributed to the U.S. Census Bureau. For a description of publication standards and the total quantity response rate, see https://www.census.gov/about/policies/quality/standards/standardf1.html.
562/7:
# Drop rows that do not have ecommerce data
#[Estimates are shown in millions of dollars and are based on data from the Annual Retail Trade Survey. Estimates have been adjusted using final results of the 2012 Economic Census.] 
retail_history_df = retail_history_df.drop([0, 8])
562/8:
retail_history_df = retail_history_df.reset_index(drop=True)
retail_history_df
562/9:
US_totalrev_df = pd.read_csv(US_totalrev_2018_2017_csv)
US_totalrev_df.head()
562/10:
# Drop Columns
US_totalrev_df = US_totalrev_df.drop(columns=['Unnamed: 0', 'NAICS Code'])
US_totalrev_df.head()
562/11:
# Drop rows that do not have ecommerce data
#[Estimates are shown in millions of dollars and are based on data from the Annual Retail Trade Survey. Estimates have been adjusted using final results of the 2012 Economic Census.] 
retail_history_df = retail_history_df.drop([0, 8])
562/12: retail_history_df = retail_history_df.reset_index(drop=True)
562/13:
retail_history_df = retail_history_df.replace({'S': None})
retail_history_df
562/14:
retail_history_df = retail_history_df.replace({'S': None,
                                               'D': None
                                              })
retail_history_df
562/15:
US_totalrev_df = pd.read_csv(US_totalrev_2018_2017_csv)
US_totalrev_df
562/16:
US_totalrev_df = pd.read_csv(US_totalrev_2018_2017_csv)
US_totalrev_df
562/17:
US_totalrev_df = pd.read_csv(US_totalrev_2018_2017_csv)
US_totalrev_df
562/18:
US_totalrev_df = pd.read_csv(US_totalrev_2018_2017_csv)
US_totalrev_df
562/19: retail_history_df = retail_history_df.drop('NaN')
562/20: US_totalrev_df = pd.read_csv(US_totalrev_2018_2017_csv)
562/21: retail_history_df = retail_history_df.drop('NaN')
563/1:
# Dependencies
import pandas as pd
import numpy as np
import os
import webbrowser
import io
563/2:
# Save file path to variable
retail_history_csv = "Datasets/EstUS_Retail_Trade_E-commercehistory.csv"
US_totalrev_2018_2017_csv = "Datasets/US_Retail_Trade_Sales_2018_2017.csv"
563/3:
# Read with pandas
retail_history_df = pd.read_csv(retail_history_csv)
retail_history_df.head()
563/4:
# Clean up DF
retail_history_df = retail_history_df.rename(columns={"Unnamed: 1": "Kind of Business",
                                 "2018": "2018 Total",
                                 "Unnamed: 3": "2018 Ecommerce Total",
                                 "2017r": "2017 Total",
                                 "Unnamed: 5": "2017 Ecommerce Total",
                                 "2016r": "2016 Total",
                                 "Unnamed: 7": "2016 Ecommerce Total",
                                 "2015r": "2015 Total",
                                 "Unnamed: 9": "2015 Ecommerce Total",
                                 "2014r": "2014 Total",
                                 "Unnamed: 11": "2014 Ecommerce Total",
                                 "1998": "1998 Total",
                                 "Unnamed: 43": "1998 Ecommerce"
                                 })
retail_history_df.head()
563/5:
# Drop columns we do not need
retail_history_df = retail_history_df.drop(columns=['NAICS Code',
                                                    '2013r',
                                                    'Unnamed: 13',
                                                    '2012r',
                                                    'Unnamed: 15',
                                                    '2011r',
                                                    'Unnamed: 17',
                                                    '2010',
                                                     'Unnamed: 19',
                                                     '2009',
                                                     'Unnamed: 21',
                                                     '2008',
                                                     'Unnamed: 23',
                                                     '2007',
                                                     'Unnamed: 25',
                                                     '2006',
                                                     'Unnamed: 27',
                                                     '2005',
                                                     'Unnamed: 29',
                                                     '2004',
                                                     'Unnamed: 31',
                                                     '2003',
                                                     'Unnamed: 33',
                                                     '2002',
                                                     'Unnamed: 35',
                                                     '2001',
                                                     'Unnamed: 37',
                                                     '2000',
                                                     'Unnamed: 39',
                                                     '1999',
                                                     'Unnamed: 41',
                                                    '2004', 
                                                    'Unnamed: 31',
                                                    '2003', 
                                                    'Unnamed: 33',
                                                    '2002',
                                                    'Unnamed: 35',
                                                    '2001',
                                                    'Unnamed: 37',
                                                    '2000',
                                                    'Unnamed: 39',
                                                    '1999',
                                                    'Unnamed: 41',
                                                   
                                                   ])
retail_history_df.head()
563/6:
retail_history_df.head()
# D - Denotes an estimate withheld to avoid disclosing data of individual companies; data are included in higher-level totals.
# S - Estimate does not meet publication standards because of high sampling variability (coefficient of variation is greater than 30%), poor response quality (total quantity response rate is less than 50%), or other concerns about the estimate's quality. Unpublished estimates derived from this table by subtraction are subject to these same limitations and should not be attributed to the U.S. Census Bureau. For a description of publication standards and the total quantity response rate, see https://www.census.gov/about/policies/quality/standards/standardf1.html.
563/7:
# Drop rows that do not have ecommerce data
#[Estimates are shown in millions of dollars and are based on data from the Annual Retail Trade Survey. Estimates have been adjusted using final results of the 2012 Economic Census.] 
retail_history_df = retail_history_df.drop([0, 8])
563/8: retail_history_df = retail_history_df.reset_index(drop=True)
563/9:
retail_history_df = retail_history_df.replace({'S': None,
                                               'D': None
                                              })
retail_history_df
563/10: US_totalrev_df = pd.read_csv(US_totalrev_2018_2017_csv)
563/11: retail_history_df = retail_history_df.drop('NaN')
563/12:
US_totalrev_df = pd.read_csv(US_totalrev_2018_2017_csv)
US_totalrev_df.head()
563/13:
US_totalrev_df = pd.read_csv(US_totalrev_2018_2017_csv)
US_totalrev_df.head()
563/14:
# Drop Columns
US_totalrev_df = US_totalrev_df.drop(columns=['Unnamed: 0']
US_totalrev_df.head()
563/15:
# Drop Columns
US_totalrev_df = US_totalrev_df.drop(columns=['Unnamed: 0'])
US_totalrev_df.head()
563/16:
# Drop Rows
US_totalrev_df = US_totalrev_df.drop([0, 1])
US_totalrev_df.head()
563/17:
US_totalrev_df = US_totalrev_df.reset_index
US_totalrev_df.head()
563/18:
# Drop Rows
US_totalrev_df = US_totalrev_df.drop([0, 1])
564/1:
# Dependencies
import pandas as pd
import numpy as np
import os
import webbrowser
import io
564/2:
# Save file path to variable
retail_history_csv = "Datasets/EstUS_Retail_Trade_E-commercehistory.csv"
US_totalrev_2018_2017_csv = "Datasets/US_Retail_Trade_Sales_2018_2017.csv"
564/3:
# Read with pandas
retail_history_df = pd.read_csv(retail_history_csv)
retail_history_df.head()
564/4:
# Clean up DF
retail_history_df = retail_history_df.rename(columns={"Unnamed: 1": "Kind of Business",
                                 "2018": "2018 Total",
                                 "Unnamed: 3": "2018 Ecommerce Total",
                                 "2017r": "2017 Total",
                                 "Unnamed: 5": "2017 Ecommerce Total",
                                 "2016r": "2016 Total",
                                 "Unnamed: 7": "2016 Ecommerce Total",
                                 "2015r": "2015 Total",
                                 "Unnamed: 9": "2015 Ecommerce Total",
                                 "2014r": "2014 Total",
                                 "Unnamed: 11": "2014 Ecommerce Total",
                                 "1998": "1998 Total",
                                 "Unnamed: 43": "1998 Ecommerce"
                                 })
retail_history_df.head()
564/5:
# Drop columns we do not need
retail_history_df = retail_history_df.drop(columns=['NAICS Code',
                                                    '2013r',
                                                    'Unnamed: 13',
                                                    '2012r',
                                                    'Unnamed: 15',
                                                    '2011r',
                                                    'Unnamed: 17',
                                                    '2010',
                                                     'Unnamed: 19',
                                                     '2009',
                                                     'Unnamed: 21',
                                                     '2008',
                                                     'Unnamed: 23',
                                                     '2007',
                                                     'Unnamed: 25',
                                                     '2006',
                                                     'Unnamed: 27',
                                                     '2005',
                                                     'Unnamed: 29',
                                                     '2004',
                                                     'Unnamed: 31',
                                                     '2003',
                                                     'Unnamed: 33',
                                                     '2002',
                                                     'Unnamed: 35',
                                                     '2001',
                                                     'Unnamed: 37',
                                                     '2000',
                                                     'Unnamed: 39',
                                                     '1999',
                                                     'Unnamed: 41',
                                                    '2004', 
                                                    'Unnamed: 31',
                                                    '2003', 
                                                    'Unnamed: 33',
                                                    '2002',
                                                    'Unnamed: 35',
                                                    '2001',
                                                    'Unnamed: 37',
                                                    '2000',
                                                    'Unnamed: 39',
                                                    '1999',
                                                    'Unnamed: 41',
                                                   
                                                   ])
retail_history_df.head()
564/6:
retail_history_df.head()
# D - Denotes an estimate withheld to avoid disclosing data of individual companies; data are included in higher-level totals.
# S - Estimate does not meet publication standards because of high sampling variability (coefficient of variation is greater than 30%), poor response quality (total quantity response rate is less than 50%), or other concerns about the estimate's quality. Unpublished estimates derived from this table by subtraction are subject to these same limitations and should not be attributed to the U.S. Census Bureau. For a description of publication standards and the total quantity response rate, see https://www.census.gov/about/policies/quality/standards/standardf1.html.
564/7:
# Drop rows that do not have ecommerce data
#[Estimates are shown in millions of dollars and are based on data from the Annual Retail Trade Survey. Estimates have been adjusted using final results of the 2012 Economic Census.] 
retail_history_df = retail_history_df.drop([0, 8])
564/8: retail_history_df = retail_history_df.reset_index(drop=True)
564/9:
retail_history_df = retail_history_df.replace({'S': None,
                                               'D': None
                                              })
retail_history_df
564/10:
US_totalrev_df = pd.read_csv(US_totalrev_2018_2017_csv)
US_totalrev_df.head()
564/11:
# Drop Columns
US_totalrev_df = US_totalrev_df.drop(columns=['Unnamed: 0'])
US_totalrev_df.head()
564/12:
# Drop Rows
US_totalrev_df = US_totalrev_df.drop([0, 1])
564/13:
US_totalrev_df = US_totalrev_df.reset_index
US_totalrev_df.head()
564/14:
US_totalrev_df = US_totalrev_df.reset_index()
US_totalrev_df
564/15:
US_totalrev_df = US_totalrev_df.reset_index(drop=True)
US_totalrev_df
564/16:
# Rename columns
US_totalrev_df = US_totalrev_df.rename(columns={"2018 E-commerce': '2018 Total ecommerce",
                                                "2017 Revised \nTotal Sales": "2017 Total Sales",
                                                "2017 Revised \nE-commerce": "2017 Total ecommerce",
                                                "Percent Distribution\nof E-commerce Sales 2018": 'Percent Distribution of ecommerce 2018'
                                                 })
US_totalrev_df.head()
564/17:
# Rename columns
US_totalrev_df = US_totalrev_df.rename(columns={"2018 E-commerce": '2018 Total ecommerce",
                                                "2017 Revised \nTotal Sales": "2017 Total Sales",
                                                "2017 Revised \nE-commerce": "2017 Total ecommerce",
                                                "Percent Distribution\nof E-commerce Sales 2018": 'Percent Distribution of ecommerce 2018'
                                                 })
US_totalrev_df.head()
564/18:
# Rename columns
US_totalrev_df = US_totalrev_df.rename(columns={"2018 E-commerce": "2018 Total ecommerce",
                                                "2017 Revised \nTotal Sales": "2017 Total Sales",
                                                "2017 Revised \nE-commerce": "2017 Total ecommerce",
                                                "Percent Distribution\nof E-commerce Sales 2018": 'Percent Distribution of ecommerce 2018'
                                                 })
US_totalrev_df.head()
565/1:
# Dependencies
import pandas as pd
import numpy as np
import os
import webbrowser
import io
565/2:
# Save file path to variable
retail_history_csv = "Datasets/EstUS_Retail_Trade_E-commercehistory.csv"
US_totalrev_2018_2017_csv = "Datasets/US_Retail_Trade_Sales_2018_2017.csv"
565/3:
# Read with pandas
retail_history_df = pd.read_csv(retail_history_csv)
retail_history_df.head()
565/4:
# Clean up DF
retail_history_df = retail_history_df.rename(columns={"Unnamed: 1": "Kind of Business",
                                 "2018": "2018 Total",
                                 "Unnamed: 3": "2018 Ecommerce Total",
                                 "2017r": "2017 Total",
                                 "Unnamed: 5": "2017 Ecommerce Total",
                                 "2016r": "2016 Total",
                                 "Unnamed: 7": "2016 Ecommerce Total",
                                 "2015r": "2015 Total",
                                 "Unnamed: 9": "2015 Ecommerce Total",
                                 "2014r": "2014 Total",
                                 "Unnamed: 11": "2014 Ecommerce Total",
                                 "1998": "1998 Total",
                                 "Unnamed: 43": "1998 Ecommerce"
                                 })
retail_history_df.head()
565/5:
# Drop columns we do not need
retail_history_df = retail_history_df.drop(columns=['NAICS Code',
                                                    '2013r',
                                                    'Unnamed: 13',
                                                    '2012r',
                                                    'Unnamed: 15',
                                                    '2011r',
                                                    'Unnamed: 17',
                                                    '2010',
                                                     'Unnamed: 19',
                                                     '2009',
                                                     'Unnamed: 21',
                                                     '2008',
                                                     'Unnamed: 23',
                                                     '2007',
                                                     'Unnamed: 25',
                                                     '2006',
                                                     'Unnamed: 27',
                                                     '2005',
                                                     'Unnamed: 29',
                                                     '2004',
                                                     'Unnamed: 31',
                                                     '2003',
                                                     'Unnamed: 33',
                                                     '2002',
                                                     'Unnamed: 35',
                                                     '2001',
                                                     'Unnamed: 37',
                                                     '2000',
                                                     'Unnamed: 39',
                                                     '1999',
                                                     'Unnamed: 41',
                                                    '2004', 
                                                    'Unnamed: 31',
                                                    '2003', 
                                                    'Unnamed: 33',
                                                    '2002',
                                                    'Unnamed: 35',
                                                    '2001',
                                                    'Unnamed: 37',
                                                    '2000',
                                                    'Unnamed: 39',
                                                    '1999',
                                                    'Unnamed: 41',
                                                   
                                                   ])
retail_history_df.head()
565/6:
retail_history_df.head()
# D - Denotes an estimate withheld to avoid disclosing data of individual companies; data are included in higher-level totals.
# S - Estimate does not meet publication standards because of high sampling variability (coefficient of variation is greater than 30%), poor response quality (total quantity response rate is less than 50%), or other concerns about the estimate's quality. Unpublished estimates derived from this table by subtraction are subject to these same limitations and should not be attributed to the U.S. Census Bureau. For a description of publication standards and the total quantity response rate, see https://www.census.gov/about/policies/quality/standards/standardf1.html.
565/7:
# Drop rows that do not have ecommerce data
#[Estimates are shown in millions of dollars and are based on data from the Annual Retail Trade Survey. Estimates have been adjusted using final results of the 2012 Economic Census.] 
retail_history_df = retail_history_df.drop([0, 8])
565/8: retail_history_df = retail_history_df.reset_index(drop=True)
565/9:
retail_history_df = retail_history_df.replace({'S': None,
                                               'D': None
                                              })
retail_history_df
565/10:
US_totalrev_df = pd.read_csv(US_totalrev_2018_2017_csv)
US_totalrev_df.head()
565/11:
# Drop Columns
US_totalrev_df = US_totalrev_df.drop(columns=['Unnamed: 0'])
US_totalrev_df.head()
565/12:
# Rename columns
US_totalrev_df = US_totalrev_df.rename(columns={"2018 E-commerce": "2018 Total ecommerce",
                                                "2017 Revised \nTotal Sales": "2017 Total Sales",
                                                "2017 Revised \nE-commerce": "2017 Total ecommerce",
                                                "Percent Distribution\nof E-commerce Sales 2018": 'Percent Distribution of ecommerce 2018'
                                                 })
US_totalrev_df.head()
565/13:
# Removing rows that do not have ecommerce data
US_totalrev_df = US_totalrev_df.drop([1, 6, 7, 14])
565/14: US_totalrev_df = US_totalrev_df.reset_index(drop=True)
565/15:
US_totalrev_df = US_totalrev_df.replace({'S': None})
US_totalrev_df
565/16:
US_totalrev_df = US_totalrev_df.style.\
    applymap(color_negative_red).\
    apply(highlight_max)
US_totalrev_df
565/17:
# Rename columns
US_totalrev_df = US_totalrev_df.rename(columns={"2018 E-commerce": "2018 Total ecommerce",
                                                "2017 Revised \nTotal Sales": "2017 Total Sales",
                                                "2017 Revised \nE-commerce": "2017 Total ecommerce",
                                                "Percent Distribution\nof E-commerce Sales 2018": 'Percent Distribution of ecommerce 2018'
                                                 })
US_totalrev_df
565/18:
# Removing rows that do not have ecommerce data
US_totalrev_df = US_totalrev_df.drop([0])
565/19: US_totalrev_df = US_totalrev_df.reset_index(drop=True)
565/20:
US_totalrev_df = US_totalrev_df.replace({'S': None})
US_totalrev_df
565/21:
# Removing rows that do not have ecommerce data
US_totalrev_df = US_totalrev_df.drop([0, 5, 8])
565/22: US_totalrev_df = US_totalrev_df.reset_index(drop=True)
565/23:
US_totalrev_df = US_totalrev_df.replace({'S': None})
US_totalrev_df
566/1:
# Dependencies
import pandas as pd
import numpy as np
import os
import webbrowser
import io
566/2:
# Save file path to variable
retail_history_csv = "Datasets/EstUS_Retail_Trade_E-commercehistory.csv"
US_totalrev_2018_2017_csv = "Datasets/US_Retail_Trade_Sales_2018_2017.csv"
566/3:
# Read with pandas
retail_history_df = pd.read_csv(retail_history_csv)
retail_history_df.head()
566/4:
# Clean up DF
retail_history_df = retail_history_df.rename(columns={"Unnamed: 1": "Kind of Business",
                                 "2018": "2018 Total",
                                 "Unnamed: 3": "2018 Ecommerce Total",
                                 "2017r": "2017 Total",
                                 "Unnamed: 5": "2017 Ecommerce Total",
                                 "2016r": "2016 Total",
                                 "Unnamed: 7": "2016 Ecommerce Total",
                                 "2015r": "2015 Total",
                                 "Unnamed: 9": "2015 Ecommerce Total",
                                 "2014r": "2014 Total",
                                 "Unnamed: 11": "2014 Ecommerce Total",
                                 "1998": "1998 Total",
                                 "Unnamed: 43": "1998 Ecommerce"
                                 })
retail_history_df.head()
566/5:
# Drop columns we do not need
retail_history_df = retail_history_df.drop(columns=['NAICS Code',
                                                    '2013r',
                                                    'Unnamed: 13',
                                                    '2012r',
                                                    'Unnamed: 15',
                                                    '2011r',
                                                    'Unnamed: 17',
                                                    '2010',
                                                     'Unnamed: 19',
                                                     '2009',
                                                     'Unnamed: 21',
                                                     '2008',
                                                     'Unnamed: 23',
                                                     '2007',
                                                     'Unnamed: 25',
                                                     '2006',
                                                     'Unnamed: 27',
                                                     '2005',
                                                     'Unnamed: 29',
                                                     '2004',
                                                     'Unnamed: 31',
                                                     '2003',
                                                     'Unnamed: 33',
                                                     '2002',
                                                     'Unnamed: 35',
                                                     '2001',
                                                     'Unnamed: 37',
                                                     '2000',
                                                     'Unnamed: 39',
                                                     '1999',
                                                     'Unnamed: 41',
                                                    '2004', 
                                                    'Unnamed: 31',
                                                    '2003', 
                                                    'Unnamed: 33',
                                                    '2002',
                                                    'Unnamed: 35',
                                                    '2001',
                                                    'Unnamed: 37',
                                                    '2000',
                                                    'Unnamed: 39',
                                                    '1999',
                                                    'Unnamed: 41',
                                                   
                                                   ])
retail_history_df.head()
566/6:
retail_history_df.head()
# D - Denotes an estimate withheld to avoid disclosing data of individual companies; data are included in higher-level totals.
# S - Estimate does not meet publication standards because of high sampling variability (coefficient of variation is greater than 30%), poor response quality (total quantity response rate is less than 50%), or other concerns about the estimate's quality. Unpublished estimates derived from this table by subtraction are subject to these same limitations and should not be attributed to the U.S. Census Bureau. For a description of publication standards and the total quantity response rate, see https://www.census.gov/about/policies/quality/standards/standardf1.html.
566/7:
# Drop rows that do not have ecommerce data
#[Estimates are shown in millions of dollars and are based on data from the Annual Retail Trade Survey. Estimates have been adjusted using final results of the 2012 Economic Census.] 
retail_history_df = retail_history_df.drop([0, 8])
566/8: retail_history_df = retail_history_df.reset_index(drop=True)
566/9:
retail_history_df = retail_history_df.replace({'S': None,
                                               'D': None
                                              })
retail_history_df
566/10:
US_totalrev_df = pd.read_csv(US_totalrev_2018_2017_csv)
US_totalrev_df.head()
566/11:
# Drop Columns
US_totalrev_df = US_totalrev_df.drop(columns=['Unnamed: 0'])
US_totalrev_df.head()
566/12:
# Rename columns
US_totalrev_df = US_totalrev_df.rename(columns={"2018 E-commerce": "2018 Total ecommerce",
                                                "2017 Revised \nTotal Sales": "2017 Total Sales",
                                                "2017 Revised \nE-commerce": "2017 Total ecommerce",
                                                "Percent Distribution\nof E-commerce Sales 2018": 'Percent Distribution of ecommerce 2018'
                                                 })
US_totalrev_df
566/13:
# Removing rows that do not have ecommerce data
US_totalrev_df = US_totalrev_df.drop([0, 5, 8])
566/14: US_totalrev_df = US_totalrev_df.reset_index(drop=True)
566/15:
US_totalrev_df = US_totalrev_df.replace({'S': None})
US_totalrev_df
566/16:
US_totalrev_df = US_totalrev_df.style.\
    applymap(color_negative_red).\
    apply(highlight_max)
US_totalrev_df
566/17:
# Removing rows that do not have ecommerce data
US_totalrev_df = US_totalrev_df.drop([0, 5, 8])
566/18: US_totalrev_df = US_totalrev_df.reset_index(drop=True)
566/19:
US_totalrev_df = US_totalrev_df.replace({'S': None})
US_totalrev_df
567/1:
# Dependencies
import pandas as pd
import numpy as np
import os
import webbrowser
import io
567/2:
# Save file path to variable
retail_history_csv = "Datasets/EstUS_Retail_Trade_E-commercehistory.csv"
US_totalrev_2018_2017_csv = "Datasets/US_Retail_Trade_Sales_2018_2017.csv"
567/3:
# Read with pandas
retail_history_df = pd.read_csv(retail_history_csv)
retail_history_df.head()
567/4:
# Clean up DF
retail_history_df = retail_history_df.rename(columns={"Unnamed: 1": "Kind of Business",
                                 "2018": "2018 Total",
                                 "Unnamed: 3": "2018 Ecommerce Total",
                                 "2017r": "2017 Total",
                                 "Unnamed: 5": "2017 Ecommerce Total",
                                 "2016r": "2016 Total",
                                 "Unnamed: 7": "2016 Ecommerce Total",
                                 "2015r": "2015 Total",
                                 "Unnamed: 9": "2015 Ecommerce Total",
                                 "2014r": "2014 Total",
                                 "Unnamed: 11": "2014 Ecommerce Total",
                                 "1998": "1998 Total",
                                 "Unnamed: 43": "1998 Ecommerce"
                                 })
retail_history_df.head()
567/5:
# Drop columns we do not need
retail_history_df = retail_history_df.drop(columns=['NAICS Code',
                                                    '2013r',
                                                    'Unnamed: 13',
                                                    '2012r',
                                                    'Unnamed: 15',
                                                    '2011r',
                                                    'Unnamed: 17',
                                                    '2010',
                                                     'Unnamed: 19',
                                                     '2009',
                                                     'Unnamed: 21',
                                                     '2008',
                                                     'Unnamed: 23',
                                                     '2007',
                                                     'Unnamed: 25',
                                                     '2006',
                                                     'Unnamed: 27',
                                                     '2005',
                                                     'Unnamed: 29',
                                                     '2004',
                                                     'Unnamed: 31',
                                                     '2003',
                                                     'Unnamed: 33',
                                                     '2002',
                                                     'Unnamed: 35',
                                                     '2001',
                                                     'Unnamed: 37',
                                                     '2000',
                                                     'Unnamed: 39',
                                                     '1999',
                                                     'Unnamed: 41',
                                                    '2004', 
                                                    'Unnamed: 31',
                                                    '2003', 
                                                    'Unnamed: 33',
                                                    '2002',
                                                    'Unnamed: 35',
                                                    '2001',
                                                    'Unnamed: 37',
                                                    '2000',
                                                    'Unnamed: 39',
                                                    '1999',
                                                    'Unnamed: 41',
                                                   
                                                   ])
retail_history_df.head()
567/6:
retail_history_df.head()
# D - Denotes an estimate withheld to avoid disclosing data of individual companies; data are included in higher-level totals.
# S - Estimate does not meet publication standards because of high sampling variability (coefficient of variation is greater than 30%), poor response quality (total quantity response rate is less than 50%), or other concerns about the estimate's quality. Unpublished estimates derived from this table by subtraction are subject to these same limitations and should not be attributed to the U.S. Census Bureau. For a description of publication standards and the total quantity response rate, see https://www.census.gov/about/policies/quality/standards/standardf1.html.
567/7:
# Drop rows that do not have ecommerce data
#[Estimates are shown in millions of dollars and are based on data from the Annual Retail Trade Survey. Estimates have been adjusted using final results of the 2012 Economic Census.] 
retail_history_df = retail_history_df.drop([0, 8])
567/8: retail_history_df = retail_history_df.reset_index(drop=True)
567/9:
retail_history_df = retail_history_df.replace({'S': None,
                                               'D': None
                                              })
retail_history_df
567/10:
US_totalrev_df = pd.read_csv(US_totalrev_2018_2017_csv)
US_totalrev_df.head()
567/11:
# Drop Columns
US_totalrev_df = US_totalrev_df.drop(columns=['Unnamed: 0'])
US_totalrev_df.head()
567/12:
# Rename columns
US_totalrev_df = US_totalrev_df.rename(columns={"2018 E-commerce": "2018 Total ecommerce",
                                                "2017 Revised \nTotal Sales": "2017 Total Sales",
                                                "2017 Revised \nE-commerce": "2017 Total ecommerce",
                                                "Percent Distribution\nof E-commerce Sales 2018": 'Percent Distribution of ecommerce 2018'
                                                 })
US_totalrev_df
567/13:
# Removing rows that do not have ecommerce data
US_totalrev_df = US_totalrev_df.drop([0])
567/14: US_totalrev_df = US_totalrev_df.reset_index(drop=True)
567/15:
US_totalrev_df = US_totalrev_df.replace({'S': None})
US_totalrev_df
567/16:
US_totalrev_df = US_totalrev_df.style.\
    applymap(color_negative_red).\
    apply(highlight_max)
US_totalrev_df
568/1:
# Dependencies
import pandas as pd
import numpy as np
import os
import webbrowser
import io
568/2:
# Save file path to variable
retail_history_csv = "Datasets/EstUS_Retail_Trade_E-commercehistory.csv"
US_totalrev_2018_2017_csv = "Datasets/US_Retail_Trade_Sales_2018_2017.csv"
568/3:
# Read with pandas
retail_history_df = pd.read_csv(retail_history_csv)
retail_history_df.head()
568/4:
# Clean up DF
retail_history_df = retail_history_df.rename(columns={"Unnamed: 1": "Kind of Business",
                                 "2018": "2018 Total",
                                 "Unnamed: 3": "2018 Ecommerce Total",
                                 "2017r": "2017 Total",
                                 "Unnamed: 5": "2017 Ecommerce Total",
                                 "2016r": "2016 Total",
                                 "Unnamed: 7": "2016 Ecommerce Total",
                                 "2015r": "2015 Total",
                                 "Unnamed: 9": "2015 Ecommerce Total",
                                 "2014r": "2014 Total",
                                 "Unnamed: 11": "2014 Ecommerce Total",
                                 "1998": "1998 Total",
                                 "Unnamed: 43": "1998 Ecommerce"
                                 })
retail_history_df.head()
568/5:
# Drop columns we do not need
retail_history_df = retail_history_df.drop(columns=['NAICS Code',
                                                    '2013r',
                                                    'Unnamed: 13',
                                                    '2012r',
                                                    'Unnamed: 15',
                                                    '2011r',
                                                    'Unnamed: 17',
                                                    '2010',
                                                     'Unnamed: 19',
                                                     '2009',
                                                     'Unnamed: 21',
                                                     '2008',
                                                     'Unnamed: 23',
                                                     '2007',
                                                     'Unnamed: 25',
                                                     '2006',
                                                     'Unnamed: 27',
                                                     '2005',
                                                     'Unnamed: 29',
                                                     '2004',
                                                     'Unnamed: 31',
                                                     '2003',
                                                     'Unnamed: 33',
                                                     '2002',
                                                     'Unnamed: 35',
                                                     '2001',
                                                     'Unnamed: 37',
                                                     '2000',
                                                     'Unnamed: 39',
                                                     '1999',
                                                     'Unnamed: 41',
                                                    '2004', 
                                                    'Unnamed: 31',
                                                    '2003', 
                                                    'Unnamed: 33',
                                                    '2002',
                                                    'Unnamed: 35',
                                                    '2001',
                                                    'Unnamed: 37',
                                                    '2000',
                                                    'Unnamed: 39',
                                                    '1999',
                                                    'Unnamed: 41',
                                                   
                                                   ])
retail_history_df.head()
568/6:
retail_history_df.head()
# D - Denotes an estimate withheld to avoid disclosing data of individual companies; data are included in higher-level totals.
# S - Estimate does not meet publication standards because of high sampling variability (coefficient of variation is greater than 30%), poor response quality (total quantity response rate is less than 50%), or other concerns about the estimate's quality. Unpublished estimates derived from this table by subtraction are subject to these same limitations and should not be attributed to the U.S. Census Bureau. For a description of publication standards and the total quantity response rate, see https://www.census.gov/about/policies/quality/standards/standardf1.html.
568/7:
# Drop rows that do not have ecommerce data
#[Estimates are shown in millions of dollars and are based on data from the Annual Retail Trade Survey. Estimates have been adjusted using final results of the 2012 Economic Census.] 
retail_history_df = retail_history_df.drop([0, 8])
568/8: retail_history_df = retail_history_df.reset_index(drop=True)
568/9:
retail_history_df = retail_history_df.replace({'S': None,
                                               'D': None
                                              })
retail_history_df
568/10:
US_totalrev_df = pd.read_csv(US_totalrev_2018_2017_csv)
US_totalrev_df.head()
568/11:
# Drop Columns
US_totalrev_df = US_totalrev_df.drop(columns=['Unnamed: 0'])
US_totalrev_df.head()
568/12:
# Rename columns
US_totalrev_df = US_totalrev_df.rename(columns={"2018 E-commerce": "2018 Total ecommerce",
                                                "2017 Revised \nTotal Sales": "2017 Total Sales",
                                                "2017 Revised \nE-commerce": "2017 Total ecommerce",
                                                "Percent Distribution\nof E-commerce Sales 2018": 'Percent Distribution of ecommerce 2018'
                                                 })
US_totalrev_df
568/13:
# Removing rows that do not have ecommerce data
US_totalrev_df = US_totalrev_df.drop([0])
568/14: US_totalrev_df = US_totalrev_df.reset_index(drop=True)
568/15:
US_totalrev_df = US_totalrev_df.replace({'S': None})
US_totalrev_df
568/16:
US_totalrev_df = US_totalrev_df.style.\
    applymap(color_negative_red).\
    apply(highlight_max)
US_totalrev_df
568/17:
US_totalrev_df = US_totalrev_df.reset_index(drop=True)
US_totalrev_df
568/18:
# Removing rows that do not have ecommerce data
US_totalrev_df = US_totalrev_df.drop([0])
568/19:
US_totalrev_df = US_totalrev_df.replace({'S': None})
US_totalrev_df
569/1:
# Dependencies
import pandas as pd
import numpy as np
import os
import webbrowser
import io
569/2:
# Save file path to variable
retail_history_csv = "Datasets/EstUS_Retail_Trade_E-commercehistory.csv"
US_totalrev_2018_2017_csv = "Datasets/US_Retail_Trade_Sales_2018_2017.csv"
569/3:
# Read with pandas
retail_history_df = pd.read_csv(retail_history_csv)
retail_history_df.head()
569/4:
# Clean up DF
retail_history_df = retail_history_df.rename(columns={"Unnamed: 1": "Kind of Business",
                                 "2018": "2018 Total",
                                 "Unnamed: 3": "2018 Ecommerce Total",
                                 "2017r": "2017 Total",
                                 "Unnamed: 5": "2017 Ecommerce Total",
                                 "2016r": "2016 Total",
                                 "Unnamed: 7": "2016 Ecommerce Total",
                                 "2015r": "2015 Total",
                                 "Unnamed: 9": "2015 Ecommerce Total",
                                 "2014r": "2014 Total",
                                 "Unnamed: 11": "2014 Ecommerce Total",
                                 "1998": "1998 Total",
                                 "Unnamed: 43": "1998 Ecommerce"
                                 })
retail_history_df.head()
569/5:
# Drop columns we do not need
retail_history_df = retail_history_df.drop(columns=['NAICS Code',
                                                    '2013r',
                                                    'Unnamed: 13',
                                                    '2012r',
                                                    'Unnamed: 15',
                                                    '2011r',
                                                    'Unnamed: 17',
                                                    '2010',
                                                     'Unnamed: 19',
                                                     '2009',
                                                     'Unnamed: 21',
                                                     '2008',
                                                     'Unnamed: 23',
                                                     '2007',
                                                     'Unnamed: 25',
                                                     '2006',
                                                     'Unnamed: 27',
                                                     '2005',
                                                     'Unnamed: 29',
                                                     '2004',
                                                     'Unnamed: 31',
                                                     '2003',
                                                     'Unnamed: 33',
                                                     '2002',
                                                     'Unnamed: 35',
                                                     '2001',
                                                     'Unnamed: 37',
                                                     '2000',
                                                     'Unnamed: 39',
                                                     '1999',
                                                     'Unnamed: 41',
                                                    '2004', 
                                                    'Unnamed: 31',
                                                    '2003', 
                                                    'Unnamed: 33',
                                                    '2002',
                                                    'Unnamed: 35',
                                                    '2001',
                                                    'Unnamed: 37',
                                                    '2000',
                                                    'Unnamed: 39',
                                                    '1999',
                                                    'Unnamed: 41',
                                                   
                                                   ])
retail_history_df.head()
569/6:
retail_history_df.head()
# D - Denotes an estimate withheld to avoid disclosing data of individual companies; data are included in higher-level totals.
# S - Estimate does not meet publication standards because of high sampling variability (coefficient of variation is greater than 30%), poor response quality (total quantity response rate is less than 50%), or other concerns about the estimate's quality. Unpublished estimates derived from this table by subtraction are subject to these same limitations and should not be attributed to the U.S. Census Bureau. For a description of publication standards and the total quantity response rate, see https://www.census.gov/about/policies/quality/standards/standardf1.html.
569/7:
# Drop rows that do not have ecommerce data
#[Estimates are shown in millions of dollars and are based on data from the Annual Retail Trade Survey. Estimates have been adjusted using final results of the 2012 Economic Census.] 
retail_history_df = retail_history_df.drop([0, 8])
569/8: retail_history_df = retail_history_df.reset_index(drop=True)
569/9:
retail_history_df = retail_history_df.replace({'S': None,
                                               'D': None
                                              })
retail_history_df
569/10:
US_totalrev_df = pd.read_csv(US_totalrev_2018_2017_csv)
US_totalrev_df.head()
569/11:
# Drop Columns
US_totalrev_df = US_totalrev_df.drop(columns=['Unnamed: 0'])
US_totalrev_df.head()
569/12:
# Rename columns
US_totalrev_df = US_totalrev_df.rename(columns={"2018 E-commerce": "2018 Total ecommerce",
                                                "2017 Revised \nTotal Sales": "2017 Total Sales",
                                                "2017 Revised \nE-commerce": "2017 Total ecommerce",
                                                "Percent Distribution\nof E-commerce Sales 2018": 'Percent Distribution of ecommerce 2018'
                                                 })
US_totalrev_df.head()
569/13:
# Removing rows that do not have ecommerce data
US_totalrev_df = US_totalrev_df.drop([0])
569/14:
US_totalrev_df = US_totalrev_df.replace({'S': None})
US_totalrev_df
569/15:
# Removing rows that do not have ecommerce data
US_totalrev_df = US_totalrev_df.drop([1])
569/16:
US_totalrev_df = US_totalrev_df.replace({'S': None})
US_totalrev_df
570/1:
# Dependencies
import pandas as pd
import numpy as np
import os
import webbrowser
import io
570/2:
# Save file path to variable
retail_history_csv = "Datasets/EstUS_Retail_Trade_E-commercehistory.csv"
US_totalrev_2018_2017_csv = "Datasets/US_Retail_Trade_Sales_2018_2017.csv"
570/3:
# Read with pandas
retail_history_df = pd.read_csv(retail_history_csv)
retail_history_df.head()
570/4:
# Clean up DF
retail_history_df = retail_history_df.rename(columns={"Unnamed: 1": "Kind of Business",
                                 "2018": "2018 Total",
                                 "Unnamed: 3": "2018 Ecommerce Total",
                                 "2017r": "2017 Total",
                                 "Unnamed: 5": "2017 Ecommerce Total",
                                 "2016r": "2016 Total",
                                 "Unnamed: 7": "2016 Ecommerce Total",
                                 "2015r": "2015 Total",
                                 "Unnamed: 9": "2015 Ecommerce Total",
                                 "2014r": "2014 Total",
                                 "Unnamed: 11": "2014 Ecommerce Total",
                                 "1998": "1998 Total",
                                 "Unnamed: 43": "1998 Ecommerce"
                                 })
retail_history_df.head()
570/5:
# Drop columns we do not need
retail_history_df = retail_history_df.drop(columns=['NAICS Code',
                                                    '2013r',
                                                    'Unnamed: 13',
                                                    '2012r',
                                                    'Unnamed: 15',
                                                    '2011r',
                                                    'Unnamed: 17',
                                                    '2010',
                                                     'Unnamed: 19',
                                                     '2009',
                                                     'Unnamed: 21',
                                                     '2008',
                                                     'Unnamed: 23',
                                                     '2007',
                                                     'Unnamed: 25',
                                                     '2006',
                                                     'Unnamed: 27',
                                                     '2005',
                                                     'Unnamed: 29',
                                                     '2004',
                                                     'Unnamed: 31',
                                                     '2003',
                                                     'Unnamed: 33',
                                                     '2002',
                                                     'Unnamed: 35',
                                                     '2001',
                                                     'Unnamed: 37',
                                                     '2000',
                                                     'Unnamed: 39',
                                                     '1999',
                                                     'Unnamed: 41',
                                                    '2004', 
                                                    'Unnamed: 31',
                                                    '2003', 
                                                    'Unnamed: 33',
                                                    '2002',
                                                    'Unnamed: 35',
                                                    '2001',
                                                    'Unnamed: 37',
                                                    '2000',
                                                    'Unnamed: 39',
                                                    '1999',
                                                    'Unnamed: 41',
                                                   
                                                   ])
retail_history_df.head()
570/6:
retail_history_df.head()
# D - Denotes an estimate withheld to avoid disclosing data of individual companies; data are included in higher-level totals.
# S - Estimate does not meet publication standards because of high sampling variability (coefficient of variation is greater than 30%), poor response quality (total quantity response rate is less than 50%), or other concerns about the estimate's quality. Unpublished estimates derived from this table by subtraction are subject to these same limitations and should not be attributed to the U.S. Census Bureau. For a description of publication standards and the total quantity response rate, see https://www.census.gov/about/policies/quality/standards/standardf1.html.
570/7:
# Drop rows that do not have ecommerce data
#[Estimates are shown in millions of dollars and are based on data from the Annual Retail Trade Survey. Estimates have been adjusted using final results of the 2012 Economic Census.] 
retail_history_df = retail_history_df.drop([0, 8])
570/8: retail_history_df = retail_history_df.reset_index(drop=True)
570/9:
retail_history_df = retail_history_df.replace({'S': None,
                                               'D': None
                                              })
retail_history_df
570/10:
US_totalrev_df = pd.read_csv(US_totalrev_2018_2017_csv)
US_totalrev_df.head()
570/11:
# Drop Columns
US_totalrev_df = US_totalrev_df.drop(columns=['Unnamed: 0'])
US_totalrev_df.head()
570/12:
# Rename columns
US_totalrev_df = US_totalrev_df.rename(columns={"2018 E-commerce": "2018 Total ecommerce",
                                                "2017 Revised \nTotal Sales": "2017 Total Sales",
                                                "2017 Revised \nE-commerce": "2017 Total ecommerce",
                                                "Percent Distribution\nof E-commerce Sales 2018": 'Percent Distribution of ecommerce 2018'
                                                 })
US_totalrev_df.head()
570/13:
# Removing rows that do not have ecommerce data
US_totalrev_df = US_totalrev_df.drop([1])
570/14: US_totalrev_df = US_totalrev_df.reset_index(drop=True)
570/15:
US_totalrev_df = US_totalrev_df.replace({'S': None})
US_totalrev_df
570/16:
# Removing rows that do not have ecommerce data
US_totalrev_df = US_totalrev_df.drop([0])
570/17: US_totalrev_df = US_totalrev_df.reset_index(drop=True)
570/18:
US_totalrev_df = US_totalrev_df.replace({'S': None})
US_totalrev_df
570/19:
# Bar Chart
US_totalrev_chart = US_totalrev_df.plot(kind="bar", title="U.S. Retail Trade Sales - Total and E-Commerce 2018 and 2017")
gender_chart.set_xlabel("Gender")
gender_chart.set_ylabel("Number of Trips Taken")

plt.show()
plt.tight_layout()
570/20: US_totalrev_df = US_totalrev_df.set_index("Industry")
570/21:
# Final DataFrame
US_totalrev_df = US_totalrev_df.replace({'S': None})
US_totalrev_df
570/22:
# Bar Chart
US_totalrev_chart = US_totalrev_df.plot(kind="bar", title="U.S. Retail Trade Sales - Total and E-Commerce 2018 and 2017")
gender_chart.set_xlabel("Gender")
gender_chart.set_ylabel("Number of Trips Taken")

plt.show()
plt.tight_layout()
570/23:
# Bar Chart
US_totalrev_chart = US_totalrev_df.plot(x="Industry", y=["2018 Total Sales", "2018 Total ecommerce", "2017 Total Sales"], kind="bar"), title="U.S. Retail Trade Sales - Total and E-Commerce 2018 and 2017")
gender_chart.set_xlabel("Gender")
gender_chart.set_ylabel("Number of Trips Taken")

plt.show()
plt.tight_layout()
570/24:
# Bar Chart
US_totalrev_chart = US_totalrev_df.plot(x="Industry", y=["2018 Total Sales", "2018 Total ecommerce", "2017 Total Sales"], kind="bar", title="U.S. Retail Trade Sales - Total and E-Commerce 2018 and 2017")
gender_chart.set_xlabel("Gender")
gender_chart.set_ylabel("Number of Trips Taken")

plt.show()
plt.tight_layout()
570/25:
# Bar Chart

# US_totalrev_chart = US_totalrev_df.plot(x="Industry", y=["2018 Total Sales", "2018 Total ecommerce", "2017 Total Sales"], kind="bar", title="U.S. Retail Trade Sales - Total and E-Commerce 2018 and 2017")
# gender_chart.set_xlabel("Gender")
# gender_chart.set_ylabel("Number of Trips Taken")

# plt.show()
# plt.tight_layout()
570/26:
total_plot_df = US_totalrev_df.loc['2018 Total Sales', '2018 Total ecommerce', '2017 Total Sales', '2017 Total ecommerce']
total_plot_df
570/27:
total_plot_df = US_totalrev_df.iloc[: , [1, 2, 3, 4]].copy() 
total_plot_df
570/28:
total_plot_df = US_totalrev_df.iloc[: , [0, 1, 2, 3]].copy() 
total_plot_df
570/29:
# Bar Chart

US_totalrev_chart = total_plot_df.plot(kind="bar", title="U.S. Retail Total Sales - In-person and E-Commerce 2018 and 2017")
gender_chart.set_xlabel("Gender")
gender_chart.set_ylabel("Number of Trips Taken")

plt.show()
plt.tight_layout()
574/1: %matplotlib notebook
574/2:
# Dependencies
import pandas as pd
import numpy as np
import os
import webbrowser
import io
import matplotlib
574/3:
# Save file path to variable
retail_history_csv = "Datasets/EstUS_Retail_Trade_E-commercehistory.csv"
US_totalrev_2018_2017_csv = "Datasets/US_Retail_Trade_Sales_2018_2017.csv"
574/4:
# Read with pandas
retail_history_df = pd.read_csv(retail_history_csv)
retail_history_df.head()
574/5:
# Clean up DF
retail_history_df = retail_history_df.rename(columns={"Unnamed: 1": "Kind of Business",
                                 "2018": "2018 Total",
                                 "Unnamed: 3": "2018 Ecommerce Total",
                                 "2017r": "2017 Total",
                                 "Unnamed: 5": "2017 Ecommerce Total",
                                 "2016r": "2016 Total",
                                 "Unnamed: 7": "2016 Ecommerce Total",
                                 "2015r": "2015 Total",
                                 "Unnamed: 9": "2015 Ecommerce Total",
                                 "2014r": "2014 Total",
                                 "Unnamed: 11": "2014 Ecommerce Total",
                                 "1998": "1998 Total",
                                 "Unnamed: 43": "1998 Ecommerce"
                                 })
retail_history_df.head()
574/6:
# Drop columns we do not need
retail_history_df = retail_history_df.drop(columns=['NAICS Code',
                                                    '2013r',
                                                    'Unnamed: 13',
                                                    '2012r',
                                                    'Unnamed: 15',
                                                    '2011r',
                                                    'Unnamed: 17',
                                                    '2010',
                                                     'Unnamed: 19',
                                                     '2009',
                                                     'Unnamed: 21',
                                                     '2008',
                                                     'Unnamed: 23',
                                                     '2007',
                                                     'Unnamed: 25',
                                                     '2006',
                                                     'Unnamed: 27',
                                                     '2005',
                                                     'Unnamed: 29',
                                                     '2004',
                                                     'Unnamed: 31',
                                                     '2003',
                                                     'Unnamed: 33',
                                                     '2002',
                                                     'Unnamed: 35',
                                                     '2001',
                                                     'Unnamed: 37',
                                                     '2000',
                                                     'Unnamed: 39',
                                                     '1999',
                                                     'Unnamed: 41',
                                                    '2004', 
                                                    'Unnamed: 31',
                                                    '2003', 
                                                    'Unnamed: 33',
                                                    '2002',
                                                    'Unnamed: 35',
                                                    '2001',
                                                    'Unnamed: 37',
                                                    '2000',
                                                    'Unnamed: 39',
                                                    '1999',
                                                    'Unnamed: 41',
                                                   
                                                   ])
retail_history_df.head()
574/7:
retail_history_df.head()
# D - Denotes an estimate withheld to avoid disclosing data of individual companies; data are included in higher-level totals.
# S - Estimate does not meet publication standards because of high sampling variability (coefficient of variation is greater than 30%), poor response quality (total quantity response rate is less than 50%), or other concerns about the estimate's quality. Unpublished estimates derived from this table by subtraction are subject to these same limitations and should not be attributed to the U.S. Census Bureau. For a description of publication standards and the total quantity response rate, see https://www.census.gov/about/policies/quality/standards/standardf1.html.
574/8:
# Drop rows that do not have ecommerce data
#[Estimates are shown in millions of dollars and are based on data from the Annual Retail Trade Survey. Estimates have been adjusted using final results of the 2012 Economic Census.] 
retail_history_df = retail_history_df.drop([0, 8])
574/9: retail_history_df = retail_history_df.reset_index(drop=True)
574/10:
# Replaced S & D, final DataFrame
retail_history_df = retail_history_df.replace({'S': None,
                                               'D': None
                                              })
retail_history_df
574/11:
US_totalrev_df = pd.read_csv(US_totalrev_2018_2017_csv)
US_totalrev_df.head()
574/12:
# Drop Columns
US_totalrev_df = US_totalrev_df.drop(columns=['Unnamed: 0'])
US_totalrev_df.head()
574/13:
# Rename columns
US_totalrev_df = US_totalrev_df.rename(columns={"2018 E-commerce": "2018 Total ecommerce",
                                                "2017 Revised \nTotal Sales": "2017 Total Sales",
                                                "2017 Revised \nE-commerce": "2017 Total ecommerce",
                                                "Percent Distribution\nof E-commerce Sales 2018": 'Percent Distribution of ecommerce 2018'
                                                 })
US_totalrev_df.head()
574/14:
# Removing rows that do not have ecommerce data
US_totalrev_df = US_totalrev_df.drop([0])
574/15:
# Reset Index
US_totalrev_df = US_totalrev_df.reset_index(drop=True)
574/16: US_totalrev_df = US_totalrev_df.set_index("Industry")
574/17:
# Final DataFrame
# Need two separate dataframes for plotting
US_totalrev_df = US_totalrev_df.replace({'S': None})
US_totalrev_df
574/18:
# Totals df for plotting
total_plot_df = US_totalrev_df.iloc[: , [0, 1, 2, 3]].copy() 
total_plot_df
574/19:
# Bar Chart
US_totalrev_chart = total_plot_df.plot(kind="bar", title="U.S. Retail Total Sales - In-person and E-Commerce 2018 and 2017")
gender_chart.set_xlabel("Gender")
gender_chart.set_ylabel("Number of Trips Taken")

plt.show()
plt.tight_layout()
574/20: total_plot_df = total_plot_df.drop([0])
575/1: %matplotlib notebook
575/2:
# Dependencies
import pandas as pd
import numpy as np
import os
import webbrowser
import io
import matplotlib
575/3:
# Save file path to variable
retail_history_csv = "Datasets/EstUS_Retail_Trade_E-commercehistory.csv"
US_totalrev_2018_2017_csv = "Datasets/US_Retail_Trade_Sales_2018_2017.csv"
575/4:
# Read with pandas
retail_history_df = pd.read_csv(retail_history_csv)
retail_history_df.head()
575/5:
# Clean up DF
retail_history_df = retail_history_df.rename(columns={"Unnamed: 1": "Kind of Business",
                                 "2018": "2018 Total",
                                 "Unnamed: 3": "2018 Ecommerce Total",
                                 "2017r": "2017 Total",
                                 "Unnamed: 5": "2017 Ecommerce Total",
                                 "2016r": "2016 Total",
                                 "Unnamed: 7": "2016 Ecommerce Total",
                                 "2015r": "2015 Total",
                                 "Unnamed: 9": "2015 Ecommerce Total",
                                 "2014r": "2014 Total",
                                 "Unnamed: 11": "2014 Ecommerce Total",
                                 "1998": "1998 Total",
                                 "Unnamed: 43": "1998 Ecommerce"
                                 })
retail_history_df.head()
575/6:
# Drop columns we do not need
retail_history_df = retail_history_df.drop(columns=['NAICS Code',
                                                    '2013r',
                                                    'Unnamed: 13',
                                                    '2012r',
                                                    'Unnamed: 15',
                                                    '2011r',
                                                    'Unnamed: 17',
                                                    '2010',
                                                     'Unnamed: 19',
                                                     '2009',
                                                     'Unnamed: 21',
                                                     '2008',
                                                     'Unnamed: 23',
                                                     '2007',
                                                     'Unnamed: 25',
                                                     '2006',
                                                     'Unnamed: 27',
                                                     '2005',
                                                     'Unnamed: 29',
                                                     '2004',
                                                     'Unnamed: 31',
                                                     '2003',
                                                     'Unnamed: 33',
                                                     '2002',
                                                     'Unnamed: 35',
                                                     '2001',
                                                     'Unnamed: 37',
                                                     '2000',
                                                     'Unnamed: 39',
                                                     '1999',
                                                     'Unnamed: 41',
                                                    '2004', 
                                                    'Unnamed: 31',
                                                    '2003', 
                                                    'Unnamed: 33',
                                                    '2002',
                                                    'Unnamed: 35',
                                                    '2001',
                                                    'Unnamed: 37',
                                                    '2000',
                                                    'Unnamed: 39',
                                                    '1999',
                                                    'Unnamed: 41',
                                                   
                                                   ])
retail_history_df.head()
575/7:
retail_history_df.head()
# D - Denotes an estimate withheld to avoid disclosing data of individual companies; data are included in higher-level totals.
# S - Estimate does not meet publication standards because of high sampling variability (coefficient of variation is greater than 30%), poor response quality (total quantity response rate is less than 50%), or other concerns about the estimate's quality. Unpublished estimates derived from this table by subtraction are subject to these same limitations and should not be attributed to the U.S. Census Bureau. For a description of publication standards and the total quantity response rate, see https://www.census.gov/about/policies/quality/standards/standardf1.html.
575/8:
# Drop rows that do not have ecommerce data
#[Estimates are shown in millions of dollars and are based on data from the Annual Retail Trade Survey. Estimates have been adjusted using final results of the 2012 Economic Census.] 
retail_history_df = retail_history_df.drop([0, 8])
575/9: retail_history_df = retail_history_df.reset_index(drop=True)
575/10:
# Replaced S & D, final DataFrame
retail_history_df = retail_history_df.replace({'S': None,
                                               'D': None
                                              })
retail_history_df
575/11:
US_totalrev_df = pd.read_csv(US_totalrev_2018_2017_csv)
US_totalrev_df.head()
575/12:
# Drop Columns
US_totalrev_df = US_totalrev_df.drop(columns=['Unnamed: 0'])
US_totalrev_df.head()
575/13:
# Rename columns
US_totalrev_df = US_totalrev_df.rename(columns={"2018 E-commerce": "2018 Total ecommerce",
                                                "2017 Revised \nTotal Sales": "2017 Total Sales",
                                                "2017 Revised \nE-commerce": "2017 Total ecommerce",
                                                "Percent Distribution\nof E-commerce Sales 2018": 'Percent Distribution of ecommerce 2018'
                                                 })
US_totalrev_df.head()
575/14:
# Removing first row
US_totalrev_df = US_totalrev_df.drop([0])
575/15:
# Reset Index
US_totalrev_df = US_totalrev_df.reset_index(drop=True)
575/16: US_totalrev_df = US_totalrev_df.set_index("Industry")
575/17:
# Final DataFrame
# Need two separate dataframes for plotting
US_totalrev_df = US_totalrev_df.replace({'S': None})
US_totalrev_df
575/18:
# Totals df for plotting
total_plot_df = US_totalrev_df.iloc[: , [0, 1, 2, 3]].copy()
575/19:
total_plot_df = total_plot_df.drop([0])
total_plot_df
575/20:
total_plot_df = total_plot_df.drop(["NaN"])
total_plot_df
575/21:
total_plot_df = total_plot_df.drop(['NaN'])
total_plot_df
575/22:
total_plot_df = total_plot_df.drop([1])
total_plot_df
576/1: %matplotlib notebook
576/2:
# Dependencies
import pandas as pd
import numpy as np
import os
import webbrowser
import io
import matplotlib
576/3:
# Save file path to variable
retail_history_csv = "Datasets/EstUS_Retail_Trade_E-commercehistory.csv"
US_totalrev_2018_2017_csv = "Datasets/US_Retail_Trade_Sales_2018_2017.csv"
576/4:
# Read with pandas
retail_history_df = pd.read_csv(retail_history_csv)
retail_history_df.head()
576/5:
# Clean up DF
retail_history_df = retail_history_df.rename(columns={"Unnamed: 1": "Kind of Business",
                                 "2018": "2018 Total",
                                 "Unnamed: 3": "2018 Ecommerce Total",
                                 "2017r": "2017 Total",
                                 "Unnamed: 5": "2017 Ecommerce Total",
                                 "2016r": "2016 Total",
                                 "Unnamed: 7": "2016 Ecommerce Total",
                                 "2015r": "2015 Total",
                                 "Unnamed: 9": "2015 Ecommerce Total",
                                 "2014r": "2014 Total",
                                 "Unnamed: 11": "2014 Ecommerce Total",
                                 "1998": "1998 Total",
                                 "Unnamed: 43": "1998 Ecommerce"
                                 })
retail_history_df.head()
576/6:
# Drop columns we do not need
retail_history_df = retail_history_df.drop(columns=['NAICS Code',
                                                    '2013r',
                                                    'Unnamed: 13',
                                                    '2012r',
                                                    'Unnamed: 15',
                                                    '2011r',
                                                    'Unnamed: 17',
                                                    '2010',
                                                     'Unnamed: 19',
                                                     '2009',
                                                     'Unnamed: 21',
                                                     '2008',
                                                     'Unnamed: 23',
                                                     '2007',
                                                     'Unnamed: 25',
                                                     '2006',
                                                     'Unnamed: 27',
                                                     '2005',
                                                     'Unnamed: 29',
                                                     '2004',
                                                     'Unnamed: 31',
                                                     '2003',
                                                     'Unnamed: 33',
                                                     '2002',
                                                     'Unnamed: 35',
                                                     '2001',
                                                     'Unnamed: 37',
                                                     '2000',
                                                     'Unnamed: 39',
                                                     '1999',
                                                     'Unnamed: 41',
                                                    '2004', 
                                                    'Unnamed: 31',
                                                    '2003', 
                                                    'Unnamed: 33',
                                                    '2002',
                                                    'Unnamed: 35',
                                                    '2001',
                                                    'Unnamed: 37',
                                                    '2000',
                                                    'Unnamed: 39',
                                                    '1999',
                                                    'Unnamed: 41',
                                                   
                                                   ])
retail_history_df.head()
576/7:
retail_history_df.head()
# D - Denotes an estimate withheld to avoid disclosing data of individual companies; data are included in higher-level totals.
# S - Estimate does not meet publication standards because of high sampling variability (coefficient of variation is greater than 30%), poor response quality (total quantity response rate is less than 50%), or other concerns about the estimate's quality. Unpublished estimates derived from this table by subtraction are subject to these same limitations and should not be attributed to the U.S. Census Bureau. For a description of publication standards and the total quantity response rate, see https://www.census.gov/about/policies/quality/standards/standardf1.html.
576/8:
# Drop rows that do not have ecommerce data
#[Estimates are shown in millions of dollars and are based on data from the Annual Retail Trade Survey. Estimates have been adjusted using final results of the 2012 Economic Census.] 
retail_history_df = retail_history_df.drop([0, 8])
576/9: retail_history_df = retail_history_df.reset_index(drop=True)
576/10:
# Replaced S & D, final DataFrame
retail_history_df = retail_history_df.replace({'S': None,
                                               'D': None
                                              })
retail_history_df
576/11:
US_totalrev_df = pd.read_csv(US_totalrev_2018_2017_csv)
US_totalrev_df.head()
576/12:
# Drop Columns
US_totalrev_df = US_totalrev_df.drop(columns=['Unnamed: 0'])
US_totalrev_df.head()
576/13:
# Rename columns
US_totalrev_df = US_totalrev_df.rename(columns={"2018 E-commerce": "2018 Total ecommerce",
                                                "2017 Revised \nTotal Sales": "2017 Total Sales",
                                                "2017 Revised \nE-commerce": "2017 Total ecommerce",
                                                "Percent Distribution\nof E-commerce Sales 2018": 'Percent Distribution of ecommerce 2018'
                                                 })
US_totalrev_df.head()
576/14:
# Removing first row
US_totalrev_df = US_totalrev_df.drop([0])
576/15:
# Reset Index
US_totalrev_df = US_totalrev_df.reset_index(drop=True)
576/16: US_totalrev_df = US_totalrev_df.set_index("Industry")
576/17:
# Final DataFrame
# Need two separate dataframes for plotting
US_totalrev_df = US_totalrev_df.replace({'S': None})
US_totalrev_df
576/18:
# Totals df for plotting
total_plot_df = US_totalrev_df.iloc[: , [0, 1, 2, 3]].copy()
576/19:
total_plot_df = total_plot_df.drop([1])
total_plot_df
576/20:
total_plot_df = total_plot_df.drop([0])
total_plot_df
577/1: %matplotlib notebook
577/2:
# Dependencies
import pandas as pd
import numpy as np
import os
import webbrowser
import io
import matplotlib
577/3:
# Save file path to variable
retail_history_csv = "Datasets/EstUS_Retail_Trade_E-commercehistory.csv"
US_totalrev_2018_2017_csv = "Datasets/US_Retail_Trade_Sales_2018_2017.csv"
577/4:
# Read with pandas
retail_history_df = pd.read_csv(retail_history_csv)
retail_history_df.head()
577/5:
# Clean up DF
retail_history_df = retail_history_df.rename(columns={"Unnamed: 1": "Kind of Business",
                                 "2018": "2018 Total",
                                 "Unnamed: 3": "2018 Ecommerce Total",
                                 "2017r": "2017 Total",
                                 "Unnamed: 5": "2017 Ecommerce Total",
                                 "2016r": "2016 Total",
                                 "Unnamed: 7": "2016 Ecommerce Total",
                                 "2015r": "2015 Total",
                                 "Unnamed: 9": "2015 Ecommerce Total",
                                 "2014r": "2014 Total",
                                 "Unnamed: 11": "2014 Ecommerce Total",
                                 "1998": "1998 Total",
                                 "Unnamed: 43": "1998 Ecommerce"
                                 })
retail_history_df.head()
577/6:
# Drop columns we do not need
retail_history_df = retail_history_df.drop(columns=['NAICS Code',
                                                    '2013r',
                                                    'Unnamed: 13',
                                                    '2012r',
                                                    'Unnamed: 15',
                                                    '2011r',
                                                    'Unnamed: 17',
                                                    '2010',
                                                     'Unnamed: 19',
                                                     '2009',
                                                     'Unnamed: 21',
                                                     '2008',
                                                     'Unnamed: 23',
                                                     '2007',
                                                     'Unnamed: 25',
                                                     '2006',
                                                     'Unnamed: 27',
                                                     '2005',
                                                     'Unnamed: 29',
                                                     '2004',
                                                     'Unnamed: 31',
                                                     '2003',
                                                     'Unnamed: 33',
                                                     '2002',
                                                     'Unnamed: 35',
                                                     '2001',
                                                     'Unnamed: 37',
                                                     '2000',
                                                     'Unnamed: 39',
                                                     '1999',
                                                     'Unnamed: 41',
                                                    '2004', 
                                                    'Unnamed: 31',
                                                    '2003', 
                                                    'Unnamed: 33',
                                                    '2002',
                                                    'Unnamed: 35',
                                                    '2001',
                                                    'Unnamed: 37',
                                                    '2000',
                                                    'Unnamed: 39',
                                                    '1999',
                                                    'Unnamed: 41',
                                                   
                                                   ])
retail_history_df.head()
577/7:
retail_history_df.head()
# D - Denotes an estimate withheld to avoid disclosing data of individual companies; data are included in higher-level totals.
# S - Estimate does not meet publication standards because of high sampling variability (coefficient of variation is greater than 30%), poor response quality (total quantity response rate is less than 50%), or other concerns about the estimate's quality. Unpublished estimates derived from this table by subtraction are subject to these same limitations and should not be attributed to the U.S. Census Bureau. For a description of publication standards and the total quantity response rate, see https://www.census.gov/about/policies/quality/standards/standardf1.html.
577/8:
# Drop rows that do not have ecommerce data
#[Estimates are shown in millions of dollars and are based on data from the Annual Retail Trade Survey. Estimates have been adjusted using final results of the 2012 Economic Census.] 
retail_history_df = retail_history_df.drop([0, 8])
577/9: retail_history_df = retail_history_df.reset_index(drop=True)
577/10:
# Replaced S & D, final DataFrame
retail_history_df = retail_history_df.replace({'S': None,
                                               'D': None
                                              })
retail_history_df
577/11:
US_totalrev_df = pd.read_csv(US_totalrev_2018_2017_csv)
US_totalrev_df.head()
577/12:
# Drop Columns
US_totalrev_df = US_totalrev_df.drop(columns=['Unnamed: 0'])
US_totalrev_df.head()
577/13:
# Rename columns
US_totalrev_df = US_totalrev_df.rename(columns={"2018 E-commerce": "2018 Total ecommerce",
                                                "2017 Revised \nTotal Sales": "2017 Total Sales",
                                                "2017 Revised \nE-commerce": "2017 Total ecommerce",
                                                "Percent Distribution\nof E-commerce Sales 2018": 'Percent Distribution of ecommerce 2018'
                                                 })
US_totalrev_df.head()
577/14:
# Removing first row
US_totalrev_df = US_totalrev_df.drop([0])
577/15:
# Reset Index
US_totalrev_df = US_totalrev_df.reset_index(drop=True)
577/16: US_totalrev_df = US_totalrev_df.set_index("Industry")
577/17:
# Final DataFrame
# Need two separate dataframes for plotting
US_totalrev_df = US_totalrev_df.replace({'S': None})
US_totalrev_df
577/18:
# Totals df for plotting
total_plot_df = US_totalrev_df.iloc[: , [0, 1, 2, 3]].copy()
577/19:
total_plot_df = total_plot_df.drop([0])
total_plot_df
578/1: %matplotlib notebook
578/2:
# Dependencies
import pandas as pd
import numpy as np
import os
import webbrowser
import io
import matplotlib
578/3:
# Save file path to variable
retail_history_csv = "Datasets/EstUS_Retail_Trade_E-commercehistory.csv"
US_totalrev_2018_2017_csv = "Datasets/US_Retail_Trade_Sales_2018_2017.csv"
578/4:
# Read with pandas
retail_history_df = pd.read_csv(retail_history_csv)
retail_history_df.head()
578/5:
# Clean up DF
retail_history_df = retail_history_df.rename(columns={"Unnamed: 1": "Kind of Business",
                                 "2018": "2018 Total",
                                 "Unnamed: 3": "2018 Ecommerce Total",
                                 "2017r": "2017 Total",
                                 "Unnamed: 5": "2017 Ecommerce Total",
                                 "2016r": "2016 Total",
                                 "Unnamed: 7": "2016 Ecommerce Total",
                                 "2015r": "2015 Total",
                                 "Unnamed: 9": "2015 Ecommerce Total",
                                 "2014r": "2014 Total",
                                 "Unnamed: 11": "2014 Ecommerce Total",
                                 "1998": "1998 Total",
                                 "Unnamed: 43": "1998 Ecommerce"
                                 })
retail_history_df.head()
578/6:
# Drop columns we do not need
retail_history_df = retail_history_df.drop(columns=['NAICS Code',
                                                    '2013r',
                                                    'Unnamed: 13',
                                                    '2012r',
                                                    'Unnamed: 15',
                                                    '2011r',
                                                    'Unnamed: 17',
                                                    '2010',
                                                     'Unnamed: 19',
                                                     '2009',
                                                     'Unnamed: 21',
                                                     '2008',
                                                     'Unnamed: 23',
                                                     '2007',
                                                     'Unnamed: 25',
                                                     '2006',
                                                     'Unnamed: 27',
                                                     '2005',
                                                     'Unnamed: 29',
                                                     '2004',
                                                     'Unnamed: 31',
                                                     '2003',
                                                     'Unnamed: 33',
                                                     '2002',
                                                     'Unnamed: 35',
                                                     '2001',
                                                     'Unnamed: 37',
                                                     '2000',
                                                     'Unnamed: 39',
                                                     '1999',
                                                     'Unnamed: 41',
                                                    '2004', 
                                                    'Unnamed: 31',
                                                    '2003', 
                                                    'Unnamed: 33',
                                                    '2002',
                                                    'Unnamed: 35',
                                                    '2001',
                                                    'Unnamed: 37',
                                                    '2000',
                                                    'Unnamed: 39',
                                                    '1999',
                                                    'Unnamed: 41',
                                                   
                                                   ])
retail_history_df.head()
578/7:
retail_history_df.head()
# D - Denotes an estimate withheld to avoid disclosing data of individual companies; data are included in higher-level totals.
# S - Estimate does not meet publication standards because of high sampling variability (coefficient of variation is greater than 30%), poor response quality (total quantity response rate is less than 50%), or other concerns about the estimate's quality. Unpublished estimates derived from this table by subtraction are subject to these same limitations and should not be attributed to the U.S. Census Bureau. For a description of publication standards and the total quantity response rate, see https://www.census.gov/about/policies/quality/standards/standardf1.html.
578/8:
# Drop rows that do not have ecommerce data
#[Estimates are shown in millions of dollars and are based on data from the Annual Retail Trade Survey. Estimates have been adjusted using final results of the 2012 Economic Census.] 
retail_history_df = retail_history_df.drop([0, 8])
578/9: retail_history_df = retail_history_df.reset_index(drop=True)
578/10:
# Replaced S & D, final DataFrame
retail_history_df = retail_history_df.replace({'S': None,
                                               'D': None
                                              })
retail_history_df
578/11:
US_totalrev_df = pd.read_csv(US_totalrev_2018_2017_csv)
US_totalrev_df.head()
578/12:
# Drop Columns
US_totalrev_df = US_totalrev_df.drop(columns=['Unnamed: 0'])
US_totalrev_df.head()
578/13:
# Rename columns
US_totalrev_df = US_totalrev_df.rename(columns={"2018 E-commerce": "2018 Total ecommerce",
                                                "2017 Revised \nTotal Sales": "2017 Total Sales",
                                                "2017 Revised \nE-commerce": "2017 Total ecommerce",
                                                "Percent Distribution\nof E-commerce Sales 2018": 'Percent Distribution of ecommerce 2018'
                                                 })
US_totalrev_df.head()
578/14:
# Removing first row
US_totalrev_df = US_totalrev_df.drop([0, 1])
578/15:
# Reset Index
US_totalrev_df = US_totalrev_df.reset_index(drop=True)
578/16: US_totalrev_df = US_totalrev_df.set_index("Industry")
578/17:
# Final DataFrame
# Need two separate dataframes for plotting
US_totalrev_df = US_totalrev_df.replace({'S': None})
US_totalrev_df
578/18:
# Totals df for plotting
total_plot_df = US_totalrev_df.iloc[: , [0, 1, 2, 3]].copy()
578/19:
# Bar Chart
US_totalrev_chart = total_plot_df.plot(kind="bar", title="U.S. Retail Total Sales - In-person and E-Commerce 2018 and 2017")
gender_chart.set_xlabel("Gender")
gender_chart.set_ylabel("Number of Trips Taken")

plt.show()
plt.tight_layout()
578/20:
# Totals df for plotting
total_plot_df = US_totalrev_df.iloc[: , [0, 1, 2, 3]].copy() 
total_plot_df
578/21: total_plot_df.loc['Total Retail Trade', "2018 Total Sales":"2017 Total ecommerce"].plot(label="United States")
578/22: total_plot_df=total_plot_df.astype(float)
578/23:
# percent change dataframe
percent_plot_df = US_totalrev_df.iloc[: , [4, 5, 6, 7, 8]].copy() 
total_plot_df
578/24:
# percent change dataframe
percent_plot_df = US_totalrev_df.iloc[: , [4, 5, 6, 7, 8]].copy() 
percent_plot_df
578/25:
US_perctrev_chart = percent_plot_df.plot(kind="bar", title="U.S. Retail Total Sales - In-person and E-Commerce 2018 and 2017")
gender_chart.set_xlabel("Gender")
gender_chart.set_ylabel("Number of Trips Taken")

plt.show()
plt.tight_layout()
578/26:
US_perctrev_chart = percent_plot_df.plot(kind="bar", title="U.S. Retail Total Sales - In-person and E-Commerce 2018 and 2017")
gender_chart.set_xlabel("Gender")
gender_chart.set_ylabel("Number of Trips Taken")

plt.show()
plt.tight_layout()
578/27:
US_perctrev_chart = percent_plot_df.plot(kind="bar", title="U.S. Retail Total Sales - In-person and E-Commerce 2018 and 2017")
US_perctrev_chart.set_xlabel("Gender")
US_perctrev_chart.set_ylabel("Number of Trips Taken")

plt.show()
plt.tight_layout()
578/28:
US_perctrev_chart = percent_plot_df.plot(kind="bar", title="U.S. Retail Total Sales - In-person and E-Commerce 2018 and 2017")
US_perctrev_chart.set_xlabel("Gender")
US_perctrev_chart.set_ylabel("Number of Trips Taken")

plot.show()
plot.tight_layout()
578/29:
# Bar Chart
total_plot_df['2018 Total Sales'] = pd.to_numeric(car_data['2018 Total Sales'])
US_totalrev_chart = total_plot_df.plot(kind="bar", title="U.S. Retail Total Sales - In-person and E-Commerce 2018 and 2017")
gender_chart.set_xlabel("Gender")
gender_chart.set_ylabel("Number of Trips Taken")

plt.show()
plt.tight_layout()
578/30:
# Bar Chart
total_plot_df['2018 Total Sales'] = pd.to_numeric(total_plot_df['2018 Total Sales'])
US_totalrev_chart = total_plot_df.plot(kind="bar", title="U.S. Retail Total Sales - In-person and E-Commerce 2018 and 2017")
gender_chart.set_xlabel("Gender")
gender_chart.set_ylabel("Number of Trips Taken")

plt.show()
plt.tight_layout()
581/1:
US_perctrev_chart = percent_plot_df.plot(kind="scatter", title="U.S. Retail Total Sales - In-person and E-Commerce 2018 and 2017")
US_perctrev_chart.set_xlabel("Gender")
US_perctrev_chart.set_ylabel("Number of Trips Taken")

plot.show()
plot.tight_layout()
582/1: %matplotlib notebook
582/2:
# Dependencies
import pandas as pd
import numpy as np
import os
import webbrowser
import io
import matplotlib
582/3:
# Save file path to variable
retail_history_csv = "Datasets/EstUS_Retail_Trade_E-commercehistory.csv"
US_totalrev_2018_2017_csv = "Datasets/US_Retail_Trade_Sales_2018_2017.csv"
582/4:
# Read with pandas
retail_history_df = pd.read_csv(retail_history_csv)
retail_history_df.head()
582/5:
# Clean up DF
retail_history_df = retail_history_df.rename(columns={"Unnamed: 1": "Kind of Business",
                                 "2018": "2018 Total",
                                 "Unnamed: 3": "2018 Ecommerce Total",
                                 "2017r": "2017 Total",
                                 "Unnamed: 5": "2017 Ecommerce Total",
                                 "2016r": "2016 Total",
                                 "Unnamed: 7": "2016 Ecommerce Total",
                                 "2015r": "2015 Total",
                                 "Unnamed: 9": "2015 Ecommerce Total",
                                 "2014r": "2014 Total",
                                 "Unnamed: 11": "2014 Ecommerce Total",
                                 "1998": "1998 Total",
                                 "Unnamed: 43": "1998 Ecommerce"
                                 })
retail_history_df.head()
582/6:
# Drop columns we do not need
retail_history_df = retail_history_df.drop(columns=['NAICS Code',
                                                    '2013r',
                                                    'Unnamed: 13',
                                                    '2012r',
                                                    'Unnamed: 15',
                                                    '2011r',
                                                    'Unnamed: 17',
                                                    '2010',
                                                     'Unnamed: 19',
                                                     '2009',
                                                     'Unnamed: 21',
                                                     '2008',
                                                     'Unnamed: 23',
                                                     '2007',
                                                     'Unnamed: 25',
                                                     '2006',
                                                     'Unnamed: 27',
                                                     '2005',
                                                     'Unnamed: 29',
                                                     '2004',
                                                     'Unnamed: 31',
                                                     '2003',
                                                     'Unnamed: 33',
                                                     '2002',
                                                     'Unnamed: 35',
                                                     '2001',
                                                     'Unnamed: 37',
                                                     '2000',
                                                     'Unnamed: 39',
                                                     '1999',
                                                     'Unnamed: 41',
                                                    '2004', 
                                                    'Unnamed: 31',
                                                    '2003', 
                                                    'Unnamed: 33',
                                                    '2002',
                                                    'Unnamed: 35',
                                                    '2001',
                                                    'Unnamed: 37',
                                                    '2000',
                                                    'Unnamed: 39',
                                                    '1999',
                                                    'Unnamed: 41',
                                                   
                                                   ])
retail_history_df.head()
582/7:
retail_history_df.head()
# D - Denotes an estimate withheld to avoid disclosing data of individual companies; data are included in higher-level totals.
# S - Estimate does not meet publication standards because of high sampling variability (coefficient of variation is greater than 30%), poor response quality (total quantity response rate is less than 50%), or other concerns about the estimate's quality. Unpublished estimates derived from this table by subtraction are subject to these same limitations and should not be attributed to the U.S. Census Bureau. For a description of publication standards and the total quantity response rate, see https://www.census.gov/about/policies/quality/standards/standardf1.html.
582/8:
# Drop rows that do not have ecommerce data
#[Estimates are shown in millions of dollars and are based on data from the Annual Retail Trade Survey. Estimates have been adjusted using final results of the 2012 Economic Census.] 
retail_history_df = retail_history_df.drop([0, 8])
582/9: retail_history_df = retail_history_df.reset_index(drop=True)
582/10:
# Replaced S & D, final DataFrame
retail_history_df = retail_history_df.replace({'S': None,
                                               'D': None
                                              })
retail_history_df
582/11:
US_totalrev_df = pd.read_csv(US_totalrev_2018_2017_csv)
US_totalrev_df.head()
582/12:
# Drop Columns
US_totalrev_df = US_totalrev_df.drop(columns=['Unnamed: 0'])
US_totalrev_df.head()
582/13:
# Rename columns
US_totalrev_df = US_totalrev_df.rename(columns={"2018 E-commerce": "2018 Total ecommerce",
                                                "2017 Revised \nTotal Sales": "2017 Total Sales",
                                                "2017 Revised \nE-commerce": "2017 Total ecommerce",
                                                "Percent Distribution\nof E-commerce Sales 2018": 'Percent Distribution of ecommerce 2018'
                                                 })
US_totalrev_df.head()
582/14:
# Removing first row
US_totalrev_df = US_totalrev_df.drop([0, 1])
582/15:
# Reset Index
US_totalrev_df = US_totalrev_df.reset_index(drop=True)
582/16: US_totalrev_df = US_totalrev_df.set_index("Industry")
582/17:
# Final DataFrame
# Need two separate dataframes for plotting
US_totalrev_df = US_totalrev_df.replace({'S': None})
US_totalrev_df
582/18:
# Totals df for plotting
total_plot_df = US_totalrev_df.iloc[: , [0, 1, 2, 3]].copy() 
total_plot_df
582/19:
# percent change dataframe
percent_plot_df = US_totalrev_df.iloc[: , [4, 5, 6, 7, 8]].copy() 
percent_plot_df
582/20:
US_perctrev_chart = percent_plot_df.plot(kind="scatter", title="U.S. Retail Total Sales - In-person and E-Commerce 2018 and 2017")
US_perctrev_chart.set_xlabel("Gender")
US_perctrev_chart.set_ylabel("Number of Trips Taken")

plot.show()
plot.tight_layout()
582/21:
# US_perctrev_chart = percent_plot_df.plot(kind="line", title="U.S. Retail Total Sales - In-person and E-Commerce 2018 and 2017")
# US_perctrev_chart.set_xlabel("Gender")
# US_perctrev_chart.set_ylabel("Number of Trips Taken")

# plot.show()
# plot.tight_layout()
582/22:
totalrev_plot_df = pd.DataFrame({
    "2018 Total Sales":[5269468, 1191321, 116895, 100205, 381313, 745736, 347454, 503925, 268163, 81179, 706298, 130130, 696849, 611743],
    "2018 Total Ecommerce Sales":[519635, 35348, 1533, 2056, 2795, 4837, 744, 0, 10981, 2617, 0, 0, 453584, 451635],
    "2017 Total Sales":[5053151, 1172367, 113035, 98570, 365622, 725137, 333338, 459463, 260566, 84188, 687123, 124538, 629204, 551298],
    "2017 Total Ecommerce Sales":[458961, 32106, 1279, 2113, 2614, 3431, 0, 0, 9374, 2441, 0, 4014, 400279, 398415]
    }, 
    index=["Total Retail Trade", "Motor vehicle and parts dealers", "Furniture and home furnishing stores", 
           "Electronics and appliance stores", "Building material and garden equipment and supplies dealers",
          "Food and beverage stores", "Health and personal care stores", "Gasoline stations", "Clothing and accessories stores",
           "Sporting goods, hobby, musical instrument, and book stores", "General merchandise stores", "Misc. store retailers",
           "Nonstore retailers", "Electronic shopping and mail-order houses"
          ]
)
plotdata.head()
582/23:
totalrev_plot_df = pd.DataFrame({
    "2018 Total Sales":[5269468, 1191321, 116895, 100205, 381313, 745736, 347454, 503925, 268163, 81179, 706298, 130130, 696849, 611743],
    "2018 Total Ecommerce Sales":[519635, 35348, 1533, 2056, 2795, 4837, 744, 0, 10981, 2617, 0, 0, 453584, 451635],
    "2017 Total Sales":[5053151, 1172367, 113035, 98570, 365622, 725137, 333338, 459463, 260566, 84188, 687123, 124538, 629204, 551298],
    "2017 Total Ecommerce Sales":[458961, 32106, 1279, 2113, 2614, 3431, 0, 0, 9374, 2441, 0, 4014, 400279, 398415]
    }, 
    index=["Total Retail Trade", "Motor vehicle and parts dealers", "Furniture and home furnishing stores", 
           "Electronics and appliance stores", "Building material and garden equipment and supplies dealers",
          "Food and beverage stores", "Health and personal care stores", "Gasoline stations", "Clothing and accessories stores",
           "Sporting goods, hobby, musical instrument, and book stores", "General merchandise stores", "Misc. store retailers",
           "Nonstore retailers", "Electronic shopping and mail-order houses"
          ]
)
totalrev_plot_df.head()
582/24:
# Plot
totalrev_plot_df.plot(kind="bar")
plt.title("Retail Totals - 2018 & 2018")
plt.xlabel("Family Member")
plt.ylabel("Pies Consumed")
582/25:
# Plot
totalrev_plot_df.plot(kind="bar")
plot.title("Retail Totals - 2018 & 2018")
plot.xlabel("Family Member")
plot.ylabel("Pies Consumed")

plt.show()
plt.tight_layout()
582/26:
# Plot
totalrev_plot_df.plot(kind="bar")
plot.title("Retail Totals - 2018 & 2018")
plot.xlabel("Industry")
plot.ylabel("Sales in Millions ($)")

plt.show()
plt.tight_layout()
583/1: %matplotlib notebook
583/2:
# Dependencies
import pandas as pd
import numpy as np
import os
import webbrowser
import io
import matplotlib
583/3:
# Save file path to variable
retail_history_csv = "Datasets/EstUS_Retail_Trade_E-commercehistory.csv"
US_totalrev_2018_2017_csv = "Datasets/US_Retail_Trade_Sales_2018_2017.csv"
583/4:
# Read with pandas
retail_history_df = pd.read_csv(retail_history_csv)
retail_history_df.head()
583/5:
# Clean up DF
retail_history_df = retail_history_df.rename(columns={"Unnamed: 1": "Kind of Business",
                                 "2018": "2018 Total",
                                 "Unnamed: 3": "2018 Ecommerce Total",
                                 "2017r": "2017 Total",
                                 "Unnamed: 5": "2017 Ecommerce Total",
                                 "2016r": "2016 Total",
                                 "Unnamed: 7": "2016 Ecommerce Total",
                                 "2015r": "2015 Total",
                                 "Unnamed: 9": "2015 Ecommerce Total",
                                 "2014r": "2014 Total",
                                 "Unnamed: 11": "2014 Ecommerce Total",
                                 "1998": "1998 Total",
                                 "Unnamed: 43": "1998 Ecommerce"
                                 })
retail_history_df.head()
583/6:
# Drop columns we do not need
retail_history_df = retail_history_df.drop(columns=['NAICS Code',
                                                    '2013r',
                                                    'Unnamed: 13',
                                                    '2012r',
                                                    'Unnamed: 15',
                                                    '2011r',
                                                    'Unnamed: 17',
                                                    '2010',
                                                     'Unnamed: 19',
                                                     '2009',
                                                     'Unnamed: 21',
                                                     '2008',
                                                     'Unnamed: 23',
                                                     '2007',
                                                     'Unnamed: 25',
                                                     '2006',
                                                     'Unnamed: 27',
                                                     '2005',
                                                     'Unnamed: 29',
                                                     '2004',
                                                     'Unnamed: 31',
                                                     '2003',
                                                     'Unnamed: 33',
                                                     '2002',
                                                     'Unnamed: 35',
                                                     '2001',
                                                     'Unnamed: 37',
                                                     '2000',
                                                     'Unnamed: 39',
                                                     '1999',
                                                     'Unnamed: 41',
                                                    '2004', 
                                                    'Unnamed: 31',
                                                    '2003', 
                                                    'Unnamed: 33',
                                                    '2002',
                                                    'Unnamed: 35',
                                                    '2001',
                                                    'Unnamed: 37',
                                                    '2000',
                                                    'Unnamed: 39',
                                                    '1999',
                                                    'Unnamed: 41',
                                                   
                                                   ])
retail_history_df.head()
583/7:
retail_history_df.head()
# D - Denotes an estimate withheld to avoid disclosing data of individual companies; data are included in higher-level totals.
# S - Estimate does not meet publication standards because of high sampling variability (coefficient of variation is greater than 30%), poor response quality (total quantity response rate is less than 50%), or other concerns about the estimate's quality. Unpublished estimates derived from this table by subtraction are subject to these same limitations and should not be attributed to the U.S. Census Bureau. For a description of publication standards and the total quantity response rate, see https://www.census.gov/about/policies/quality/standards/standardf1.html.
583/8:
# Drop rows that do not have ecommerce data
#[Estimates are shown in millions of dollars and are based on data from the Annual Retail Trade Survey. Estimates have been adjusted using final results of the 2012 Economic Census.] 
retail_history_df = retail_history_df.drop([0, 8])
583/9: retail_history_df = retail_history_df.reset_index(drop=True)
583/10:
# Replaced S & D, final DataFrame
retail_history_df = retail_history_df.replace({'S': None,
                                               'D': None
                                              })
retail_history_df
583/11:
US_totalrev_df = pd.read_csv(US_totalrev_2018_2017_csv)
US_totalrev_df.head()
583/12:
# Drop Columns
US_totalrev_df = US_totalrev_df.drop(columns=['Unnamed: 0'])
US_totalrev_df.head()
583/13:
# Rename columns
US_totalrev_df = US_totalrev_df.rename(columns={"2018 E-commerce": "2018 Total ecommerce",
                                                "2017 Revised \nTotal Sales": "2017 Total Sales",
                                                "2017 Revised \nE-commerce": "2017 Total ecommerce",
                                                "Percent Distribution\nof E-commerce Sales 2018": 'Percent Distribution of ecommerce 2018'
                                                 })
US_totalrev_df.head()
583/14:
# Removing first row
US_totalrev_df = US_totalrev_df.drop([0, 1])
583/15:
# Reset Index
US_totalrev_df = US_totalrev_df.reset_index(drop=True)
583/16: US_totalrev_df = US_totalrev_df.set_index("Industry")
583/17:
# Final DataFrame
# Need two separate dataframes for plotting
US_totalrev_df = US_totalrev_df.replace({'S': None})
US_totalrev_df
583/18:
# Totals df for plotting
# Non-store retail = do not have brick and mortar stores
total_plot_df = US_totalrev_df.iloc[: , [0, 1, 2, 3]].copy() 
total_plot_df
583/19:
totalrev_plot_df = pd.DataFrame({
    "2018 Total Sales":[5269468, 1191321, 116895, 100205, 381313, 745736, 347454, 503925, 268163, 81179, 706298, 130130, 696849, 611743],
    "2018 Total Ecommerce Sales":[519635, 35348, 1533, 2056, 2795, 4837, 744, 0, 10981, 2617, 0, 0, 453584, 451635],
    "2017 Total Sales":[5053151, 1172367, 113035, 98570, 365622, 725137, 333338, 459463, 260566, 84188, 687123, 124538, 629204, 551298],
    "2017 Total Ecommerce Sales":[458961, 32106, 1279, 2113, 2614, 3431, 0, 0, 9374, 2441, 0, 4014, 400279, 398415]
    }, 
    index=["Total Retail Trade", "Motor vehicle and parts dealers", "Furniture and home furnishing stores", 
           "Electronics and appliance stores", "Building material and garden equipment and supplies dealers",
          "Food and beverage stores", "Health and personal care stores", "Gasoline stations", "Clothing and accessories stores",
           "Sporting goods, hobby, musical instrument, and book stores", "General merchandise stores", "Misc. store retailers",
           "Nonstore retailers", "Electronic shopping and mail-order houses"
          ]
)
totalrev_plot_df.head()
583/20:
# Plot
totalrev_plot_df.plot(kind="bar")
plot.title("Retail Totals - 2018 & 2018")
plot.xlabel("Industry")
plot.ylabel("Sales in Millions ($)")

plt.show()
plt.tight_layout()
583/21:
# Plot
totalrev_plot_df.plot(kind="bar")
plt.title("Retail Totals - 2018 & 2018")
plt.xlabel("Industry")
plt.ylabel("Sales in Millions ($)")

plt.show()
plt.tight_layout()
583/22:
# Replaced S & D, final DataFrame
retail_history_df = retail_history_df.replace({'S': None,
                                               'D': None,
                                               'NaN': None
                                              })
retail_history_df
583/23:
x_axis = np.arange(len(total_plot_df))
tick_locations = [value+0.4 for value in x_axis]

plt.figure(figsize=(20,3))
plt.bar(x_axis, total_plot_df["Industry"], color='r', alpha=0.5, align="edge")
plt.xticks(tick_locations, rain_df["State"], rotation="vertical")
585/1: %matplotlib notebook
585/2:
# Dependencies
import pandas as pd
import numpy as np
import os
import webbrowser
import io
import matplotlib as plt
585/3:
# Save file path to variable
retail_history_csv = "Datasets/EstUS_Retail_Trade_E-commercehistory.csv"
US_totalrev_2018_2017_csv = "Datasets/US_Retail_Trade_Sales_2018_2017.csv"
585/4:
# Read with pandas
retail_history_df = pd.read_csv(retail_history_csv)
retail_history_df.head()
585/5:
# Clean up DF
retail_history_df = retail_history_df.rename(columns={"Unnamed: 1": "Kind of Business",
                                 "2018": "2018 Total",
                                 "Unnamed: 3": "2018 Ecommerce Total",
                                 "2017r": "2017 Total",
                                 "Unnamed: 5": "2017 Ecommerce Total",
                                 "2016r": "2016 Total",
                                 "Unnamed: 7": "2016 Ecommerce Total",
                                 "2015r": "2015 Total",
                                 "Unnamed: 9": "2015 Ecommerce Total",
                                 "2014r": "2014 Total",
                                 "Unnamed: 11": "2014 Ecommerce Total",
                                 "1998": "1998 Total",
                                 "Unnamed: 43": "1998 Ecommerce"
                                 })
retail_history_df.head()
585/6:
# Drop columns we do not need
retail_history_df = retail_history_df.drop(columns=['NAICS Code',
                                                    '2013r',
                                                    'Unnamed: 13',
                                                    '2012r',
                                                    'Unnamed: 15',
                                                    '2011r',
                                                    'Unnamed: 17',
                                                    '2010',
                                                     'Unnamed: 19',
                                                     '2009',
                                                     'Unnamed: 21',
                                                     '2008',
                                                     'Unnamed: 23',
                                                     '2007',
                                                     'Unnamed: 25',
                                                     '2006',
                                                     'Unnamed: 27',
                                                     '2005',
                                                     'Unnamed: 29',
                                                     '2004',
                                                     'Unnamed: 31',
                                                     '2003',
                                                     'Unnamed: 33',
                                                     '2002',
                                                     'Unnamed: 35',
                                                     '2001',
                                                     'Unnamed: 37',
                                                     '2000',
                                                     'Unnamed: 39',
                                                     '1999',
                                                     'Unnamed: 41',
                                                    '2004', 
                                                    'Unnamed: 31',
                                                    '2003', 
                                                    'Unnamed: 33',
                                                    '2002',
                                                    'Unnamed: 35',
                                                    '2001',
                                                    'Unnamed: 37',
                                                    '2000',
                                                    'Unnamed: 39',
                                                    '1999',
                                                    'Unnamed: 41',
                                                   
                                                   ])
retail_history_df.head()
585/7:
retail_history_df.head()
# D - Denotes an estimate withheld to avoid disclosing data of individual companies; data are included in higher-level totals.
# S - Estimate does not meet publication standards because of high sampling variability (coefficient of variation is greater than 30%), poor response quality (total quantity response rate is less than 50%), or other concerns about the estimate's quality. Unpublished estimates derived from this table by subtraction are subject to these same limitations and should not be attributed to the U.S. Census Bureau. For a description of publication standards and the total quantity response rate, see https://www.census.gov/about/policies/quality/standards/standardf1.html.
585/8:
# Drop rows that do not have ecommerce data
#[Estimates are shown in millions of dollars and are based on data from the Annual Retail Trade Survey. Estimates have been adjusted using final results of the 2012 Economic Census.] 
retail_history_df = retail_history_df.drop([0, 8])
585/9: retail_history_df = retail_history_df.reset_index(drop=True)
585/10:
# Replaced S & D, final DataFrame
retail_history_df = retail_history_df.replace({'S': None,
                                               'D': None,
                                              })
retail_history_df
585/11:
US_totalrev_df = pd.read_csv(US_totalrev_2018_2017_csv)
US_totalrev_df.head()
585/12:
# Drop Columns
US_totalrev_df = US_totalrev_df.drop(columns=['Unnamed: 0'])
US_totalrev_df.head()
585/13:
# Rename columns
US_totalrev_df = US_totalrev_df.rename(columns={"2018 E-commerce": "2018 Total ecommerce",
                                                "2017 Revised \nTotal Sales": "2017 Total Sales",
                                                "2017 Revised \nE-commerce": "2017 Total ecommerce",
                                                "Percent Distribution\nof E-commerce Sales 2018": 'Percent Distribution of ecommerce 2018'
                                                 })
US_totalrev_df.head()
585/14:
# Removing first row
US_totalrev_df = US_totalrev_df.drop([0, 1])
585/15:
# Reset Index
US_totalrev_df = US_totalrev_df.reset_index(drop=True)
585/16: US_totalrev_df = US_totalrev_df.set_index("Industry")
585/17:
# Final DataFrame
# Need two separate dataframes for plotting
US_totalrev_df = US_totalrev_df.replace({'S': None})
US_totalrev_df
585/18:
# Totals df for plotting
# Non-store retail = do not have brick and mortar stores
total_plot_df = US_totalrev_df.iloc[: , [0, 1, 2, 3]].copy() 
total_plot_df
585/19:
# totalrev_plot_df = pd.DataFrame({
#     "2018 Total Sales":[5269468, 1191321, 116895, 100205, 381313, 745736, 347454, 503925, 268163, 81179, 706298, 130130, 696849, 611743],
#     "2018 Total Ecommerce Sales":[519635, 35348, 1533, 2056, 2795, 4837, 744, 0, 10981, 2617, 0, 0, 453584, 451635],
#     "2017 Total Sales":[5053151, 1172367, 113035, 98570, 365622, 725137, 333338, 459463, 260566, 84188, 687123, 124538, 629204, 551298],
#     "2017 Total Ecommerce Sales":[458961, 32106, 1279, 2113, 2614, 3431, 0, 0, 9374, 2441, 0, 4014, 400279, 398415]
#     }, 
#     index=["Total Retail Trade", "Motor vehicle and parts dealers", "Furniture and home furnishing stores", 
#            "Electronics and appliance stores", "Building material and garden equipment and supplies dealers",
#           "Food and beverage stores", "Health and personal care stores", "Gasoline stations", "Clothing and accessories stores",
#            "Sporting goods, hobby, musical instrument, and book stores", "General merchandise stores", "Misc. store retailers",
#            "Nonstore retailers", "Electronic shopping and mail-order houses"
#           ]
# )
# totalrev_plot_df.head()
585/20:
# Plot
totalrev_plot_df.plot(kind="bar")
plt.title("Retail Totals - 2018 & 2018")
plt.xlabel("Industry")
plt.ylabel("Sales in Millions ($)")

plt.show()
plt.tight_layout()
585/21:
# Plot
# totalrev_plot_df.plot(kind="bar")
# plt.title("Retail Totals - 2018 & 2018")
# plt.xlabel("Industry")
# plt.ylabel("Sales in Millions ($)")

# plt.show()
# plt.tight_layout()
585/22:
x_axis = np.arange(len(total_plot_df))
tick_locations = [value+0.4 for value in x_axis]

plt.figure(figsize=(20,3))
plt.bar(x_axis, total_plot_df["Industry"], color='r', alpha=0.5, align="edge")
plt.xticks(tick_locations, rain_df["State"], rotation="vertical")
585/23:
x_axis = np.arange(len(total_plot_df))
tick_locations = [value+0.4 for value in x_axis]


plt.bar(x_axis, total_plot_df["Industry"], color='r', alpha=0.5, align="edge")
plt.xticks(tick_locations, rain_df["State"], rotation="vertical")
585/24:
totalrev_plot_df = pd.DataFrame({
    "2018 Total Sales":[5269468, 1191321, 116895, 100205, 381313, 745736, 347454, 503925, 268163, 81179, 706298, 130130, 696849, 611743],
    "2018 Total Ecommerce Sales":[519635, 35348, 1533, 2056, 2795, 4837, 744, 0, 10981, 2617, 0, 0, 453584, 451635],
    "2017 Total Sales":[5053151, 1172367, 113035, 98570, 365622, 725137, 333338, 459463, 260566, 84188, 687123, 124538, 629204, 551298],
    "2017 Total Ecommerce Sales":[458961, 32106, 1279, 2113, 2614, 3431, 0, 0, 9374, 2441, 0, 4014, 400279, 398415]
    }, 
    index=["Total Retail Trade", "Motor vehicle and parts dealers", "Furniture and home furnishing stores", 
           "Electronics and appliance stores", "Building material and garden equipment and supplies dealers",
          "Food and beverage stores", "Health and personal care stores", "Gasoline stations", "Clothing and accessories stores",
           "Sporting goods, hobby, musical instrument, and book stores", "General merchandise stores", "Misc. store retailers",
           "Nonstore retailers", "Electronic shopping and mail-order houses"
          ]
)
totalrev_plot_df.head()
585/25:
# Plot
totalrev_plot_df.plot(kind="bar")
plt.title("Retail Totals - 2018 & 2018")
plt.xlabel("Industry")
plt.ylabel("Sales in Millions ($)")

plt.show()
plt.tight_layout()
586/1: %matplotlib notebook
586/2:
# Dependencies
import pandas as pd
import numpy as np
import os
import webbrowser
import io
import matplotlib.pyplot as plt
586/3:
# Save file path to variable
retail_history_csv = "Datasets/EstUS_Retail_Trade_E-commercehistory.csv"
US_totalrev_2018_2017_csv = "Datasets/US_Retail_Trade_Sales_2018_2017.csv"
586/4:
# Read with pandas
retail_history_df = pd.read_csv(retail_history_csv)
retail_history_df.head()
586/5:
# Clean up DF
retail_history_df = retail_history_df.rename(columns={"Unnamed: 1": "Kind of Business",
                                 "2018": "2018 Total",
                                 "Unnamed: 3": "2018 Ecommerce Total",
                                 "2017r": "2017 Total",
                                 "Unnamed: 5": "2017 Ecommerce Total",
                                 "2016r": "2016 Total",
                                 "Unnamed: 7": "2016 Ecommerce Total",
                                 "2015r": "2015 Total",
                                 "Unnamed: 9": "2015 Ecommerce Total",
                                 "2014r": "2014 Total",
                                 "Unnamed: 11": "2014 Ecommerce Total",
                                 "1998": "1998 Total",
                                 "Unnamed: 43": "1998 Ecommerce"
                                 })
retail_history_df.head()
586/6:
# Drop columns we do not need
retail_history_df = retail_history_df.drop(columns=['NAICS Code',
                                                    '2013r',
                                                    'Unnamed: 13',
                                                    '2012r',
                                                    'Unnamed: 15',
                                                    '2011r',
                                                    'Unnamed: 17',
                                                    '2010',
                                                     'Unnamed: 19',
                                                     '2009',
                                                     'Unnamed: 21',
                                                     '2008',
                                                     'Unnamed: 23',
                                                     '2007',
                                                     'Unnamed: 25',
                                                     '2006',
                                                     'Unnamed: 27',
                                                     '2005',
                                                     'Unnamed: 29',
                                                     '2004',
                                                     'Unnamed: 31',
                                                     '2003',
                                                     'Unnamed: 33',
                                                     '2002',
                                                     'Unnamed: 35',
                                                     '2001',
                                                     'Unnamed: 37',
                                                     '2000',
                                                     'Unnamed: 39',
                                                     '1999',
                                                     'Unnamed: 41',
                                                    '2004', 
                                                    'Unnamed: 31',
                                                    '2003', 
                                                    'Unnamed: 33',
                                                    '2002',
                                                    'Unnamed: 35',
                                                    '2001',
                                                    'Unnamed: 37',
                                                    '2000',
                                                    'Unnamed: 39',
                                                    '1999',
                                                    'Unnamed: 41',
                                                   
                                                   ])
retail_history_df.head()
586/7:
retail_history_df.head()
# D - Denotes an estimate withheld to avoid disclosing data of individual companies; data are included in higher-level totals.
# S - Estimate does not meet publication standards because of high sampling variability (coefficient of variation is greater than 30%), poor response quality (total quantity response rate is less than 50%), or other concerns about the estimate's quality. Unpublished estimates derived from this table by subtraction are subject to these same limitations and should not be attributed to the U.S. Census Bureau. For a description of publication standards and the total quantity response rate, see https://www.census.gov/about/policies/quality/standards/standardf1.html.
586/8:
# Drop rows that do not have ecommerce data
#[Estimates are shown in millions of dollars and are based on data from the Annual Retail Trade Survey. Estimates have been adjusted using final results of the 2012 Economic Census.] 
retail_history_df = retail_history_df.drop([0, 8])
586/9: retail_history_df = retail_history_df.reset_index(drop=True)
586/10:
# Replaced S & D, final DataFrame
retail_history_df = retail_history_df.replace({'S': None,
                                               'D': None,
                                              })
retail_history_df
586/11:
US_totalrev_df = pd.read_csv(US_totalrev_2018_2017_csv)
US_totalrev_df.head()
586/12:
# Drop Columns
US_totalrev_df = US_totalrev_df.drop(columns=['Unnamed: 0'])
US_totalrev_df.head()
586/13:
# Rename columns
US_totalrev_df = US_totalrev_df.rename(columns={"2018 E-commerce": "2018 Total ecommerce",
                                                "2017 Revised \nTotal Sales": "2017 Total Sales",
                                                "2017 Revised \nE-commerce": "2017 Total ecommerce",
                                                "Percent Distribution\nof E-commerce Sales 2018": 'Percent Distribution of ecommerce 2018'
                                                 })
US_totalrev_df.head()
586/14:
# Removing first row
US_totalrev_df = US_totalrev_df.drop([0, 1])
586/15:
# Reset Index
US_totalrev_df = US_totalrev_df.reset_index(drop=True)
586/16: US_totalrev_df = US_totalrev_df.set_index("Industry")
586/17:
# Final DataFrame
# Need two separate dataframes for plotting
US_totalrev_df = US_totalrev_df.replace({'S': None})
US_totalrev_df
586/18:
# Totals df for plotting
# Non-store retail = do not have brick and mortar stores
total_plot_df = US_totalrev_df.iloc[: , [0, 1, 2, 3]].copy() 
total_plot_df
586/19:
totalrev_plot_df = pd.DataFrame({
    "2018 Total Sales":[5269468, 1191321, 116895, 100205, 381313, 745736, 347454, 503925, 268163, 81179, 706298, 130130, 696849, 611743],
    "2018 Total Ecommerce Sales":[519635, 35348, 1533, 2056, 2795, 4837, 744, 0, 10981, 2617, 0, 0, 453584, 451635],
    "2017 Total Sales":[5053151, 1172367, 113035, 98570, 365622, 725137, 333338, 459463, 260566, 84188, 687123, 124538, 629204, 551298],
    "2017 Total Ecommerce Sales":[458961, 32106, 1279, 2113, 2614, 3431, 0, 0, 9374, 2441, 0, 4014, 400279, 398415]
    }, 
    index=["Total Retail Trade", "Motor vehicle and parts dealers", "Furniture and home furnishing stores", 
           "Electronics and appliance stores", "Building material and garden equipment and supplies dealers",
          "Food and beverage stores", "Health and personal care stores", "Gasoline stations", "Clothing and accessories stores",
           "Sporting goods, hobby, musical instrument, and book stores", "General merchandise stores", "Misc. store retailers",
           "Nonstore retailers", "Electronic shopping and mail-order houses"
          ]
)
totalrev_plot_df.head()
586/20:
# Plot
totalrev_plot_df.plot(kind="bar")
plt.title("Retail Totals - 2018 & 2018")
plt.xlabel("Industry")
plt.ylabel("Sales in Millions ($)")

plt.show()
plt.tight_layout()
586/21:
x_axis = np.arange(len(total_plot_df))
tick_locations = [value+0.4 for value in x_axis]


plt.bar(x_axis, total_plot_df["Industry"], color='r', alpha=0.5, align="edge")
plt.xticks(tick_locations, rain_df["State"], rotation="vertical")
586/22:
# Plot
totalrev_plot_df.plot(kind="bar")
plt.title("Retail Totals - 2018 & 2018")
plt.xlabel("Industry")
plt.ylabel("Sales in Millions ($)")

plt.show()
586/23:
totalrev_plot_df = pd.DataFrame({
    "2018 Total Sales":[1191321, 116895, 100205, 381313, 745736, 347454, 503925, 268163, 81179, 706298, 130130, 696849, 611743],
    "2018 Total Ecommerce Sales":[35348, 1533, 2056, 2795, 4837, 744, 0, 10981, 2617, 0, 0, 453584, 451635],
    "2017 Total Sales":[1172367, 113035, 98570, 365622, 725137, 333338, 459463, 260566, 84188, 687123, 124538, 629204, 551298],
    "2017 Total Ecommerce Sales":[32106, 1279, 2113, 2614, 3431, 0, 0, 9374, 2441, 0, 4014, 400279, 398415]
    }, 
    index=["Motor vehicle and parts dealers", "Furniture and home furnishing stores", 
           "Electronics and appliance stores", "Building material and garden equipment and supplies dealers",
          "Food and beverage stores", "Health and personal care stores", "Gasoline stations", "Clothing and accessories stores",
           "Sporting goods, hobby, musical instrument, and book stores", "General merchandise stores", "Misc. store retailers",
           "Nonstore retailers", "Electronic shopping and mail-order houses"
          ]
)
totalrev_plot_df.head()
586/24:
# Plot
totalrev_plot_df.plot(kind="bar")
plt.title("Retail Totals - 2018 & 2017")
plt.xlabel("Industry")
plt.ylabel("Sales in Millions ($)")

plt.show()
plt.savefig("Retail Totals, 2018 & 2017.png")
586/25:
# Plot
totalrev_plot_df.plot(kind="bar", stacked=True)
plt.title("Retail Totals - 2018 & 2017")
plt.xlabel("Industry")
plt.ylabel("Sales in Millions ($)")

plt.show()
plt.savefig("Retail Totals, 2018 & 2017.png")
586/26:
# Plot
totalrev_plot_df.plot(kind="bar", stacked=True)
plt.title("Retail Totals - 2018 & 2017")
plt.xlabel("Industry")
plt.ylabel("Sales in Millions ($)")

plt.show()
plt.savefig("Retail Totals Stacked, 2018 & 2017.png")
586/27:
# Plot
totalrev_plot_df.plot(kind="barh", stacked=True)
plt.title("Retail Totals - 2018 & 2017")
plt.xlabel("Industry")
plt.ylabel("Sales in Millions ($)")

plt.show()
plt.savefig("Retail Totals Stacked, 2018 & 2017.png")
586/28:
# Plot
totalrev_plot_df.plot(kind="barh", stacked=True)
plt.title("Retail Totals - 2018 & 2017")
plt.xlabel("Industry")
plt.ylabel("Sales in Millions ($)")

plt.show()
plt.tight_figure
plt.savefig("Retail Totals Stacked, 2018 & 2017.png")
586/29:
# Plot
totalrev_plot_df.plot(kind="barh", stacked=True)
plt.title("Retail Totals - 2018 & 2017")
plt.xlabel("Industry")
plt.ylabel("Sales in Millions ($)")

plt.show()
plt.tight_figure()
plt.savefig("Retail Totals Stacked, 2018 & 2017.png")
586/30:
# Plot
totalrev_plot_df.plot(kind="barh", stacked=True)
plt.title("Retail Totals - 2018 & 2017")
plt.xlabel("Industry")
plt.ylabel("Sales in Millions ($)")

plt.show()
plt.tight_layout
plt.savefig("Retail Totals Stacked, 2018 & 2017.png")
586/31:
totalrev_plot_df = pd.DataFrame({
    "2018 Total Sales":[1191321, 116895, 100205, 381313, 745736, 347454, 503925, 268163, 81179, 706298, 130130, 696849, 611743],
    "2018 Total Ecommerce Sales":[35348, 1533, 2056, 2795, 4837, 744, 0, 10981, 2617, 0, 0, 453584, 451635],
    "2017 Total Sales":[1172367, 113035, 98570, 365622, 725137, 333338, 459463, 260566, 84188, 687123, 124538, 629204, 551298],
    "2017 Total Ecommerce Sales":[32106, 1279, 2113, 2614, 3431, 0, 0, 9374, 2441, 0, 4014, 400279, 398415]
    }, 
    index=["Vehicle", "Furniture", 
           "Electronics", "Building material/Garden equip.",
          "Food & beverage", "Health & Personal", "Gasoline stations", "Clothing",
           "Sporting goods, hobby, musical instrument, and book stores", "General merchandise stores", "Misc. store retailers",
           "Nonstore retailers", "E-shopping and mail-order houses"
          ]
)
totalrev_plot_df.head()
586/32:
# Plot
totalrev_plot_df.plot(kind="bar")
plt.title("Retail Totals - 2018 & 2017")
plt.xlabel("Industry")
plt.ylabel("Sales in Millions ($)")

plt.show()
plt.savefig("Retail Totals, 2018 & 2017.png")
586/33:
# Plot
plt.figure(figsize=(20,3))
totalrev_plot_df.plot(kind="bar")
plt.title("Retail Totals - 2018 & 2017")
plt.xlabel("Industry")
plt.ylabel("Sales in Millions ($)")

plt.show()
plt.savefig("Retail Totals, 2018 & 2017.png")
586/34:
# Plot
plt.figure(figsize=(15,3))
totalrev_plot_df.plot(kind="bar")
plt.title("Retail Totals - 2018 & 2017")
plt.xlabel("Industry")
plt.ylabel("Sales in Millions ($)")

plt.show()
plt.savefig("Retail Totals, 2018 & 2017.png")
586/35:
# Plot
plt.figure(figsize=(10,2))
totalrev_plot_df.plot(kind="barh", stacked=True)
plt.title("Retail Totals - 2018 & 2017")
plt.xlabel("Industry")
plt.ylabel("Sales in Millions ($)")

plt.show()
plt.tight_layout()
plt.savefig("Retail Totals Stacked, 2018 & 2017.png")
586/36:
# Plot
plt.figure(figsize=(10,8))
totalrev_plot_df.plot(kind="bar")
plt.title("Retail Totals - 2018 & 2017")
plt.xlabel("Industry")
plt.ylabel("Sales in Millions ($)")

plt.show()
plt.savefig("Retail Totals, 2018 & 2017.png")
586/37:
# Plot
plt.figure(figsize=(10,2))
totalrev_plot_df.plot(kind="barh", stacked=True)
plt.title("Retail Totals - 2018 & 2017")
plt.xlabel("Industry")
plt.ylabel("Sales in Millions ($)")

# Set xticks and lables
xticks(np.arange(0, 1, step=0.2)) 


plt.show()
plt.tight_layout()
plt.savefig("Retail Totals Stacked, 2018 & 2017.png")
586/38:
# Plot
plt.figure(figsize=(10,8))
totalrev_plot_df.plot(kind="bar")
plt.title("Retail Totals - 2018 & 2017")
plt.xlabel("Industry")
plt.ylabel("Sales in Millions ($)")

for index, value in enumerate(y):
    plt.text(value, index, str(value))

plt.show()
plt.savefig("Retail Totals, 2018 & 2017.png")
586/39:
# Plot
plt.figure(figsize=(10,8))
totalrev_plot_df.plot(kind="bar")
plt.title("Retail Totals - 2018 & 2017")
plt.xlabel("Industry")
plt.ylabel("Sales in Millions ($)")

for index, value in enumerate(index):
    plt.text(value, index, str(value))

plt.show()
plt.savefig("Retail Totals, 2018 & 2017.png")
588/1: %matplotlib notebook
588/2:
# Dependencies
import pandas as pd
import numpy as np
import os
import webbrowser
import io
import matplotlib
import matplotlib.pyplot as plt
588/3:
# Save file path to variable
retail_history_csv = "Datasets/EstUS_Retail_Trade_E-commercehistory.csv"
US_totalrev_2018_2017_csv = "Datasets/US_Retail_Trade_Sales_2018_2017.csv"
588/4:
# Read with pandas
retail_history_df = pd.read_csv(retail_history_csv)
retail_history_df.head()
588/5:
# Clean up DF
retail_history_df = retail_history_df.rename(columns={"Unnamed: 1": "Kind of Business",
                                 "2018": "2018 Total",
                                 "Unnamed: 3": "2018 Ecommerce Total",
                                 "2017r": "2017 Total",
                                 "Unnamed: 5": "2017 Ecommerce Total",
                                 "2016r": "2016 Total",
                                 "Unnamed: 7": "2016 Ecommerce Total",
                                 "2015r": "2015 Total",
                                 "Unnamed: 9": "2015 Ecommerce Total",
                                 "2014r": "2014 Total",
                                 "Unnamed: 11": "2014 Ecommerce Total",
                                 "1998": "1998 Total",
                                 "Unnamed: 43": "1998 Ecommerce"
                                 })
retail_history_df.head()
588/6:
# Drop columns we do not need
retail_history_df = retail_history_df.drop(columns=['NAICS Code',
                                                    '2013r',
                                                    'Unnamed: 13',
                                                    '2012r',
                                                    'Unnamed: 15',
                                                    '2011r',
                                                    'Unnamed: 17',
                                                    '2010',
                                                     'Unnamed: 19',
                                                     '2009',
                                                     'Unnamed: 21',
                                                     '2008',
                                                     'Unnamed: 23',
                                                     '2007',
                                                     'Unnamed: 25',
                                                     '2006',
                                                     'Unnamed: 27',
                                                     '2005',
                                                     'Unnamed: 29',
                                                     '2004',
                                                     'Unnamed: 31',
                                                     '2003',
                                                     'Unnamed: 33',
                                                     '2002',
                                                     'Unnamed: 35',
                                                     '2001',
                                                     'Unnamed: 37',
                                                     '2000',
                                                     'Unnamed: 39',
                                                     '1999',
                                                     'Unnamed: 41',
                                                    '2004', 
                                                    'Unnamed: 31',
                                                    '2003', 
                                                    'Unnamed: 33',
                                                    '2002',
                                                    'Unnamed: 35',
                                                    '2001',
                                                    'Unnamed: 37',
                                                    '2000',
                                                    'Unnamed: 39',
                                                    '1999',
                                                    'Unnamed: 41',
                                                   
                                                   ])
retail_history_df.head()
588/7:
retail_history_df.head()
# D - Denotes an estimate withheld to avoid disclosing data of individual companies; data are included in higher-level totals.
# S - Estimate does not meet publication standards because of high sampling variability (coefficient of variation is greater than 30%), poor response quality (total quantity response rate is less than 50%), or other concerns about the estimate's quality. Unpublished estimates derived from this table by subtraction are subject to these same limitations and should not be attributed to the U.S. Census Bureau. For a description of publication standards and the total quantity response rate, see https://www.census.gov/about/policies/quality/standards/standardf1.html.
588/8:
# Drop rows that do not have ecommerce data
#[Estimates are shown in millions of dollars and are based on data from the Annual Retail Trade Survey. Estimates have been adjusted using final results of the 2012 Economic Census.] 
retail_history_df = retail_history_df.drop([0, 8])
588/9: retail_history_df = retail_history_df.reset_index(drop=True)
588/10:
# Replaced S & D, final DataFrame
retail_history_df = retail_history_df.replace({'S': None,
                                               'D': None,
                                              })
retail_history_df
588/11: # Plot
588/12:
US_totalrev_df = pd.read_csv(US_totalrev_2018_2017_csv)
US_totalrev_df.head()
588/13:
# Drop Columns
US_totalrev_df = US_totalrev_df.drop(columns=['Unnamed: 0'])
US_totalrev_df.head()
588/14:
# Rename columns
US_totalrev_df = US_totalrev_df.rename(columns={"2018 E-commerce": "2018 Total ecommerce",
                                                "2017 Revised \nTotal Sales": "2017 Total Sales",
                                                "2017 Revised \nE-commerce": "2017 Total ecommerce",
                                                "Percent Distribution\nof E-commerce Sales 2018": 'Percent Distribution of ecommerce 2018'
                                                 })
US_totalrev_df.head()
588/15:
# Removing first row
US_totalrev_df = US_totalrev_df.drop([0, 1])
588/16:
# Reset Index
US_totalrev_df = US_totalrev_df.reset_index(drop=True)
588/17: US_totalrev_df = US_totalrev_df.set_index("Industry")
588/18:
# Final DataFrame
# Need two separate dataframes for plotting
US_totalrev_df = US_totalrev_df.replace({'S': None})
US_totalrev_df
588/19:
# Totals df for plotting
# Non-store retail = do not have brick and mortar stores
total_plot_df = US_totalrev_df.iloc[: , [0, 1, 2, 3]].copy() 
total_plot_df
588/20:
totalrev_plot_df = pd.DataFrame({
    "2018 Total Sales":[1191321, 116895, 100205, 381313, 745736, 347454, 503925, 268163, 81179, 706298, 130130, 696849, 611743],
    "2018 Total Ecommerce Sales":[35348, 1533, 2056, 2795, 4837, 744, 0, 10981, 2617, 0, 0, 453584, 451635],
    "2017 Total Sales":[1172367, 113035, 98570, 365622, 725137, 333338, 459463, 260566, 84188, 687123, 124538, 629204, 551298],
    "2017 Total Ecommerce Sales":[32106, 1279, 2113, 2614, 3431, 0, 0, 9374, 2441, 0, 4014, 400279, 398415]
    }, 
    index=["Vehicle", "Furniture", 
           "Electronics", "Building material/Garden equip.",
          "Food & beverage", "Health & Personal", "Gasoline stations", "Clothing",
           "Sporting goods, hobby, musical instrument, and book stores", "General merchandise stores", "Misc. store retailers",
           "Nonstore retailers", "E-shopping and mail-order houses"
          ]
)
totalrev_plot_df.head()
588/21:
# Plot
plt.figure(figsize=(10,8))
totalrev_plot_df.plot(kind="bar")
plt.title("Retail Totals - 2018 & 2017")
plt.xlabel("Industry")
plt.ylabel("Sales in Millions ($)")

for index, value in enumerate(index):
    plt.text(value, index, str(value))

plt.show()
plt.savefig("Retail Totals, 2018 & 2017.png")
588/22:
x = ["Vehicle", "Furniture", "Electronics", "Building material/Garden equip.", "Food & beverage", "Health & Personal", "Gasoline stations", 
     "Clothing", "Sporting goods, hobby, musical instrument, and book stores", "General merchandise stores", 
     "Misc. store retailers", "Nonstore retailers", "E-shopping and mail-order houses"]
totalsales2018 = [1191321, 116895, 100205, 381313, 745736, 347454, 503925, 268163, 81179, 706298, 130130, 696849, 611743]
totalecom2018 = [35348, 1533, 2056, 2795, 4837, 744, 0, 10981, 2617, 0, 0, 453584, 451635]
totalsales2017 = [1172367, 113035, 98570, 365622, 725137, 333338, 459463, 260566, 84188, 687123, 124538, 629204, 551298]
totalecom2017 = [32106, 1279, 2113, 2614, 3431, 0, 0, 9374, 2441, 0, 4014, 400279, 398415]
x_pos = [i for i, _ in enumerate(x)]

fig, ax = plt.subplots()
rects1 = ax.bar(x_pos, totalsales2018, totalecom2018, totalsales2017, totalecom2017, color='b')
plt.xlabel("Product Category")
plt.ylabel("Total Sales ($ in Millions)")
plt.title("Total Sales & Ecommerce Sales by Product Category" + "United States, 2018 and 2017")
plt.xticks(x_pos, x)
# Turn on the grid
plt.minorticks_on()
plt.grid(which='major', linestyle='-', linewidth='0.5', color='red')
# Customize the minor grid
plt.grid(which='minor', linestyle=':', linewidth='0.5', color='black')
def autolabel(rects):
588/23:
x = ["Vehicle", "Furniture", "Electronics", "Building material/Garden equip.", "Food & beverage", "Health & Personal", "Gasoline stations", 
     "Clothing", "Sporting goods, hobby, musical instrument, and book stores", "General merchandise stores", 
     "Misc. store retailers", "Nonstore retailers", "E-shopping and mail-order houses"]
totalsales2018 = [1191321, 116895, 100205, 381313, 745736, 347454, 503925, 268163, 81179, 706298, 130130, 696849, 611743]
totalecom2018 = [35348, 1533, 2056, 2795, 4837, 744, 0, 10981, 2617, 0, 0, 453584, 451635]
totalsales2017 = [1172367, 113035, 98570, 365622, 725137, 333338, 459463, 260566, 84188, 687123, 124538, 629204, 551298]
totalecom2017 = [32106, 1279, 2113, 2614, 3431, 0, 0, 9374, 2441, 0, 4014, 400279, 398415]
x_pos = [i for i, _ in enumerate(x)]

fig, ax = plt.subplots()
rects1 = ax.bar(x_pos, totalsales2018, totalecom2018, totalsales2017, totalecom2017, color='b')
plt.xlabel("Product Category")
plt.ylabel("Total Sales ($ in Millions)")
plt.title("Total Sales & Ecommerce Sales by Product Category" + "United States, 2018 and 2017")
plt.xticks(x_pos, x)
# Turn on the grid
plt.minorticks_on()
plt.grid(which='major', linestyle='-', linewidth='0.5', color='red')
# Customize the minor grid
plt.grid(which='minor', linestyle=':', linewidth='0.5', color='black')
def autolabel(rects):
    
    """
    Attach a text label above each bar displaying its height
    """
    for rect in rects:
        height = rect.get_height()
        ax.text(rect.get_x() + rect.get_width()/2., 1.05*height,
                '%f' % float(height),
        ha='center', va='bottom')
autolabel(rects1)

plt.show()
588/24:
totalrev_plot_df = pd.DataFrame({
    "2018 Total Sales":[1191321, 116895, 100205, 381313, 745736, 347454, 503925, 268163, 81179, 706298, 130130, 696849, 611743],
    "2018 Total Ecommerce Sales":[35348, 1533, 2056, 2795, 4837, 744, 0, 10981, 2617, 0, 0, 453584, 451635],
    "2017 Total Sales":[1172367, 113035, 98570, 365622, 725137, 333338, 459463, 260566, 84188, 687123, 124538, 629204, 551298],
    "2017 Total Ecommerce Sales":[32106, 1279, 2113, 2614, 3431, 0, 0, 9374, 2441, 0, 4014, 400279, 398415]
    }, 
    index=["Vehicle", "Furniture", 
           "Electronics", "Building material/Garden equip.",
          "Food & beverage", "Health & Personal", "Gasoline stations", "Clothing",
           "Sporting goods, hobby, musical instrument, and book stores", "General merchandise stores", "Misc. store retailers",
           "Nonstore retailers", "E-shopping and mail-order houses"
          ]
)
totalrev_plot_df.head()
588/25:
# Plot
plt.figure(figsize=(10,8))
totalrev_plot_df.plot(kind="bar")
plt.title("Retail Totals - 2018 & 2017")
plt.xlabel("Industry")
plt.ylabel("Sales in Millions ($)")

labels = []

plt.show()
plt.savefig("Retail Totals, 2018 & 2017.png")
588/26:
# Plot
plt.figure(figsize=(10,2))
totalrev_plot_df.plot(kind="barh", stacked=True)
plt.title("Retail Totals - 2018 & 2017")
plt.xlabel("Industry")
plt.ylabel("Sales in Millions ($)")

# Set xticks and lables
xticks = 
xticks(np.arange(0, 1, step=0.2)) 


plt.show()
plt.tight_layout()
plt.savefig("Retail Totals Stacked, 2018 & 2017.png")
588/27:
totalrev_plot_df.plot.bar(rot=0, subplots=True, color={"Total Sales 2018": "red", "Total Ecommerce Sales 2018": "lightred", 
                                                       "Total Sales 2017": "green", "Total Ecommerce Sales 2017": "lightgreen"})
589/1: %matplotlib notebook
589/2:
# Dependencies
import pandas as pd
import numpy as np
import os
import webbrowser
import io
import matplotlib
import matplotlib.pyplot as plt
589/3:
# Save file path to variable
retail_history_csv = "Datasets/EstUS_Retail_Trade_E-commercehistory.csv"
US_totalrev_2018_2017_csv = "Datasets/US_Retail_Trade_Sales_2018_2017.csv"
589/4:
# Read with pandas
retail_history_df = pd.read_csv(retail_history_csv)
retail_history_df.head()
589/5:
# Clean up DF
retail_history_df = retail_history_df.rename(columns={"Unnamed: 1": "Kind of Business",
                                 "2018": "2018 Total",
                                 "Unnamed: 3": "2018 Ecommerce Total",
                                 "2017r": "2017 Total",
                                 "Unnamed: 5": "2017 Ecommerce Total",
                                 "2016r": "2016 Total",
                                 "Unnamed: 7": "2016 Ecommerce Total",
                                 "2015r": "2015 Total",
                                 "Unnamed: 9": "2015 Ecommerce Total",
                                 "2014r": "2014 Total",
                                 "Unnamed: 11": "2014 Ecommerce Total",
                                 "1998": "1998 Total",
                                 "Unnamed: 43": "1998 Ecommerce"
                                 })
retail_history_df.head()
589/6:
# Drop columns we do not need
retail_history_df = retail_history_df.drop(columns=['NAICS Code',
                                                    '2013r',
                                                    'Unnamed: 13',
                                                    '2012r',
                                                    'Unnamed: 15',
                                                    '2011r',
                                                    'Unnamed: 17',
                                                    '2010',
                                                     'Unnamed: 19',
                                                     '2009',
                                                     'Unnamed: 21',
                                                     '2008',
                                                     'Unnamed: 23',
                                                     '2007',
                                                     'Unnamed: 25',
                                                     '2006',
                                                     'Unnamed: 27',
                                                     '2005',
                                                     'Unnamed: 29',
                                                     '2004',
                                                     'Unnamed: 31',
                                                     '2003',
                                                     'Unnamed: 33',
                                                     '2002',
                                                     'Unnamed: 35',
                                                     '2001',
                                                     'Unnamed: 37',
                                                     '2000',
                                                     'Unnamed: 39',
                                                     '1999',
                                                     'Unnamed: 41',
                                                    '2004', 
                                                    'Unnamed: 31',
                                                    '2003', 
                                                    'Unnamed: 33',
                                                    '2002',
                                                    'Unnamed: 35',
                                                    '2001',
                                                    'Unnamed: 37',
                                                    '2000',
                                                    'Unnamed: 39',
                                                    '1999',
                                                    'Unnamed: 41',
                                                   
                                                   ])
retail_history_df.head()
589/7:
retail_history_df.head()
# D - Denotes an estimate withheld to avoid disclosing data of individual companies; data are included in higher-level totals.
# S - Estimate does not meet publication standards because of high sampling variability (coefficient of variation is greater than 30%), poor response quality (total quantity response rate is less than 50%), or other concerns about the estimate's quality. Unpublished estimates derived from this table by subtraction are subject to these same limitations and should not be attributed to the U.S. Census Bureau. For a description of publication standards and the total quantity response rate, see https://www.census.gov/about/policies/quality/standards/standardf1.html.
589/8:
# Drop rows that do not have ecommerce data
#[Estimates are shown in millions of dollars and are based on data from the Annual Retail Trade Survey. Estimates have been adjusted using final results of the 2012 Economic Census.] 
retail_history_df = retail_history_df.drop([0, 8])
589/9: retail_history_df = retail_history_df.reset_index(drop=True)
589/10:
# Replaced S & D, final DataFrame
retail_history_df = retail_history_df.replace({'S': None,
                                               'D': None,
                                              })
retail_history_df
589/11: # Plot
589/12:
US_totalrev_df = pd.read_csv(US_totalrev_2018_2017_csv)
US_totalrev_df.head()
589/13:
# Drop Columns
US_totalrev_df = US_totalrev_df.drop(columns=['Unnamed: 0'])
US_totalrev_df.head()
589/14:
# Rename columns
US_totalrev_df = US_totalrev_df.rename(columns={"2018 E-commerce": "2018 Total ecommerce",
                                                "2017 Revised \nTotal Sales": "2017 Total Sales",
                                                "2017 Revised \nE-commerce": "2017 Total ecommerce",
                                                "Percent Distribution\nof E-commerce Sales 2018": 'Percent Distribution of ecommerce 2018'
                                                 })
US_totalrev_df.head()
589/15:
# Removing first row
US_totalrev_df = US_totalrev_df.drop([0, 1])
589/16:
# Reset Index
US_totalrev_df = US_totalrev_df.reset_index(drop=True)
589/17: US_totalrev_df = US_totalrev_df.set_index("Industry")
589/18:
# Final DataFrame
# Need two separate dataframes for plotting
US_totalrev_df = US_totalrev_df.replace({'S': None})
US_totalrev_df
589/19:
# Totals df for plotting
# Non-store retail = do not have brick and mortar stores
total_plot_df = US_totalrev_df.iloc[: , [0, 1, 2, 3]].copy() 
total_plot_df
589/20:
totalrev_plot_df = pd.DataFrame({
    "2018 Total Sales":[1191321, 116895, 100205, 381313, 745736, 347454, 503925, 268163, 81179, 706298, 130130, 696849, 611743],
    "2018 Total Ecommerce Sales":[35348, 1533, 2056, 2795, 4837, 744, 0, 10981, 2617, 0, 0, 453584, 451635],
    "2017 Total Sales":[1172367, 113035, 98570, 365622, 725137, 333338, 459463, 260566, 84188, 687123, 124538, 629204, 551298],
    "2017 Total Ecommerce Sales":[32106, 1279, 2113, 2614, 3431, 0, 0, 9374, 2441, 0, 4014, 400279, 398415]
    }, 
    index=["Vehicle", "Furniture", 
           "Electronics", "Building material/Garden equip.",
          "Food & beverage", "Health & Personal", "Gasoline stations", "Clothing",
           "Sporting goods, hobby, musical instrument, and book stores", "General merchandise stores", "Misc. store retailers",
           "Nonstore retailers", "E-shopping and mail-order houses"
          ]
)
totalrev_plot_df.head()
589/21:
totalsales2018 = [1191321, 116895, 100205, 381313, 745736, 347454, 503925, 268163, 81179, 706298, 130130, 696849, 611743]
totalecom2018 = [35348, 1533, 2056, 2795, 4837, 744, 0, 10981, 2617, 0, 0, 453584, 451635]
totalsales2017 = [1172367, 113035, 98570, 365622, 725137, 333338, 459463, 260566, 84188, 687123, 124538, 629204, 551298]
totalecom2017 = [32106, 1279, 2113, 2614, 3431, 0, 0, 9374, 2441, 0, 4014, 400279, 398415]
index = ["Vehicle", "Furniture", "Electronics", "Building material/Garden equip.",
         "Food & beverage", "Health & Personal", "Gasoline stations", "Clothing", 
         "Sporting goods, hobby, musical instrument, and book stores", "General merchandise stores", 
         "Misc. store retailers", "Nonstore retailers", "E-shopping and mail-order houses"]
totalrev_plot_df = pd.DataFrame({'Total Sales 2018': totalsales2018,
                                 'Total Ecommerce Sales 2018': totalecom2018,
                                 'Total Sales 2017': totalsales2017,
                                 'Total Ecommerce Sales 2017': totalecom2017
                                }, index=index)
589/22:
totalrev_plot_df.plot.bar(rot=0, subplots=True, color={"Total Sales 2018": "red", "Total Ecommerce Sales 2018": "lightred", 
                                                       "Total Sales 2017": "green", "Total Ecommerce Sales 2017": "lightgreen"})
589/23:
axes = totalrev_plot_df.plot.bar(rot=0, subplots=True, color={"Total Sales 2018": "red", "Total Ecommerce Sales 2018": "pink", 
                                                       "Total Sales 2017": "green", "Total Ecommerce Sales 2017": "blue"})
axes[1].legend(loc=2)
589/24:
axes = totalrev_plot_df.plot.bar(rot=0, subplots=True, color={"Total Sales 2018": "red", "Total Ecommerce Sales 2018": "pink", 
                                                       "Total Sales 2017": "green", "Total Ecommerce Sales 2017": "blue"})
axes[1].legend(loc=2)
plt.tight_layout()
589/25:
axes = totalrev_plot_df.plot.bar(rot=0, subplots=True, color={"Total Sales 2018": "red", "Total Ecommerce Sales 2018": "pink", 
                                                       "Total Sales 2017": "green", "Total Ecommerce Sales 2017": "blue"}, )
axes[1].legend(loc=2)
axes[1].xticks(index)
plt.tight_layout()
589/26:
axes = totalrev_plot_df.plot.bar(rot=0, subplots=True, color={"Total Sales 2018": "red", "Total Ecommerce Sales 2018": "pink", 
                                                       "Total Sales 2017": "green", "Total Ecommerce Sales 2017": "blue"}, )
axes[1].legend(loc=2)


plt.tight_layout()
589/27:
# Plot
plt.figure(figsize=(10,8))
totalrev_plot_df.plot(kind="bar")
plt.title("Retail Totals - 2018 & 2017")
plt.xlabel("Industry")
plt.ylabel("Sales in Millions ($)")

labels = []

plt.show()
plt.savefig("Retail Totals, 2018 & 2017.png")
589/28:
# Plot
plt.figure(figsize=(10,2))
totalrev_plot_df.plot.bar(kind="barh", stacked=True)
plt.title("Retail Totals - 2018 & 2017")
plt.xlabel("Industry")
plt.ylabel("Sales in Millions ($)")

# Set xticks and lables
xticks = 
xticks(np.arange(0, 1, step=0.2)) 


plt.show()
plt.tight_layout()
plt.savefig("Retail Totals Stacked, 2018 & 2017.png")
589/29:
# Plot
plt.figure(figsize=(10,8))
totalrev_plot_df.plot(kind="bar", stacked=True)
plt.title("Retail Totals - 2018 & 2017")
plt.xlabel("Industry")
plt.ylabel("Sales in Millions ($)")

labels = []

plt.show()
plt.savefig("Retail Totals, 2018 & 2017.png")
589/30:
# Plot
plt.figure(figsize=(10,2))
totalrev_plot_df.plot.bar(kind="barh", stacked=True)
plt.title("Retail Totals - 2018 & 2017")
plt.xlabel("Industry")
plt.ylabel("Sales in Millions ($)")

# Set xticks and lables
xticks = 
xticks(np.arange(0, 1, step=0.2)) 


plt.show()
plt.tight_layout()
plt.savefig("Retail Totals Stacked, 2018 & 2017.png")
589/31:
axes = totalrev_plot_df.plot.bar(rot=1, subplots=True, color={"Total Sales 2018": "red", "Total Ecommerce Sales 2018": "pink", 
                                                       "Total Sales 2017": "green", "Total Ecommerce Sales 2017": "blue"}, )
axes[1].legend(loc=2)
plt.tight_layout()
589/32:
axes = totalrev_plot_df.plot.bar(rot=1, subplots=True, color={"Total Sales 2018": "red", "Total Ecommerce Sales 2018": "pink", 
                                                       "Total Sales 2017": "green", "Total Ecommerce Sales 2017": "blue"}, labels=index)
axes[1].legend(loc=2)
plt.tight_layout()
589/33:
axes = totalrev_plot_df.plot.bar(rot=1, subplots=True, color={"Total Sales 2018": "red", "Total Ecommerce Sales 2018": "pink", 
                                                       "Total Sales 2017": "green", "Total Ecommerce Sales 2017": "blue")
axes[1].legend(loc=2)
plt.tight_layout()
589/34:
axes = totalrev_plot_df.plot.bar(rot=1, subplots=True, color={"Total Sales 2018": "red", "Total Ecommerce Sales 2018": "pink", 
                                                       "Total Sales 2017": "green", "Total Ecommerce Sales 2017": "blue"})
axes[1].legend(loc=2)
plt.tight_layout()
589/35:
axes = totalrev_plot_df.plot.bar(rot=1, subplots=True, color={"Total Sales 2018": "red", "Total Ecommerce Sales 2018": "pink", 
                                                       "Total Sales 2017": "green", "Total Ecommerce Sales 2017": "blue"})
axes[1].legend(loc=2)
axes[1].xlabel(index)
plt.tight_layout()
589/36:
historyecom_plot_df = retail_history_df.iloc[: , [2, 4, 6, 8, 10, 12]].copy() 
historyecom_plot_df.head()
589/37:
historytotal_plot_df = retail_history_df.iloc[: , [1, 3, 5, 7, 9, 11]].copy() 
historytotal_plot_df.head()
589/38:
historyecom_plot_df = retail_history_df.iloc[: , [0, 2, 4, 6, 8, 10, 12]].copy() 
historyecom_plot_df.head()
589/39:
historytotal_plot_df = retail_history_df.iloc[: , [0, 1, 3, 5, 7, 9, 11]].copy() 
historytotal_plot_df.head()
589/40:
retail_history_df = retail_history_df.set_index("Kind of Business")
retail_history_df
589/41:
historyecom_plot_df = retail_history_df.iloc[: , [0, 2, 4, 6, 8, 10, 12]].copy() 
historyecom_plot_df.head()
589/42:
historyecom_plot_df = retail_history_df.iloc[: , [0, 2, 4, 6, 8, 10, 12]].copy() 
historyecom_plot_df.head()
590/1: %matplotlib notebook
590/2:
# Dependencies
import pandas as pd
import numpy as np
import os
import webbrowser
import io
import matplotlib
import matplotlib.pyplot as plt
590/3:
# Save file path to variable
retail_history_csv = "Datasets/EstUS_Retail_Trade_E-commercehistory.csv"
US_totalrev_2018_2017_csv = "Datasets/US_Retail_Trade_Sales_2018_2017.csv"
590/4:
# Read with pandas
retail_history_df = pd.read_csv(retail_history_csv)
retail_history_df.head()
590/5:
# Clean up DF
retail_history_df = retail_history_df.rename(columns={"Unnamed: 1": "Kind of Business",
                                 "2018": "2018 Total",
                                 "Unnamed: 3": "2018 Ecommerce Total",
                                 "2017r": "2017 Total",
                                 "Unnamed: 5": "2017 Ecommerce Total",
                                 "2016r": "2016 Total",
                                 "Unnamed: 7": "2016 Ecommerce Total",
                                 "2015r": "2015 Total",
                                 "Unnamed: 9": "2015 Ecommerce Total",
                                 "2014r": "2014 Total",
                                 "Unnamed: 11": "2014 Ecommerce Total",
                                 "1998": "1998 Total",
                                 "Unnamed: 43": "1998 Ecommerce"
                                 })
retail_history_df.head()
590/6:
# Drop columns we do not need
retail_history_df = retail_history_df.drop(columns=['NAICS Code',
                                                    '2013r',
                                                    'Unnamed: 13',
                                                    '2012r',
                                                    'Unnamed: 15',
                                                    '2011r',
                                                    'Unnamed: 17',
                                                    '2010',
                                                     'Unnamed: 19',
                                                     '2009',
                                                     'Unnamed: 21',
                                                     '2008',
                                                     'Unnamed: 23',
                                                     '2007',
                                                     'Unnamed: 25',
                                                     '2006',
                                                     'Unnamed: 27',
                                                     '2005',
                                                     'Unnamed: 29',
                                                     '2004',
                                                     'Unnamed: 31',
                                                     '2003',
                                                     'Unnamed: 33',
                                                     '2002',
                                                     'Unnamed: 35',
                                                     '2001',
                                                     'Unnamed: 37',
                                                     '2000',
                                                     'Unnamed: 39',
                                                     '1999',
                                                     'Unnamed: 41',
                                                    '2004', 
                                                    'Unnamed: 31',
                                                    '2003', 
                                                    'Unnamed: 33',
                                                    '2002',
                                                    'Unnamed: 35',
                                                    '2001',
                                                    'Unnamed: 37',
                                                    '2000',
                                                    'Unnamed: 39',
                                                    '1999',
                                                    'Unnamed: 41',
                                                   
                                                   ])
retail_history_df.head()
590/7:
retail_history_df.head()
# D - Denotes an estimate withheld to avoid disclosing data of individual companies; data are included in higher-level totals.
# S - Estimate does not meet publication standards because of high sampling variability (coefficient of variation is greater than 30%), poor response quality (total quantity response rate is less than 50%), or other concerns about the estimate's quality. Unpublished estimates derived from this table by subtraction are subject to these same limitations and should not be attributed to the U.S. Census Bureau. For a description of publication standards and the total quantity response rate, see https://www.census.gov/about/policies/quality/standards/standardf1.html.
590/8:
# Drop rows that do not have ecommerce data
#[Estimates are shown in millions of dollars and are based on data from the Annual Retail Trade Survey. Estimates have been adjusted using final results of the 2012 Economic Census.] 
retail_history_df = retail_history_df.drop([0, 8])
590/9: retail_history_df = retail_history_df.reset_index(drop=True)
590/10:
# Replaced S & D, final DataFrame
retail_history_df = retail_history_df.replace({'S': None,
                                               'D': None,
                                              })
retail_history_df
590/11:
historyecom_plot_df = retail_history_df.iloc[: , [0, 2, 4, 6, 8, 10, 12]].copy() 
historyecom_plot_df.head()
590/12:
historytotal_plot_df = retail_history_df.iloc[: , [0, 1, 3, 5, 7, 9, 11]].copy() 
historytotal_plot_df.head()
590/13:
# Plot
year = [2018, 2017, 2016, 2015, 2014, 1998]
population_by_continent = {
    'africa': [228, 284, 365, 477, 631, 814, 1044, 1275],
    'americas': [340, 425, 519, 619, 727, 840, 943, 1006],
    'asia': [1394, 1686, 2120, 2625, 3202, 3714, 4169, 4560],
    'europe': [220, 253, 276, 295, 310, 303, 294, 293],
    'oceania': [12, 15, 19, 22, 26, 31, 36, 39],
}

fig, ax = plt.subplots()
ax.stackplot(year, population_by_continent.values(),
             labels=population_by_continent.keys())
ax.legend(loc='upper left')
ax.set_title('World population')
ax.set_xlabel('Year')
ax.set_ylabel('Number of people (millions)')

plt.show()
590/14:
historytotal_plot_df = retail_history_df.iloc[: , [0, 1, 3, 5, 7, 9, 11]].copy() 
historytotal_plot_df
590/15:
historytotal_plot_df = historytotal_plot_df.set_index("Kind of Business")
historytotal_plot_df
590/16:
# Plot
year = [2018, 2017, 2016, 2015, 2014, 1998]
totalrev_byproductcategory = {
    'Total Retail Trade': [5269468, 5053151, 4852958, 4725993, 4639440, 2581762],
    'Motor Vehicle & Parts': [1191321, 1172367, 1140614, 1094112, 1020851, 688415],
    'Furniture & Home Stores': [116895, 113035, 110404, 106570, 99718, 77412],
    'Electronics & Appliances': [100205, 98570, 99043, 103658, 105197, 82731],
    'Building Materials & Garden Equip': [381313, 365622, 348697, 331611, 318352, 202423],
    'Food & Beverage': [745736, 725137, 699349, 685381, 669165, 416525],
    'Health & Personal Care': [347454, 333338, 327153, 315244, 299263, 129583],
    'Clothing & Accessories': [268163, 260566, 260050, 255798, 250409, 149151],
    'Sporting Goods, Hobby, Musical Inst., & Books': [81179, 84188, 86483, 85701, 83787, 60441],
    'General Merchandise': [706298, 687123, 675389, 674889, 667163, 351081],
    'Misc. Store Retailers': [130130, 124538, 121572, 119350, 115878, 98369],
    'Nonstore Retailers': [696849, 629204, 561412, 509652, 470867, 133904],
    'Electronic Shopping & Mail-order Houses': [611743, 551298, 488840, 433987, 386065, 80297],
}

fig, ax = plt.subplots()
ax.stackplot(year, totalrev_byproductcategory.values(),
             labels=totalrev_byproductcategory.keys())
ax.legend(loc='upper right')
ax.set_title('Total Revenue by Product Category')
ax.set_xlabel('Year')
ax.set_ylabel('Total Sales (millions)')

plt.show()
590/17:
# Plot
year = [2018, 2017, 2016, 2015, 2014, 1998]
totalrev_byproductcategory = {
    'Total Retail Trade': [5269468, 5053151, 4852958, 4725993, 4639440, 2581762],
    'Motor Vehicle & Parts': [1191321, 1172367, 1140614, 1094112, 1020851, 688415],
    'Furniture & Home Stores': [116895, 113035, 110404, 106570, 99718, 77412],
    'Electronics & Appliances': [100205, 98570, 99043, 103658, 105197, 82731],
    'Building Materials & Garden Equip': [381313, 365622, 348697, 331611, 318352, 202423],
    'Food & Beverage': [745736, 725137, 699349, 685381, 669165, 416525],
    'Health & Personal Care': [347454, 333338, 327153, 315244, 299263, 129583],
    'Clothing & Accessories': [268163, 260566, 260050, 255798, 250409, 149151],
    'Sporting Goods, Hobby, Musical Inst., & Books': [81179, 84188, 86483, 85701, 83787, 60441],
    'General Merchandise': [706298, 687123, 675389, 674889, 667163, 351081],
    'Misc. Store Retailers': [130130, 124538, 121572, 119350, 115878, 98369],
    'Nonstore Retailers': [696849, 629204, 561412, 509652, 470867, 133904],
    'Electronic Shopping & Mail-order Houses': [611743, 551298, 488840, 433987, 386065, 80297],
}

fig, ax = plt.subplots()
ax.plot(year, totalrev_byproductcategory.values(),
             labels=totalrev_byproductcategory.keys())
ax.legend(loc='upper right')
ax.set_title('Total Revenue by Product Category')
ax.set_xlabel('Year')
ax.set_ylabel('Total Sales (millions)')

plt.show()
590/18:
# Plot
year = [2018, 2017, 2016, 2015, 2014, 1998]
totalrev_byproductcategory = {
    'Total Retail Trade': [5269468, 5053151, 4852958, 4725993, 4639440, 2581762],
    'Motor Vehicle & Parts': [1191321, 1172367, 1140614, 1094112, 1020851, 688415],
    'Furniture & Home Stores': [116895, 113035, 110404, 106570, 99718, 77412],
    'Electronics & Appliances': [100205, 98570, 99043, 103658, 105197, 82731],
    'Building Materials & Garden Equip': [381313, 365622, 348697, 331611, 318352, 202423],
    'Food & Beverage': [745736, 725137, 699349, 685381, 669165, 416525],
    'Health & Personal Care': [347454, 333338, 327153, 315244, 299263, 129583],
    'Clothing & Accessories': [268163, 260566, 260050, 255798, 250409, 149151],
    'Sporting Goods, Hobby, Musical Inst., & Books': [81179, 84188, 86483, 85701, 83787, 60441],
    'General Merchandise': [706298, 687123, 675389, 674889, 667163, 351081],
    'Misc. Store Retailers': [130130, 124538, 121572, 119350, 115878, 98369],
    'Nonstore Retailers': [696849, 629204, 561412, 509652, 470867, 133904],
    'Electronic Shopping & Mail-order Houses': [611743, 551298, 488840, 433987, 386065, 80297],
}

fig, ax = plt.subplots()
ax.stackplot(year, totalrev_byproductcategory.values(),
             labels=totalrev_byproductcategory.keys())
ax.legend(loc='upper right')
ax.set_title('Total Revenue by Product Category')
ax.set_xlabel('Year')
ax.set_ylabel('Total Sales (millions)')

plt.show()
590/19: retail_history_df.head()
590/20: historytotal_plot_df = retail_history_df.iloc[: , [0, 1, 3, 5, 7, 9, 11]].copy()
590/21:
historytotal_plot_df = historytotal_plot_df.set_index("Kind of Business")
historytotal_plot_df
590/22: retail_history_df = retail_history_df.set_index("Kind of Business")
590/23:
# Plot the world average as a line chart
total_retail, = plt.plot(years, retail_history_df.loc['Total Retail Trade', ["2018 Total", "2018 Ecommerce Total", 
                                                                             "2017 Total", "2017 Ecommerce Total", 
                                                                             "2016 Total", "2016 Ecommerce Total", 
                                                                             "2015 Total", "2015 Ecommerce Total", 
                                                                             "2014 Total", "2014 Ecommerce Total", 
                                                                             "1998 Total", "1998 Ecommerce"]], color="blue", label="Total Retail Trade Sales" )

# Plot the unemployment values for a single country
motor_vehicles, = plt.plot(years, retail_history_df.loc['Motor vehicle and parts dealers',["2018 Total", "2018 Ecommerce Total", 
                                                                             "2017 Total", "2017 Ecommerce Total", 
                                                                             "2016 Total", "2016 Ecommerce Total", 
                                                                             "2015 Total", "2015 Ecommerce Total", 
                                                                             "2014 Total", "2014 Ecommerce Total", 
                                                                             "1998 Total", "1998 Ecommerce"]], color="green",label="Motor Vehicles & Parts"])

# Create a legend for our chart
plt.legend(handles=[world_avg, country_one], loc="best")

# Show the chart
plt.show()
590/24:
# Plot the world average as a line chart
total_retail, = plt.plot(years, retail_history_df.loc['Total Retail Trade', ["2018 Total", "2018 Ecommerce Total", 
                                                                             "2017 Total", "2017 Ecommerce Total", 
                                                                             "2016 Total", "2016 Ecommerce Total", 
                                                                             "2015 Total", "2015 Ecommerce Total", 
                                                                             "2014 Total", "2014 Ecommerce Total", 
                                                                             "1998 Total", "1998 Ecommerce"]], color="blue", label="Total Retail Trade Sales" )

# Plot the unemployment values for a single country
motor_vehicles, = plt.plot(years, retail_history_df.loc['Motor vehicle and parts dealers',["2018 Total", "2018 Ecommerce Total", 
                                                                             "2017 Total", "2017 Ecommerce Total", 
                                                                             "2016 Total", "2016 Ecommerce Total", 
                                                                             "2015 Total", "2015 Ecommerce Total", 
                                                                             "2014 Total", "2014 Ecommerce Total", 
                                                                             "1998 Total", "1998 Ecommerce"]], color="green",label="Motor Vehicles & Parts" )

# Create a legend for our chart
plt.legend(handles=[world_avg, country_one], loc="best")

# Show the chart
plt.show()
590/25: years = [2018, 2017, 2016, 2015, 2014, 1998]
590/26:
# Plot the world average as a line chart
total_retail, = plt.plot(years, retail_history_df.loc['Total Retail Trade', ["2018 Total", "2018 Ecommerce Total", 
                                                                             "2017 Total", "2017 Ecommerce Total", 
                                                                             "2016 Total", "2016 Ecommerce Total", 
                                                                             "2015 Total", "2015 Ecommerce Total", 
                                                                             "2014 Total", "2014 Ecommerce Total", 
                                                                             "1998 Total", "1998 Ecommerce"]], color="blue", label="Total Retail Trade Sales" )

# Plot the unemployment values for a single country
motor_vehicles, = plt.plot(years, retail_history_df.loc['Motor vehicle and parts dealers',["2018 Total", "2018 Ecommerce Total", 
                                                                             "2017 Total", "2017 Ecommerce Total", 
                                                                             "2016 Total", "2016 Ecommerce Total", 
                                                                             "2015 Total", "2015 Ecommerce Total", 
                                                                             "2014 Total", "2014 Ecommerce Total", 
                                                                             "1998 Total", "1998 Ecommerce"]], color="green",label="Motor Vehicles & Parts" )

# Create a legend for our chart
plt.legend(handles=[world_avg, country_one], loc="best")

# Show the chart
plt.show()
590/27:
# Plot the world average as a line chart
total_retail, = plt.plot(years, retail_history_df.loc['Total Retail Trade',["2018 Total", "2018 Ecommerce Total", 
                                                                             "2017 Total", "2017 Ecommerce Total", 
                                                                             "2016 Total", "2016 Ecommerce Total", 
                                                                             "2015 Total", "2015 Ecommerce Total", 
                                                                             "2014 Total", "2014 Ecommerce Total", 
                                                                             "1998 Total", "1998 Ecommerce"]], color="blue", label="Total Retail Trade Sales" )

# Plot the unemployment values for a single country
motor_vehicles, = plt.plot(years, retail_history_df.loc['Motor vehicle and parts dealers',["2018 Total", "2018 Ecommerce Total", 
                                                                             "2017 Total", "2017 Ecommerce Total", 
                                                                             "2016 Total", "2016 Ecommerce Total", 
                                                                             "2015 Total", "2015 Ecommerce Total", 
                                                                             "2014 Total", "2014 Ecommerce Total", 
                                                                             "1998 Total", "1998 Ecommerce"]], color="green",label="Motor Vehicles & Parts" )

# Create a legend for our chart
plt.legend(handles=[world_avg, country_one], loc="best")

# Show the chart
plt.show()
592/1: %matplotlib notebook
592/2:
# Dependencies
import pandas as pd
import numpy as np
import os
import webbrowser
import io
import matplotlib
import matplotlib.pyplot as plt
592/3:
# Save file path to variable
retail_history_csv = "Datasets/EstUS_Retail_Trade_E-commercehistory.csv"
US_totalrev_2018_2017_csv = "Datasets/US_Retail_Trade_Sales_2018_2017.csv"
592/4:
# Read with pandas
retail_history_df = pd.read_csv(retail_history_csv)
retail_history_df.head()
592/5:
# Clean up DF
retail_history_df = retail_history_df.rename(columns={"Unnamed: 1": "Kind of Business",
                                 "2018": "2018 Total",
                                 "Unnamed: 3": "2018 Ecommerce Total",
                                 "2017r": "2017 Total",
                                 "Unnamed: 5": "2017 Ecommerce Total",
                                 "2016r": "2016 Total",
                                 "Unnamed: 7": "2016 Ecommerce Total",
                                 "2015r": "2015 Total",
                                 "Unnamed: 9": "2015 Ecommerce Total",
                                 "2014r": "2014 Total",
                                 "Unnamed: 11": "2014 Ecommerce Total",
                                 "1998": "1998 Total",
                                 "Unnamed: 43": "1998 Ecommerce"
                                 })
retail_history_df.head()
592/6:
# Drop columns we do not need
retail_history_df = retail_history_df.drop(columns=['NAICS Code',
                                                    '2013r',
                                                    'Unnamed: 13',
                                                    '2012r',
                                                    'Unnamed: 15',
                                                    '2011r',
                                                    'Unnamed: 17',
                                                    '2010',
                                                     'Unnamed: 19',
                                                     '2009',
                                                     'Unnamed: 21',
                                                     '2008',
                                                     'Unnamed: 23',
                                                     '2007',
                                                     'Unnamed: 25',
                                                     '2006',
                                                     'Unnamed: 27',
                                                     '2005',
                                                     'Unnamed: 29',
                                                     '2004',
                                                     'Unnamed: 31',
                                                     '2003',
                                                     'Unnamed: 33',
                                                     '2002',
                                                     'Unnamed: 35',
                                                     '2001',
                                                     'Unnamed: 37',
                                                     '2000',
                                                     'Unnamed: 39',
                                                     '1999',
                                                     'Unnamed: 41',
                                                    '2004', 
                                                    'Unnamed: 31',
                                                    '2003', 
                                                    'Unnamed: 33',
                                                    '2002',
                                                    'Unnamed: 35',
                                                    '2001',
                                                    'Unnamed: 37',
                                                    '2000',
                                                    'Unnamed: 39',
                                                    '1999',
                                                    'Unnamed: 41',
                                                   
                                                   ])
retail_history_df.head()
592/7:
retail_history_df.head()
# D - Denotes an estimate withheld to avoid disclosing data of individual companies; data are included in higher-level totals.
# S - Estimate does not meet publication standards because of high sampling variability (coefficient of variation is greater than 30%), poor response quality (total quantity response rate is less than 50%), or other concerns about the estimate's quality. Unpublished estimates derived from this table by subtraction are subject to these same limitations and should not be attributed to the U.S. Census Bureau. For a description of publication standards and the total quantity response rate, see https://www.census.gov/about/policies/quality/standards/standardf1.html.
592/8:
# Drop rows that do not have ecommerce data
#[Estimates are shown in millions of dollars and are based on data from the Annual Retail Trade Survey. Estimates have been adjusted using final results of the 2012 Economic Census.] 
retail_history_df = retail_history_df.drop([0, 8])
592/9: retail_history_df = retail_history_df.reset_index(drop=True)
592/10:
# Replaced S & D, final DataFrame
retail_history_df = retail_history_df.replace({'S': None,
                                               'D': None,
                                              })
retail_history_df
592/11:
historyecom_plot_df = retail_history_df.iloc[: , [0, 2, 4, 6, 8, 10, 12]].copy() 
historyecom_plot_df.head()
592/12: historytotal_plot_df = retail_history_df.iloc[: , [0, 1, 3, 5, 7, 9, 11]].copy()
592/13:
historytotal_plot_df = historytotal_plot_df.set_index("Kind of Business")
historytotal_plot_df
592/14:
# Plot
year = [2018, 2017, 2016, 2015, 2014, 1998]
totalrev_byproductcategory = {
    'Total Retail Trade': [5269468, 5053151, 4852958, 4725993, 4639440, 2581762],
    'Motor Vehicle & Parts': [1191321, 1172367, 1140614, 1094112, 1020851, 688415],
    'Furniture & Home Stores': [116895, 113035, 110404, 106570, 99718, 77412],
    'Electronics & Appliances': [100205, 98570, 99043, 103658, 105197, 82731],
    'Building Materials & Garden Equip': [381313, 365622, 348697, 331611, 318352, 202423],
    'Food & Beverage': [745736, 725137, 699349, 685381, 669165, 416525],
    'Health & Personal Care': [347454, 333338, 327153, 315244, 299263, 129583],
    'Clothing & Accessories': [268163, 260566, 260050, 255798, 250409, 149151],
    'Sporting Goods, Hobby, Musical Inst., & Books': [81179, 84188, 86483, 85701, 83787, 60441],
    'General Merchandise': [706298, 687123, 675389, 674889, 667163, 351081],
    'Misc. Store Retailers': [130130, 124538, 121572, 119350, 115878, 98369],
    'Nonstore Retailers': [696849, 629204, 561412, 509652, 470867, 133904],
    'Electronic Shopping & Mail-order Houses': [611743, 551298, 488840, 433987, 386065, 80297],
}

fig, ax = plt.subplots()
ax.stackplot(year, totalrev_byproductcategory.values(),
             labels=totalrev_byproductcategory.keys())
ax.legend(loc='upper right')
ax.set_title('Total Revenue by Product Category')
ax.set_xlabel('Year')
ax.set_ylabel('Total Sales (millions)')

plt.show()
592/15: retail_history_df.head()
592/16: retail_history_df = retail_history_df.set_index("Kind of Business")
592/17: years = [2018, 2017, 2016, 2015, 2014, 1998]
592/18:
# Plot the world average as a line chart
total_retail, = plt.plot(years, retail_history_df.loc['Total Retail Trade',["2018 Total", "2018 Ecommerce Total", 
                                                                             "2017 Total", "2017 Ecommerce Total", 
                                                                             "2016 Total", "2016 Ecommerce Total", 
                                                                             "2015 Total", "2015 Ecommerce Total", 
                                                                             "2014 Total", "2014 Ecommerce Total", 
                                                                             "1998 Total", "1998 Ecommerce"]], color="blue", label="Total Retail Trade Sales" )

# Plot the unemployment values for a single country
motor_vehicles, = plt.plot(years, retail_history_df.loc['Motor vehicle and parts dealers',["2018 Total", "2018 Ecommerce Total", 
                                                                             "2017 Total", "2017 Ecommerce Total", 
                                                                             "2016 Total", "2016 Ecommerce Total", 
                                                                             "2015 Total", "2015 Ecommerce Total", 
                                                                             "2014 Total", "2014 Ecommerce Total", 
                                                                             "1998 Total", "1998 Ecommerce"]], color="green",label="Motor Vehicles & Parts" )

# Create a legend for our chart
plt.legend(handles=[world_avg, country_one], loc="best")

# Show the chart
plt.show()
592/19:
# Plot the world average as a line chart
total_retail, = plt.plot(years, retail_history_df.loc['Total Retail Trade ...',["2018 Total", "2018 Ecommerce Total", 
                                                                             "2017 Total", "2017 Ecommerce Total", 
                                                                             "2016 Total", "2016 Ecommerce Total", 
                                                                             "2015 Total", "2015 Ecommerce Total", 
                                                                             "2014 Total", "2014 Ecommerce Total", 
                                                                             "1998 Total", "1998 Ecommerce"]], color="blue", label="Total Retail Trade Sales" )

# Plot the unemployment values for a single country
motor_vehicles, = plt.plot(years, retail_history_df.loc['Motor vehicle and parts dealers',["2018 Total", "2018 Ecommerce Total", 
                                                                             "2017 Total", "2017 Ecommerce Total", 
                                                                             "2016 Total", "2016 Ecommerce Total", 
                                                                             "2015 Total", "2015 Ecommerce Total", 
                                                                             "2014 Total", "2014 Ecommerce Total", 
                                                                             "1998 Total", "1998 Ecommerce"]], color="green",label="Motor Vehicles & Parts" )

# Create a legend for our chart
plt.legend(handles=[world_avg, country_one], loc="best")

# Show the chart
plt.show()
592/20: # years = [2018, 2017, 2016, 2015, 2014, 1998]
592/21:
# # Plot the world average as a line chart
# total_retail, = plt.plot(years, retail_history_df.loc['Total Retail Trade ...',["2018 Total", "2018 Ecommerce Total", 
#                                                                              "2017 Total", "2017 Ecommerce Total", 
#                                                                              "2016 Total", "2016 Ecommerce Total", 
#                                                                              "2015 Total", "2015 Ecommerce Total", 
#                                                                              "2014 Total", "2014 Ecommerce Total", 
#                                                                              "1998 Total", "1998 Ecommerce"]], color="blue", label="Total Retail Trade Sales" )

# # Plot the unemployment values for a single country
# motor_vehicles, = plt.plot(years, retail_history_df.loc['Motor vehicle and parts dealers',["2018 Total", "2018 Ecommerce Total", 
#                                                                              "2017 Total", "2017 Ecommerce Total", 
#                                                                              "2016 Total", "2016 Ecommerce Total", 
#                                                                              "2015 Total", "2015 Ecommerce Total", 
#                                                                              "2014 Total", "2014 Ecommerce Total", 
#                                                                              "1998 Total", "1998 Ecommerce"]], color="green",label="Motor Vehicles & Parts" )

# # Create a legend for our chart
# plt.legend(handles=[world_avg, country_one], loc="best")

# # Show the chart
# plt.show()
592/22:
ax = plt.gca()

retail_history_df.plot(kind='line',x='Kind of Business',y='2018 Total',ax=ax)
retail_history_df.plot(kind='line',x='Kind of Business',y='2018 Ecommerce Total', color='red', ax=ax)

plt.show()
592/23:
# Plot
year = [2018, 2017, 2016, 2015, 2014, 1998]
totalrev_byproductcategory = {
    'Total Retail Trade': [5269468, 5053151, 4852958, 4725993, 4639440, 2581762],
    'Motor Vehicle & Parts': [1191321, 1172367, 1140614, 1094112, 1020851, 688415],
    'Furniture & Home Stores': [116895, 113035, 110404, 106570, 99718, 77412],
    'Electronics & Appliances': [100205, 98570, 99043, 103658, 105197, 82731],
    'Building Materials & Garden Equip': [381313, 365622, 348697, 331611, 318352, 202423],
    'Food & Beverage': [745736, 725137, 699349, 685381, 669165, 416525],
    'Health & Personal Care': [347454, 333338, 327153, 315244, 299263, 129583],
    'Clothing & Accessories': [268163, 260566, 260050, 255798, 250409, 149151],
    'Sporting Goods, Hobby, Musical Inst., & Books': [81179, 84188, 86483, 85701, 83787, 60441],
    'General Merchandise': [706298, 687123, 675389, 674889, 667163, 351081],
    'Misc. Store Retailers': [130130, 124538, 121572, 119350, 115878, 98369],
    'Nonstore Retailers': [696849, 629204, 561412, 509652, 470867, 133904],
    'Electronic Shopping & Mail-order Houses': [611743, 551298, 488840, 433987, 386065, 80297],
}

fig, ax = plt.subplots()
ax.stackplot(year, totalrev_byproductcategory.values(),
             labels=totalrev_byproductcategory.keys())
ax.legend(loc='upper right')
ax.set_title('Total Revenue by Product Category')
ax.set_xlabel('Year')
ax.set_ylabel('Total Sales (millions)')

plt.show()
592/24:
# Clean up DF
retail_history_df = retail_history_df.rename(columns={"Unnamed: 1": "Kind_of_Business",
                                 "2018": "2018 Total",
                                 "Unnamed: 3": "2018 Ecommerce Total",
                                 "2017r": "2017 Total",
                                 "Unnamed: 5": "2017 Ecommerce Total",
                                 "2016r": "2016 Total",
                                 "Unnamed: 7": "2016 Ecommerce Total",
                                 "2015r": "2015 Total",
                                 "Unnamed: 9": "2015 Ecommerce Total",
                                 "2014r": "2014 Total",
                                 "Unnamed: 11": "2014 Ecommerce Total",
                                 "1998": "1998 Total",
                                 "Unnamed: 43": "1998 Ecommerce"
                                 })
retail_history_df.head()
594/1: %matplotlib notebook
594/2:
# Dependencies
import pandas as pd
import numpy as np
import os
import webbrowser
import io
import matplotlib
import matplotlib.pyplot as plt
594/3:
# Save file path to variable
retail_history_csv = "Datasets/EstUS_Retail_Trade_E-commercehistory.csv"
US_totalrev_2018_2017_csv = "Datasets/US_Retail_Trade_Sales_2018_2017.csv"
594/4:
# Read with pandas
retail_history_df = pd.read_csv(retail_history_csv)
retail_history_df.head()
594/5:
# Clean up DF
retail_history_df = retail_history_df.rename(columns={"Unnamed: 1": "Kind_of_Business",
                                 "2018": "2018 Total",
                                 "Unnamed: 3": "2018 Ecommerce Total",
                                 "2017r": "2017 Total",
                                 "Unnamed: 5": "2017 Ecommerce Total",
                                 "2016r": "2016 Total",
                                 "Unnamed: 7": "2016 Ecommerce Total",
                                 "2015r": "2015 Total",
                                 "Unnamed: 9": "2015 Ecommerce Total",
                                 "2014r": "2014 Total",
                                 "Unnamed: 11": "2014 Ecommerce Total",
                                 "1998": "1998 Total",
                                 "Unnamed: 43": "1998 Ecommerce"
                                 })
retail_history_df.head()
594/6:
# Drop columns we do not need
retail_history_df = retail_history_df.drop(columns=['NAICS Code',
                                                    '2013r',
                                                    'Unnamed: 13',
                                                    '2012r',
                                                    'Unnamed: 15',
                                                    '2011r',
                                                    'Unnamed: 17',
                                                    '2010',
                                                     'Unnamed: 19',
                                                     '2009',
                                                     'Unnamed: 21',
                                                     '2008',
                                                     'Unnamed: 23',
                                                     '2007',
                                                     'Unnamed: 25',
                                                     '2006',
                                                     'Unnamed: 27',
                                                     '2005',
                                                     'Unnamed: 29',
                                                     '2004',
                                                     'Unnamed: 31',
                                                     '2003',
                                                     'Unnamed: 33',
                                                     '2002',
                                                     'Unnamed: 35',
                                                     '2001',
                                                     'Unnamed: 37',
                                                     '2000',
                                                     'Unnamed: 39',
                                                     '1999',
                                                     'Unnamed: 41',
                                                    '2004', 
                                                    'Unnamed: 31',
                                                    '2003', 
                                                    'Unnamed: 33',
                                                    '2002',
                                                    'Unnamed: 35',
                                                    '2001',
                                                    'Unnamed: 37',
                                                    '2000',
                                                    'Unnamed: 39',
                                                    '1999',
                                                    'Unnamed: 41',
                                                   
                                                   ])
retail_history_df.head()
594/7:
retail_history_df.head()
# D - Denotes an estimate withheld to avoid disclosing data of individual companies; data are included in higher-level totals.
# S - Estimate does not meet publication standards because of high sampling variability (coefficient of variation is greater than 30%), poor response quality (total quantity response rate is less than 50%), or other concerns about the estimate's quality. Unpublished estimates derived from this table by subtraction are subject to these same limitations and should not be attributed to the U.S. Census Bureau. For a description of publication standards and the total quantity response rate, see https://www.census.gov/about/policies/quality/standards/standardf1.html.
594/8:
# Drop rows that do not have ecommerce data
#[Estimates are shown in millions of dollars and are based on data from the Annual Retail Trade Survey. Estimates have been adjusted using final results of the 2012 Economic Census.] 
retail_history_df = retail_history_df.drop([0, 8])
594/9: retail_history_df = retail_history_df.reset_index(drop=True)
594/10:
# Replaced S & D, final DataFrame
retail_history_df = retail_history_df.replace({'S': None,
                                               'D': None,
                                              })
retail_history_df
594/11:
historyecom_plot_df = retail_history_df.iloc[: , [0, 2, 4, 6, 8, 10, 12]].copy() 
historyecom_plot_df.head()
594/12: historytotal_plot_df = retail_history_df.iloc[: , [0, 1, 3, 5, 7, 9, 11]].copy()
594/13:
historytotal_plot_df = historytotal_plot_df.set_index("Kind of Business")
historytotal_plot_df
594/14:
historytotal_plot_df = historytotal_plot_df.set_index("Kind_of_Business")
historytotal_plot_df
594/15:
# Plot
year = [2018, 2017, 2016, 2015, 2014, 1998]
totalrev_byproductcategory = {
    'Total Retail Trade': [5269468, 5053151, 4852958, 4725993, 4639440, 2581762],
    'Motor Vehicle & Parts': [1191321, 1172367, 1140614, 1094112, 1020851, 688415],
    'Furniture & Home Stores': [116895, 113035, 110404, 106570, 99718, 77412],
    'Electronics & Appliances': [100205, 98570, 99043, 103658, 105197, 82731],
    'Building Materials & Garden Equip': [381313, 365622, 348697, 331611, 318352, 202423],
    'Food & Beverage': [745736, 725137, 699349, 685381, 669165, 416525],
    'Health & Personal Care': [347454, 333338, 327153, 315244, 299263, 129583],
    'Clothing & Accessories': [268163, 260566, 260050, 255798, 250409, 149151],
    'Sporting Goods, Hobby, Musical Inst., & Books': [81179, 84188, 86483, 85701, 83787, 60441],
    'General Merchandise': [706298, 687123, 675389, 674889, 667163, 351081],
    'Misc. Store Retailers': [130130, 124538, 121572, 119350, 115878, 98369],
    'Nonstore Retailers': [696849, 629204, 561412, 509652, 470867, 133904],
    'Electronic Shopping & Mail-order Houses': [611743, 551298, 488840, 433987, 386065, 80297],
}

fig, ax = plt.subplots()
ax.stackplot(year, totalrev_byproductcategory.values(),
             labels=totalrev_byproductcategory.keys())
ax.legend(loc='upper right')
ax.set_title('Total Revenue by Product Category')
ax.set_xlabel('Year')
ax.set_ylabel('Total Sales (millions)')

plt.show()
594/16: retail_history_df.head()
594/17: retail_history_df = retail_history_df.set_index("Kind_of_Business")
594/18:
# Split up our data into groups based upon 'gender'
categories = retail_history_df.groupby('Kind_of_Business')

# Find out how many of each gender took bike trips
top_categories = categories['2018 Total'].max(5)


# Chart our data, give it a title, and label the axes
categories_chart = top_categories.plot(kind="bar", title="Bike Trips by Gender")
gender_chart.set_xlabel("Gender")
gender_chart.set_ylabel("Number of Trips Taken")

plt.show()
plt.tight_layout()
594/19:
ax = plt.gca()

retail_history_df.plot(kind='line',x='Kind of Business',ax=ax)

plt.show()
594/20:
ax = plt.gca()

retail_history_df.plot(kind='line',x='Kind_of_Business',ax=ax)

plt.show()
594/21: retail_history_df = retail_history_df.set_index("Kind_of_Business").plot()
594/22: retail_history_df.set_index("Kind_of_Business").plot()
594/23: retail_history_df.set_index("Kind_of_Business").plot();
594/24: retail_history_df = retail_history_df.set_index("Kind_of_Business")
595/1: %matplotlib notebook
595/2:
# Dependencies
import pandas as pd
import numpy as np
import os
import webbrowser
import io
import matplotlib
import matplotlib.pyplot as plt
595/3:
# Save file path to variable
retail_history_csv = "Datasets/EstUS_Retail_Trade_E-commercehistory.csv"
US_totalrev_2018_2017_csv = "Datasets/US_Retail_Trade_Sales_2018_2017.csv"
595/4:
# Read with pandas
retail_history_df = pd.read_csv(retail_history_csv)
retail_history_df.head()
595/5:
# Clean up DF
retail_history_df = retail_history_df.rename(columns={"Unnamed: 1": "Kind_of_Business",
                                 "2018": "2018 Total",
                                 "Unnamed: 3": "2018 Ecommerce Total",
                                 "2017r": "2017 Total",
                                 "Unnamed: 5": "2017 Ecommerce Total",
                                 "2016r": "2016 Total",
                                 "Unnamed: 7": "2016 Ecommerce Total",
                                 "2015r": "2015 Total",
                                 "Unnamed: 9": "2015 Ecommerce Total",
                                 "2014r": "2014 Total",
                                 "Unnamed: 11": "2014 Ecommerce Total",
                                 "1998": "1998 Total",
                                 "Unnamed: 43": "1998 Ecommerce"
                                 })
retail_history_df.head()
595/6:
# Drop columns we do not need
retail_history_df = retail_history_df.drop(columns=['NAICS Code',
                                                    '2013r',
                                                    'Unnamed: 13',
                                                    '2012r',
                                                    'Unnamed: 15',
                                                    '2011r',
                                                    'Unnamed: 17',
                                                    '2010',
                                                     'Unnamed: 19',
                                                     '2009',
                                                     'Unnamed: 21',
                                                     '2008',
                                                     'Unnamed: 23',
                                                     '2007',
                                                     'Unnamed: 25',
                                                     '2006',
                                                     'Unnamed: 27',
                                                     '2005',
                                                     'Unnamed: 29',
                                                     '2004',
                                                     'Unnamed: 31',
                                                     '2003',
                                                     'Unnamed: 33',
                                                     '2002',
                                                     'Unnamed: 35',
                                                     '2001',
                                                     'Unnamed: 37',
                                                     '2000',
                                                     'Unnamed: 39',
                                                     '1999',
                                                     'Unnamed: 41',
                                                    '2004', 
                                                    'Unnamed: 31',
                                                    '2003', 
                                                    'Unnamed: 33',
                                                    '2002',
                                                    'Unnamed: 35',
                                                    '2001',
                                                    'Unnamed: 37',
                                                    '2000',
                                                    'Unnamed: 39',
                                                    '1999',
                                                    'Unnamed: 41',
                                                   
                                                   ])
retail_history_df.head()
595/7:
retail_history_df.head()
# D - Denotes an estimate withheld to avoid disclosing data of individual companies; data are included in higher-level totals.
# S - Estimate does not meet publication standards because of high sampling variability (coefficient of variation is greater than 30%), poor response quality (total quantity response rate is less than 50%), or other concerns about the estimate's quality. Unpublished estimates derived from this table by subtraction are subject to these same limitations and should not be attributed to the U.S. Census Bureau. For a description of publication standards and the total quantity response rate, see https://www.census.gov/about/policies/quality/standards/standardf1.html.
595/8:
# Drop rows that do not have ecommerce data
#[Estimates are shown in millions of dollars and are based on data from the Annual Retail Trade Survey. Estimates have been adjusted using final results of the 2012 Economic Census.] 
retail_history_df = retail_history_df.drop([0, 8])
595/9: retail_history_df = retail_history_df.reset_index(drop=True)
595/10:
# Replaced S & D, final DataFrame
retail_history_df = retail_history_df.replace({'S': None,
                                               'D': None,
                                              })
retail_history_df
595/11:
historyecom_plot_df = retail_history_df.iloc[: , [0, 2, 4, 6, 8, 10, 12]].copy() 
historyecom_plot_df.head()
595/12: historytotal_plot_df = retail_history_df.iloc[: , [0, 1, 3, 5, 7, 9, 11]].copy()
595/13:
historytotal_plot_df = historytotal_plot_df.set_index("Kind_of_Business")
historytotal_plot_df
595/14:
# Plot
year = [2018, 2017, 2016, 2015, 2014, 1998]
totalrev_byproductcategory = {
    'Total Retail Trade': [5269468, 5053151, 4852958, 4725993, 4639440, 2581762],
    'Motor Vehicle & Parts': [1191321, 1172367, 1140614, 1094112, 1020851, 688415],
    'Furniture & Home Stores': [116895, 113035, 110404, 106570, 99718, 77412],
    'Electronics & Appliances': [100205, 98570, 99043, 103658, 105197, 82731],
    'Building Materials & Garden Equip': [381313, 365622, 348697, 331611, 318352, 202423],
    'Food & Beverage': [745736, 725137, 699349, 685381, 669165, 416525],
    'Health & Personal Care': [347454, 333338, 327153, 315244, 299263, 129583],
    'Clothing & Accessories': [268163, 260566, 260050, 255798, 250409, 149151],
    'Sporting Goods, Hobby, Musical Inst., & Books': [81179, 84188, 86483, 85701, 83787, 60441],
    'General Merchandise': [706298, 687123, 675389, 674889, 667163, 351081],
    'Misc. Store Retailers': [130130, 124538, 121572, 119350, 115878, 98369],
    'Nonstore Retailers': [696849, 629204, 561412, 509652, 470867, 133904],
    'Electronic Shopping & Mail-order Houses': [611743, 551298, 488840, 433987, 386065, 80297],
}

fig, ax = plt.subplots()
ax.stackplot(year, totalrev_byproductcategory.values(),
             labels=totalrev_byproductcategory.keys())
ax.legend(loc='upper right')
ax.set_title('Total Revenue by Product Category')
ax.set_xlabel('Year')
ax.set_ylabel('Total Sales (millions)')

plt.show()
595/15: retail_history_df.head()
595/16: retail_history_df = retail_history_df.set_index("Kind_of_Business").plot()
595/17: historytotal_plot_df = historytotal_plot_df.set_index("Kind_of_Business").plot()
595/18: historytotal_plot_df = historytotal_plot_df.set_index("Kind_of_Business").plot();
595/19:
historytotal_plot_df = historytotal_plot_df.set_index("Kind_of_Business")
historytotal_plot_df
595/20:
retail_history_df=retail_history_df.astype(float)
retail_history_df = retail_history_df.set_index("Kind_of_Business").plot()
595/21: retail_history_df
595/22:
retail_history_df = retail_history_df.drop(rows={'Total Retail Trade ...': 'Total Retail Trade'})
retail_history_df.head()
596/1:
retail_history_df.head()
# D - Denotes an estimate withheld to avoid disclosing data of individual companies; data are included in higher-level totals.
# S - Estimate does not meet publication standards because of high sampling variability (coefficient of variation is greater than 30%), poor response quality (total quantity response rate is less than 50%), or other concerns about the estimate's quality. Unpublished estimates derived from this table by subtraction are subject to these same limitations and should not be attributed to the U.S. Census Bureau. For a description of publication standards and the total quantity response rate, see https://www.census.gov/about/policies/quality/standards/standardf1.html.
597/1: %matplotlib notebook
597/2:
# Dependencies
import pandas as pd
import numpy as np
import os
import webbrowser
import io
import matplotlib
import matplotlib.pyplot as plt
597/3:
# Save file path to variable
retail_history_csv = "Datasets/EstUS_Retail_Trade_E-commercehistory.csv"
US_totalrev_2018_2017_csv = "Datasets/US_Retail_Trade_Sales_2018_2017.csv"
597/4:
# Read with pandas
retail_history_df = pd.read_csv(retail_history_csv)
retail_history_df.head()
597/5:
# Clean up DF
retail_history_df = retail_history_df.rename(columns={"Unnamed: 1": "Kind_of_Business",
                                 "2018": "2018 Total",
                                 "Unnamed: 3": "2018 Ecommerce Total",
                                 "2017r": "2017 Total",
                                 "Unnamed: 5": "2017 Ecommerce Total",
                                 "2016r": "2016 Total",
                                 "Unnamed: 7": "2016 Ecommerce Total",
                                 "2015r": "2015 Total",
                                 "Unnamed: 9": "2015 Ecommerce Total",
                                 "2014r": "2014 Total",
                                 "Unnamed: 11": "2014 Ecommerce Total",
                                 "1998": "1998 Total",
                                 "Unnamed: 43": "1998 Ecommerce"
                                 })
retail_history_df.head()
597/6:
# Drop columns we do not need
retail_history_df = retail_history_df.drop(columns=['NAICS Code',
                                                    '2013r',
                                                    'Unnamed: 13',
                                                    '2012r',
                                                    'Unnamed: 15',
                                                    '2011r',
                                                    'Unnamed: 17',
                                                    '2010',
                                                     'Unnamed: 19',
                                                     '2009',
                                                     'Unnamed: 21',
                                                     '2008',
                                                     'Unnamed: 23',
                                                     '2007',
                                                     'Unnamed: 25',
                                                     '2006',
                                                     'Unnamed: 27',
                                                     '2005',
                                                     'Unnamed: 29',
                                                     '2004',
                                                     'Unnamed: 31',
                                                     '2003',
                                                     'Unnamed: 33',
                                                     '2002',
                                                     'Unnamed: 35',
                                                     '2001',
                                                     'Unnamed: 37',
                                                     '2000',
                                                     'Unnamed: 39',
                                                     '1999',
                                                     'Unnamed: 41',
                                                    '2004', 
                                                    'Unnamed: 31',
                                                    '2003', 
                                                    'Unnamed: 33',
                                                    '2002',
                                                    'Unnamed: 35',
                                                    '2001',
                                                    'Unnamed: 37',
                                                    '2000',
                                                    'Unnamed: 39',
                                                    '1999',
                                                    'Unnamed: 41',
                                                   
                                                   ])
retail_history_df.head()
597/7: retail_history_df
597/8:
retail_history_df.head()
# D - Denotes an estimate withheld to avoid disclosing data of individual companies; data are included in higher-level totals.
# S - Estimate does not meet publication standards because of high sampling variability (coefficient of variation is greater than 30%), poor response quality (total quantity response rate is less than 50%), or other concerns about the estimate's quality. Unpublished estimates derived from this table by subtraction are subject to these same limitations and should not be attributed to the U.S. Census Bureau. For a description of publication standards and the total quantity response rate, see https://www.census.gov/about/policies/quality/standards/standardf1.html.
597/9:
# Drop rows that do not have ecommerce data
#[Estimates are shown in millions of dollars and are based on data from the Annual Retail Trade Survey. Estimates have been adjusted using final results of the 2012 Economic Census.] 
retail_history_df = retail_history_df.drop([0, 8])
597/10: retail_history_df = retail_history_df.reset_index(drop=True)
597/11:
# Replaced S & D, final DataFrame
retail_history_df = retail_history_df.replace({'S': None,
                                               'D': None,
                                              })
retail_history_df
597/12:
historyecom_plot_df = retail_history_df.iloc[: , [0, 2, 4, 6, 8, 10, 12]].copy() 
historyecom_plot_df.head()
597/13: historytotal_plot_df = retail_history_df.iloc[: , [0, 1, 3, 5, 7, 9, 11]].copy()
597/14:
historytotal_plot_df = historytotal_plot_df.set_index("Kind_of_Business")
historytotal_plot_df
597/15:
# Plot
year = [2018, 2017, 2016, 2015, 2014, 1998]
totalrev_byproductcategory = {
    'Total Retail Trade': [5269468, 5053151, 4852958, 4725993, 4639440, 2581762],
    'Motor Vehicle & Parts': [1191321, 1172367, 1140614, 1094112, 1020851, 688415],
    'Furniture & Home Stores': [116895, 113035, 110404, 106570, 99718, 77412],
    'Electronics & Appliances': [100205, 98570, 99043, 103658, 105197, 82731],
    'Building Materials & Garden Equip': [381313, 365622, 348697, 331611, 318352, 202423],
    'Food & Beverage': [745736, 725137, 699349, 685381, 669165, 416525],
    'Health & Personal Care': [347454, 333338, 327153, 315244, 299263, 129583],
    'Clothing & Accessories': [268163, 260566, 260050, 255798, 250409, 149151],
    'Sporting Goods, Hobby, Musical Inst., & Books': [81179, 84188, 86483, 85701, 83787, 60441],
    'General Merchandise': [706298, 687123, 675389, 674889, 667163, 351081],
    'Misc. Store Retailers': [130130, 124538, 121572, 119350, 115878, 98369],
    'Nonstore Retailers': [696849, 629204, 561412, 509652, 470867, 133904],
    'Electronic Shopping & Mail-order Houses': [611743, 551298, 488840, 433987, 386065, 80297],
}

fig, ax = plt.subplots()
ax.stackplot(year, totalrev_byproductcategory.values(),
             labels=totalrev_byproductcategory.keys())
ax.legend(loc='upper right')
ax.set_title('Total Revenue by Product Category')
ax.set_xlabel('Year')
ax.set_ylabel('Total Sales (millions)')

plt.show()
597/16: retail_history_df.head()
597/17:
retail_history_df=retail_history_df.astype(float)
retail_history_df = retail_history_df.set_index("Kind_of_Business").plot()
597/18:
# Clean up DF
retail_history_df = retail_history_df.rename(columns={"Unnamed: 1": "Kind_of_Business",
                                 "2018": "2018 Total",
                                 "Unnamed: 3": "2018 Ecommerce Total",
                                 "2017r": "2017 Total",
                                 "Unnamed: 5": "2017 Ecommerce Total",
                                 "2016r": "2016 Total",
                                 "Unnamed: 7": "2016 Ecommerce Total",
                                 "2015r": "2015 Total",
                                 "Unnamed: 9": "2015 Ecommerce Total",
                                 "2014r": "2014 Total",
                                 "Unnamed: 11": "2014 Ecommerce Total",
                                 "1998": "1998 Total",
                                 "Unnamed: 43": "1998 Ecommerce"
                                 }, index={'Total Retail Trade ...': 'Total Retail Trade'})
retail_history_df.head()
597/19:
# Clean up DF
retail_history_df = retail_history_df.rename(columns={"Unnamed: 1": "Kind_of_Business",
                                 "2018": "2018 Total",
                                 "Unnamed: 3": "2018 Ecommerce Total",
                                 "2017r": "2017 Total",
                                 "Unnamed: 5": "2017 Ecommerce Total",
                                 "2016r": "2016 Total",
                                 "Unnamed: 7": "2016 Ecommerce Total",
                                 "2015r": "2015 Total",
                                 "Unnamed: 9": "2015 Ecommerce Total",
                                 "2014r": "2014 Total",
                                 "Unnamed: 11": "2014 Ecommerce Total",
                                 "1998": "1998 Total",
                                 "Unnamed: 43": "1998 Ecommerce"
                                 }, index={'Total Retail Trade ...': 'Total Retail Trade'})
retail_history_df.head()
597/20:
# Clean up DF
retail_history_df = retail_history_df.rename(columns={"Unnamed: 1": "Kind_of_Business",
                                 "2018": "2018 Total",
                                 "Unnamed: 3": "2018 Ecommerce Total",
                                 "2017r": "2017 Total",
                                 "Unnamed: 5": "2017 Ecommerce Total",
                                 "2016r": "2016 Total",
                                 "Unnamed: 7": "2016 Ecommerce Total",
                                 "2015r": "2015 Total",
                                 "Unnamed: 9": "2015 Ecommerce Total",
                                 "2014r": "2014 Total",
                                 "Unnamed: 11": "2014 Ecommerce Total",
                                 "1998": "1998 Total",
                                 "Unnamed: 43": "1998 Ecommerce"
                                 }, 
                                index={'Total Retail Trade ...': 'Total Retail Trade'})
retail_history_df.head()
597/21:
# Clean up DF
retail_history_df = retail_history_df.rename(columns={"Unnamed: 1": "Kind_of_Business",
                                 "2018": "2018 Total",
                                 "Unnamed: 3": "2018 Ecommerce Total",
                                 "2017r": "2017 Total",
                                 "Unnamed: 5": "2017 Ecommerce Total",
                                 "2016r": "2016 Total",
                                 "Unnamed: 7": "2016 Ecommerce Total",
                                 "2015r": "2015 Total",
                                 "Unnamed: 9": "2015 Ecommerce Total",
                                 "2014r": "2014 Total",
                                 "Unnamed: 11": "2014 Ecommerce Total",
                                 "1998": "1998 Total",
                                 "Unnamed: 43": "1998 Ecommerce"
                                 }, 
                                index={'Total Retail Trade ...': 'Total Retail Trade',
                                       'Motor vehicle and parts dealers .....': 'Motor vehicle & Parts'})
retail_history_df.head()
597/22:
# Clean up DF
retail_history_df = retail_history_df.rename(columns={"Unnamed: 1": "Kind_of_Business",
                                 "2018": "2018 Total",
                                 "Unnamed: 3": "2018 Ecommerce Total",
                                 "2017r": "2017 Total",
                                 "Unnamed: 5": "2017 Ecommerce Total",
                                 "2016r": "2016 Total",
                                 "Unnamed: 7": "2016 Ecommerce Total",
                                 "2015r": "2015 Total",
                                 "Unnamed: 9": "2015 Ecommerce Total",
                                 "2014r": "2014 Total",
                                 "Unnamed: 11": "2014 Ecommerce Total",
                                 "1998": "1998 Total",
                                 "Unnamed: 43": "1998 Ecommerce"
                                 }, 
                                index={'Total Retail Trade ...': 'Total Retail Trade',
                                       'Motor vehicle and parts dealers .....': 'Motor vehicle & Parts'}, inplace=True)
retail_history_df.head()
598/1: %matplotlib notebook
598/2:
# Dependencies
import pandas as pd
import numpy as np
import os
import webbrowser
import io
import matplotlib
import matplotlib.pyplot as plt
598/3:
# Save file path to variable
retail_history_csv = "Datasets/EstUS_Retail_Trade_E-commercehistory.csv"
US_totalrev_2018_2017_csv = "Datasets/US_Retail_Trade_Sales_2018_2017.csv"
598/4:
# Read with pandas
retail_history_df = pd.read_csv(retail_history_csv)
retail_history_df.head()
598/5:
# Clean up DF
retail_history_df = retail_history_df.rename(columns={"Unnamed: 1": "Kind_of_Business",
                                 "2018": "2018 Total",
                                 "Unnamed: 3": "2018 Ecommerce Total",
                                 "2017r": "2017 Total",
                                 "Unnamed: 5": "2017 Ecommerce Total",
                                 "2016r": "2016 Total",
                                 "Unnamed: 7": "2016 Ecommerce Total",
                                 "2015r": "2015 Total",
                                 "Unnamed: 9": "2015 Ecommerce Total",
                                 "2014r": "2014 Total",
                                 "Unnamed: 11": "2014 Ecommerce Total",
                                 "1998": "1998 Total",
                                 "Unnamed: 43": "1998 Ecommerce"
                                 }, 
                                index={'Total Retail Trade ...': 'Total Retail Trade',
                                       'Motor vehicle and parts dealers .....': 'Motor vehicle & Parts'}, inplace=True)
retail_history_df.head()
600/1: %matplotlib notebook
600/2:
# Dependencies
import pandas as pd
import numpy as np
import os
import webbrowser
import io
import matplotlib
import matplotlib.pyplot as plt
600/3:
# Save file path to variable
retail_history_csv = "Datasets/EstUS_Retail_Trade_E-commercehistory.csv"
US_totalrev_2018_2017_csv = "Datasets/US_Retail_Trade_Sales_2018_2017.csv"
600/4:
# Read with pandas
retail_history_df = pd.read_csv(retail_history_csv)
retail_history_df.head()
600/5:
# Clean up DF
retail_history_df = retail_history_df.rename(columns={"Unnamed: 1": "Kind_of_Business",
                                 "2018": "2018 Total",
                                 "Unnamed: 3": "2018 Ecommerce Total",
                                 "2017r": "2017 Total",
                                 "Unnamed: 5": "2017 Ecommerce Total",
                                 "2016r": "2016 Total",
                                 "Unnamed: 7": "2016 Ecommerce Total",
                                 "2015r": "2015 Total",
                                 "Unnamed: 9": "2015 Ecommerce Total",
                                 "2014r": "2014 Total",
                                 "Unnamed: 11": "2014 Ecommerce Total",
                                 "1998": "1998 Total",
                                 "Unnamed: 43": "1998 Ecommerce"
                                 }, 
                                index={'Total Retail Trade ...': 'Total Retail Trade',
                                       'Motor vehicle and parts dealers .....': 'Motor vehicle & Parts'}, inplace=True)
retail_history_df.head()
600/6:
# Clean up DF
retail_history_df = retail_history_df.rename(columns={"Unnamed: 1": "Kind_of_Business",
                                 "2018": "2018 Total",
                                 "Unnamed: 3": "2018 Ecommerce Total",
                                 "2017r": "2017 Total",
                                 "Unnamed: 5": "2017 Ecommerce Total",
                                 "2016r": "2016 Total",
                                 "Unnamed: 7": "2016 Ecommerce Total",
                                 "2015r": "2015 Total",
                                 "Unnamed: 9": "2015 Ecommerce Total",
                                 "2014r": "2014 Total",
                                 "Unnamed: 11": "2014 Ecommerce Total",
                                 "1998": "1998 Total",
                                 "Unnamed: 43": "1998 Ecommerce"
                                 }, 
                                index={'Total Retail Trade ...': 'Total Retail Trade',
                                       'Motor vehicle and parts dealers .....': 'Motor vehicle & Parts'})
retail_history_df.head()
600/7:
# Clean up DF
retail_history_df = retail_history_df.rename(columns={"Unnamed: 1": "Kind_of_Business",
                                 "2018": "2018 Total",
                                 "Unnamed: 3": "2018 Ecommerce Total",
                                 "2017r": "2017 Total",
                                 "Unnamed: 5": "2017 Ecommerce Total",
                                 "2016r": "2016 Total",
                                 "Unnamed: 7": "2016 Ecommerce Total",
                                 "2015r": "2015 Total",
                                 "Unnamed: 9": "2015 Ecommerce Total",
                                 "2014r": "2014 Total",
                                 "Unnamed: 11": "2014 Ecommerce Total",
                                 "1998": "1998 Total",
                                 "Unnamed: 43": "1998 Ecommerce"
                                 })
retail_history_df.head()
601/1: %matplotlib notebook
601/2:
# Dependencies
import pandas as pd
import numpy as np
import os
import webbrowser
import io
import matplotlib
import matplotlib.pyplot as plt
601/3:
# Save file path to variable
retail_history_csv = "Datasets/EstUS_Retail_Trade_E-commercehistory.csv"
US_totalrev_2018_2017_csv = "Datasets/US_Retail_Trade_Sales_2018_2017.csv"
601/4:
# Read with pandas
retail_history_df = pd.read_csv(retail_history_csv)
retail_history_df.head()
601/5:
# Clean up DF
retail_history_df = retail_history_df.rename(columns={"Unnamed: 1": "Kind_of_Business",
                                 "2018": "2018 Total",
                                 "Unnamed: 3": "2018 Ecommerce Total",
                                 "2017r": "2017 Total",
                                 "Unnamed: 5": "2017 Ecommerce Total",
                                 "2016r": "2016 Total",
                                 "Unnamed: 7": "2016 Ecommerce Total",
                                 "2015r": "2015 Total",
                                 "Unnamed: 9": "2015 Ecommerce Total",
                                 "2014r": "2014 Total",
                                 "Unnamed: 11": "2014 Ecommerce Total",
                                 "1998": "1998 Total",
                                 "Unnamed: 43": "1998 Ecommerce"
                                 })
retail_history_df.head()
601/6:
# Drop columns we do not need
retail_history_df = retail_history_df.drop(columns=['NAICS Code',
                                                    '2013r',
                                                    'Unnamed: 13',
                                                    '2012r',
                                                    'Unnamed: 15',
                                                    '2011r',
                                                    'Unnamed: 17',
                                                    '2010',
                                                     'Unnamed: 19',
                                                     '2009',
                                                     'Unnamed: 21',
                                                     '2008',
                                                     'Unnamed: 23',
                                                     '2007',
                                                     'Unnamed: 25',
                                                     '2006',
                                                     'Unnamed: 27',
                                                     '2005',
                                                     'Unnamed: 29',
                                                     '2004',
                                                     'Unnamed: 31',
                                                     '2003',
                                                     'Unnamed: 33',
                                                     '2002',
                                                     'Unnamed: 35',
                                                     '2001',
                                                     'Unnamed: 37',
                                                     '2000',
                                                     'Unnamed: 39',
                                                     '1999',
                                                     'Unnamed: 41',
                                                    '2004', 
                                                    'Unnamed: 31',
                                                    '2003', 
                                                    'Unnamed: 33',
                                                    '2002',
                                                    'Unnamed: 35',
                                                    '2001',
                                                    'Unnamed: 37',
                                                    '2000',
                                                    'Unnamed: 39',
                                                    '1999',
                                                    'Unnamed: 41',
                                                   
                                                   ])
retail_history_df.head()
601/7: retail_history_df
601/8:
retail_history_df.head()
# D - Denotes an estimate withheld to avoid disclosing data of individual companies; data are included in higher-level totals.
# S - Estimate does not meet publication standards because of high sampling variability (coefficient of variation is greater than 30%), poor response quality (total quantity response rate is less than 50%), or other concerns about the estimate's quality. Unpublished estimates derived from this table by subtraction are subject to these same limitations and should not be attributed to the U.S. Census Bureau. For a description of publication standards and the total quantity response rate, see https://www.census.gov/about/policies/quality/standards/standardf1.html.
601/9:
# Drop rows that do not have ecommerce data
#[Estimates are shown in millions of dollars and are based on data from the Annual Retail Trade Survey. Estimates have been adjusted using final results of the 2012 Economic Census.] 
retail_history_df = retail_history_df.drop([0, 8])
601/10: retail_history_df = retail_history_df.reset_index(drop=True)
601/11:
# Replaced S & D, final DataFrame
retail_history_df = retail_history_df.replace({'S': None,
                                               'D': None,
                                              })
retail_history_df
601/12:
historyecom_plot_df = retail_history_df.iloc[: , [0, 2, 4, 6, 8, 10, 12]].copy() 
historyecom_plot_df.head()
601/13: historytotal_plot_df = retail_history_df.iloc[: , [0, 1, 3, 5, 7, 9, 11]].copy()
601/14:
historytotal_plot_df = historytotal_plot_df.set_index("Kind_of_Business")
historytotal_plot_df
601/15:
# Plot
year = [2018, 2017, 2016, 2015, 2014, 1998]
totalrev_byproductcategory = {
    'Total Retail Trade': [5269468, 5053151, 4852958, 4725993, 4639440, 2581762],
    'Motor Vehicle & Parts': [1191321, 1172367, 1140614, 1094112, 1020851, 688415],
    'Furniture & Home Stores': [116895, 113035, 110404, 106570, 99718, 77412],
    'Electronics & Appliances': [100205, 98570, 99043, 103658, 105197, 82731],
    'Building Materials & Garden Equip': [381313, 365622, 348697, 331611, 318352, 202423],
    'Food & Beverage': [745736, 725137, 699349, 685381, 669165, 416525],
    'Health & Personal Care': [347454, 333338, 327153, 315244, 299263, 129583],
    'Clothing & Accessories': [268163, 260566, 260050, 255798, 250409, 149151],
    'Sporting Goods, Hobby, Musical Inst., & Books': [81179, 84188, 86483, 85701, 83787, 60441],
    'General Merchandise': [706298, 687123, 675389, 674889, 667163, 351081],
    'Misc. Store Retailers': [130130, 124538, 121572, 119350, 115878, 98369],
    'Nonstore Retailers': [696849, 629204, 561412, 509652, 470867, 133904],
    'Electronic Shopping & Mail-order Houses': [611743, 551298, 488840, 433987, 386065, 80297],
}

fig, ax = plt.subplots()
ax.stackplot(year, totalrev_byproductcategory.values(),
             labels=totalrev_byproductcategory.keys())
ax.legend(loc='upper right')
ax.set_title('Total Revenue by Product Category')
ax.set_xlabel('Year')
ax.set_ylabel('Total Sales (millions)')

plt.show()
601/16: retail_history_df.head()
601/17:
retail_history_df=retail_history_df.astype(float)
retail_history_df = retail_history_df.set_index("Kind_of_Business").plot()
601/18:
# retail_history_df=retail_history_df.astype(float)
# retail_history_df = retail_history_df.set_index("Kind_of_Business").plot()
601/19:
ax = plt.gca()

retail_history_df.plot(kind='line',x='Kind_of_Business',ax=ax)

plt.show()
601/20:
# # Plot the world average as a line chart
total_retail, = plt.plot(years, retail_history_df.loc['Total Retail Trade ...',["2018 Total", "2018 Ecommerce Total", 
                                                                             "2017 Total", "2017 Ecommerce Total", 
                                                                             "2016 Total", "2016 Ecommerce Total", 
                                                                             "2015 Total", "2015 Ecommerce Total", 
                                                                             "2014 Total", "2014 Ecommerce Total", 
                                                                             "1998 Total", "1998 Ecommerce"]], color="blue", label="Total Retail Trade Sales" )

# Plot the unemployment values for a single country
motor_vehicles, = plt.plot(years, retail_history_df.loc['Motor vehicle and parts dealers',["2018 Total", "2018 Ecommerce Total", 
                                                                             "2017 Total", "2017 Ecommerce Total", 
                                                                             "2016 Total", "2016 Ecommerce Total", 
                                                                             "2015 Total", "2015 Ecommerce Total", 
                                                                             "2014 Total", "2014 Ecommerce Total", 
                                                                             "1998 Total", "1998 Ecommerce"]], color="green",label="Motor Vehicles & Parts" )

# Create a legend for our chart
plt.legend(handles=[world_avg, country_one], loc="best")

# Show the chart
plt.show()
601/21: years = [2018, 2017, 2016, 2015, 2014, 1998]
601/22:
# # Plot the world average as a line chart
total_retail, = plt.plot(years, retail_history_df.loc['Total Retail Trade ...',["2018 Total", "2018 Ecommerce Total", 
                                                                             "2017 Total", "2017 Ecommerce Total", 
                                                                             "2016 Total", "2016 Ecommerce Total", 
                                                                             "2015 Total", "2015 Ecommerce Total", 
                                                                             "2014 Total", "2014 Ecommerce Total", 
                                                                             "1998 Total", "1998 Ecommerce"]], color="blue", label="Total Retail Trade Sales" )

# Plot the unemployment values for a single country
motor_vehicles, = plt.plot(years, retail_history_df.loc['Motor vehicle and parts dealers',["2018 Total", "2018 Ecommerce Total", 
                                                                             "2017 Total", "2017 Ecommerce Total", 
                                                                             "2016 Total", "2016 Ecommerce Total", 
                                                                             "2015 Total", "2015 Ecommerce Total", 
                                                                             "2014 Total", "2014 Ecommerce Total", 
                                                                             "1998 Total", "1998 Ecommerce"]], color="green",label="Motor Vehicles & Parts" )

# Create a legend for our chart
plt.legend(handles=[world_avg, country_one], loc="best")

# Show the chart
plt.show()
601/23:
# # Plot the world average as a line chart
# total_retail, = plt.plot(years, retail_history_df.loc['Total Retail Trade ...',["2018 Total", "2018 Ecommerce Total", 
#                                                                              "2017 Total", "2017 Ecommerce Total", 
#                                                                              "2016 Total", "2016 Ecommerce Total", 
#                                                                              "2015 Total", "2015 Ecommerce Total", 
#                                                                              "2014 Total", "2014 Ecommerce Total", 
#                                                                              "1998 Total", "1998 Ecommerce"]], color="blue", label="Total Retail Trade Sales" )

# # Plot the unemployment values for a single country
# motor_vehicles, = plt.plot(years, retail_history_df.loc['Motor vehicle and parts dealers',["2018 Total", "2018 Ecommerce Total", 
#                                                                              "2017 Total", "2017 Ecommerce Total", 
#                                                                              "2016 Total", "2016 Ecommerce Total", 
#                                                                              "2015 Total", "2015 Ecommerce Total", 
#                                                                              "2014 Total", "2014 Ecommerce Total", 
#                                                                              "1998 Total", "1998 Ecommerce"]], color="green",label="Motor Vehicles & Parts" )

# # Create a legend for our chart
# plt.legend(handles=[world_avg, country_one], loc="best")

# # Show the chart
# plt.show()
601/24:
US_totalrev_df = pd.read_csv(US_totalrev_2018_2017_csv)
US_totalrev_df.head()
601/25:
# Drop Columns
US_totalrev_df = US_totalrev_df.drop(columns=['Unnamed: 0'])
US_totalrev_df.head()
601/26:
# Rename columns
US_totalrev_df = US_totalrev_df.rename(columns={"2018 E-commerce": "2018 Total ecommerce",
                                                "2017 Revised \nTotal Sales": "2017 Total Sales",
                                                "2017 Revised \nE-commerce": "2017 Total ecommerce",
                                                "Percent Distribution\nof E-commerce Sales 2018": 'Percent Distribution of ecommerce 2018'
                                                 })
US_totalrev_df.head()
601/27:
# Removing first row
US_totalrev_df = US_totalrev_df.drop([0, 1])
601/28:
# Reset Index
US_totalrev_df = US_totalrev_df.reset_index(drop=True)
601/29: US_totalrev_df = US_totalrev_df.set_index("Industry")
601/30:
# Final DataFrame
# Need two separate dataframes for plotting
US_totalrev_df = US_totalrev_df.replace({'S': None})
US_totalrev_df
601/31:
# Totals df for plotting
# Non-store retail = do not have brick and mortar stores
total_plot_df = US_totalrev_df.iloc[: , [0, 1, 2, 3]].copy() 
total_plot_df
601/32:
totalrev_plot_df = pd.DataFrame({
    "2018 Total Sales":[1191321, 116895, 100205, 381313, 745736, 347454, 503925, 268163, 81179, 706298, 130130, 696849, 611743],
    "2018 Total Ecommerce Sales":[35348, 1533, 2056, 2795, 4837, 744, 0, 10981, 2617, 0, 0, 453584, 451635],
    "2017 Total Sales":[1172367, 113035, 98570, 365622, 725137, 333338, 459463, 260566, 84188, 687123, 124538, 629204, 551298],
    "2017 Total Ecommerce Sales":[32106, 1279, 2113, 2614, 3431, 0, 0, 9374, 2441, 0, 4014, 400279, 398415]
    }, 
    index=["Vehicle", "Furniture", 
           "Electronics", "Building material/Garden equip.",
          "Food & beverage", "Health & Personal", "Gasoline stations", "Clothing",
           "Sporting goods, hobby, musical instrument, and book stores", "General merchandise stores", "Misc. store retailers",
           "Nonstore retailers", "E-shopping and mail-order houses"
          ]
)
totalrev_plot_df.head()
601/33:
totalsales2018 = [1191321, 116895, 100205, 381313, 745736, 347454, 503925, 268163, 81179, 706298, 130130, 696849, 611743]
totalecom2018 = [35348, 1533, 2056, 2795, 4837, 744, 0, 10981, 2617, 0, 0, 453584, 451635]
totalsales2017 = [1172367, 113035, 98570, 365622, 725137, 333338, 459463, 260566, 84188, 687123, 124538, 629204, 551298]
totalecom2017 = [32106, 1279, 2113, 2614, 3431, 0, 0, 9374, 2441, 0, 4014, 400279, 398415]
index = ["Vehicle", "Furniture", "Electronics", "Building material/Garden equip.",
         "Food & beverage", "Health & Personal", "Gasoline stations", "Clothing", 
         "Sporting goods, hobby, musical instrument, and book stores", "General merchandise stores", 
         "Misc. store retailers", "Nonstore retailers", "E-shopping and mail-order houses"]
totalrev_plot_df = pd.DataFrame({'Total Sales 2018': totalsales2018,
                                 'Total Ecommerce Sales 2018': totalecom2018,
                                 'Total Sales 2017': totalsales2017,
                                 'Total Ecommerce Sales 2017': totalecom2017
                                }, index=index)
601/34:
# Plot
plt.figure(figsize=(10,8))
totalrev_plot_df.plot(kind="bar", stacked=True)
plt.title("Retail Totals - 2018 & 2017")
plt.xlabel("Industry")
plt.ylabel("Sales in Millions ($)")

labels = []

plt.show()
plt.savefig("Retail Totals, 2018 & 2017.png")
601/35:
# Plot
plt.figure(figsize=(10,2))
totalrev_plot_df.plot.bar(kind="barh", stacked=True)
plt.title("Retail Totals - 2018 & 2017")
plt.xlabel("Industry")
plt.ylabel("Sales in Millions ($)")

# Set xticks and lables
xticks = 
xticks(np.arange(0, 1, step=0.2)) 


plt.show()
plt.tight_layout()
plt.savefig("Retail Totals Stacked, 2018 & 2017.png")
601/36:
# Plot
plt.figure(figsize=(10,2))
totalrev_plot_df.plot.bar(kind="barh", stacked=True)
plt.title("Retail Totals - 2018 & 2017")
plt.xlabel("Industry")
plt.ylabel("Sales in Millions ($)")

# Set xticks and lables
# xticks = 
# xticks(np.arange(0, 1, step=0.2)) 


plt.show()
plt.tight_layout()
plt.savefig("Retail Totals Stacked, 2018 & 2017.png")
601/37:
# Plot
plt.figure(figsize=(10,2))
totalrev_plot_df.plot(kind="barh", stacked=True)
plt.title("Retail Totals - 2018 & 2017")
plt.xlabel("Industry")
plt.ylabel("Sales in Millions ($)")

# Set xticks and lables
# xticks = 
# xticks(np.arange(0, 1, step=0.2)) 


plt.show()
plt.tight_layout()
plt.savefig("Retail Totals Stacked, 2018 & 2017.png")
601/38:
# Bar Chart
total_plot_df['2018 Total Sales'] = pd.to_numeric(total_plot_df['2018 Total Sales'])
US_totalrev_chart = total_plot_df.plot(kind="bar", title="U.S. Retail Total Sales - In-person and E-Commerce 2018 and 2017")
gender_chart.set_xlabel("Gender")
gender_chart.set_ylabel("Number of Trips Taken")

plt.show()
plt.tight_layout()
601/39:
# percent change dataframe
percent_plot_df = US_totalrev_df.iloc[: , [4, 5, 6, 7, 8]].copy() 
percent_plot_df
601/40:
plt.figure(figsize=(10,2))
percent_plot_df.plot(kind="barh", stacked=True)
plt.title("Retail Totals - 2018 & 2017")
plt.xlabel("Industry")
plt.ylabel("Sales in Millions ($)")
601/41:
plt.figure(figsize=(10,2))
percent_plot_df.plot(kind="barh", stacked=True)
plt.title("Retail Totals - 2018 & 2017")
plt.xlabel("Percent y/y change")
plt.ylabel("Industry")

plt.savefig("Retail Totals Stacked, 2018 & 2017.png")
601/42:
plt.figure(figsize=(10,2))
percent_plot_df.plot(kind="line", stacked=True)
plt.title("Retail Totals - 2018 & 2017")
plt.xlabel("Percent y/y change")
plt.ylabel("Industry")

plt.savefig("Retail Totals Stacked, 2018 & 2017.png")
601/43:
plt.figure(figsize=(10,2))
percent_plot_df.plot(kind="line")
plt.title("Retail Totals - 2018 & 2017")
plt.xlabel("Percent y/y change")
plt.ylabel("Industry")

plt.savefig("Retail Totals Stacked, 2018 & 2017.png")
601/44:
retail_history_df=retail_history_df.astype(float)
retail_history_df = retail_history_df.set_index("Kind_of_Business").plot()
601/45:
# retail_history_df=retail_history_df.astype(float)
# retail_history_df = retail_history_df.set_index("Kind_of_Business").plot()
601/46:
plt.plot(year, 'Total Retail Trade', data=totalrev_byproductcategory, marker='o', markerfacecolor='blue', markersize=12, color='skyblue', linewidth=4)
plt.plot('x', 'y2', data=df, marker='', color='olive', linewidth=2)
plt.plot('x', 'y3', data=df, marker='', color='olive', linewidth=2, linestyle='dashed', label="toto")
plt.legend()
601/47:
plt.plot(year, 'Total Retail Trade', data=totalrev_byproductcategory, marker='o', markerfacecolor='blue', markersize=12, color='skyblue', linewidth=4)
# plt.plot('x', 'y2', data=df, marker='', color='olive', linewidth=2)
# plt.plot('x', 'y3', data=df, marker='', color='olive', linewidth=2, linestyle='dashed', label="toto")
plt.legend()
601/48:
plt.plot(year, 'Total Retail Trade', data=totalrev_byproductcategory, marker='o', markerfacecolor='blue', markersize=12, color='skyblue', linewidth=4)
# plt.plot('x', 'y2', data=df, marker='', color='olive', linewidth=2)
# plt.plot('x', 'y3', data=df, marker='', color='olive', linewidth=2, linestyle='dashed', label="toto")
plt.legend()
plt.show()
601/49:
plt.plot(year, 'Total Retail Trade', data=totalrev_byproductcategory, marker='o', markerfacecolor='blue', markersize=12, color='skyblue', linewidth=4)
# plt.plot('x', 'y2', data=df, marker='', color='olive', linewidth=2)
# plt.plot('x', 'y3', data=df, marker='', color='olive', linewidth=2, linestyle='dashed', label="toto")
plt.legend()
plt.show()
601/50:
plt.plot(year, 'Total Retail Trade', data=totalrev_byproductcategory, color='blue', linewidth=2)
plt.plot(year, 'Motor Vehicle & Parts', data=totalrev_byproductcategory, color='olive', linewidth=2)
# plt.plot('x', 'y3', data=df, marker='', color='olive', linewidth=2, linestyle='dashed', label="toto")
plt.legend()
plt.show()
602/1: %matplotlib notebook
602/2:
# Dependencies
import pandas as pd
import numpy as np
import os
import webbrowser
import io
import matplotlib
import matplotlib.pyplot as plt
602/3:
# Save file path to variable
retail_history_csv = "Datasets/EstUS_Retail_Trade_E-commercehistory.csv"
US_totalrev_2018_2017_csv = "Datasets/US_Retail_Trade_Sales_2018_2017.csv"
602/4:
# Read with pandas
retail_history_df = pd.read_csv(retail_history_csv)
retail_history_df.head()
602/5:
# Clean up DF
retail_history_df = retail_history_df.rename(columns={"Unnamed: 1": "Kind_of_Business",
                                 "2018": "2018 Total",
                                 "Unnamed: 3": "2018 Ecommerce Total",
                                 "2017r": "2017 Total",
                                 "Unnamed: 5": "2017 Ecommerce Total",
                                 "2016r": "2016 Total",
                                 "Unnamed: 7": "2016 Ecommerce Total",
                                 "2015r": "2015 Total",
                                 "Unnamed: 9": "2015 Ecommerce Total",
                                 "2014r": "2014 Total",
                                 "Unnamed: 11": "2014 Ecommerce Total",
                                 "1998": "1998 Total",
                                 "Unnamed: 43": "1998 Ecommerce"
                                 })
retail_history_df.head()
602/6:
# Drop columns we do not need
retail_history_df = retail_history_df.drop(columns=['NAICS Code',
                                                    '2013r',
                                                    'Unnamed: 13',
                                                    '2012r',
                                                    'Unnamed: 15',
                                                    '2011r',
                                                    'Unnamed: 17',
                                                    '2010',
                                                     'Unnamed: 19',
                                                     '2009',
                                                     'Unnamed: 21',
                                                     '2008',
                                                     'Unnamed: 23',
                                                     '2007',
                                                     'Unnamed: 25',
                                                     '2006',
                                                     'Unnamed: 27',
                                                     '2005',
                                                     'Unnamed: 29',
                                                     '2004',
                                                     'Unnamed: 31',
                                                     '2003',
                                                     'Unnamed: 33',
                                                     '2002',
                                                     'Unnamed: 35',
                                                     '2001',
                                                     'Unnamed: 37',
                                                     '2000',
                                                     'Unnamed: 39',
                                                     '1999',
                                                     'Unnamed: 41',
                                                    '2004', 
                                                    'Unnamed: 31',
                                                    '2003', 
                                                    'Unnamed: 33',
                                                    '2002',
                                                    'Unnamed: 35',
                                                    '2001',
                                                    'Unnamed: 37',
                                                    '2000',
                                                    'Unnamed: 39',
                                                    '1999',
                                                    'Unnamed: 41',
                                                   
                                                   ])
retail_history_df.head()
602/7: retail_history_df
602/8:
retail_history_df.head()
# D - Denotes an estimate withheld to avoid disclosing data of individual companies; data are included in higher-level totals.
# S - Estimate does not meet publication standards because of high sampling variability (coefficient of variation is greater than 30%), poor response quality (total quantity response rate is less than 50%), or other concerns about the estimate's quality. Unpublished estimates derived from this table by subtraction are subject to these same limitations and should not be attributed to the U.S. Census Bureau. For a description of publication standards and the total quantity response rate, see https://www.census.gov/about/policies/quality/standards/standardf1.html.
602/9:
# Drop rows that do not have ecommerce data
#[Estimates are shown in millions of dollars and are based on data from the Annual Retail Trade Survey. Estimates have been adjusted using final results of the 2012 Economic Census.] 
retail_history_df = retail_history_df.drop([0, 8])
602/10: retail_history_df = retail_history_df.reset_index(drop=True)
602/11:
# Replaced S & D, final DataFrame
retail_history_df = retail_history_df.replace({'S': None,
                                               'D': None,
                                              })
retail_history_df
602/12:
historyecom_plot_df = retail_history_df.iloc[: , [0, 2, 4, 6, 8, 10, 12]].copy() 
historyecom_plot_df.head()
602/13: historytotal_plot_df = retail_history_df.iloc[: , [0, 1, 3, 5, 7, 9, 11]].copy()
602/14:
historytotal_plot_df = historytotal_plot_df.set_index("Kind_of_Business")
historytotal_plot_df
602/15:
# Plot
year = [2018, 2017, 2016, 2015, 2014, 1998]
totalrev_byproductcategory = {
    'Total Retail Trade': [5269468, 5053151, 4852958, 4725993, 4639440, 2581762],
    'Motor Vehicle & Parts': [1191321, 1172367, 1140614, 1094112, 1020851, 688415],
    'Furniture & Home Stores': [116895, 113035, 110404, 106570, 99718, 77412],
    'Electronics & Appliances': [100205, 98570, 99043, 103658, 105197, 82731],
    'Building Materials & Garden Equip': [381313, 365622, 348697, 331611, 318352, 202423],
    'Food & Beverage': [745736, 725137, 699349, 685381, 669165, 416525],
    'Health & Personal Care': [347454, 333338, 327153, 315244, 299263, 129583],
    'Clothing & Accessories': [268163, 260566, 260050, 255798, 250409, 149151],
    'Sporting Goods, Hobby, Musical Inst., & Books': [81179, 84188, 86483, 85701, 83787, 60441],
    'General Merchandise': [706298, 687123, 675389, 674889, 667163, 351081],
    'Misc. Store Retailers': [130130, 124538, 121572, 119350, 115878, 98369],
    'Nonstore Retailers': [696849, 629204, 561412, 509652, 470867, 133904],
    'Electronic Shopping & Mail-order Houses': [611743, 551298, 488840, 433987, 386065, 80297],
}

fig, ax = plt.subplots()
ax.stackplot(year, totalrev_byproductcategory.values(),
             labels=totalrev_byproductcategory.keys())
ax.legend(loc='upper right')
ax.set_title('Total Revenue by Product Category')
ax.set_xlabel('Year')
ax.set_ylabel('Total Sales (millions)')

plt.show()
plt.sav
602/16:
# Plot
year = [2018, 2017, 2016, 2015, 2014, 1998]
totalrev_byproductcategory = {
    'Total Retail Trade': [5269468, 5053151, 4852958, 4725993, 4639440, 2581762],
    'Motor Vehicle & Parts': [1191321, 1172367, 1140614, 1094112, 1020851, 688415],
    'Furniture & Home Stores': [116895, 113035, 110404, 106570, 99718, 77412],
    'Electronics & Appliances': [100205, 98570, 99043, 103658, 105197, 82731],
    'Building Materials & Garden Equip': [381313, 365622, 348697, 331611, 318352, 202423],
    'Food & Beverage': [745736, 725137, 699349, 685381, 669165, 416525],
    'Health & Personal Care': [347454, 333338, 327153, 315244, 299263, 129583],
    'Clothing & Accessories': [268163, 260566, 260050, 255798, 250409, 149151],
    'Sporting Goods, Hobby, Musical Inst., & Books': [81179, 84188, 86483, 85701, 83787, 60441],
    'General Merchandise': [706298, 687123, 675389, 674889, 667163, 351081],
    'Misc. Store Retailers': [130130, 124538, 121572, 119350, 115878, 98369],
    'Nonstore Retailers': [696849, 629204, 561412, 509652, 470867, 133904],
    'Electronic Shopping & Mail-order Houses': [611743, 551298, 488840, 433987, 386065, 80297],
}

fig, ax = plt.subplots()
ax.stackplot(year, totalrev_byproductcategory.values(),
             labels=totalrev_byproductcategory.keys())
ax.legend(loc='upper right')
ax.set_title('Total Revenue by Product Category')
ax.set_xlabel('Year')
ax.set_ylabel('Total Sales (millions)')

plt.show()
602/17:
plt.plot(year, 'Total Retail Trade', data=totalrev_byproductcategory, color='blue', linewidth=2)
plt.plot(year, 'Motor Vehicle & Parts', data=totalrev_byproductcategory, color='olive', linewidth=2)
# plt.plot('x', 'y3', data=df, marker='', color='olive', linewidth=2, linestyle='dashed', label="toto")
plt.legend()
plt.show()
602/18: retail_history_df.head()
602/19:
US_totalrev_df = pd.read_csv(US_totalrev_2018_2017_csv)
US_totalrev_df.head()
602/20:
# Drop Columns
US_totalrev_df = US_totalrev_df.drop(columns=['Unnamed: 0'])
US_totalrev_df.head()
602/21:
# Rename columns
US_totalrev_df = US_totalrev_df.rename(columns={"2018 E-commerce": "2018 Total ecommerce",
                                                "2017 Revised \nTotal Sales": "2017 Total Sales",
                                                "2017 Revised \nE-commerce": "2017 Total ecommerce",
                                                "Percent Distribution\nof E-commerce Sales 2018": 'Percent Distribution of ecommerce 2018'
                                                 })
US_totalrev_df.head()
602/22:
# Removing first row
US_totalrev_df = US_totalrev_df.drop([0, 1])
602/23:
# Reset Index
US_totalrev_df = US_totalrev_df.reset_index(drop=True)
602/24: US_totalrev_df = US_totalrev_df.set_index("Industry")
602/25:
# Final DataFrame
# Need two separate dataframes for plotting
US_totalrev_df = US_totalrev_df.replace({'S': None})
US_totalrev_df
602/26:
# Totals df for plotting
# Non-store retail = do not have brick and mortar stores
total_plot_df = US_totalrev_df.iloc[: , [0, 1, 2, 3]].copy() 
total_plot_df
602/27:
totalrev_plot_df = pd.DataFrame({
    "2018 Total Sales":[1191321, 116895, 100205, 381313, 745736, 347454, 503925, 268163, 81179, 706298, 130130, 696849, 611743],
    "2018 Total Ecommerce Sales":[35348, 1533, 2056, 2795, 4837, 744, 0, 10981, 2617, 0, 0, 453584, 451635],
    "2017 Total Sales":[1172367, 113035, 98570, 365622, 725137, 333338, 459463, 260566, 84188, 687123, 124538, 629204, 551298],
    "2017 Total Ecommerce Sales":[32106, 1279, 2113, 2614, 3431, 0, 0, 9374, 2441, 0, 4014, 400279, 398415]
    }, 
    index=["Vehicle", "Furniture", 
           "Electronics", "Building material/Garden equip.",
          "Food & beverage", "Health & Personal", "Gasoline stations", "Clothing",
           "Sporting goods, hobby, musical instrument, and book stores", "General merchandise stores", "Misc. store retailers",
           "Nonstore retailers", "E-shopping and mail-order houses"
          ]
)
totalrev_plot_df.head()
602/28:
totalsales2018 = [1191321, 116895, 100205, 381313, 745736, 347454, 503925, 268163, 81179, 706298, 130130, 696849, 611743]
totalecom2018 = [35348, 1533, 2056, 2795, 4837, 744, 0, 10981, 2617, 0, 0, 453584, 451635]
totalsales2017 = [1172367, 113035, 98570, 365622, 725137, 333338, 459463, 260566, 84188, 687123, 124538, 629204, 551298]
totalecom2017 = [32106, 1279, 2113, 2614, 3431, 0, 0, 9374, 2441, 0, 4014, 400279, 398415]
index = ["Vehicle", "Furniture", "Electronics", "Building material/Garden equip.",
         "Food & beverage", "Health & Personal", "Gasoline stations", "Clothing", 
         "Sporting goods, hobby, musical instrument, and book stores", "General merchandise stores", 
         "Misc. store retailers", "Nonstore retailers", "E-shopping and mail-order houses"]
totalrev_plot_df = pd.DataFrame({'Total Sales 2018': totalsales2018,
                                 'Total Ecommerce Sales 2018': totalecom2018,
                                 'Total Sales 2017': totalsales2017,
                                 'Total Ecommerce Sales 2017': totalecom2017
                                }, index=index)
602/29:
# Plot
plt.figure(figsize=(10,8))
totalrev_plot_df.plot(kind="bar", stacked=True)
plt.title("Retail Totals - 2018 & 2017")
plt.xlabel("Industry")
plt.ylabel("Sales in Millions ($)")

labels = []

plt.show()
plt.savefig("Retail Totals, 2018 & 2017.png")
602/30:
# Plot
plt.figure(figsize=(10,2))
totalrev_plot_df.plot(kind="barh", stacked=True)
plt.title("Retail Totals - 2018 & 2017")
plt.xlabel("Industry")
plt.ylabel("Sales in Millions ($)")

# Set xticks and lables
# xticks = 
# xticks(np.arange(0, 1, step=0.2)) 


plt.show()
plt.tight_layout()
plt.savefig("Retail Totals Stacked, 2018 & 2017.png")
602/31: # Line Chart
602/32:
# percent change dataframe
percent_plot_df = US_totalrev_df.iloc[: , [4, 5, 6, 7, 8]].copy() 
percent_plot_df
602/33:
# Set Index
historyecom_plot_df = historyecom_plot_df.set_index("Kind_of_Business")
historyecom_plot_df
602/34:
# Find top five product categories
historyecom_plot_df.nlargest(5, ['2018 Ecommerce Total'])
602/35:
# Replaced S & D, final DataFrame
retail_history_df = retail_history_df.replace({'S': '0',
                                               'D': '0',
                                              })
retail_history_df
603/1: %matplotlib notebook
603/2:
# Dependencies
import pandas as pd
import numpy as np
import os
import webbrowser
import io
import matplotlib
import matplotlib.pyplot as plt
603/3:
# Save file path to variable
retail_history_csv = "Datasets/EstUS_Retail_Trade_E-commercehistory.csv"
US_totalrev_2018_2017_csv = "Datasets/US_Retail_Trade_Sales_2018_2017.csv"
603/4:
# Read with pandas
retail_history_df = pd.read_csv(retail_history_csv)
retail_history_df.head()
603/5:
# Rename Columns
retail_history_df = retail_history_df.rename(columns={"Unnamed: 1": "Kind_of_Business",
                                 "2018": "2018 Total",
                                 "Unnamed: 3": "2018 Ecommerce Total",
                                 "2017r": "2017 Total",
                                 "Unnamed: 5": "2017 Ecommerce Total",
                                 "2016r": "2016 Total",
                                 "Unnamed: 7": "2016 Ecommerce Total",
                                 "2015r": "2015 Total",
                                 "Unnamed: 9": "2015 Ecommerce Total",
                                 "2014r": "2014 Total",
                                 "Unnamed: 11": "2014 Ecommerce Total",
                                 "1998": "1998 Total",
                                 "Unnamed: 43": "1998 Ecommerce"
                                 })
retail_history_df.head()
603/6:
# Drop columns we do not need
retail_history_df = retail_history_df.drop(columns=['NAICS Code',
                                                    '2013r',
                                                    'Unnamed: 13',
                                                    '2012r',
                                                    'Unnamed: 15',
                                                    '2011r',
                                                    'Unnamed: 17',
                                                    '2010',
                                                     'Unnamed: 19',
                                                     '2009',
                                                     'Unnamed: 21',
                                                     '2008',
                                                     'Unnamed: 23',
                                                     '2007',
                                                     'Unnamed: 25',
                                                     '2006',
                                                     'Unnamed: 27',
                                                     '2005',
                                                     'Unnamed: 29',
                                                     '2004',
                                                     'Unnamed: 31',
                                                     '2003',
                                                     'Unnamed: 33',
                                                     '2002',
                                                     'Unnamed: 35',
                                                     '2001',
                                                     'Unnamed: 37',
                                                     '2000',
                                                     'Unnamed: 39',
                                                     '1999',
                                                     'Unnamed: 41',
                                                    '2004', 
                                                    'Unnamed: 31',
                                                    '2003', 
                                                    'Unnamed: 33',
                                                    '2002',
                                                    'Unnamed: 35',
                                                    '2001',
                                                    'Unnamed: 37',
                                                    '2000',
                                                    'Unnamed: 39',
                                                    '1999',
                                                    'Unnamed: 41',
                                                   
                                                   ])
retail_history_df.head()
603/7:
retail_history_df
# D - Denotes an estimate withheld to avoid disclosing data of individual companies; data are included in higher-level totals.
# S - Estimate does not meet publication standards because of high sampling variability (coefficient of variation is greater than 30%), poor response quality (total quantity response rate is less than 50%), or other concerns about the estimate's quality. Unpublished estimates derived from this table by subtraction are subject to these same limitations and should not be attributed to the U.S. Census Bureau. For a description of publication standards and the total quantity response rate, see https://www.census.gov/about/policies/quality/standards/standardf1.html.
603/8:
# Drop rows that do not have ecommerce data
#[Estimates are shown in millions of dollars and are based on data from the Annual Retail Trade Survey. Estimates have been adjusted using final results of the 2012 Economic Census.] 
retail_history_df = retail_history_df.drop([0, 8])
603/9: retail_history_df = retail_history_df.reset_index(drop=True)
603/10:
# Replaced S & D, final DataFrame
retail_history_df = retail_history_df.replace({'S': '0',
                                               'D': '0',
                                              })
retail_history_df
603/11:
# Separate by ecommerce
historyecom_plot_df = retail_history_df.iloc[: , [0, 2, 4, 6, 8, 10, 12]].copy()
603/12:
# Set Index
historyecom_plot_df = historyecom_plot_df.set_index("Kind_of_Business")
historyecom_plot_df
603/13:
# Find top five product categories
historyecom_plot_df.nlargest(5, ['2018 Ecommerce Total'])
603/14:
# Find top five product categories
top_five_ecom = historyecom_plot_df.nlargest(5, ['2018 Ecommerce Total'])
603/15:
# Find top five product categories
histroyecom_plot_df['2018 Ecommerce Total'].sort_values(ascending=False).head(n)
604/1: %matplotlib notebook
604/2:
# Dependencies
import pandas as pd
import numpy as np
import os
import webbrowser
import io
import matplotlib
import matplotlib.pyplot as plt
604/3:
# Save file path to variable
retail_history_csv = "Datasets/EstUS_Retail_Trade_E-commercehistory.csv"
US_totalrev_2018_2017_csv = "Datasets/US_Retail_Trade_Sales_2018_2017.csv"
604/4:
# Read with pandas
retail_history_df = pd.read_csv(retail_history_csv)
retail_history_df.head()
604/5:
# Rename Columns
retail_history_df = retail_history_df.rename(columns={"Unnamed: 1": "Kind_of_Business",
                                 "2018": "2018 Total",
                                 "Unnamed: 3": "2018 Ecommerce Total",
                                 "2017r": "2017 Total",
                                 "Unnamed: 5": "2017 Ecommerce Total",
                                 "2016r": "2016 Total",
                                 "Unnamed: 7": "2016 Ecommerce Total",
                                 "2015r": "2015 Total",
                                 "Unnamed: 9": "2015 Ecommerce Total",
                                 "2014r": "2014 Total",
                                 "Unnamed: 11": "2014 Ecommerce Total",
                                 "1998": "1998 Total",
                                 "Unnamed: 43": "1998 Ecommerce"
                                 })
retail_history_df.head()
604/6:
# Drop columns we do not need
retail_history_df = retail_history_df.drop(columns=['NAICS Code',
                                                    '2013r',
                                                    'Unnamed: 13',
                                                    '2012r',
                                                    'Unnamed: 15',
                                                    '2011r',
                                                    'Unnamed: 17',
                                                    '2010',
                                                     'Unnamed: 19',
                                                     '2009',
                                                     'Unnamed: 21',
                                                     '2008',
                                                     'Unnamed: 23',
                                                     '2007',
                                                     'Unnamed: 25',
                                                     '2006',
                                                     'Unnamed: 27',
                                                     '2005',
                                                     'Unnamed: 29',
                                                     '2004',
                                                     'Unnamed: 31',
                                                     '2003',
                                                     'Unnamed: 33',
                                                     '2002',
                                                     'Unnamed: 35',
                                                     '2001',
                                                     'Unnamed: 37',
                                                     '2000',
                                                     'Unnamed: 39',
                                                     '1999',
                                                     'Unnamed: 41',
                                                    '2004', 
                                                    'Unnamed: 31',
                                                    '2003', 
                                                    'Unnamed: 33',
                                                    '2002',
                                                    'Unnamed: 35',
                                                    '2001',
                                                    'Unnamed: 37',
                                                    '2000',
                                                    'Unnamed: 39',
                                                    '1999',
                                                    'Unnamed: 41',
                                                   
                                                   ])
retail_history_df.head()
604/7:
retail_history_df
# D - Denotes an estimate withheld to avoid disclosing data of individual companies; data are included in higher-level totals.
# S - Estimate does not meet publication standards because of high sampling variability (coefficient of variation is greater than 30%), poor response quality (total quantity response rate is less than 50%), or other concerns about the estimate's quality. Unpublished estimates derived from this table by subtraction are subject to these same limitations and should not be attributed to the U.S. Census Bureau. For a description of publication standards and the total quantity response rate, see https://www.census.gov/about/policies/quality/standards/standardf1.html.
604/8:
# Drop rows that do not have ecommerce data
#[Estimates are shown in millions of dollars and are based on data from the Annual Retail Trade Survey. Estimates have been adjusted using final results of the 2012 Economic Census.] 
retail_history_df = retail_history_df.drop([0, 8])
604/9: retail_history_df = retail_history_df.reset_index(drop=True)
604/10:
# Replaced S & D, final DataFrame
retail_history_df = retail_history_df.replace({'S': '0',
                                               'D': '0',
                                              })
retail_history_df
604/11:
# Separate by ecommerce
historyecom_df = retail_history_df.iloc[: , [0, 2, 4, 6, 8, 10, 12]].copy()
604/12:
# Set Index
historyecom_df = historyecom_plot_df.set_index("Kind_of_Business")
historyecom_df
604/13:
# Set Index
historyecom_df = historyecom_df.set_index("Kind_of_Business")
historyecom_df
604/14:
# Find top five product categories
histroyecom_plot_df['2018 Ecommerce Total'].sort_values(ascending=False).head(n)
historyecom_plot_df
604/15:
# Find top five product categories
histroyecom_plot_df['2018 Ecommerce Total'].sort_values(ascending=False).head(n)
historyecom_plot_df
604/16:
# Find top five product categories
topfive_ecom = histroyecom_plot_df['2018 Ecommerce Total'].sort_values(ascending=False).head(n)
604/17:
# Find top five product categories
topfive_ecom = histroyecom_df['2018 Ecommerce Total'].sort_values(ascending=False).head(n)
605/1: %matplotlib notebook
605/2:
# Dependencies
import pandas as pd
import numpy as np
import os
import webbrowser
import io
import matplotlib
import matplotlib.pyplot as plt
605/3:
# Save file path to variable
retail_history_csv = "Datasets/EstUS_Retail_Trade_E-commercehistory.csv"
US_totalrev_2018_2017_csv = "Datasets/US_Retail_Trade_Sales_2018_2017.csv"
605/4:
# Read with pandas
retail_history_df = pd.read_csv(retail_history_csv)
retail_history_df.head()
605/5:
# Rename Columns
retail_history_df = retail_history_df.rename(columns={"Unnamed: 1": "Kind_of_Business",
                                 "2018": "2018 Total",
                                 "Unnamed: 3": "2018 Ecommerce Total",
                                 "2017r": "2017 Total",
                                 "Unnamed: 5": "2017 Ecommerce Total",
                                 "2016r": "2016 Total",
                                 "Unnamed: 7": "2016 Ecommerce Total",
                                 "2015r": "2015 Total",
                                 "Unnamed: 9": "2015 Ecommerce Total",
                                 "2014r": "2014 Total",
                                 "Unnamed: 11": "2014 Ecommerce Total",
                                 "1998": "1998 Total",
                                 "Unnamed: 43": "1998 Ecommerce"
                                 })
retail_history_df.head()
605/6:
# Drop columns we do not need
retail_history_df = retail_history_df.drop(columns=['NAICS Code',
                                                    '2013r',
                                                    'Unnamed: 13',
                                                    '2012r',
                                                    'Unnamed: 15',
                                                    '2011r',
                                                    'Unnamed: 17',
                                                    '2010',
                                                     'Unnamed: 19',
                                                     '2009',
                                                     'Unnamed: 21',
                                                     '2008',
                                                     'Unnamed: 23',
                                                     '2007',
                                                     'Unnamed: 25',
                                                     '2006',
                                                     'Unnamed: 27',
                                                     '2005',
                                                     'Unnamed: 29',
                                                     '2004',
                                                     'Unnamed: 31',
                                                     '2003',
                                                     'Unnamed: 33',
                                                     '2002',
                                                     'Unnamed: 35',
                                                     '2001',
                                                     'Unnamed: 37',
                                                     '2000',
                                                     'Unnamed: 39',
                                                     '1999',
                                                     'Unnamed: 41',
                                                    '2004', 
                                                    'Unnamed: 31',
                                                    '2003', 
                                                    'Unnamed: 33',
                                                    '2002',
                                                    'Unnamed: 35',
                                                    '2001',
                                                    'Unnamed: 37',
                                                    '2000',
                                                    'Unnamed: 39',
                                                    '1999',
                                                    'Unnamed: 41',
                                                   
                                                   ])
retail_history_df.head()
605/7:
retail_history_df
# D - Denotes an estimate withheld to avoid disclosing data of individual companies; data are included in higher-level totals.
# S - Estimate does not meet publication standards because of high sampling variability (coefficient of variation is greater than 30%), poor response quality (total quantity response rate is less than 50%), or other concerns about the estimate's quality. Unpublished estimates derived from this table by subtraction are subject to these same limitations and should not be attributed to the U.S. Census Bureau. For a description of publication standards and the total quantity response rate, see https://www.census.gov/about/policies/quality/standards/standardf1.html.
605/8:
# Drop rows that do not have ecommerce data
#[Estimates are shown in millions of dollars and are based on data from the Annual Retail Trade Survey. Estimates have been adjusted using final results of the 2012 Economic Census.] 
retail_history_df = retail_history_df.drop([0, 8])
605/9: retail_history_df = retail_history_df.reset_index(drop=True)
605/10:
# Replaced S & D, final DataFrame
retail_history_df = retail_history_df.replace({'S': '0',
                                               'D': '0',
                                              })
retail_history_df
605/11:
# Separate by ecommerce
historyecom_df = retail_history_df.iloc[: , [0, 2, 4, 6, 8, 10, 12]].copy()
605/12:
# Set Index
historyecom_df = historyecom_df.set_index("Kind_of_Business")
historyecom_df
605/13:
# Find top five product categories
topfive_ecom = histroyecom_df['2018 Ecommerce Total'].sort_values(ascending=False).head(n)
605/14:
# Find top five product categories
topfive_ecom = historyecom_df['2018 Ecommerce Total'].sort_values(ascending=False).head(n)
605/15:
# Find top five product categories
topfive_ecom = historyecom_df['2018 Ecommerce Total'].sort_values(ascending=False).head(5)
605/16:
# Find top five product categories
topfive_ecom = historyecom_df['2018 Ecommerce Total'].sort_values(ascending=False).head(5)
topfive_ecom
605/17:
# Find top five product categories
topfive_ecom = pd.DataFrame(historyecom_df['2018 Ecommerce Total'].sort_values(ascending=False).head(5))
topfive_ecom
605/18:
# Find top five product categories
topfive_ecom = pd.DataFrame(historyecom_df['2018 Ecommerce Total'].sort_values(ascending=True).head(5))
topfive_ecom
605/19:
# Find top five product categories
topfive_ecom = pd.DataFrame(historyecom_df['2018 Ecommerce Total'].sort_values(ascending=False).head(5))
topfive_ecom
605/20:
# Find top five product categories
topfive_ecom = historyecom_df.nlargest(6, '2018 Ecommerce Total')
topfive_ecom
605/21:
# Find top five product categories
topfive_ecom = historyecom_df["2018 Ecommerce Total"].max(5)
topfive_ecom
605/22:
# Find top five product categories
topfive_ecom = historyecom_df["2018 Ecommerce Total"].max()
topfive_ecom
605/23:
# Find top five product categories
historyecom_df.plot()
605/24:
historyecom_df = historyecom_df.apply(pd.to_numeric)
historyecom_df
605/25: retail_history_df = rertail_history_df.apply(pd.to_numeric)
605/26: retail_history_df = retail_history_df.apply(pd.to_numeric)
605/27: retail_history_df = retail_history_df.set_index('Kind_of_Business')
605/28:
# Replaced S & D, final DataFrame
retail_history_df = retail_history_df.replace({'S': '0',
                                               'D': '0',
                                              })
605/29:
# Replaced S & D, final DataFrame
retail_history_df = retail_history_df.replace({'S': '0',
                                               'D': '0',
                                              })
retail_history_df
605/30: retail_history_df = retail_history_df.apply(pd.to_numeric)
605/31:
# Find top five product categories
topfive_ecom = historyecom_df.sort_values(by=['2018 Ecommerce Total']).head(5)
topfive_ecom
605/32:
# Find top five product categories
topfive_ecom = historyecom_df['2018 Ecommerce Total'].value_counts.head(5)
# historyecom_df.sort_values(by=['2018 Ecommerce Total']).head(5)
topfive_ecom
605/33:
# Find top five product categories
topfive_ecom = historyecom_df.sort_values(by=['2018 Ecommerce Total']).head(5)
topfive_ecom
605/34:
# Find top five product categories
topfive_ecom = historyecom_df.sort_values(by=['2018 Ecommerce Total'], ascending=True).head(5)
topfive_ecom
605/35:
# Find top five product categories
topfive_ecom = historyecom_df.sort_values(by=['2018 Ecommerce Total'], ascending=False).head(5)
topfive_ecom
605/36:
# remove commas from dataframe
retail_history_df.replace(',','', regex=True, inplace=True)
605/37:
# remove commas from dataframe
retail_history_df.replace(',','', regex=True, inplace=True)
retail_history_df.head()
605/38:
# Separate by ecommerce
historyecom_df = retail_history_df.iloc[: , [0, 2, 4, 6, 8, 10, 12]].copy()
606/1: %matplotlib notebook
606/2:
# Dependencies
import pandas as pd
import numpy as np
import os
import webbrowser
import io
import matplotlib
import matplotlib.pyplot as plt
606/3:
# Save file path to variable
retail_history_csv = "Datasets/EstUS_Retail_Trade_E-commercehistory.csv"
US_totalrev_2018_2017_csv = "Datasets/US_Retail_Trade_Sales_2018_2017.csv"
606/4:
# Read with pandas
retail_history_df = pd.read_csv(retail_history_csv)
retail_history_df.head()
606/5:
# Rename Columns
retail_history_df = retail_history_df.rename(columns={"Unnamed: 1": "Kind_of_Business",
                                 "2018": "2018 Total",
                                 "Unnamed: 3": "2018 Ecommerce Total",
                                 "2017r": "2017 Total",
                                 "Unnamed: 5": "2017 Ecommerce Total",
                                 "2016r": "2016 Total",
                                 "Unnamed: 7": "2016 Ecommerce Total",
                                 "2015r": "2015 Total",
                                 "Unnamed: 9": "2015 Ecommerce Total",
                                 "2014r": "2014 Total",
                                 "Unnamed: 11": "2014 Ecommerce Total",
                                 "1998": "1998 Total",
                                 "Unnamed: 43": "1998 Ecommerce"
                                 })
retail_history_df.head()
606/6:
# Drop columns we do not need
retail_history_df = retail_history_df.drop(columns=['NAICS Code',
                                                    '2013r',
                                                    'Unnamed: 13',
                                                    '2012r',
                                                    'Unnamed: 15',
                                                    '2011r',
                                                    'Unnamed: 17',
                                                    '2010',
                                                     'Unnamed: 19',
                                                     '2009',
                                                     'Unnamed: 21',
                                                     '2008',
                                                     'Unnamed: 23',
                                                     '2007',
                                                     'Unnamed: 25',
                                                     '2006',
                                                     'Unnamed: 27',
                                                     '2005',
                                                     'Unnamed: 29',
                                                     '2004',
                                                     'Unnamed: 31',
                                                     '2003',
                                                     'Unnamed: 33',
                                                     '2002',
                                                     'Unnamed: 35',
                                                     '2001',
                                                     'Unnamed: 37',
                                                     '2000',
                                                     'Unnamed: 39',
                                                     '1999',
                                                     'Unnamed: 41',
                                                    '2004', 
                                                    'Unnamed: 31',
                                                    '2003', 
                                                    'Unnamed: 33',
                                                    '2002',
                                                    'Unnamed: 35',
                                                    '2001',
                                                    'Unnamed: 37',
                                                    '2000',
                                                    'Unnamed: 39',
                                                    '1999',
                                                    'Unnamed: 41',
                                                   
                                                   ])
retail_history_df.head()
606/7:
retail_history_df
# D - Denotes an estimate withheld to avoid disclosing data of individual companies; data are included in higher-level totals.
# S - Estimate does not meet publication standards because of high sampling variability (coefficient of variation is greater than 30%), poor response quality (total quantity response rate is less than 50%), or other concerns about the estimate's quality. Unpublished estimates derived from this table by subtraction are subject to these same limitations and should not be attributed to the U.S. Census Bureau. For a description of publication standards and the total quantity response rate, see https://www.census.gov/about/policies/quality/standards/standardf1.html.
606/8:
# Drop rows that do not have ecommerce data
#[Estimates are shown in millions of dollars and are based on data from the Annual Retail Trade Survey. Estimates have been adjusted using final results of the 2012 Economic Census.] 
retail_history_df = retail_history_df.drop([0, 8])
606/9: retail_history_df = retail_history_df.reset_index(drop=True)
606/10: retail_history_df = retail_history_df.set_index('Kind_of_Business')
606/11:
# Replaced S & D, final DataFrame
retail_history_df = retail_history_df.replace({'S': '0',
                                               'D': '0',
                                              })
retail_history_df
606/12:
# remove commas from dataframe
retail_history_df.replace(',','', regex=True, inplace=True)
retail_history_df.head()
606/13:
# Separate by ecommerce
historyecom_df = retail_history_df.iloc[: , [0, 2, 4, 6, 8, 10, 12]].copy()
606/14:
# Separate by ecommerce
historyecom_df = retail_history_df.iloc[: , [2, 4, 6, 8, 10, 12]].copy()
606/15:
# Separate by ecommerce
historyecom_df = retail_history_df.iloc[: , [1, 3, 5, 7, 9, 11]].copy()
606/16:
# Set Index
historyecom_df = historyecom_df.set_index("Kind_of_Business")
606/17:
# Separate by ecommerce
historyecom_df = retail_history_df.iloc[: , [1, 3, 5, 7, 9, 11]].copy() 
historyecom_df
606/18:
# Find top five product categories
topfive_ecom = historyecom_df.sort_values(by=['2018 Ecommerce Total'], ascending=False).head(5)
topfive_ecom
606/19:
# Find top five product categories
topfive_ecom = historyecom_df.sort_values(by=['2018 Ecommerce Total'], ascending=True).head(5)
topfive_ecom
606/20:
topfive_ecom = historyecom_df.nlargest(5, '2018 Ecommerce Total')
topfive_ecom
606/21:
# Find top five product categories
topfive_ecom = historyecom_df.sort_values(by=['2018 Ecommerce Total'], ascending=False).head(6)
topfive_ecom
606/22:
# topfive_ecom = historyecom_df.nlargest(5, '2018 Ecommerce Total')
# topfive_ecom
606/23:
# Separate by total
historytotal_plot_df = retail_history_df.iloc[: , [0, 1, 3, 5, 7, 9, 11]].copy()
606/24:
historytotal_plot_df = historytotal_plot_df.set_index("Kind_of_Business")
historytotal_plot_df
606/25:
# Separate by total
historytotal_df = retail_history_df.iloc[: , [0, 1, 3, 5, 7, 9, 11]].copy()
historytotal_df
606/26:
# Plot
year = [2018, 2017, 2016, 2015, 2014, 1998]
totalrev_byproductcategory = {
    'Total Retail Trade': [5269468, 5053151, 4852958, 4725993, 4639440, 2581762],
    'Motor Vehicle & Parts': [1191321, 1172367, 1140614, 1094112, 1020851, 688415],
    'Furniture & Home Stores': [116895, 113035, 110404, 106570, 99718, 77412],
    'Electronics & Appliances': [100205, 98570, 99043, 103658, 105197, 82731],
    'Building Materials & Garden Equip': [381313, 365622, 348697, 331611, 318352, 202423],
    'Food & Beverage': [745736, 725137, 699349, 685381, 669165, 416525],
    'Health & Personal Care': [347454, 333338, 327153, 315244, 299263, 129583],
    'Clothing & Accessories': [268163, 260566, 260050, 255798, 250409, 149151],
    'Sporting Goods, Hobby, Musical Inst., & Books': [81179, 84188, 86483, 85701, 83787, 60441],
    'General Merchandise': [706298, 687123, 675389, 674889, 667163, 351081],
    'Misc. Store Retailers': [130130, 124538, 121572, 119350, 115878, 98369],
    'Nonstore Retailers': [696849, 629204, 561412, 509652, 470867, 133904],
    'Electronic Shopping & Mail-order Houses': [611743, 551298, 488840, 433987, 386065, 80297],
}

fig, ax = plt.subplots()
ax.stackplot(year, totalrev_byproductcategory.values(),
             labels=totalrev_byproductcategory.keys())
ax.legend(loc='upper right')
ax.set_title('Total Revenue by Product Category')
ax.set_xlabel('Year')
ax.set_ylabel('Total Sales (millions)')

plt.show()
606/27:
# Plot
year = [2018, 2017, 2016, 2015, 2014, 1998]
totalrev_byproductcategory = {
    'Total Retail Trade': [5269468, 5053151, 4852958, 4725993, 4639440, 2581762],
    'Motor Vehicle & Parts': [1191321, 1172367, 1140614, 1094112, 1020851, 688415],
    'Furniture & Home Stores': [116895, 113035, 110404, 106570, 99718, 77412],
    'Electronics & Appliances': [100205, 98570, 99043, 103658, 105197, 82731],
    'Building Materials & Garden Equip': [381313, 365622, 348697, 331611, 318352, 202423],
    'Food & Beverage': [745736, 725137, 699349, 685381, 669165, 416525],
    'Health & Personal Care': [347454, 333338, 327153, 315244, 299263, 129583],
    'Clothing & Accessories': [268163, 260566, 260050, 255798, 250409, 149151],
    'Sporting Goods, Hobby, Musical Inst., & Books': [81179, 84188, 86483, 85701, 83787, 60441],
    'General Merchandise': [706298, 687123, 675389, 674889, 667163, 351081],
    'Misc. Store Retailers': [130130, 124538, 121572, 119350, 115878, 98369],
    'Nonstore Retailers': [696849, 629204, 561412, 509652, 470867, 133904],
    'Electronic Shopping & Mail-order Houses': [611743, 551298, 488840, 433987, 386065, 80297],
}

fig, ax = plt.subplots()
ax.stackplot(year, totalrev_byproductcategory.values(),
             labels=totalrev_byproductcategory.keys())
ax.legend(loc='upper right')
ax.set_title('Total Revenue by Product Category')
ax.set_xlabel('Year')
ax.set_ylabel('Total Sales (millions)')
plt.figure(figsize=(10,2))

plt.show()
606/28:
# Plot
year = [2018, 2017, 2016, 2015, 2014, 1998]
totalrev_byproductcategory = {
    'Total Retail Trade': [5269468, 5053151, 4852958, 4725993, 4639440, 2581762],
    'Motor Vehicle & Parts': [1191321, 1172367, 1140614, 1094112, 1020851, 688415],
    'Furniture & Home Stores': [116895, 113035, 110404, 106570, 99718, 77412],
    'Electronics & Appliances': [100205, 98570, 99043, 103658, 105197, 82731],
    'Building Materials & Garden Equip': [381313, 365622, 348697, 331611, 318352, 202423],
    'Food & Beverage': [745736, 725137, 699349, 685381, 669165, 416525],
    'Health & Personal Care': [347454, 333338, 327153, 315244, 299263, 129583],
    'Clothing & Accessories': [268163, 260566, 260050, 255798, 250409, 149151],
    'Sporting Goods, Hobby, Musical Inst., & Books': [81179, 84188, 86483, 85701, 83787, 60441],
    'General Merchandise': [706298, 687123, 675389, 674889, 667163, 351081],
    'Misc. Store Retailers': [130130, 124538, 121572, 119350, 115878, 98369],
    'Nonstore Retailers': [696849, 629204, 561412, 509652, 470867, 133904],
    'Electronic Shopping & Mail-order Houses': [611743, 551298, 488840, 433987, 386065, 80297],
}

fig, ax = plt.subplots()
ax.stackplot(year, totalrev_byproductcategory.values(),
             labels=totalrev_byproductcategory.keys())
ax.legend(loc='upper right')
ax.set_title('Total Revenue by Product Category')
ax.set_xlabel('Year')
ax.set_ylabel('Total Sales (millions)')
plt.figure(figsize=(10,10))

plt.show()
606/29:
# Plot
year = [2018, 2017, 2016, 2015, 2014, 1998]
totalrev_byproductcategory = {
    'Total Retail Trade': [5269468, 5053151, 4852958, 4725993, 4639440, 2581762],
    'Motor Vehicle & Parts': [1191321, 1172367, 1140614, 1094112, 1020851, 688415],
    'Furniture & Home Stores': [116895, 113035, 110404, 106570, 99718, 77412],
    'Electronics & Appliances': [100205, 98570, 99043, 103658, 105197, 82731],
    'Building Materials & Garden Equip': [381313, 365622, 348697, 331611, 318352, 202423],
    'Food & Beverage': [745736, 725137, 699349, 685381, 669165, 416525],
    'Health & Personal Care': [347454, 333338, 327153, 315244, 299263, 129583],
    'Clothing & Accessories': [268163, 260566, 260050, 255798, 250409, 149151],
    'Sporting Goods, Hobby, Musical Inst., & Books': [81179, 84188, 86483, 85701, 83787, 60441],
    'General Merchandise': [706298, 687123, 675389, 674889, 667163, 351081],
    'Misc. Store Retailers': [130130, 124538, 121572, 119350, 115878, 98369],
    'Nonstore Retailers': [696849, 629204, 561412, 509652, 470867, 133904],
    'Electronic Shopping & Mail-order Houses': [611743, 551298, 488840, 433987, 386065, 80297],
}

fig, ax = plt.subplots()
ax.stackplot(year, totalrev_byproductcategory.values(),
             labels=totalrev_byproductcategory.keys(), size=(10,10))
ax.legend(loc='upper right')
ax.set_title('Total Revenue by Product Category')
ax.set_xlabel('Year')
ax.set_ylabel('Total Sales (millions)')

plt.show()
606/30:
# Plot
year = [2018, 2017, 2016, 2015, 2014, 1998]
totalrev_byproductcategory = {
    'Total Retail Trade': [5269468, 5053151, 4852958, 4725993, 4639440, 2581762],
    'Motor Vehicle & Parts': [1191321, 1172367, 1140614, 1094112, 1020851, 688415],
    'Furniture & Home Stores': [116895, 113035, 110404, 106570, 99718, 77412],
    'Electronics & Appliances': [100205, 98570, 99043, 103658, 105197, 82731],
    'Building Materials & Garden Equip': [381313, 365622, 348697, 331611, 318352, 202423],
    'Food & Beverage': [745736, 725137, 699349, 685381, 669165, 416525],
    'Health & Personal Care': [347454, 333338, 327153, 315244, 299263, 129583],
    'Clothing & Accessories': [268163, 260566, 260050, 255798, 250409, 149151],
    'Sporting Goods, Hobby, Musical Inst., & Books': [81179, 84188, 86483, 85701, 83787, 60441],
    'General Merchandise': [706298, 687123, 675389, 674889, 667163, 351081],
    'Misc. Store Retailers': [130130, 124538, 121572, 119350, 115878, 98369],
    'Nonstore Retailers': [696849, 629204, 561412, 509652, 470867, 133904],
    'Electronic Shopping & Mail-order Houses': [611743, 551298, 488840, 433987, 386065, 80297],
}

fig, ax = plt.subplots()
ax.stackplot(year, totalrev_byproductcategory.values(),
             labels=totalrev_byproductcategory.keys(), figsize=(10,10))
ax.legend(loc='upper right')
ax.set_title('Total Revenue by Product Category')
ax.set_xlabel('Year')
ax.set_ylabel('Total Sales (millions)')

plt.show()
606/31:
# Plot
year = [2018, 2017, 2016, 2015, 2014, 1998]
totalrev_byproductcategory = {
    'Total Retail Trade': [5269468, 5053151, 4852958, 4725993, 4639440, 2581762],
    'Motor Vehicle & Parts': [1191321, 1172367, 1140614, 1094112, 1020851, 688415],
    'Furniture & Home Stores': [116895, 113035, 110404, 106570, 99718, 77412],
    'Electronics & Appliances': [100205, 98570, 99043, 103658, 105197, 82731],
    'Building Materials & Garden Equip': [381313, 365622, 348697, 331611, 318352, 202423],
    'Food & Beverage': [745736, 725137, 699349, 685381, 669165, 416525],
    'Health & Personal Care': [347454, 333338, 327153, 315244, 299263, 129583],
    'Clothing & Accessories': [268163, 260566, 260050, 255798, 250409, 149151],
    'Sporting Goods, Hobby, Musical Inst., & Books': [81179, 84188, 86483, 85701, 83787, 60441],
    'General Merchandise': [706298, 687123, 675389, 674889, 667163, 351081],
    'Misc. Store Retailers': [130130, 124538, 121572, 119350, 115878, 98369],
    'Nonstore Retailers': [696849, 629204, 561412, 509652, 470867, 133904],
    'Electronic Shopping & Mail-order Houses': [611743, 551298, 488840, 433987, 386065, 80297],
}

fig, ax = plt.subplots()
ax.stackplot(year, totalrev_byproductcategory.values(),
             labels=totalrev_byproductcategory.keys())
ax.legend(loc='upper right')
ax.set_title('Total Revenue by Product Category')
ax.set_xlabel('Year')
ax.set_ylabel('Total Sales (millions)')

plt.show()
606/32:
# Plot
year = [2018, 2017, 2016, 2015, 2014, 1998]
# totalrev_byproductcategory = {
#     'Total Retail Trade': [5269468, 5053151, 4852958, 4725993, 4639440, 2581762],
#     'Motor Vehicle & Parts': [1191321, 1172367, 1140614, 1094112, 1020851, 688415],
#     'Furniture & Home Stores': [116895, 113035, 110404, 106570, 99718, 77412],
#     'Electronics & Appliances': [100205, 98570, 99043, 103658, 105197, 82731],
#     'Building Materials & Garden Equip': [381313, 365622, 348697, 331611, 318352, 202423],
#     'Food & Beverage': [745736, 725137, 699349, 685381, 669165, 416525],
#     'Health & Personal Care': [347454, 333338, 327153, 315244, 299263, 129583],
#     'Clothing & Accessories': [268163, 260566, 260050, 255798, 250409, 149151],
#     'Sporting Goods, Hobby, Musical Inst., & Books': [81179, 84188, 86483, 85701, 83787, 60441],
#     'General Merchandise': [706298, 687123, 675389, 674889, 667163, 351081],
#     'Misc. Store Retailers': [130130, 124538, 121572, 119350, 115878, 98369],
#     'Nonstore Retailers': [696849, 629204, 561412, 509652, 470867, 133904],
#     'Electronic Shopping & Mail-order Houses': [611743, 551298, 488840, 433987, 386065, 80297],
# }

fig, ax = plt.subplots()
ax.stackplot(year, historytotal_df.values(),
             labels=historytotal_df.keys())
ax.legend(loc='upper right')
ax.set_title('Total Revenue by Product Category')
ax.set_xlabel('Year')
ax.set_ylabel('Total Sales (millions)')

plt.show()
606/33:
# Plot
year = [2018, 2017, 2016, 2015, 2014, 1998]
totalrev_byproductcategory = {
    'Total Retail Trade': [5269468, 5053151, 4852958, 4725993, 4639440, 2581762],
    'Motor Vehicle & Parts': [1191321, 1172367, 1140614, 1094112, 1020851, 688415],
    'Furniture & Home Stores': [116895, 113035, 110404, 106570, 99718, 77412],
    'Electronics & Appliances': [100205, 98570, 99043, 103658, 105197, 82731],
    'Building Materials & Garden Equip': [381313, 365622, 348697, 331611, 318352, 202423],
    'Food & Beverage': [745736, 725137, 699349, 685381, 669165, 416525],
    'Health & Personal Care': [347454, 333338, 327153, 315244, 299263, 129583],
    'Clothing & Accessories': [268163, 260566, 260050, 255798, 250409, 149151],
    'Sporting Goods, Hobby, Musical Inst., & Books': [81179, 84188, 86483, 85701, 83787, 60441],
    'General Merchandise': [706298, 687123, 675389, 674889, 667163, 351081],
    'Misc. Store Retailers': [130130, 124538, 121572, 119350, 115878, 98369],
    'Nonstore Retailers': [696849, 629204, 561412, 509652, 470867, 133904],
    'Electronic Shopping & Mail-order Houses': [611743, 551298, 488840, 433987, 386065, 80297],
}

fig, ax = plt.subplots()
ax.stackplot(year, totalrev_byproductcategory.values(),
             labels=totalrev_byproductcategory_df.keys())
ax.legend(loc='upper right')
ax.set_title('Total Revenue by Product Category')
ax.set_xlabel('Year')
ax.set_ylabel('Total Sales (millions)')

plt.show()
plt.tight_layout()
607/1: %matplotlib notebook
607/2:
# Dependencies
import pandas as pd
import numpy as np
import os
import webbrowser
import io
import matplotlib
import matplotlib.pyplot as plt
607/3:
# Save file path to variable
retail_history_csv = "Datasets/EstUS_Retail_Trade_E-commercehistory.csv"
US_totalrev_2018_2017_csv = "Datasets/US_Retail_Trade_Sales_2018_2017.csv"
607/4:
# Read with pandas
retail_history_df = pd.read_csv(retail_history_csv)
retail_history_df.head()
607/5:
# Rename Columns
retail_history_df = retail_history_df.rename(columns={"Unnamed: 1": "Kind_of_Business",
                                 "2018": "2018 Total",
                                 "Unnamed: 3": "2018 Ecommerce Total",
                                 "2017r": "2017 Total",
                                 "Unnamed: 5": "2017 Ecommerce Total",
                                 "2016r": "2016 Total",
                                 "Unnamed: 7": "2016 Ecommerce Total",
                                 "2015r": "2015 Total",
                                 "Unnamed: 9": "2015 Ecommerce Total",
                                 "2014r": "2014 Total",
                                 "Unnamed: 11": "2014 Ecommerce Total",
                                 "1998": "1998 Total",
                                 "Unnamed: 43": "1998 Ecommerce"
                                 })
retail_history_df.head()
607/6:
# Drop columns we do not need
retail_history_df = retail_history_df.drop(columns=['NAICS Code',
                                                    '2013r',
                                                    'Unnamed: 13',
                                                    '2012r',
                                                    'Unnamed: 15',
                                                    '2011r',
                                                    'Unnamed: 17',
                                                    '2010',
                                                     'Unnamed: 19',
                                                     '2009',
                                                     'Unnamed: 21',
                                                     '2008',
                                                     'Unnamed: 23',
                                                     '2007',
                                                     'Unnamed: 25',
                                                     '2006',
                                                     'Unnamed: 27',
                                                     '2005',
                                                     'Unnamed: 29',
                                                     '2004',
                                                     'Unnamed: 31',
                                                     '2003',
                                                     'Unnamed: 33',
                                                     '2002',
                                                     'Unnamed: 35',
                                                     '2001',
                                                     'Unnamed: 37',
                                                     '2000',
                                                     'Unnamed: 39',
                                                     '1999',
                                                     'Unnamed: 41',
                                                    '2004', 
                                                    'Unnamed: 31',
                                                    '2003', 
                                                    'Unnamed: 33',
                                                    '2002',
                                                    'Unnamed: 35',
                                                    '2001',
                                                    'Unnamed: 37',
                                                    '2000',
                                                    'Unnamed: 39',
                                                    '1999',
                                                    'Unnamed: 41',
                                                   
                                                   ])
retail_history_df.head()
607/7:
retail_history_df
# D - Denotes an estimate withheld to avoid disclosing data of individual companies; data are included in higher-level totals.
# S - Estimate does not meet publication standards because of high sampling variability (coefficient of variation is greater than 30%), poor response quality (total quantity response rate is less than 50%), or other concerns about the estimate's quality. Unpublished estimates derived from this table by subtraction are subject to these same limitations and should not be attributed to the U.S. Census Bureau. For a description of publication standards and the total quantity response rate, see https://www.census.gov/about/policies/quality/standards/standardf1.html.
607/8:
# Drop rows that do not have ecommerce data
#[Estimates are shown in millions of dollars and are based on data from the Annual Retail Trade Survey. Estimates have been adjusted using final results of the 2012 Economic Census.] 
retail_history_df = retail_history_df.drop([0, 8])
607/9: retail_history_df = retail_history_df.reset_index(drop=True)
607/10: retail_history_df = retail_history_df.set_index('Kind_of_Business')
607/11:
# Replaced S & D, final DataFrame
retail_history_df = retail_history_df.replace({'S': '0',
                                               'D': '0',
                                              })
retail_history_df
607/12:
# remove commas from dataframe
retail_history_df.replace(',','', regex=True, inplace=True)
retail_history_df.head()
607/13:
# Separate by ecommerce
historyecom_df = retail_history_df.iloc[: , [1, 3, 5, 7, 9, 11]].copy() 
historyecom_df
607/14:
# Find top five product categories
topfive_ecom = historyecom_df.sort_values(by=['2018 Ecommerce Total'], ascending=False).head(6)
topfive_ecom
607/15:
# topfive_ecom = historyecom_df.nlargest(5, '2018 Ecommerce Total')
# topfive_ecom
607/16:
# Separate by total
historytotal_df = retail_history_df.iloc[: , [0, 1, 3, 5, 7, 9, 11]].copy()
historytotal_df
607/17:
# Plot
year = [2018, 2017, 2016, 2015, 2014, 1998]
totalrev_byproductcategory = {
    'Total Retail Trade': [5269468, 5053151, 4852958, 4725993, 4639440, 2581762],
    'Motor Vehicle & Parts': [1191321, 1172367, 1140614, 1094112, 1020851, 688415],
    'Furniture & Home Stores': [116895, 113035, 110404, 106570, 99718, 77412],
    'Electronics & Appliances': [100205, 98570, 99043, 103658, 105197, 82731],
    'Building Materials & Garden Equip': [381313, 365622, 348697, 331611, 318352, 202423],
    'Food & Beverage': [745736, 725137, 699349, 685381, 669165, 416525],
    'Health & Personal Care': [347454, 333338, 327153, 315244, 299263, 129583],
    'Clothing & Accessories': [268163, 260566, 260050, 255798, 250409, 149151],
    'Sporting Goods, Hobby, Musical Inst., & Books': [81179, 84188, 86483, 85701, 83787, 60441],
    'General Merchandise': [706298, 687123, 675389, 674889, 667163, 351081],
    'Misc. Store Retailers': [130130, 124538, 121572, 119350, 115878, 98369],
    'Nonstore Retailers': [696849, 629204, 561412, 509652, 470867, 133904],
    'Electronic Shopping & Mail-order Houses': [611743, 551298, 488840, 433987, 386065, 80297],
}

fig, ax = plt.subplots()
ax.stackplot(year, totalrev_byproductcategory.values(),
             labels=totalrev_byproductcategory_df.keys())
ax.legend(loc='upper right')
ax.set_title('Total Revenue by Product Category')
ax.set_xlabel('Year')
ax.set_ylabel('Total Sales (millions)')

plt.show()
plt.tight_layout()
607/18:
# Plot
year = [2018, 2017, 2016, 2015, 2014, 1998]
totalrev_byproductcategory = {
    'Total Retail Trade': [5269468, 5053151, 4852958, 4725993, 4639440, 2581762],
    'Motor Vehicle & Parts': [1191321, 1172367, 1140614, 1094112, 1020851, 688415],
    'Furniture & Home Stores': [116895, 113035, 110404, 106570, 99718, 77412],
    'Electronics & Appliances': [100205, 98570, 99043, 103658, 105197, 82731],
    'Building Materials & Garden Equip': [381313, 365622, 348697, 331611, 318352, 202423],
    'Food & Beverage': [745736, 725137, 699349, 685381, 669165, 416525],
    'Health & Personal Care': [347454, 333338, 327153, 315244, 299263, 129583],
    'Clothing & Accessories': [268163, 260566, 260050, 255798, 250409, 149151],
    'Sporting Goods, Hobby, Musical Inst., & Books': [81179, 84188, 86483, 85701, 83787, 60441],
    'General Merchandise': [706298, 687123, 675389, 674889, 667163, 351081],
    'Misc. Store Retailers': [130130, 124538, 121572, 119350, 115878, 98369],
    'Nonstore Retailers': [696849, 629204, 561412, 509652, 470867, 133904],
    'Electronic Shopping & Mail-order Houses': [611743, 551298, 488840, 433987, 386065, 80297],
}

fig, ax = plt.subplots()
ax.stackplot(year, totalrev_byproductcategory.values(),
             labels=totalrev_byproductcategory.keys())
ax.legend(loc='upper right')
ax.set_title('Total Revenue by Product Category')
ax.set_xlabel('Year')
ax.set_ylabel('Total Sales (millions)')

plt.show()
plt.tight_layout()
607/19:
# Plot
year = [2018, 2017, 2016, 2015, 2014, 1998]
totalrev_byproductcategory = {
    'Total Retail Trade': [5269468, 5053151, 4852958, 4725993, 4639440, 2581762],
    'Motor Vehicle & Parts': [1191321, 1172367, 1140614, 1094112, 1020851, 688415],
    'Furniture & Home Stores': [116895, 113035, 110404, 106570, 99718, 77412],
    'Electronics & Appliances': [100205, 98570, 99043, 103658, 105197, 82731],
    'Building Materials & Garden Equip': [381313, 365622, 348697, 331611, 318352, 202423],
    'Food & Beverage': [745736, 725137, 699349, 685381, 669165, 416525],
    'Health & Personal Care': [347454, 333338, 327153, 315244, 299263, 129583],
    'Clothing & Accessories': [268163, 260566, 260050, 255798, 250409, 149151],
    'Sporting Goods, Hobby, Musical Inst., & Books': [81179, 84188, 86483, 85701, 83787, 60441],
    'General Merchandise': [706298, 687123, 675389, 674889, 667163, 351081],
    'Misc. Store Retailers': [130130, 124538, 121572, 119350, 115878, 98369],
    'Nonstore Retailers': [696849, 629204, 561412, 509652, 470867, 133904],
    'Electronic Shopping & Mail-order Houses': [611743, 551298, 488840, 433987, 386065, 80297],
}

fig, ax = plt.subplots()
ax.stackplot(year, totalrev_byproductcategory.values(),
             labels=totalrev_byproductcategory.keys())
ax.legend(loc='upper right')
ax.set_title('Total Revenue by Product Category')
ax.set_xlabel('Year')
ax.set_ylabel('Total Sales (millions)')

plt.show()
plt.tight_layout()
plt.savefig("Total Revenue by Product Category")
607/20:
plt.plot(year, 'Total Retail Trade', data=totalrev_byproductcategory, color='blue', linewidth=2)
plt.plot(year, 'Motor Vehicle & Parts', data=totalrev_byproductcategory, color='olive', linewidth=2)
# plt.plot('x', 'y3', data=df, marker='', color='olive', linewidth=2, linestyle='dashed', label="toto")
plt.legend()
plt.show()
607/21: retail_history_df.head()
607/22:
# retail_history_df=retail_history_df.astype(float)
# retail_history_df = retail_history_df.set_index("Kind_of_Business").plot()
607/23:
US_totalrev_df = pd.read_csv(US_totalrev_2018_2017_csv)
US_totalrev_df.head()
607/24:
# Drop Columns
US_totalrev_df = US_totalrev_df.drop(columns=['Unnamed: 0'])
US_totalrev_df.head()
607/25:
# Rename columns
US_totalrev_df = US_totalrev_df.rename(columns={"2018 E-commerce": "2018 Total ecommerce",
                                                "2017 Revised \nTotal Sales": "2017 Total Sales",
                                                "2017 Revised \nE-commerce": "2017 Total ecommerce",
                                                "Percent Distribution\nof E-commerce Sales 2018": 'Percent Distribution of ecommerce 2018'
                                                 })
US_totalrev_df.head()
607/26:
# Removing first row
US_totalrev_df = US_totalrev_df.drop([0, 1])
607/27:
# Reset Index
US_totalrev_df = US_totalrev_df.reset_index(drop=True)
607/28: US_totalrev_df = US_totalrev_df.set_index("Industry")
607/29:
# Final DataFrame
# Need two separate dataframes for plotting
US_totalrev_df = US_totalrev_df.replace({'S': None})
US_totalrev_df
607/30:
# Rename columns
US_totalrev_df = US_totalrev_df.rename(columns={"2018 E-commerce": "2018 Total ecommerce",
                                                "2017 Revised \nTotal Sales": "2017 Total Sales",
                                                "2017 Revised \nE-commerce": "2017 Total ecommerce",
                                                "Percent Distribution\nof E-commerce Sales 2018": 'Percent Distribution of ecommerce 2018'
                                                 })
US_totalrev_df.head()
607/31:
# Removing first row
US_totalrev_df = US_totalrev_df.drop([0, 1])
607/32:
# Reset Index
US_totalrev_df = US_totalrev_df.reset_index(drop=True)
607/33:
# Removing first row
# US_totalrev_df = US_totalrev_df.drop([0, 1])
607/34:
# Reset Index
US_totalrev_df = US_totalrev_df.reset_index(drop=True)
607/35: US_totalrev_df = US_totalrev_df.set_index("Industry")
607/36:
# Final DataFrame
# Need two separate dataframes for plotting
US_totalrev_df = US_totalrev_df.replace({'S': None})
US_totalrev_df
608/1: %matplotlib notebook
608/2:
# Dependencies
import pandas as pd
import numpy as np
import os
import webbrowser
import io
import matplotlib
import matplotlib.pyplot as plt
608/3:
# Save file path to variable
retail_history_csv = "Datasets/EstUS_Retail_Trade_E-commercehistory.csv"
US_totalrev_2018_2017_csv = "Datasets/US_Retail_Trade_Sales_2018_2017.csv"
608/4:
# Read with pandas
retail_history_df = pd.read_csv(retail_history_csv)
retail_history_df.head()
608/5:
# Rename Columns
retail_history_df = retail_history_df.rename(columns={"Unnamed: 1": "Kind_of_Business",
                                 "2018": "2018 Total",
                                 "Unnamed: 3": "2018 Ecommerce Total",
                                 "2017r": "2017 Total",
                                 "Unnamed: 5": "2017 Ecommerce Total",
                                 "2016r": "2016 Total",
                                 "Unnamed: 7": "2016 Ecommerce Total",
                                 "2015r": "2015 Total",
                                 "Unnamed: 9": "2015 Ecommerce Total",
                                 "2014r": "2014 Total",
                                 "Unnamed: 11": "2014 Ecommerce Total",
                                 "1998": "1998 Total",
                                 "Unnamed: 43": "1998 Ecommerce"
                                 })
retail_history_df.head()
608/6:
# Drop columns we do not need
retail_history_df = retail_history_df.drop(columns=['NAICS Code',
                                                    '2013r',
                                                    'Unnamed: 13',
                                                    '2012r',
                                                    'Unnamed: 15',
                                                    '2011r',
                                                    'Unnamed: 17',
                                                    '2010',
                                                     'Unnamed: 19',
                                                     '2009',
                                                     'Unnamed: 21',
                                                     '2008',
                                                     'Unnamed: 23',
                                                     '2007',
                                                     'Unnamed: 25',
                                                     '2006',
                                                     'Unnamed: 27',
                                                     '2005',
                                                     'Unnamed: 29',
                                                     '2004',
                                                     'Unnamed: 31',
                                                     '2003',
                                                     'Unnamed: 33',
                                                     '2002',
                                                     'Unnamed: 35',
                                                     '2001',
                                                     'Unnamed: 37',
                                                     '2000',
                                                     'Unnamed: 39',
                                                     '1999',
                                                     'Unnamed: 41',
                                                    '2004', 
                                                    'Unnamed: 31',
                                                    '2003', 
                                                    'Unnamed: 33',
                                                    '2002',
                                                    'Unnamed: 35',
                                                    '2001',
                                                    'Unnamed: 37',
                                                    '2000',
                                                    'Unnamed: 39',
                                                    '1999',
                                                    'Unnamed: 41',
                                                   
                                                   ])
retail_history_df.head()
608/7:
retail_history_df
# D - Denotes an estimate withheld to avoid disclosing data of individual companies; data are included in higher-level totals.
# S - Estimate does not meet publication standards because of high sampling variability (coefficient of variation is greater than 30%), poor response quality (total quantity response rate is less than 50%), or other concerns about the estimate's quality. Unpublished estimates derived from this table by subtraction are subject to these same limitations and should not be attributed to the U.S. Census Bureau. For a description of publication standards and the total quantity response rate, see https://www.census.gov/about/policies/quality/standards/standardf1.html.
608/8:
# Drop rows that do not have ecommerce data
#[Estimates are shown in millions of dollars and are based on data from the Annual Retail Trade Survey. Estimates have been adjusted using final results of the 2012 Economic Census.] 
retail_history_df = retail_history_df.drop([0, 8])
608/9: retail_history_df = retail_history_df.reset_index(drop=True)
608/10: retail_history_df = retail_history_df.set_index('Kind_of_Business')
608/11:
# Replaced S & D, final DataFrame
retail_history_df = retail_history_df.replace({'S': '0',
                                               'D': '0',
                                              })
retail_history_df
608/12:
# remove commas from dataframe
retail_history_df.replace(',','', regex=True, inplace=True)
retail_history_df.head()
608/13:
# Separate by ecommerce
historyecom_df = retail_history_df.iloc[: , [1, 3, 5, 7, 9, 11]].copy() 
historyecom_df
608/14:
# Find top five product categories
topfive_ecom = historyecom_df.sort_values(by=['2018 Ecommerce Total'], ascending=False).head(6)
topfive_ecom
608/15:
# topfive_ecom = historyecom_df.nlargest(5, '2018 Ecommerce Total')
# topfive_ecom
608/16:
# Separate by total
historytotal_df = retail_history_df.iloc[: , [0, 1, 3, 5, 7, 9, 11]].copy()
historytotal_df
608/17:
# Plot
year = [2018, 2017, 2016, 2015, 2014, 1998]
totalrev_byproductcategory = {
    'Total Retail Trade': [5269468, 5053151, 4852958, 4725993, 4639440, 2581762],
    'Motor Vehicle & Parts': [1191321, 1172367, 1140614, 1094112, 1020851, 688415],
    'Furniture & Home Stores': [116895, 113035, 110404, 106570, 99718, 77412],
    'Electronics & Appliances': [100205, 98570, 99043, 103658, 105197, 82731],
    'Building Materials & Garden Equip': [381313, 365622, 348697, 331611, 318352, 202423],
    'Food & Beverage': [745736, 725137, 699349, 685381, 669165, 416525],
    'Health & Personal Care': [347454, 333338, 327153, 315244, 299263, 129583],
    'Clothing & Accessories': [268163, 260566, 260050, 255798, 250409, 149151],
    'Sporting Goods, Hobby, Musical Inst., & Books': [81179, 84188, 86483, 85701, 83787, 60441],
    'General Merchandise': [706298, 687123, 675389, 674889, 667163, 351081],
    'Misc. Store Retailers': [130130, 124538, 121572, 119350, 115878, 98369],
    'Nonstore Retailers': [696849, 629204, 561412, 509652, 470867, 133904],
    'Electronic Shopping & Mail-order Houses': [611743, 551298, 488840, 433987, 386065, 80297],
}

fig, ax = plt.subplots()
ax.stackplot(year, totalrev_byproductcategory.values(),
             labels=totalrev_byproductcategory.keys())
ax.legend(loc='upper right')
ax.set_title('Total Revenue by Product Category')
ax.set_xlabel('Year')
ax.set_ylabel('Total Sales (millions)')

plt.show()
plt.tight_layout()
plt.savefig("Total Revenue by Product Category")
608/18:
plt.plot(year, 'Total Retail Trade', data=totalrev_byproductcategory, color='blue', linewidth=2)
plt.plot(year, 'Motor Vehicle & Parts', data=totalrev_byproductcategory, color='olive', linewidth=2)
# plt.plot('x', 'y3', data=df, marker='', color='olive', linewidth=2, linestyle='dashed', label="toto")
plt.legend()
plt.show()
608/19: retail_history_df.head()
608/20:
# retail_history_df=retail_history_df.astype(float)
# retail_history_df = retail_history_df.set_index("Kind_of_Business").plot()
608/21:
US_totalrev_df = pd.read_csv(US_totalrev_2018_2017_csv)
US_totalrev_df.head()
608/22:
# Drop Columns
US_totalrev_df = US_totalrev_df.drop(columns=['Unnamed: 0'])
US_totalrev_df.head()
608/23:
# Rename columns
US_totalrev_df = US_totalrev_df.rename(columns={"2018 E-commerce": "2018 Total ecommerce",
                                                "2017 Revised \nTotal Sales": "2017 Total Sales",
                                                "2017 Revised \nE-commerce": "2017 Total ecommerce",
                                                "Percent Distribution\nof E-commerce Sales 2018": 'Percent Distribution of ecommerce 2018'
                                                 })
US_totalrev_df.head()
608/24:
# Removing first row
US_totalrev_df = US_totalrev_df.drop([0, 1])
608/25:
# Reset Index
US_totalrev_df = US_totalrev_df.reset_index(drop=True)
608/26: US_totalrev_df = US_totalrev_df.set_index("Industry")
608/27:
# Final DataFrame
# Need two separate dataframes for plotting
US_totalrev_df = US_totalrev_df.replace({'S': None})
US_totalrev_df
608/28:
# Totals df for plotting
# Non-store retail = do not have brick and mortar stores
total_plot_df = US_totalrev_df.iloc[: , [0, 1, 2, 3]].copy() 
total_plot_df
608/29:
totalrev_plot_df = pd.DataFrame({
    "2018 Total Sales":[1191321, 116895, 100205, 381313, 745736, 347454, 503925, 268163, 81179, 706298, 130130, 696849, 611743],
    "2018 Total Ecommerce Sales":[35348, 1533, 2056, 2795, 4837, 744, 0, 10981, 2617, 0, 0, 453584, 451635],
    "2017 Total Sales":[1172367, 113035, 98570, 365622, 725137, 333338, 459463, 260566, 84188, 687123, 124538, 629204, 551298],
    "2017 Total Ecommerce Sales":[32106, 1279, 2113, 2614, 3431, 0, 0, 9374, 2441, 0, 4014, 400279, 398415]
    }, 
    index=["Vehicle", "Furniture", 
           "Electronics", "Building material/Garden equip.",
          "Food & beverage", "Health & Personal", "Gasoline stations", "Clothing",
           "Sporting goods, hobby, musical instrument, and book stores", "General merchandise stores", "Misc. store retailers",
           "Nonstore retailers", "E-shopping and mail-order houses"
          ]
)
totalrev_plot_df.head()
608/30:
totalsales2018 = [1191321, 116895, 100205, 381313, 745736, 347454, 503925, 268163, 81179, 706298, 130130, 696849, 611743]
totalecom2018 = [35348, 1533, 2056, 2795, 4837, 744, 0, 10981, 2617, 0, 0, 453584, 451635]
totalsales2017 = [1172367, 113035, 98570, 365622, 725137, 333338, 459463, 260566, 84188, 687123, 124538, 629204, 551298]
totalecom2017 = [32106, 1279, 2113, 2614, 3431, 0, 0, 9374, 2441, 0, 4014, 400279, 398415]
index = ["Vehicle", "Furniture", "Electronics", "Building material/Garden equip.",
         "Food & beverage", "Health & Personal", "Gasoline stations", "Clothing", 
         "Sporting goods, hobby, musical instrument, and book stores", "General merchandise stores", 
         "Misc. store retailers", "Nonstore retailers", "E-shopping and mail-order houses"]
totalrev_plot_df = pd.DataFrame({'Total Sales 2018': totalsales2018,
                                 'Total Ecommerce Sales 2018': totalecom2018,
                                 'Total Sales 2017': totalsales2017,
                                 'Total Ecommerce Sales 2017': totalecom2017
                                }, index=index)
608/31:
# Plot
plt.figure(figsize=(10,8))
totalrev_plot_df.plot(kind="bar", stacked=True)
plt.title("Retail Totals - 2018 & 2017")
plt.xlabel("Industry")
plt.ylabel("Sales in Millions ($)")

labels = []

plt.show()
plt.savefig("Retail Totals, 2018 & 2017.png")
608/32:
# Plot
plt.figure(figsize=(10,2))
totalrev_plot_df.plot(kind="barh", stacked=True)
plt.title("Retail Totals - 2018 & 2017")
plt.xlabel("Industry")
plt.ylabel("Sales in Millions ($)")

# Set xticks and lables
# xticks = 
# xticks(np.arange(0, 1, step=0.2)) 


plt.show()
plt.tight_layout()
plt.savefig("Retail Totals Stacked, 2018 & 2017.png")
608/33: # Line Chart
608/34:
# percent change dataframe
percent_plot_df = US_totalrev_df.iloc[: , [4, 5, 6, 7, 8]].copy() 
percent_plot_df
608/35:
plt.figure(figsize=(10,2))
percent_plot_df.plot(kind="line")
plt.title("Retail Totals - 2018 & 2017")
plt.xlabel("Percent y/y change")
plt.ylabel("Industry")

plt.savefig("Retail Totals Stacked, 2018 & 2017.png")
608/36:
# Final DataFrame
# Need two separate dataframes for plotting
US_totalrev_df = US_totalrev_df.replace({'S': '0'})
US_totalrev_df
608/37:
# Remove commas from the data
# remove commas from dataframe
US_totalrev_df.replace(',','', regex=True, inplace=True)
US_totalrev_df.head()
608/38:
# Removing first row
US_totalrev_df = US_totalrev_df.drop([0, 1])
608/39:
# Reset Index
US_totalrev_df = US_totalrev_df.reset_index(drop=True)
609/1: %matplotlib notebook
609/2:
# Dependencies
import pandas as pd
import numpy as np
import os
import webbrowser
import io
import matplotlib
import matplotlib.pyplot as plt
609/3:
# Save file path to variable
retail_history_csv = "Datasets/EstUS_Retail_Trade_E-commercehistory.csv"
US_totalrev_2018_2017_csv = "Datasets/US_Retail_Trade_Sales_2018_2017.csv"
609/4:
# Read with pandas
retail_history_df = pd.read_csv(retail_history_csv)
retail_history_df.head()
609/5:
# Rename Columns
retail_history_df = retail_history_df.rename(columns={"Unnamed: 1": "Kind_of_Business",
                                 "2018": "2018 Total",
                                 "Unnamed: 3": "2018 Ecommerce Total",
                                 "2017r": "2017 Total",
                                 "Unnamed: 5": "2017 Ecommerce Total",
                                 "2016r": "2016 Total",
                                 "Unnamed: 7": "2016 Ecommerce Total",
                                 "2015r": "2015 Total",
                                 "Unnamed: 9": "2015 Ecommerce Total",
                                 "2014r": "2014 Total",
                                 "Unnamed: 11": "2014 Ecommerce Total",
                                 "1998": "1998 Total",
                                 "Unnamed: 43": "1998 Ecommerce"
                                 })
retail_history_df.head()
609/6:
# Drop columns we do not need
retail_history_df = retail_history_df.drop(columns=['NAICS Code',
                                                    '2013r',
                                                    'Unnamed: 13',
                                                    '2012r',
                                                    'Unnamed: 15',
                                                    '2011r',
                                                    'Unnamed: 17',
                                                    '2010',
                                                     'Unnamed: 19',
                                                     '2009',
                                                     'Unnamed: 21',
                                                     '2008',
                                                     'Unnamed: 23',
                                                     '2007',
                                                     'Unnamed: 25',
                                                     '2006',
                                                     'Unnamed: 27',
                                                     '2005',
                                                     'Unnamed: 29',
                                                     '2004',
                                                     'Unnamed: 31',
                                                     '2003',
                                                     'Unnamed: 33',
                                                     '2002',
                                                     'Unnamed: 35',
                                                     '2001',
                                                     'Unnamed: 37',
                                                     '2000',
                                                     'Unnamed: 39',
                                                     '1999',
                                                     'Unnamed: 41',
                                                    '2004', 
                                                    'Unnamed: 31',
                                                    '2003', 
                                                    'Unnamed: 33',
                                                    '2002',
                                                    'Unnamed: 35',
                                                    '2001',
                                                    'Unnamed: 37',
                                                    '2000',
                                                    'Unnamed: 39',
                                                    '1999',
                                                    'Unnamed: 41',
                                                   
                                                   ])
retail_history_df.head()
609/7:
retail_history_df
# D - Denotes an estimate withheld to avoid disclosing data of individual companies; data are included in higher-level totals.
# S - Estimate does not meet publication standards because of high sampling variability (coefficient of variation is greater than 30%), poor response quality (total quantity response rate is less than 50%), or other concerns about the estimate's quality. Unpublished estimates derived from this table by subtraction are subject to these same limitations and should not be attributed to the U.S. Census Bureau. For a description of publication standards and the total quantity response rate, see https://www.census.gov/about/policies/quality/standards/standardf1.html.
609/8:
# Drop rows that do not have ecommerce data
#[Estimates are shown in millions of dollars and are based on data from the Annual Retail Trade Survey. Estimates have been adjusted using final results of the 2012 Economic Census.] 
retail_history_df = retail_history_df.drop([0, 8])
609/9: retail_history_df = retail_history_df.reset_index(drop=True)
609/10: retail_history_df = retail_history_df.set_index('Kind_of_Business')
609/11:
# Replaced S & D, final DataFrame
retail_history_df = retail_history_df.replace({'S': '0',
                                               'D': '0',
                                              })
retail_history_df
609/12:
# remove commas from dataframe
retail_history_df.replace(',','', regex=True, inplace=True)
retail_history_df.head()
609/13:
# Separate by ecommerce
historyecom_df = retail_history_df.iloc[: , [1, 3, 5, 7, 9, 11]].copy() 
historyecom_df
609/14:
# Find top five product categories
topfive_ecom = historyecom_df.sort_values(by=['2018 Ecommerce Total'], ascending=False).head(6)
topfive_ecom
609/15:
# topfive_ecom = historyecom_df.nlargest(5, '2018 Ecommerce Total')
# topfive_ecom
609/16:
# Separate by total
historytotal_df = retail_history_df.iloc[: , [0, 1, 3, 5, 7, 9, 11]].copy()
historytotal_df
609/17:
# Plot
year = [2018, 2017, 2016, 2015, 2014, 1998]
totalrev_byproductcategory = {
    'Total Retail Trade': [5269468, 5053151, 4852958, 4725993, 4639440, 2581762],
    'Motor Vehicle & Parts': [1191321, 1172367, 1140614, 1094112, 1020851, 688415],
    'Furniture & Home Stores': [116895, 113035, 110404, 106570, 99718, 77412],
    'Electronics & Appliances': [100205, 98570, 99043, 103658, 105197, 82731],
    'Building Materials & Garden Equip': [381313, 365622, 348697, 331611, 318352, 202423],
    'Food & Beverage': [745736, 725137, 699349, 685381, 669165, 416525],
    'Health & Personal Care': [347454, 333338, 327153, 315244, 299263, 129583],
    'Clothing & Accessories': [268163, 260566, 260050, 255798, 250409, 149151],
    'Sporting Goods, Hobby, Musical Inst., & Books': [81179, 84188, 86483, 85701, 83787, 60441],
    'General Merchandise': [706298, 687123, 675389, 674889, 667163, 351081],
    'Misc. Store Retailers': [130130, 124538, 121572, 119350, 115878, 98369],
    'Nonstore Retailers': [696849, 629204, 561412, 509652, 470867, 133904],
    'Electronic Shopping & Mail-order Houses': [611743, 551298, 488840, 433987, 386065, 80297],
}

fig, ax = plt.subplots()
ax.stackplot(year, totalrev_byproductcategory.values(),
             labels=totalrev_byproductcategory.keys())
ax.legend(loc='upper right')
ax.set_title('Total Revenue by Product Category')
ax.set_xlabel('Year')
ax.set_ylabel('Total Sales (millions)')

plt.show()
plt.tight_layout()
plt.savefig("Total Revenue by Product Category")
609/18:
plt.plot(year, 'Total Retail Trade', data=totalrev_byproductcategory, color='blue', linewidth=2)
plt.plot(year, 'Motor Vehicle & Parts', data=totalrev_byproductcategory, color='olive', linewidth=2)
# plt.plot('x', 'y3', data=df, marker='', color='olive', linewidth=2, linestyle='dashed', label="toto")
plt.legend()
plt.show()
609/19: retail_history_df.head()
609/20:
# retail_history_df=retail_history_df.astype(float)
# retail_history_df = retail_history_df.set_index("Kind_of_Business").plot()
609/21:
US_totalrev_df = pd.read_csv(US_totalrev_2018_2017_csv)
US_totalrev_df.head()
609/22:
# Drop Columns
US_totalrev_df = US_totalrev_df.drop(columns=['Unnamed: 0'])
US_totalrev_df.head()
609/23:
# Rename columns
US_totalrev_df = US_totalrev_df.rename(columns={"2018 E-commerce": "2018 Total ecommerce",
                                                "2017 Revised \nTotal Sales": "2017 Total Sales",
                                                "2017 Revised \nE-commerce": "2017 Total ecommerce",
                                                "Percent Distribution\nof E-commerce Sales 2018": 'Percent Distribution of ecommerce 2018'
                                                 })
US_totalrev_df.head()
609/24:
# Remove commas from the data
# remove commas from dataframe
US_totalrev_df.replace(',','', regex=True, inplace=True)
US_totalrev_df.head()
609/25:
# Removing first row
US_totalrev_df = US_totalrev_df.drop([0, 1])
609/26:
# Reset Index
US_totalrev_df = US_totalrev_df.reset_index(drop=True)
609/27: US_totalrev_df = US_totalrev_df.set_index("Industry")
609/28:
# Final DataFrame
# Need two separate dataframes for plotting
US_totalrev_df = US_totalrev_df.replace({'S': '0'})
US_totalrev_df
609/29:
# Totals df for plotting
# Non-store retail = do not have brick and mortar stores
total_plot_df = US_totalrev_df.iloc[: , [0, 1, 2, 3]].copy() 
total_plot_df
609/30:
totalrev_plot_df = pd.DataFrame({
    "2018 Total Sales":[1191321, 116895, 100205, 381313, 745736, 347454, 503925, 268163, 81179, 706298, 130130, 696849, 611743],
    "2018 Total Ecommerce Sales":[35348, 1533, 2056, 2795, 4837, 744, 0, 10981, 2617, 0, 0, 453584, 451635],
    "2017 Total Sales":[1172367, 113035, 98570, 365622, 725137, 333338, 459463, 260566, 84188, 687123, 124538, 629204, 551298],
    "2017 Total Ecommerce Sales":[32106, 1279, 2113, 2614, 3431, 0, 0, 9374, 2441, 0, 4014, 400279, 398415]
    }, 
    index=["Vehicle", "Furniture", 
           "Electronics", "Building material/Garden equip.",
          "Food & beverage", "Health & Personal", "Gasoline stations", "Clothing",
           "Sporting goods, hobby, musical instrument, and book stores", "General merchandise stores", "Misc. store retailers",
           "Nonstore retailers", "E-shopping and mail-order houses"
          ]
)
totalrev_plot_df.head()
609/31:
totalsales2018 = [1191321, 116895, 100205, 381313, 745736, 347454, 503925, 268163, 81179, 706298, 130130, 696849, 611743]
totalecom2018 = [35348, 1533, 2056, 2795, 4837, 744, 0, 10981, 2617, 0, 0, 453584, 451635]
totalsales2017 = [1172367, 113035, 98570, 365622, 725137, 333338, 459463, 260566, 84188, 687123, 124538, 629204, 551298]
totalecom2017 = [32106, 1279, 2113, 2614, 3431, 0, 0, 9374, 2441, 0, 4014, 400279, 398415]
index = ["Vehicle", "Furniture", "Electronics", "Building material/Garden equip.",
         "Food & beverage", "Health & Personal", "Gasoline stations", "Clothing", 
         "Sporting goods, hobby, musical instrument, and book stores", "General merchandise stores", 
         "Misc. store retailers", "Nonstore retailers", "E-shopping and mail-order houses"]
totalrev_plot_df = pd.DataFrame({'Total Sales 2018': totalsales2018,
                                 'Total Ecommerce Sales 2018': totalecom2018,
                                 'Total Sales 2017': totalsales2017,
                                 'Total Ecommerce Sales 2017': totalecom2017
                                }, index=index)
609/32:
# Plot
plt.figure(figsize=(10,8))
totalrev_plot_df.plot(kind="bar", stacked=True)
plt.title("Retail Totals - 2018 & 2017")
plt.xlabel("Industry")
plt.ylabel("Sales in Millions ($)")

labels = []

plt.show()
plt.savefig("Retail Totals, 2018 & 2017.png")
609/33:
# Plot
plt.figure(figsize=(10,2))
totalrev_plot_df.plot(kind="barh", stacked=True)
plt.title("Retail Totals - 2018 & 2017")
plt.xlabel("Industry")
plt.ylabel("Sales in Millions ($)")

# Set xticks and lables
# xticks = 
# xticks(np.arange(0, 1, step=0.2)) 


plt.show()
plt.tight_layout()
plt.savefig("Retail Totals Stacked, 2018 & 2017.png")
609/34: # Line Chart
609/35:
# percent change dataframe
percent_plot_df = US_totalrev_df.iloc[: , [4, 5, 6, 7, 8]].copy() 
percent_plot_df
609/36:
plt.figure(figsize=(10,2))
percent_plot_df.plot(kind="line")
plt.title("Retail Totals - 2018 & 2017")
plt.xlabel("Percent y/y change")
plt.ylabel("Industry")

plt.savefig("Retail Totals Stacked, 2018 & 2017.png")
609/37:
totalrev_plot_df = pd.DataFrame({
    "2018 Total Sales":[1191321, 116895, 100205, 381313, 745736, 347454, 503925, 268163, 81179, 706298, 130130, 696849, 611743],
    "2018 Total Ecommerce Sales":[35348, 1533, 2056, 2795, 4837, 744, 0, 10981, 2617, 0, 0, 453584, 451635],
    "2017 Total Sales":[1172367, 113035, 98570, 365622, 725137, 333338, 459463, 260566, 84188, 687123, 124538, 629204, 551298],
    "2017 Total Ecommerce Sales":[32106, 1279, 2113, 2614, 3431, 0, 0, 9374, 2441, 0, 4014, 400279, 398415]
    }, 
    index=["Vehicle", "Furniture", 
           "Electronics", "Building material/Garden equip.",
          "Food & beverage", "Health & Personal", "Gasoline stations", "Clothing",
           "Sporting goods, hobby, musical instrument, and book stores", "General merchandise stores", "Misc. store retailers",
           "Nonstore retailers", "E-shopping and mail-order houses"
          ]
)
totalrev_plot_df
609/38:
# top five 2018 data
topfive_2018_df = pd.DataFrame({ '2018 Ecommerce Sales': [453584, 451635, 35348, 10981, 4837]},
                               index=['Nonstore  Retailers', 'E-Shopping & Mail-Order Houses', 'Vehicle & Accessories',
                                     'Clothing', 'Food & Beverage'])
topfive_2018_df
609/39: topfive_2018_df.plot()
609/40: topfive_2018_df.plt.bar()
609/41: topfive_2018_df.plot.bar()
609/42:
# set x-axis 
x_axis = np.arange(len(topfive_2018_df))
tick_locations = [value+0.4 for value in x_axis]
609/43:
plt.figure(figsize=(10,3))
plt.bar(x_axis, topfive_2018_df["2018 Ecommerce Sales"], color='b', alpha=0.5, align="edge")
plt.xticks(tick_locations, topfive_2018_df[index], rotation="vertical")
609/44:
plt.figure(figsize=(10,3))
plt.bar(x_axis, topfive_2018_df["2018 Ecommerce Sales"], color='b', alpha=0.5, align="edge")
plt.xticks(tick_locations, topfive_2018_df["index"], rotation="vertical")
609/45:
plt.figure(figsize=(10,3))
plt.bar(x_axis, topfive_2018_df["2018 Ecommerce Sales"], color='b', alpha=0.5, align="edge")
plt.xticks(tick_locations, topfive_2018_df[index], rotation="vertical")
609/46:
plt.figure(figsize=(10,3))
plt.bar(x_axis, topfive_2018_df["2018 Ecommerce Sales"], color='b', alpha=0.5, align="edge")
plt.xticks(tick_locations, topfive_2018_df[index], rotation="vertical")

plt.title("Top Five Product Categories, 2018")
plt.xlabel("Product Category")
plt.ylabel("Total Sales in millions($)")
609/47:
# top five 2018 data
topfive_2018_df = pd.DataFrame({ '2018 Ecommerce Sales': [453584, 451635, 35348, 10981, 4837]},
                               {'Nonstore  Retailers', 'E-Shopping & Mail-Order Houses', 'Vehicle & Accessories',
                                     'Clothing', 'Food & Beverage'})
topfive_2018_df
609/48:
# top five 2018 data
topfive_2018_df = pd.DataFrame({ '2018 Ecommerce Sales': [453584, 451635, 35348, 10981, 4837]},
                               index=['Nonstore  Retailers', 'E-Shopping & Mail-Order Houses', 'Vehicle & Accessories',
                                     'Clothing', 'Food & Beverage'])
topfive_2018_df
609/49:
# plt.figure(figsize=(10,3))
# plt.bar(x_axis, topfive_2018_df["2018 Ecommerce Sales"], color='b', alpha=0.5, align="edge")
# plt.xticks(tick_locations, topfive_2018_df[index], rotation="vertical")

# plt.title("Top Five Product Categories, 2018")
# plt.xlabel("Product Category")
# plt.ylabel("Total Sales in millions($)")
609/50: topfive_2018_df.plot.bar()
609/51: topfive_2018_df.plot.bar(x=index, y='2018 Ecommerce Sales', rot=0)
609/52: topfive_2018_df.plot.bar()
609/53:
fig, ax = plt.subplots(figsize=(10,7))
ax.barh(topfive_2018.iloc[:,0], topfive_2018.iloc[:,1])
plt.show()
609/54:
fig, ax = plt.subplots(figsize=(10,7))
ax.barh(topfive_2018_df.iloc[:,0], topfive_2018_df.iloc[:,1])
plt.show()
609/55:
topfive_2018_df['2018 Total Ecommerce'].plot(kind="bar", title="test")
# Rotate the x-labels by 30 degrees, and keep the text aligned horizontally
plt.xticks(rotation=30, horizontalalignment="center")
plt.title("Mince Pie Consumption Study Results")
plt.xlabel("Family Member")
plt.ylabel("Pies Consumed")
609/56:
topfive_2018_df['2018 Ecommerce Sales'].plot(kind="bar", title="test")
# Rotate the x-labels by 30 degrees, and keep the text aligned horizontally
plt.xticks(rotation=30, horizontalalignment="center")
plt.title("Mince Pie Consumption Study Results")
plt.xlabel("Family Member")
plt.ylabel("Pies Consumed")
609/57:
topfive_2018_df['2018 Ecommerce Sales'].plot(kind="bar", title="test")
# Rotate the x-labels by 30 degrees, and keep the text aligned horizontally
plt.xticks(rotation=30, horizontalalignment="center")
plt.title("Mince Pie Consumption Study Results")
plt.xlabel("Family Member")
plt.ylabel("Pies Consumed")

plt.show()
609/58:
topfive_2018_df['2018 Ecommerce Sales'].plot(kind="bar", title="test")
# Rotate the x-labels by 30 degrees, and keep the text aligned horizontally
plt.xticks(rotation=30, horizontalalignment="center")
plt.title("Mince Pie Consumption Study Results")
plt.xlabel("Family Member")
plt.ylabel("Pies Consumed")

plt.show()
609/59: topfive_2018_df.plot(kind='bar', color='p').legend(loc='best')
609/60: topfive_2018_df.plot(kind="bar")
609/61: topfive_2018_df.plot(kind="bar", title="Top Five Product Categories, 2018")
609/62:
topfive_2018_df.plot(kind="bar", title="Top Five Product Categories, 2018")
plt.xlabel = "Product Category"
plt.ylabel = "Total Sales (Millions)"
609/63:
plt.figure(figsize=(10,10))
topfive_2018_df.plot(kind="bar", title="Top Five Product Categories, 2018")
plt.xlabel = "Product Category"
plt.ylabel = "Total Sales (Millions)"
609/64:
topfive_2018_df.plot(kind="bar", title="Top Five Product Categories, 2018", figsize=(10,10))
plt.xlabel = "Product Category"
plt.ylabel = "Total Sales (Millions)"
609/65:
topfive_2018_df.plot(kind="bar", title="Top Five Product Categories, 2018", figsize=(10,20))
plt.xlabel = "Product Category"
plt.ylabel = "Total Sales (Millions)"
609/66:
topfive_2018_df.plot(kind="bar", title="Top Five Product Categories, 2018", figsize=(10,30))
plt.xlabel = "Product Category"
plt.ylabel = "Total Sales (Millions)"
609/67:
# top five 2018 data
topfive_2018_df = pd.DataFrame({ '2018 Ecommerce Sales': [453584, 451635, 35348, 10981, 4837]},
                               index=['Nonstore  Retailers', 'E-Shopping & Mail-Order', 'Vehicle & Accessories',
                                     'Clothing', 'Food & Beverage'])
topfive_2018_df
609/68:
# set x-axis 
x_axis = np.arange(len(topfive_2018_df))
tick_locations = [value+0.4 for value in x_axis]
609/69:
# plt.figure(figsize=(10,3))
# plt.bar(x_axis, topfive_2018_df["2018 Ecommerce Sales"], color='b', alpha=0.5, align="edge")
# plt.xticks(tick_locations, topfive_2018_df[index], rotation="vertical")

# plt.title("Top Five Product Categories, 2018")
# plt.xlabel("Product Category")
# plt.ylabel("Total Sales in millions($)")
609/70:
topfive_2018_df.plot(kind="bar", title="Top Five Product Categories, 2018", figsize=(20,30))
plt.xlabel = "Product Category"
plt.ylabel = "Total Sales (Millions)"
609/71:
topfive_2018_df.plot(kind="bar", title="Top Five Product Categories, 2018", figsize=(20,30))
plt.xlabel = "Product Category"
plt.ylabel = "Total Sales (Millions)"
plt.savefig("Top Five Product Categories, 2018.png")
609/72:
# top five 2017 data
topfive_2017_df = pd.DataFrame({'2017 Ecommerce Sales': [400279, 398415, 32106, 9374, 4014]},
                               index=['Nonstore  Retailers', 'E-Shopping & Mail-Order', 'Vehicle & Accessories',
                                     'Clothing', 'Misc. store retailers'])
topfive_2017_df
609/73:
# top five 2017 plot
topfive_2017_df.plot(kind="bar", title="Top Five Product Categories, 2017", figsize=(20,30))
plt.xlabel = "Product Category"
plt.ylabel = "Total Sales (Millions)"
plt.savefig("Top Five Product Categories, 2017.png")
609/74:
# top five 2017 plot
topfive_2017_df.plot(kind="bar", color='r', title="Top Five Product Categories, 2017", figsize=(20,30))
plt.xlabel = "Product Category"
plt.ylabel = "Total Sales (Millions)"
plt.savefig("Top Five Product Categories, 2017.png")
609/75:
# top five 2017 plot
topfive_2017_df.plot(kind="bar", color='r', title="Top Five Product Categories, 2017", figsize=(30,30))
plt.xlabel = "Product Category"
plt.ylabel = "Total Sales (Millions)"
plt.savefig("Top Five Product Categories, 2017.png")
609/76:
# top five 2017 plot
topfive_2017_df.plot(kind="bar", color='r', title="Top Five Product Categories, 2017", figsize=(30,30))
plt.xlabel = "Product Category"
plt.ylabel = "Total Sales (Millions)"
plt.savefig("Top Five Product Categories, 2017.png")
plt.tight_layout()
609/77:
# top five 2017 plot
topfive_2017_df.plot(kind="bar", color='r', title="Top Five Product Categories, 2017", figsize=(10,30))
plt.xlabel = "Product Category"
plt.ylabel = "Total Sales (Millions)"
plt.savefig("Top Five Product Categories, 2017.png")
plt.tight_layout()
609/78:
# top five 2017 plot
topfive_2017_df.plot(kind="bar", color='r', title="Top Five Product Categories, 2017", figsize=(10,10))
plt.xlabel = "Product Category"
plt.ylabel = "Total Sales (Millions)"
plt.savefig("Top Five Product Categories, 2017.png")
plt.tight_layout()
609/79:
topfive_2018_df.plot(kind="bar", title="Top Five Product Categories, 2018", figsize=(10,10))
plt.xlabel = "Product Category"
plt.ylabel = "Total Sales (Millions)"
plt.savefig("Top Five Product Categories, 2018.png")
plt.tight_layout()
609/80: retail_history_df
615/1:
# Read with pandas
IN_Data_df = pd.read_csv(IN_Data_csv)
IN_Data_df.head()
615/2:
# Dependencies
import pandas as pd
import numpy as np
import os
import webbrowser
import io
import matplotlib
import matplotlib.pyplot as plt
615/3: IN_Data_csv = "Datasets/Indiana Retail Trade Data.csv"
615/4:
# Read with pandas
IN_Data_df = pd.read_csv(IN_Data_csv)
IN_Data_df.head()
615/5:
# Delete extraneous column
IN_Data_df = IN_Data_df.drop(columns=['Geo Footnote', '2017 NAICS Footnote', 'id', '2017 NAICS code', 
                            'Annual payroll ($1,000)', 'First-quarter payroll ($1,000)', 
                            'Number of employees', 'Range indicating percent of total annual payroll imputed', 
                            'Range indicating percent of total employees imputed', 
                           ])


IN_Data_df.head()
616/1:
# Dependencies
import pandas as pd
import numpy as np
import os
import webbrowser
import io
import matplotlib
import matplotlib.pyplot as plt
616/2: IN_Data_csv = "Datasets/Indiana Retail Trade Data.csv"
616/3:
# Read with pandas
IN_Data_df = pd.read_csv(IN_Data_csv)
IN_Data_df.head()
616/4:
# Delete extraneous column
IN_Data_df = IN_Data_df.drop(columns=['Geo Footnote', '2017 NAICS Footnote', 'id', '2017 NAICS code', 
                            'Annual payroll ($1,000)', 'First-quarter payroll ($1,000)', 
                            'Number of employees', 'Range indicating percent of total annual payroll imputed', 
                            'Range indicating percent of total employees imputed', 
                           ])


IN_Data_df.head()
616/5:
IN_Data_df.loc[IN_Data_df['Meaning of NAICS code']=='Electronic shopping and mail-order houses', 'eCommerce?']= 1
IN_Data_df.loc[IN_Data_df['Meaning of NAICS code']!='Electronic shopping and mail-order houses', 'eCommerce?']= 0

IN_Data_df.head()
616/6: IN_Data_df.groupby('eCommerce?')['Sales, value of shipments, or revenue ($1,000)'].sum()
616/7:
#Create Pie Chart
labels = "eCommerce", "Retail"
value = [257001507, 2717305378]
colors = ["purple", "grey"]
explode = (0.3,0)
616/8:
# Create a pie chart based upon the above data
plt.pie(value, labels=labels, colors=colors, explode=explode,
        autopct="%1.1f%%", shadow=True, startangle=180)
plt.title("eCommerce vs Retail 2017")

# Create axes which are equal so we have a perfect circle
plt.axis("equal")
plt.show()
#plt.savefig("../Images/PyPies.png")
616/9:
# Create a pie chart based upon the above data
plt.pie(value, labels=labels, colors=colors, explode=explode,
        autopct="%1.1f%%", shadow=True, startangle=180)
plt.title("eCommerce vs Retail 2017, Indiana")

# Create axes which are equal so we have a perfect circle
plt.axis("equal")
plt.show()
#plt.savefig("../Images/PyPies.png")
616/10:
# Create a pie chart based upon the above data
plt.pie(value, labels=labels, colors='r', explode=explode,
        autopct="%1.1f%%", shadow=True, startangle=180)
plt.title("eCommerce vs Retail 2017, Indiana")

# Create axes which are equal so we have a perfect circle
plt.axis("equal")
plt.show()
#plt.savefig("../Images/PyPies.png")
616/11:
# Create a pie chart based upon the above data
plt.pie(value, labels=labels, colors='r','b', explode=explode,
        autopct="%1.1f%%", shadow=True, startangle=180)
plt.title("eCommerce vs Retail 2017, Indiana")

# Create axes which are equal so we have a perfect circle
plt.axis("equal")
plt.show()
#plt.savefig("../Images/PyPies.png")
616/12:
# Create a pie chart based upon the above data
plt.pie(value, labels=labels, colors='r, b', explode=explode,
        autopct="%1.1f%%", shadow=True, startangle=180)
plt.title("eCommerce vs Retail 2017, Indiana")

# Create axes which are equal so we have a perfect circle
plt.axis("equal")
plt.show()
#plt.savefig("../Images/PyPies.png")
616/13:
# Create a pie chart based upon the above data
plt.pie(value, labels=labels, colors=colors, explode=explode,
        autopct="%1.1f%%", shadow=True, startangle=180)
plt.title("eCommerce vs Retail 2017, Indiana")

# Create axes which are equal so we have a perfect circle
plt.axis("equal")
plt.show()
#plt.savefig("../Images/PyPies.png")
616/14:
#Create Pie Chart
labels = "eCommerce", "Retail"
value = [257001507, 2717305378]
colors = ["purple", "blue"]
explode = (0.3,0)
616/15:
# Create a pie chart based upon the above data
plt.pie(value, labels=labels, colors=colors, explode=explode,
        autopct="%1.1f%%", shadow=True, startangle=180)
plt.title("eCommerce vs Retail 2017, Indiana")

# Create axes which are equal so we have a perfect circle
plt.axis("equal")
plt.show()
#plt.savefig("../Images/PyPies.png")
616/16:
IN_Data_df_df.loc[CA_df['Meaning of NAICS code']=='Electronic shopping and mail-order houses', 'eCommerce?']= 1
IN_Data_df_df.loc[CA_df['Meaning of NAICS code']!='Electronic shopping and mail-order houses', 'eCommerce?']= 0

IN_Data_df_df.head()
616/17:
IN_Data_df_df.loc[CA_df['Meaning of NAICS code']=='Electronic shopping and mail-order houses', 'eCommerce?']= 1
IN_Data_df_df.loc[CA_df['Meaning of NAICS code']!='Electronic shopping and mail-order houses', 'eCommerce?']= 0

IN_Data_df.head()
616/18:
IN_Data_df_df.loc[IN_Data_df['Meaning of NAICS code']=='Electronic shopping and mail-order houses', 'eCommerce?']= 1
IN_Data_df_df.loc[IN_Data_df['Meaning of NAICS code']!='Electronic shopping and mail-order houses', 'eCommerce?']= 0

IN_Data_df.head()
616/19:
IN_Data_df.loc[IN_Data_df['Meaning of NAICS code']=='Electronic shopping and mail-order houses', 'eCommerce?']= 1
IN_Data_df.loc[IN_Data_df['Meaning of NAICS code']!='Electronic shopping and mail-order houses', 'eCommerce?']= 0

IN_Data_df.head()
616/20: CA_df.groupby('eCommerce?')['Number of establishments'].sum()
616/21: CA_df.groupby('eCommerce?')['Number of establishments'].sum()
616/22: IN_Data_df.groupby('eCommerce?')['Number of establishments'].sum()
616/23:
Sum_Data = IN_Data_df.groupby('eCommerce?')['Sales, value of shipments, or revenue ($1,000)']
Sum_Data.sum()
616/24: IN_Data_df.groupby('eCommerce?')['Sales, value of shipments, or revenue ($1,000)'].sum()
616/25: IN_Data_df.astype(float)
616/26: IN_Data_df['Sales, value of shipments, or revenue ($1,000)'] = IN_Data_df['Sales, value of shipments, or revenue ($1,000)'].astype(float)
616/27:
# Delete extraneous column
IN_Data_df = IN_Data_df.drop(columns=['Geo Footnote', '2017 NAICS Footnote', 'id', '2017 NAICS code', 
                            'Annual payroll ($1,000)', 'First-quarter payroll ($1,000)', 
                            'Number of employees', 'Range indicating percent of total annual payroll imputed', 
                            'Range indicating percent of total employees imputed', 
                           ])


IN_Data_df
617/1:
# Dependencies
import pandas as pd
import numpy as np
import os
import webbrowser
import io
import matplotlib
import matplotlib.pyplot as plt
617/2: IN_Data_csv = "Datasets/Indiana Retail Trade Data.csv"
617/3:
# Read with pandas
IN_Data_df = pd.read_csv(IN_Data_csv)
IN_Data_df.head()
617/4:
# Delete extraneous column
IN_Data_df = IN_Data_df.drop(columns=['Geo Footnote', '2017 NAICS Footnote', 'id', '2017 NAICS code', 
                            'Annual payroll ($1,000)', 'First-quarter payroll ($1,000)', 
                            'Number of employees', 'Range indicating percent of total annual payroll imputed', 
                            'Range indicating percent of total employees imputed', 
                           ])


IN_Data_df
617/5:
US_totalrev_df = US_totalrev_df.replace({'S': '0'})
US_totalrev_df
617/6:
IN_Data_df = US_totalrev_df.replace({'S': '0'})
IN_Data_df
617/7:
IN_Data_df = IN_Data_df.replace({'S': '0'})
IN_Data_df
617/8:
IN_Data_df.loc[IN_Data_df['Meaning of NAICS code']=='Electronic shopping and mail-order houses', 'eCommerce?']= 1
IN_Data_df.loc[IN_Data_df['Meaning of NAICS code']!='Electronic shopping and mail-order houses', 'eCommerce?']= 0

IN_Data_df.head()
617/9: IN_Data_df.groupby('eCommerce?')['Sales, value of shipments, or revenue ($1,000)'].sum()
617/10: IN_Data_df['Sales, value of shipments, or revenue ($1,000)'] = IN_Data_df['Sales, value of shipments, or revenue ($1,000)'].astype(float)
617/11:
IN_Data_df = IN_Data_df.replace({'D': '0'})
IN_Data_df
617/12:
IN_Data_df.loc[IN_Data_df['Meaning of NAICS code']=='Electronic shopping and mail-order houses', 'eCommerce?']= 1
IN_Data_df.loc[IN_Data_df['Meaning of NAICS code']!='Electronic shopping and mail-order houses', 'eCommerce?']= 0

IN_Data_df.head()
617/13: IN_Data_df['Sales, value of shipments, or revenue ($1,000)'] = IN_Data_df['Sales, value of shipments, or revenue ($1,000)'].astype(float)
617/14: IN_Data_df.groupby('eCommerce?')['Sales, value of shipments, or revenue ($1,000)'].sum()
617/15:
#Create Pie Chart
labels = "eCommerce", "Retail"
value = [ecommerce_totals]
colors = ["purple", "blue"]
explode = (0.3,0)
617/16: ecommerce_totals = IN_Data_df.groupby('eCommerce?')['Sales, value of shipments, or revenue ($1,000)'].sum()
617/17:
#Create Pie Chart
labels = "eCommerce", "Retail"
value = [ecommerce_totals[1.0], ecommerce_totals[0.0]]
colors = ["purple", "blue"]
explode = (0.3,0)
617/18:
# Create a pie chart based upon the above data
plt.pie(value, labels=labels, colors=colors, explode=explode,
        autopct="%1.1f%%", shadow=True, startangle=180)
plt.title("eCommerce vs Retail 2017, Indiana")

# Create axes which are equal so we have a perfect circle
plt.axis("equal")
plt.show()
#plt.savefig("../Images/PyPies.png")
617/19: IN_Data_df['Number of establishments'] = IN_Data_df['Number of establishments'].astype(float)
617/20: IN_Data_df.groupby('eCommerce?')['Number of establishments'].sum()
617/21: establishments_total = IN_Data_df.groupby('eCommerce?')['Number of establishments'].sum()
617/22:
#Create Pie Chart
labels = "eCommerce", "Retail"
value = [establishments_total[1.0], establishments_total[0.0]]
colors = ["pink", "green"]
explode = (0.3,0)
617/23:
# Create a pie chart based upon the above data
plt.pie(value, labels=labels, colors=colors, explode=explode,
        autopct="%1.1f%%", shadow=True, startangle=180)
plt.title("eCommerce vs Retail 2017, Indiana")

# Create axes which are equal so we have a perfect circle
plt.axis("equal")
plt.show()
#plt.savefig("../Images/PyPies.png")
618/1:
# Read with pandas
PA_Data_df = pd.read_csv(PA_Data_csv)
PA_Data_df.head()
618/2:
# Dependencies
import pandas as pd
import numpy as np
import os
import webbrowser
import io
import matplotlib
import matplotlib.pyplot as plt
618/3: PA_Data_csv = "Datasets/PA Retail Data.csv"
618/4:
# Read with pandas
PA_Data_df = pd.read_csv(PA_Data_csv)
PA_Data_df.head()
618/5:
# Dependencies
import pandas as pd
import numpy as np
import os
import webbrowser
import io
import matplotlib
import matplotlib.pyplot as plt
618/6: PA_Data_csv = "Datasets/PA Retail Data.csv"
618/7:
# Read with pandas
PA_Data_df = pd.read_csv(PA_Data_csv)
PA_Data_df.head()
618/8:
# Delete extraneous column
PA_Data_df = PA_Data_df.drop(columns=['Geo Footnote', '2017 NAICS Footnote', 'id', '2017 NAICS code', 
                            'Annual payroll ($1,000)', 'First-quarter payroll ($1,000)', 
                            'Number of employees', 'Range indicating percent of total annual payroll imputed', 
                            'Range indicating percent of total employees imputed', 
                           ])


IN_Data_df
618/9:
# Delete extraneous column
PA_Data_df = PA_Data_df.drop(columns=['Geo Footnote', '2017 NAICS Footnote', 'id', '2017 NAICS code', 
                            'Annual payroll ($1,000)', 'First-quarter payroll ($1,000)', 
                            'Number of employees', 'Range indicating percent of total annual payroll imputed', 
                            'Range indicating percent of total employees imputed', 
                           ])


PA_Data_df
619/1:
# Dependencies
import pandas as pd
import numpy as np
import os
import webbrowser
import io
import matplotlib
import matplotlib.pyplot as plt
619/2: PA_Data_csv = "Datasets/PA Retail Data.csv"
619/3:
# Read with pandas
PA_Data_df = pd.read_csv(PA_Data_csv)
PA_Data_df.head()
619/4:
# Delete extraneous column
PA_Data_df = PA_Data_df.drop(columns=['Geo Footnote', '2017 NAICS Footnote', 'id', '2017 NAICS code', 
                            'Annual payroll ($1,000)', 'First-quarter payroll ($1,000)', 
                            'Number of employees', 'Range indicating percent of total annual payroll imputed', 
                            'Range indicating percent of total employees imputed', 
                           ])


PA_Data_df
619/5:
IN_Data_df = IN_Data_df.replace({'D': '0'})
IN_Data_df
619/6:
PA_Data_df = PA_Data_df.replace({'D': '0'})
PA_Data_df
619/7:
PA_Data_df.loc[PA_Data_df['Meaning of NAICS code']=='Electronic shopping and mail-order houses', 'eCommerce?']= 1
PA_Data_df.loc[PA_Data_df['Meaning of NAICS code']!='Electronic shopping and mail-order houses', 'eCommerce?']= 0

PA_Data_df.head()
619/8: PA_Data_df['Sales, value of shipments, or revenue ($1,000)'] = PA_Data_df['Sales, value of shipments, or revenue ($1,000)'].astype(float)
619/9: ecommerce_totals = PA_Data_df.groupby('eCommerce?')['Sales, value of shipments, or revenue ($1,000)'].sum()
619/10:
#Create Pie Chart
labels = "eCommerce", "Retail"
value = [ecommerce_totals[1.0], ecommerce_totals[0.0]]
colors = ["purple", "blue"]
explode = (0.3,0)
619/11:
# Create a pie chart based upon the above data
plt.pie(value, labels=labels, colors=colors, explode=explode,
        autopct="%1.1f%%", shadow=True, startangle=180)
plt.title("eCommerce vs Retail 2017, Indiana")

# Create axes which are equal so we have a perfect circle
plt.axis("equal")
plt.show()
#plt.savefig("../Images/PyPies.png")
619/12:
ecommerce_totals = PA_Data_df.groupby('eCommerce?')['Sales, value of shipments, or revenue ($1,000)'].sum()
ecommerce_totals
619/13:
PA_Data_df.loc[PA_Data_df['Meaning of NAICS code']=='Electronic shopping and mail-order houses', 'eCommerce?']= 1
PA_Data_df.loc[PA_Data_df['Meaning of NAICS code']!='Electronic shopping and mail-order houses', 'eCommerce?']= 0

PA_Data_df
619/14: PA_Data_df['Number of establishments'] = PA_Data_df['Number of establishments'].astype(float)
619/15:
establishments_total = PA_Data_df.groupby('eCommerce?')['Number of establishments'].sum()
establishments_total
619/16:
#Create Pie Chart
labels = "eCommerce", "Retail"
value = [establishments_total[1.0], establishments_total[0.0]]
colors = ["pink", "green"]
explode = (0.3,0)
619/17:
# Create a pie chart based upon the above data
plt.pie(value, labels=labels, colors=colors, explode=explode,
        autopct="%1.1f%%", shadow=True, startangle=180)
plt.title("eCommerce vs Retail 2017, Indiana")

# Create axes which are equal so we have a perfect circle
plt.axis("equal")
plt.show()
#plt.savefig("../Images/PyPies.png")
619/18:
# Create a pie chart based upon the above data
plt.pie(value, labels=labels, colors=colors, explode=explode,
        autopct="%1.1f%%", shadow=True, startangle=180)
plt.title("eCommerce vs Retail 2017, Pennsylvania")

# Create axes which are equal so we have a perfect circle
plt.axis("equal")
plt.show()
#plt.savefig("../Images/PyPies.png")
619/19:
# Create a pie chart based upon the above data
plt.pie(value, labels=labels, colors=colors, explode=explode,
        autopct="%1.1f%%", shadow=True, startangle=180)
plt.title("eCommerce vs Retail 2017, Pennsylvania")

# Create axes which are equal so we have a perfect circle
plt.axis("equal")
plt.show()
#plt.savefig("../Images/PyPies.png")
620/1:
# Dependencies
import pandas as pd
import numpy as np
import os
import webbrowser
import io
import matplotlib
import matplotlib.pyplot as plt
620/2: PA_Data_csv = "Datasets/PA Retail Data.csv"
620/3:
# Read with pandas
PA_Data_df = pd.read_csv(PA_Data_csv)
PA_Data_df.head()
620/4:
# Delete extraneous column
PA_Data_df = PA_Data_df.drop(columns=['Geo Footnote', '2017 NAICS Footnote', 'id', '2017 NAICS code', 
                            'Annual payroll ($1,000)', 'First-quarter payroll ($1,000)', 
                            'Number of employees', 'Range indicating percent of total annual payroll imputed', 
                            'Range indicating percent of total employees imputed', 
                           ])


PA_Data_df
620/5:
PA_Data_df = PA_Data_df.replace({'D': '0'})
PA_Data_df
620/6:
PA_Data_df.loc[PA_Data_df['Meaning of NAICS code']=='Electronic shopping and mail-order houses', 'eCommerce?']= 1
PA_Data_df.loc[PA_Data_df['Meaning of NAICS code']!='Electronic shopping and mail-order houses', 'eCommerce?']= 0

PA_Data_df.head()
620/7: PA_Data_df['Sales, value of shipments, or revenue ($1,000)'] = PA_Data_df['Sales, value of shipments, or revenue ($1,000)'].astype(float)
620/8:
ecommerce_totals = PA_Data_df.groupby('eCommerce?')['Sales, value of shipments, or revenue ($1,000)'].sum()
ecommerce_totals
620/9:
#Create Pie Chart
labels = "eCommerce", "Retail"
value = [ecommerce_totals[1.0], ecommerce_totals[0.0]]
colors = ["purple", "blue"]
explode = (0.3,0)
620/10:
# Create a pie chart based upon the above data
plt.pie(value, labels=labels, colors=colors, explode=explode,
        autopct="%1.1f%%", shadow=True, startangle=180)
plt.title("eCommerce vs Retail 2017, Pennsylvania")

# Create axes which are equal so we have a perfect circle
plt.axis("equal")
plt.show()
#plt.savefig("../Images/PyPies.png")
620/11:
PA_Data_df.loc[PA_Data_df['Meaning of NAICS code']=='Electronic shopping and mail-order houses', 'eCommerce?']= 1
PA_Data_df.loc[PA_Data_df['Meaning of NAICS code']!='Electronic shopping and mail-order houses', 'eCommerce?']= 0

PA_Data_df
620/12: PA_Data_df['Number of establishments'] = PA_Data_df['Number of establishments'].astype(float)
620/13:
establishments_total = PA_Data_df.groupby('eCommerce?')['Number of establishments'].sum()
establishments_total
620/14:
#Create Pie Chart
labels = "eCommerce", "Retail"
value = [establishments_total[1.0], establishments_total[0.0]]
colors = ["pink", "green"]
explode = (0.3,0)
620/15:
# Create a pie chart based upon the above data
plt.pie(value, labels=labels, colors=colors, explode=explode,
        autopct="%1.1f%%", shadow=True, startangle=180)
plt.title("eCommerce vs Retail 2017, Pennsylvania")

# Create axes which are equal so we have a perfect circle
plt.axis("equal")
plt.show()
#plt.savefig("../Images/PyPies.png")
623/1: %matplotlib notebook
623/2:
# Dependencies
import pandas as pd
import numpy as np
import os
import webbrowser
import io
import matplotlib
import matplotlib.pyplot as plt
623/3:
# Save file path to variable
retail_history_csv = "Datasets/EstUS_Retail_Trade_E-commercehistory.csv"
US_totalrev_2018_2017_csv = "Datasets/US_Retail_Trade_Sales_2018_2017.csv"
623/4:
# Read with pandas
retail_history_df = pd.read_csv(retail_history_csv)
retail_history_df.head()
623/5:
# Rename Columns
retail_history_df = retail_history_df.rename(columns={"Unnamed: 1": "Kind_of_Business",
                                 "2018": "2018 Total",
                                 "Unnamed: 3": "2018 Ecommerce Total",
                                 "2017r": "2017 Total",
                                 "Unnamed: 5": "2017 Ecommerce Total",
                                 "2016r": "2016 Total",
                                 "Unnamed: 7": "2016 Ecommerce Total",
                                 "2015r": "2015 Total",
                                 "Unnamed: 9": "2015 Ecommerce Total",
                                 "2014r": "2014 Total",
                                 "Unnamed: 11": "2014 Ecommerce Total",
                                 "1998": "1998 Total",
                                 "Unnamed: 43": "1998 Ecommerce"
                                 })
retail_history_df.head()
623/6:
# Drop columns we do not need
retail_history_df = retail_history_df.drop(columns=['NAICS Code',
                                                    '2013r',
                                                    'Unnamed: 13',
                                                    '2012r',
                                                    'Unnamed: 15',
                                                    '2011r',
                                                    'Unnamed: 17',
                                                    '2010',
                                                     'Unnamed: 19',
                                                     '2009',
                                                     'Unnamed: 21',
                                                     '2008',
                                                     'Unnamed: 23',
                                                     '2007',
                                                     'Unnamed: 25',
                                                     '2006',
                                                     'Unnamed: 27',
                                                     '2005',
                                                     'Unnamed: 29',
                                                     '2004',
                                                     'Unnamed: 31',
                                                     '2003',
                                                     'Unnamed: 33',
                                                     '2002',
                                                     'Unnamed: 35',
                                                     '2001',
                                                     'Unnamed: 37',
                                                     '2000',
                                                     'Unnamed: 39',
                                                     '1999',
                                                     'Unnamed: 41',
                                                    '2004', 
                                                    'Unnamed: 31',
                                                    '2003', 
                                                    'Unnamed: 33',
                                                    '2002',
                                                    'Unnamed: 35',
                                                    '2001',
                                                    'Unnamed: 37',
                                                    '2000',
                                                    'Unnamed: 39',
                                                    '1999',
                                                    'Unnamed: 41',
                                                   
                                                   ])
retail_history_df.head()
623/7:
retail_history_df
# D - Denotes an estimate withheld to avoid disclosing data of individual companies; data are included in higher-level totals.
# S - Estimate does not meet publication standards because of high sampling variability (coefficient of variation is greater than 30%), poor response quality (total quantity response rate is less than 50%), or other concerns about the estimate's quality. Unpublished estimates derived from this table by subtraction are subject to these same limitations and should not be attributed to the U.S. Census Bureau. For a description of publication standards and the total quantity response rate, see https://www.census.gov/about/policies/quality/standards/standardf1.html.
623/8:
# Drop rows that do not have ecommerce data
#[Estimates are shown in millions of dollars and are based on data from the Annual Retail Trade Survey. Estimates have been adjusted using final results of the 2012 Economic Census.] 
retail_history_df = retail_history_df.drop([0, 8])
623/9: retail_history_df = retail_history_df.reset_index(drop=True)
623/10: retail_history_df = retail_history_df.set_index('Kind_of_Business')
623/11:
# Replaced S & D, final DataFrame
retail_history_df = retail_history_df.replace({'S': '0',
                                               'D': '0',
                                              })
retail_history_df
623/12:
# remove commas from dataframe
retail_history_df.replace(',','', regex=True, inplace=True)
retail_history_df.head()
623/13:
# Separate by ecommerce
historyecom_df = retail_history_df.iloc[: , [1, 3, 5, 7, 9, 11]].copy() 
historyecom_df
623/14:
# Find top five product categories
topfive_ecom = historyecom_df.sort_values(by=['2018 Ecommerce Total'], ascending=False).head(6)
topfive_ecom
623/15:
# Separate by total
historytotal_df = retail_history_df.iloc[: , [0, 1, 3, 5, 7, 9, 11]].copy()
historytotal_df
623/16:
# Plot
year = [2018, 2017, 2016, 2015, 2014, 1998]
totalrev_byproductcategory = {
    'Total Retail Trade': [5269468, 5053151, 4852958, 4725993, 4639440, 2581762],
    'Motor Vehicle & Parts': [1191321, 1172367, 1140614, 1094112, 1020851, 688415],
    'Furniture & Home Stores': [116895, 113035, 110404, 106570, 99718, 77412],
    'Electronics & Appliances': [100205, 98570, 99043, 103658, 105197, 82731],
    'Building Materials & Garden Equip': [381313, 365622, 348697, 331611, 318352, 202423],
    'Food & Beverage': [745736, 725137, 699349, 685381, 669165, 416525],
    'Health & Personal Care': [347454, 333338, 327153, 315244, 299263, 129583],
    'Clothing & Accessories': [268163, 260566, 260050, 255798, 250409, 149151],
    'Sporting Goods, Hobby, Musical Inst., & Books': [81179, 84188, 86483, 85701, 83787, 60441],
    'General Merchandise': [706298, 687123, 675389, 674889, 667163, 351081],
    'Misc. Store Retailers': [130130, 124538, 121572, 119350, 115878, 98369],
    'Nonstore Retailers': [696849, 629204, 561412, 509652, 470867, 133904],
    'Electronic Shopping & Mail-order Houses': [611743, 551298, 488840, 433987, 386065, 80297],
}

fig, ax = plt.subplots()
ax.stackplot(year, totalrev_byproductcategory.values(),
             labels=totalrev_byproductcategory.keys())
ax.legend(loc='upper right')
ax.set_title('Total Revenue by Product Category')
ax.set_xlabel('Year')
ax.set_ylabel('Total Sales (millions)')

plt.show()
plt.tight_layout()
plt.savefig("Total Revenue by Product Category")
623/17: retail_history_df
623/18:
# Line Plot for History Data
plt.figure(figsize=(10,2))
retail_history_df.plot(kind="line")
plt.title("Retail History - 2018 & 2017")
plt.xlabel("Percent y/y change")
plt.ylabel("Industry")

plt.savefig("Retail Totals Stacked, 2018 & 2017.png")
623/19: %matplotlib notebook
623/20:
# Dependencies
import pandas as pd
import numpy as np
import os
import webbrowser
import io
import matplotlib
import matplotlib.pyplot as plt
623/21:
# Save file path to variable
retail_history_csv = "Datasets/Estimated Annual U.S. Retail Trade Sales Total and E-commerce1- 1998-2018.csv"
US_totalrev_2018_2017_csv = "Datasets/US_Retail_Trade_Sales_2018_2017.csv"
623/22:
# Read with pandas
retail_history_df = pd.read_csv(retail_history_csv)
retail_history_df.head()
623/23:
# Rename Columns
retail_history_df = retail_history_df.rename(columns={"Unnamed: 1": "Kind_of_Business",
                                 "2018": "2018 Total",
                                 "Unnamed: 3": "2018 Ecommerce Total",
                                 "2017r": "2017 Total",
                                 "Unnamed: 5": "2017 Ecommerce Total",
                                 "2016r": "2016 Total",
                                 "Unnamed: 7": "2016 Ecommerce Total",
                                 "2015r": "2015 Total",
                                 "Unnamed: 9": "2015 Ecommerce Total",
                                 "2014r": "2014 Total",
                                 "Unnamed: 11": "2014 Ecommerce Total",
                                 "1998": "1998 Total",
                                 "Unnamed: 43": "1998 Ecommerce"
                                 })
retail_history_df.head()
623/24:
# Drop columns we do not need
retail_history_df = retail_history_df.drop(columns=['NAICS Code',
                                                    '2013r',
                                                    'Unnamed: 13',
                                                    '2012r',
                                                    'Unnamed: 15',
                                                    '2011r',
                                                    'Unnamed: 17',
                                                    '2010',
                                                     'Unnamed: 19',
                                                     '2009',
                                                     'Unnamed: 21',
                                                     '2008',
                                                     'Unnamed: 23',
                                                     '2007',
                                                     'Unnamed: 25',
                                                     '2006',
                                                     'Unnamed: 27',
                                                     '2005',
                                                     'Unnamed: 29',
                                                     '2004',
                                                     'Unnamed: 31',
                                                     '2003',
                                                     'Unnamed: 33',
                                                     '2002',
                                                     'Unnamed: 35',
                                                     '2001',
                                                     'Unnamed: 37',
                                                     '2000',
                                                     'Unnamed: 39',
                                                     '1999',
                                                     'Unnamed: 41',
                                                    '2004', 
                                                    'Unnamed: 31',
                                                    '2003', 
                                                    'Unnamed: 33',
                                                    '2002',
                                                    'Unnamed: 35',
                                                    '2001',
                                                    'Unnamed: 37',
                                                    '2000',
                                                    'Unnamed: 39',
                                                    '1999',
                                                    'Unnamed: 41',
                                                   
                                                   ])
retail_history_df.head()
623/25:
retail_history_df
# D - Denotes an estimate withheld to avoid disclosing data of individual companies; data are included in higher-level totals.
# S - Estimate does not meet publication standards because of high sampling variability (coefficient of variation is greater than 30%), poor response quality (total quantity response rate is less than 50%), or other concerns about the estimate's quality. Unpublished estimates derived from this table by subtraction are subject to these same limitations and should not be attributed to the U.S. Census Bureau. For a description of publication standards and the total quantity response rate, see https://www.census.gov/about/policies/quality/standards/standardf1.html.
623/26:
# Drop rows that do not have ecommerce data
#[Estimates are shown in millions of dollars and are based on data from the Annual Retail Trade Survey. Estimates have been adjusted using final results of the 2012 Economic Census.] 
retail_history_df = retail_history_df.drop([0, 8])
623/27: retail_history_df = retail_history_df.reset_index(drop=True)
623/28: retail_history_df = retail_history_df.set_index('Kind_of_Business')
623/29:
# Replaced S & D, final DataFrame
retail_history_df = retail_history_df.replace({'S': '0',
                                               'D': '0',
                                              })
retail_history_df
623/30:
# remove commas from dataframe
retail_history_df.replace(',','', regex=True, inplace=True)
retail_history_df.head()
623/31:
# Separate by ecommerce
historyecom_df = retail_history_df.iloc[: , [1, 3, 5, 7, 9, 11]].copy() 
historyecom_df
623/32:
# Find top five product categories
topfive_ecom = historyecom_df.sort_values(by=['2018 Ecommerce Total'], ascending=False).head(6)
topfive_ecom
623/33:
# Separate by total
historytotal_df = retail_history_df.iloc[: , [0, 1, 3, 5, 7, 9, 11]].copy()
historytotal_df
623/34:
# Plot
year = [2018, 2017, 2016, 2015, 2014, 1998]
totalrev_byproductcategory = {
    'Total Retail Trade': [5269468, 5053151, 4852958, 4725993, 4639440, 2581762],
    'Motor Vehicle & Parts': [1191321, 1172367, 1140614, 1094112, 1020851, 688415],
    'Furniture & Home Stores': [116895, 113035, 110404, 106570, 99718, 77412],
    'Electronics & Appliances': [100205, 98570, 99043, 103658, 105197, 82731],
    'Building Materials & Garden Equip': [381313, 365622, 348697, 331611, 318352, 202423],
    'Food & Beverage': [745736, 725137, 699349, 685381, 669165, 416525],
    'Health & Personal Care': [347454, 333338, 327153, 315244, 299263, 129583],
    'Clothing & Accessories': [268163, 260566, 260050, 255798, 250409, 149151],
    'Sporting Goods, Hobby, Musical Inst., & Books': [81179, 84188, 86483, 85701, 83787, 60441],
    'General Merchandise': [706298, 687123, 675389, 674889, 667163, 351081],
    'Misc. Store Retailers': [130130, 124538, 121572, 119350, 115878, 98369],
    'Nonstore Retailers': [696849, 629204, 561412, 509652, 470867, 133904],
    'Electronic Shopping & Mail-order Houses': [611743, 551298, 488840, 433987, 386065, 80297],
}

fig, ax = plt.subplots()
ax.stackplot(year, totalrev_byproductcategory.values(),
             labels=totalrev_byproductcategory.keys())
ax.legend(loc='upper right')
ax.set_title('Total Revenue by Product Category')
ax.set_xlabel('Year')
ax.set_ylabel('Total Sales (millions)')

plt.show()
plt.tight_layout()
plt.savefig("Total Revenue by Product Category")
623/35: retail_history_df
623/36: retail_history_df.reset_index()
623/37: retail_history_df = retail_history_df.reset_index()
623/38: retail_history_df
623/39: retail_historymelted_df = retail_history_df.melt(id_vars=['Kind_of_Business'])
623/40: retail_historymelted_df
623/41:
# Line Plot for History Data
# plt.figure(figsize=(10,2))
# retail_history_df.plot(kind="line")
# plt.title("Retail History - 2018 & 2017")
# plt.xlabel("Percent y/y change")
# plt.ylabel("Industry")

# plt.savefig("Retail Totals Stacked, 2018 & 2017.png")
623/42: retail_historymelted_df['Type'] = ""
623/43: retail_historymelted_df
623/44: retail_historymelted_df
623/45:
retail_historymelted_df.loc[retail_historymelted_df['variable'].str.contains('Total', case=False), 'Type']= 'Total'
retail_historymelted_df.loc[retail_historymelted_df['variable'].str.contains('Ecommerce', case=False),'Type']= 'Ecommerce'
623/46: retail_historymelted_df
623/47: retail_history_df.columns()
623/48: retail_history_df.columns
623/49:
# Dependencies
import pandas as pd
import numpy as np
import os
import webbrowser
import io
import matplotlib
import matplotlib.pyplot as plt
623/50:
# Save file path to variable
retail_history_csv = "Datasets/Estimated Annual U.S. Retail Trade Sales Total and E-commerce1- 1998-2018.csv"
US_totalrev_2018_2017_csv = "Datasets/US_Retail_Trade_Sales_2018_2017.csv"
623/51:
# Read with pandas
retail_history_df = pd.read_csv(retail_history_csv)
retail_history_df.head()
623/52:
# Rename Columns
retail_history_df = retail_history_df.rename(columns={"Unnamed: 1": "Kind_of_Business",
                                 "2018": "2018 Total",
                                 "Unnamed: 3": "2018 Ecommerce Total",
                                 "2017r": "2017 Total",
                                 "Unnamed: 5": "2017 Ecommerce Total",
                                 "2016r": "2016 Total",
                                 "Unnamed: 7": "2016 Ecommerce Total",
                                 "2015r": "2015 Total",
                                 "Unnamed: 9": "2015 Ecommerce Total",
                                 "2014r": "2014 Total",
                                 "Unnamed: 11": "2014 Ecommerce Total",
                                 "1998": "1998 Total",
                                 "Unnamed: 43": "1998 Ecommerce Total"
                                 })
retail_history_df.head()
623/53:
# Drop columns we do not need
retail_history_df = retail_history_df.drop(columns=['NAICS Code',
                                                    '2013r',
                                                    'Unnamed: 13',
                                                    '2012r',
                                                    'Unnamed: 15',
                                                    '2011r',
                                                    'Unnamed: 17',
                                                    '2010',
                                                     'Unnamed: 19',
                                                     '2009',
                                                     'Unnamed: 21',
                                                     '2008',
                                                     'Unnamed: 23',
                                                     '2007',
                                                     'Unnamed: 25',
                                                     '2006',
                                                     'Unnamed: 27',
                                                     '2005',
                                                     'Unnamed: 29',
                                                     '2004',
                                                     'Unnamed: 31',
                                                     '2003',
                                                     'Unnamed: 33',
                                                     '2002',
                                                     'Unnamed: 35',
                                                     '2001',
                                                     'Unnamed: 37',
                                                     '2000',
                                                     'Unnamed: 39',
                                                     '1999',
                                                     'Unnamed: 41',
                                                    '2004', 
                                                    'Unnamed: 31',
                                                    '2003', 
                                                    'Unnamed: 33',
                                                    '2002',
                                                    'Unnamed: 35',
                                                    '2001',
                                                    'Unnamed: 37',
                                                    '2000',
                                                    'Unnamed: 39',
                                                    '1999',
                                                    'Unnamed: 41',
                                                   
                                                   ])
retail_history_df.head()
623/54:
retail_history_df
# D - Denotes an estimate withheld to avoid disclosing data of individual companies; data are included in higher-level totals.
# S - Estimate does not meet publication standards because of high sampling variability (coefficient of variation is greater than 30%), poor response quality (total quantity response rate is less than 50%), or other concerns about the estimate's quality. Unpublished estimates derived from this table by subtraction are subject to these same limitations and should not be attributed to the U.S. Census Bureau. For a description of publication standards and the total quantity response rate, see https://www.census.gov/about/policies/quality/standards/standardf1.html.
623/55:
# Drop rows that do not have ecommerce data
#[Estimates are shown in millions of dollars and are based on data from the Annual Retail Trade Survey. Estimates have been adjusted using final results of the 2012 Economic Census.] 
retail_history_df = retail_history_df.drop([0, 8])
623/56: retail_history_df = retail_history_df.reset_index(drop=True)
623/57: retail_history_df = retail_history_df.set_index('Kind_of_Business')
623/58:
# Replaced S & D, final DataFrame
retail_history_df = retail_history_df.replace({'S': '0',
                                               'D': '0',
                                              })
retail_history_df
623/59:
# remove commas from dataframe
retail_history_df.replace(',','', regex=True, inplace=True)
retail_history_df.head()
623/60:
# Separate by ecommerce
historyecom_df = retail_history_df.iloc[: , [1, 3, 5, 7, 9, 11]].copy() 
historyecom_df
623/61:
# Find top five product categories
topfive_ecom = historyecom_df.sort_values(by=['2018 Ecommerce Total'], ascending=False).head(6)
topfive_ecom
623/62:
# Separate by total
historytotal_df = retail_history_df.iloc[: , [0, 1, 3, 5, 7, 9, 11]].copy()
historytotal_df
623/63: retail_history_df = retail_history_df.reset_index()
623/64: retail_history_df.columns
623/65: retail_historymelted_df = retail_history_df.melt(id_vars=['Kind_of_Business'])
623/66: retail_historymelted_df['Type'] = ""
623/67: retail_historymelted_df
623/68:
retail_historymelted_df.loc[retail_historymelted_df['variable'].str.contains('Total', case=False), 'Type']= 'Retail Total'
retail_historymelted_df.loc[retail_historymelted_df['variable'].str.contains('Ecommerce Total', case=False),'Type']= 'Ecommerce'
623/69: retail_historymelted_df
623/70:
# Looking for only numbers
retail_historymelted_df['Year'] = retail_historymelted_df['variable'].str.extract('(\d+)', expand=False)
623/71: retail_historymelted_df
623/72:
# Looking for only numbers - reg.ex = regular expression.
retail_historymelted_df['Year'] = retail_historymelted_df['variable'].str.extract('(\d+)', expand=False)
retail_historymelted_df['Year'] = retail_historymelted_df['Year'].astype(int)
623/73: retail_historymelted_df
623/74:
# Line Plot for History Data

plt.figure(figsize=(10,2))
retail_historymelted_df.plot(kind="line", x='Year', y='value', color='Kind_of_Business')
plt.title("US Retail History - 2018-1998")
plt.xlabel("Year")
plt.ylabel("Total Sales in Millions($)")

# plt.savefig("Retail Totals Stacked, 2018 & 2017.png")
623/75: retail_historymelted_df.info
623/76: retail_historymelted_df.info()
623/77:
# Looking for only numbers - reg.ex = regular expression.
retail_historymelted_df['Year'] = retail_historymelted_df['variable'].str.extract('(\d+)', expand=False)
retail_historymelted_df['Year'] = retail_historymelted_df['Year'].astype(int)
retail_historymelted_df['value'] = retail_historymelted_df['value'].astype(int)
623/78:
# Looking for only numbers - reg.ex = regular expression.
retail_historymelted_df['Year'] = retail_historymelted_df['variable'].str.extract('(\d+)', expand=False)
retail_historymelted_df['Year'] = retail_historymelted_df['Year'].astype(int)
retail_historymelted_df['value'] = retail_historymelted_df['value'].astype(float)
623/79: retail_historymelted_df.info()
623/80:
# Line Plot for History Data

plt.figure(figsize=(10,2))
retail_historymelted_df.plot(kind="line", x='Year', y='value', color='Kind_of_Business')
plt.title("US Retail History - 2018-1998")
plt.xlabel("Year")
plt.ylabel("Total Sales in Millions($)")

# plt.savefig("Retail Totals Stacked, 2018 & 2017.png")
623/81:
# Line Plot for History Data

plt.figure(figsize=(10,2))
retail_historymelted_df.plot(kind="line", x='Year', y='value', color='Kind_of_Business')
plt.title("US Retail History - 2018-1998")
plt.xlabel("Year")
plt.ylabel("Total Sales in Millions($)")

plt.show()
plt.tight_layout()

# plt.savefig("Retail Totals Stacked, 2018 & 2017.png")
623/82:
# Line Plot for History Data

plt.figure(figsize=20,20))
retail_historymelted_df.plot(kind="line", x='Year', y='value', color='Kind_of_Business')
plt.title("US Retail History - 2018-1998")
plt.xlabel("Year")
plt.ylabel("Total Sales in Millions($)")

plt.show()
plt.tight_layout()

# plt.savefig("Retail Totals Stacked, 2018 & 2017.png")
623/83:
# Line Plot for History Data

plt.figure(figsize=20,20)
retail_historymelted_df.plot(kind="line", x='Year', y='value', color='Kind_of_Business')
plt.title("US Retail History - 2018-1998")
plt.xlabel("Year")
plt.ylabel("Total Sales in Millions($)")

plt.show()
plt.tight_layout()

# plt.savefig("Retail Totals Stacked, 2018 & 2017.png")
623/84:
# Line Plot for History Data

plt.figure(figsize=(20,20))
retail_historymelted_df.plot(kind="line", x='Year', y='value', color='Kind_of_Business')
plt.title("US Retail History - 2018-1998")
plt.xlabel("Year")
plt.ylabel("Total Sales in Millions($)")

plt.show()
plt.tight_layout()

# plt.savefig("Retail Totals Stacked, 2018 & 2017.png")
623/85: !pip install seaborn
623/86: !pip install seaborn
623/87:
# Dependencies
import pandas as pd
import numpy as np
import os
import webbrowser
import io
import matplotlib
import matplotlib.pyplot as plt
import seaborn as sns
623/88:
#Sns line plot
sns.lineplot(data=retail_historymelted_df, x='Year', y='value', hue='Kind_of_Business')
623/89:
#Sns line plot
sns.lineplot(data=retail_historymelted_df, x='Year', y='value', hue='Kind_of_Business')
sns.show()
623/90:
#Sns line plot
sns.lineplot(data=retail_historymelted_df, x='Year', y='value', hue='Kind_of_Business')
plt.show()
624/1: %matplotlib notebook
624/2:
# Dependencies
import pandas as pd
import numpy as np
import os
import webbrowser
import io
import matplotlib
import matplotlib.pyplot as plt
import seaborn as sns
624/3:
# Save file path to variable
retail_history_csv = "Datasets/Estimated Annual U.S. Retail Trade Sales Total and E-commerce1- 1998-2018.csv"
US_totalrev_2018_2017_csv = "Datasets/US_Retail_Trade_Sales_2018_2017.csv"
624/4:
# Read with pandas
retail_history_df = pd.read_csv(retail_history_csv)
retail_history_df.head()
624/5:
# Rename Columns
retail_history_df = retail_history_df.rename(columns={"Unnamed: 1": "Kind_of_Business",
                                 "2018": "2018 Total",
                                 "Unnamed: 3": "2018 Ecommerce Total",
                                 "2017r": "2017 Total",
                                 "Unnamed: 5": "2017 Ecommerce Total",
                                 "2016r": "2016 Total",
                                 "Unnamed: 7": "2016 Ecommerce Total",
                                 "2015r": "2015 Total",
                                 "Unnamed: 9": "2015 Ecommerce Total",
                                 "2014r": "2014 Total",
                                 "Unnamed: 11": "2014 Ecommerce Total",
                                 "1998": "1998 Total",
                                 "Unnamed: 43": "1998 Ecommerce Total"
                                 })
retail_history_df.head()
624/6:
# Drop columns we do not need
retail_history_df = retail_history_df.drop(columns=['NAICS Code',
                                                    '2013r',
                                                    'Unnamed: 13',
                                                    '2012r',
                                                    'Unnamed: 15',
                                                    '2011r',
                                                    'Unnamed: 17',
                                                    '2010',
                                                     'Unnamed: 19',
                                                     '2009',
                                                     'Unnamed: 21',
                                                     '2008',
                                                     'Unnamed: 23',
                                                     '2007',
                                                     'Unnamed: 25',
                                                     '2006',
                                                     'Unnamed: 27',
                                                     '2005',
                                                     'Unnamed: 29',
                                                     '2004',
                                                     'Unnamed: 31',
                                                     '2003',
                                                     'Unnamed: 33',
                                                     '2002',
                                                     'Unnamed: 35',
                                                     '2001',
                                                     'Unnamed: 37',
                                                     '2000',
                                                     'Unnamed: 39',
                                                     '1999',
                                                     'Unnamed: 41',
                                                    '2004', 
                                                    'Unnamed: 31',
                                                    '2003', 
                                                    'Unnamed: 33',
                                                    '2002',
                                                    'Unnamed: 35',
                                                    '2001',
                                                    'Unnamed: 37',
                                                    '2000',
                                                    'Unnamed: 39',
                                                    '1999',
                                                    'Unnamed: 41',
                                                   
                                                   ])
retail_history_df.head()
624/7:
retail_history_df
# D - Denotes an estimate withheld to avoid disclosing data of individual companies; data are included in higher-level totals.
# S - Estimate does not meet publication standards because of high sampling variability (coefficient of variation is greater than 30%), poor response quality (total quantity response rate is less than 50%), or other concerns about the estimate's quality. Unpublished estimates derived from this table by subtraction are subject to these same limitations and should not be attributed to the U.S. Census Bureau. For a description of publication standards and the total quantity response rate, see https://www.census.gov/about/policies/quality/standards/standardf1.html.
624/8:
# Drop rows that do not have ecommerce data
#[Estimates are shown in millions of dollars and are based on data from the Annual Retail Trade Survey. Estimates have been adjusted using final results of the 2012 Economic Census.] 
retail_history_df = retail_history_df.drop([0, 8])
624/9: retail_history_df = retail_history_df.reset_index(drop=True)
624/10: retail_history_df = retail_history_df.set_index('Kind_of_Business')
624/11:
# Replaced S & D, final DataFrame
retail_history_df = retail_history_df.replace({'S': '0',
                                               'D': '0',
                                              })
retail_history_df
624/12:
# remove commas from dataframe
retail_history_df.replace(',','', regex=True, inplace=True)
retail_history_df.head()
624/13:
# Separate by ecommerce
historyecom_df = retail_history_df.iloc[: , [1, 3, 5, 7, 9, 11]].copy() 
historyecom_df
624/14:
# Find top five product categories
topfive_ecom = historyecom_df.sort_values(by=['2018 Ecommerce Total'], ascending=False).head(6)
topfive_ecom
624/15:
# Separate by total
historytotal_df = retail_history_df.iloc[: , [0, 1, 3, 5, 7, 9, 11]].copy()
historytotal_df
624/16:
# Plot
year = [2018, 2017, 2016, 2015, 2014, 1998]
totalrev_byproductcategory = {
    'Total Retail Trade': [5269468, 5053151, 4852958, 4725993, 4639440, 2581762],
    'Motor Vehicle & Parts': [1191321, 1172367, 1140614, 1094112, 1020851, 688415],
    'Furniture & Home Stores': [116895, 113035, 110404, 106570, 99718, 77412],
    'Electronics & Appliances': [100205, 98570, 99043, 103658, 105197, 82731],
    'Building Materials & Garden Equip': [381313, 365622, 348697, 331611, 318352, 202423],
    'Food & Beverage': [745736, 725137, 699349, 685381, 669165, 416525],
    'Health & Personal Care': [347454, 333338, 327153, 315244, 299263, 129583],
    'Clothing & Accessories': [268163, 260566, 260050, 255798, 250409, 149151],
    'Sporting Goods, Hobby, Musical Inst., & Books': [81179, 84188, 86483, 85701, 83787, 60441],
    'General Merchandise': [706298, 687123, 675389, 674889, 667163, 351081],
    'Misc. Store Retailers': [130130, 124538, 121572, 119350, 115878, 98369],
    'Nonstore Retailers': [696849, 629204, 561412, 509652, 470867, 133904],
    'Electronic Shopping & Mail-order Houses': [611743, 551298, 488840, 433987, 386065, 80297],
}

fig, ax = plt.subplots()
ax.stackplot(year, totalrev_byproductcategory.values(),
             labels=totalrev_byproductcategory.keys())
ax.legend(loc='upper right')
ax.set_title('Total Revenue by Product Category')
ax.set_xlabel('Year')
ax.set_ylabel('Total Sales (millions)')

plt.show()
plt.tight_layout()
plt.savefig("Total Revenue by Product Category")
624/17: retail_history_df
624/18: retail_history_df = retail_history_df.reset_index()
624/19: retail_history_df.columns
624/20: retail_historymelted_df = retail_history_df.melt(id_vars=['Kind_of_Business'])
624/21: retail_historymelted_df
624/22:
retail_historymelted_df.loc[retail_historymelted_df['variable'].str.contains('Total', case=False), 'Type']= 'Retail Total'
retail_historymelted_df.loc[retail_historymelted_df['variable'].str.contains('Ecommerce Total', case=False),'Type']= 'Ecommerce'
624/23:
# Looking for only numbers - reg.ex = regular expression.
retail_historymelted_df['Year'] = retail_historymelted_df['variable'].str.extract('(\d+)', expand=False)
retail_historymelted_df['Year'] = retail_historymelted_df['Year'].astype(int)
retail_historymelted_df['value'] = retail_historymelted_df['value'].astype(float)
624/24: retail_historymelted_df.info()
624/25:
#Sns line plot
sns.lineplot(data=retail_historymelted_df, x='Year', y='value', hue='Kind_of_Business')
plt.show()
624/26:
# Line Plot for History Data

plt.figure(figsize=(20,20))
retail_historymelted_df.plot(kind="line", x='Year', y='value', color='Kind_of_Business')
plt.title("US Retail History - 2018-1998")
plt.xlabel("Year")
plt.ylabel("Total Sales in Millions($)")

plt.show()
plt.tight_layout()

# plt.savefig("Retail Totals Stacked, 2018 & 2017.png")
624/27:
topfive_2018_df = pd.DataFrame({ '2018 Ecommerce Sales': [453584, 451635, 35348, 10981, 4837]},
                               index=['Nonstore  Retailers', 'E-Shopping & Mail-Order', 'Vehicle & Accessories',
                                     'Clothing', 'Food & Beverage'])
topfive_2018_df
624/28:
topfive_2018_df.plot(kind="bar", title="Top Five Product Categories, 2018", figsize=(20,30))
plt.xlabel = "Product Category"
plt.ylabel = "Total Sales (Millions)"
plt.savefig("Top Five Product Categories, 2018.png")
624/29:
US_totalrev_df = pd.read_csv(US_totalrev_2018_2017_csv)
US_totalrev_df.head()
624/30:
# Drop Columns
US_totalrev_df = US_totalrev_df.drop(columns=['Unnamed: 0'])
US_totalrev_df.head()
624/31:
# Rename columns
US_totalrev_df = US_totalrev_df.rename(columns={"2018 E-commerce": "2018 Total ecommerce",
                                                "2017 Revised \nTotal Sales": "2017 Total Sales",
                                                "2017 Revised \nE-commerce": "2017 Total ecommerce",
                                                "Percent Distribution\nof E-commerce Sales 2018": 'Percent Distribution of ecommerce 2018'
                                                 })
US_totalrev_df.head()
624/32:
# Remove commas from the data
# remove commas from dataframe
US_totalrev_df.replace(',','', regex=True, inplace=True)
US_totalrev_df.head()
624/33:
# Removing first row
US_totalrev_df = US_totalrev_df.drop([0, 1])
624/34:
# Reset Index
US_totalrev_df = US_totalrev_df.reset_index(drop=True)
624/35: US_totalrev_df = US_totalrev_df.set_index("Industry")
624/36:
# Final DataFrame
# Need two separate dataframes for plotting
US_totalrev_df = US_totalrev_df.replace({'S': '0'})
US_totalrev_df
624/37:
# Totals df for plotting
# Non-store retail = do not have brick and mortar stores
total_plot_df = US_totalrev_df.iloc[: , [0, 1, 2, 3]].copy() 
total_plot_df
624/38:
totalrev_plot_df = pd.DataFrame({
    "2018 Total Sales":[1191321, 116895, 100205, 381313, 745736, 347454, 503925, 268163, 81179, 706298, 130130, 696849, 611743],
    "2018 Total Ecommerce Sales":[35348, 1533, 2056, 2795, 4837, 744, 0, 10981, 2617, 0, 0, 453584, 451635],
    "2017 Total Sales":[1172367, 113035, 98570, 365622, 725137, 333338, 459463, 260566, 84188, 687123, 124538, 629204, 551298],
    "2017 Total Ecommerce Sales":[32106, 1279, 2113, 2614, 3431, 0, 0, 9374, 2441, 0, 4014, 400279, 398415]
    }, 
    index=["Vehicle", "Furniture", 
           "Electronics", "Building material/Garden equip.",
          "Food & beverage", "Health & Personal", "Gasoline stations", "Clothing",
           "Sporting goods, hobby, musical instrument, and book stores", "General merchandise stores", "Misc. store retailers",
           "Nonstore retailers", "E-shopping and mail-order houses"
          ]
)
totalrev_plot_df
624/39:
# top five 2018 data
topfive_2018_df = pd.DataFrame({ '2018 Ecommerce Sales': [453584, 451635, 35348, 10981, 4837]},
                               index=['Nonstore  Retailers', 'E-Shopping & Mail-Order', 'Vehicle & Accessories',
                                     'Clothing', 'Food & Beverage'])
topfive_2018_df
624/40:
topfive_2018_df.plot(kind="bar", title="Top Five Product Categories, 2018", figsize=(10,10))
plt.xlabel = "Product Category"
plt.ylabel = "Total Sales (Millions)"
plt.savefig("Top Five Product Categories, 2018.png")
plt.tight_layout()
624/41:
# top five 2017 data
topfive_2017_df = pd.DataFrame({'2017 Ecommerce Sales': [400279, 398415, 32106, 9374, 4014]},
                               index=['Nonstore  Retailers', 'E-Shopping & Mail-Order', 'Vehicle & Accessories',
                                     'Clothing', 'Misc. store retailers'])
topfive_2017_df
624/42:
# top five 2017 plot
topfive_2017_df.plot(kind="bar", color='r', title="Top Five Product Categories, 2017", figsize=(10,10))
plt.xlabel = "Product Category"
plt.ylabel = "Total Sales (Millions)"
plt.savefig("Top Five Product Categories, 2017.png")
plt.tight_layout()
624/43:
topfive_ecom2018 = [453584, 451635, 35348, 10981, 4837]
topfive_ecom2017 = [400279, 398415, 32106, 9374, 4014]
index = ["Vehicle", "Furniture", "Electronics", "Building material/Garden equip.",
         "Food & beverage", "Health & Personal", "Gasoline stations", "Clothing", 
         "Sporting goods, hobby, musical instrument, and book stores", "General merchandise stores", 
         "Misc. store retailers", "Nonstore retailers", "E-shopping and mail-order houses"]
totalrev_plot_df = pd.DataFrame({'Total Sales 2018': totalsales2018,
                                 'Total Ecommerce Sales 2018': totalecom2018,
                                 'Total Sales 2017': totalsales2017,
                                 'Total Ecommerce Sales 2017': totalecom2017
                                }, index=index)
624/44: %matplotlib inline
624/45:
# Dependencies
import pandas as pd
import numpy as np
import os
import webbrowser
import io
import matplotlib
import matplotlib.pyplot as plt
import seaborn as sns
624/46:
# Save file path to variable
retail_history_csv = "Datasets/Estimated Annual U.S. Retail Trade Sales Total and E-commerce1- 1998-2018.csv"
US_totalrev_2018_2017_csv = "Datasets/US_Retail_Trade_Sales_2018_2017.csv"
624/47:
# Read with pandas
retail_history_df = pd.read_csv(retail_history_csv)
retail_history_df.head()
624/48:
# Rename Columns
retail_history_df = retail_history_df.rename(columns={"Unnamed: 1": "Kind_of_Business",
                                 "2018": "2018 Total",
                                 "Unnamed: 3": "2018 Ecommerce Total",
                                 "2017r": "2017 Total",
                                 "Unnamed: 5": "2017 Ecommerce Total",
                                 "2016r": "2016 Total",
                                 "Unnamed: 7": "2016 Ecommerce Total",
                                 "2015r": "2015 Total",
                                 "Unnamed: 9": "2015 Ecommerce Total",
                                 "2014r": "2014 Total",
                                 "Unnamed: 11": "2014 Ecommerce Total",
                                 "1998": "1998 Total",
                                 "Unnamed: 43": "1998 Ecommerce Total"
                                 })
retail_history_df.head()
624/49:
# Drop columns we do not need
retail_history_df = retail_history_df.drop(columns=['NAICS Code',
                                                    '2013r',
                                                    'Unnamed: 13',
                                                    '2012r',
                                                    'Unnamed: 15',
                                                    '2011r',
                                                    'Unnamed: 17',
                                                    '2010',
                                                     'Unnamed: 19',
                                                     '2009',
                                                     'Unnamed: 21',
                                                     '2008',
                                                     'Unnamed: 23',
                                                     '2007',
                                                     'Unnamed: 25',
                                                     '2006',
                                                     'Unnamed: 27',
                                                     '2005',
                                                     'Unnamed: 29',
                                                     '2004',
                                                     'Unnamed: 31',
                                                     '2003',
                                                     'Unnamed: 33',
                                                     '2002',
                                                     'Unnamed: 35',
                                                     '2001',
                                                     'Unnamed: 37',
                                                     '2000',
                                                     'Unnamed: 39',
                                                     '1999',
                                                     'Unnamed: 41',
                                                    '2004', 
                                                    'Unnamed: 31',
                                                    '2003', 
                                                    'Unnamed: 33',
                                                    '2002',
                                                    'Unnamed: 35',
                                                    '2001',
                                                    'Unnamed: 37',
                                                    '2000',
                                                    'Unnamed: 39',
                                                    '1999',
                                                    'Unnamed: 41',
                                                   
                                                   ])
retail_history_df.head()
624/50:
retail_history_df
# D - Denotes an estimate withheld to avoid disclosing data of individual companies; data are included in higher-level totals.
# S - Estimate does not meet publication standards because of high sampling variability (coefficient of variation is greater than 30%), poor response quality (total quantity response rate is less than 50%), or other concerns about the estimate's quality. Unpublished estimates derived from this table by subtraction are subject to these same limitations and should not be attributed to the U.S. Census Bureau. For a description of publication standards and the total quantity response rate, see https://www.census.gov/about/policies/quality/standards/standardf1.html.
624/51:
# Drop rows that do not have ecommerce data
#[Estimates are shown in millions of dollars and are based on data from the Annual Retail Trade Survey. Estimates have been adjusted using final results of the 2012 Economic Census.] 
retail_history_df = retail_history_df.drop([0, 8])
624/52: retail_history_df = retail_history_df.reset_index(drop=True)
624/53: retail_history_df = retail_history_df.set_index('Kind_of_Business')
624/54:
# Replaced S & D, final DataFrame
retail_history_df = retail_history_df.replace({'S': '0',
                                               'D': '0',
                                              })
retail_history_df
624/55:
# remove commas from dataframe
retail_history_df.replace(',','', regex=True, inplace=True)
retail_history_df.head()
624/56:
# Separate by ecommerce
historyecom_df = retail_history_df.iloc[: , [1, 3, 5, 7, 9, 11]].copy() 
historyecom_df
624/57:
# Find top five product categories
topfive_ecom = historyecom_df.sort_values(by=['2018 Ecommerce Total'], ascending=False).head(6)
topfive_ecom
624/58:
# Separate by total
historytotal_df = retail_history_df.iloc[: , [0, 1, 3, 5, 7, 9, 11]].copy()
historytotal_df
624/59:
# Plot
year = [2018, 2017, 2016, 2015, 2014, 1998]
totalrev_byproductcategory = {
    'Total Retail Trade': [5269468, 5053151, 4852958, 4725993, 4639440, 2581762],
    'Motor Vehicle & Parts': [1191321, 1172367, 1140614, 1094112, 1020851, 688415],
    'Furniture & Home Stores': [116895, 113035, 110404, 106570, 99718, 77412],
    'Electronics & Appliances': [100205, 98570, 99043, 103658, 105197, 82731],
    'Building Materials & Garden Equip': [381313, 365622, 348697, 331611, 318352, 202423],
    'Food & Beverage': [745736, 725137, 699349, 685381, 669165, 416525],
    'Health & Personal Care': [347454, 333338, 327153, 315244, 299263, 129583],
    'Clothing & Accessories': [268163, 260566, 260050, 255798, 250409, 149151],
    'Sporting Goods, Hobby, Musical Inst., & Books': [81179, 84188, 86483, 85701, 83787, 60441],
    'General Merchandise': [706298, 687123, 675389, 674889, 667163, 351081],
    'Misc. Store Retailers': [130130, 124538, 121572, 119350, 115878, 98369],
    'Nonstore Retailers': [696849, 629204, 561412, 509652, 470867, 133904],
    'Electronic Shopping & Mail-order Houses': [611743, 551298, 488840, 433987, 386065, 80297],
}

fig, ax = plt.subplots()
ax.stackplot(year, totalrev_byproductcategory.values(),
             labels=totalrev_byproductcategory.keys())
ax.legend(loc='upper right')
ax.set_title('Total Revenue by Product Category')
ax.set_xlabel('Year')
ax.set_ylabel('Total Sales (millions)')

plt.show()
plt.tight_layout()
plt.savefig("Total Revenue by Product Category")
624/60: retail_history_df
624/61: retail_history_df = retail_history_df.reset_index()
624/62: retail_history_df.columns
624/63: retail_historymelted_df = retail_history_df.melt(id_vars=['Kind_of_Business'])
624/64: retail_historymelted_df
624/65:
retail_historymelted_df.loc[retail_historymelted_df['variable'].str.contains('Total', case=False), 'Type']= 'Retail Total'
retail_historymelted_df.loc[retail_historymelted_df['variable'].str.contains('Ecommerce Total', case=False),'Type']= 'Ecommerce'
624/66:
# Looking for only numbers - reg.ex = regular expression.
retail_historymelted_df['Year'] = retail_historymelted_df['variable'].str.extract('(\d+)', expand=False)
retail_historymelted_df['Year'] = retail_historymelted_df['Year'].astype(int)
retail_historymelted_df['value'] = retail_historymelted_df['value'].astype(float)
624/67: retail_historymelted_df.info()
624/68:
#Sns line plot
sns.lineplot(data=retail_historymelted_df, x='Year', y='value', hue='Kind_of_Business')
plt.show()
624/69:
# Line Plot for History Data

plt.figure(figsize=(20,20))
retail_historymelted_df.plot(kind="line", x='Year', y='value', color='Kind_of_Business')
plt.title("US Retail History - 2018-1998")
plt.xlabel("Year")
plt.ylabel("Total Sales in Millions($)")

plt.show()
plt.tight_layout()

# plt.savefig("Retail Totals Stacked, 2018 & 2017.png")
624/70:
#Sns line plot
sns.lineplot(data=retail_historymelted_df, x='Year', y='value', hue='Kind_of_Business')
plt.show()
plt.tight_layout()
624/71:
# Line Plot for History Data

plt.figure(figsize=(20,20))
retail_historymelted_df.plot(kind="line", x='Year', y='value', color='Kind_of_Business')
plt.title("US Retail History - 2018-1998")
plt.xlabel("Year")
plt.ylabel("Total Sales in Millions($)")

plt.show()
plt.tight_layout()

# plt.savefig("Retail Totals Stacked, 2018 & 2017.png")
624/72:
# # Line Plot for History Data

# plt.figure(figsize=(20,20))
# retail_historymelted_df.plot(kind="line", x='Year', y='value', color='Kind_of_Business')
# plt.title("US Retail History - 2018-1998")
# plt.xlabel("Year")
# plt.ylabel("Total Sales in Millions($)")

# plt.show()
# plt.tight_layout()

# # plt.savefig("Retail Totals Stacked, 2018 & 2017.png")
624/73:
#Sns line plot
sns.lineplot(data=retail_historymelted_df, x='Year', y='value', hue='Kind_of_Business')
plt.figure(figsize=20,20)
plt.show()
plt.tight_layout()
624/74:
#Sns line plot
sns.lineplot(data=retail_historymelted_df, x='Year', y='value', hue='Kind_of_Business')
plt.figure(figsize=(20,20))
plt.show()
plt.tight_layout()
624/75:
#Sns line plot
sns.lineplot(data=retail_historymelted_df, x='Year', y='value', hue='Kind_of_Business')
plt.figure(figsize=(20,20))
plt.show()
624/76:
#Sns line plot
retail_history_plot = sns.lineplot(data=retail_historymelted_df, x='Year', y='value', hue='Kind_of_Business')
retail_history_plot.figure(figsize=(20,20))
plt.show()
624/77: dir(retail_history_plot)
624/78:
#Sns line plot
plt.figure(figsize=(20,20))
retail_history_plot = sns.lineplot(data=retail_historymelted_df, x='Year', y='value', hue='Kind_of_Business')
plt.show()
624/79:
topfive_2018_df.plot(kind="bar", title="Top Five Product Categories, 2018", figsize=(20,30))
plt.xlabel = "Product Category"
plt.ylabel = "Total Sales (Millions)"
plt.savefig("Top Five Product Categories, 2018.png")
624/80:
topfive_2018_df.plot(kind="bar", title="Top Five Product Categories, 2018", figsize=(10,10))
plt.xlabel = "Product Category"
plt.ylabel = "Total Sales (Millions)"
plt.savefig("Top Five Product Categories, 2018.png")
plt.tight_layout()
624/81:
# top five 2017 data
topfive_2017_df = pd.DataFrame({'2017 Ecommerce Sales': [400279, 398415, 32106, 9374, 4014]},
                               index=['Nonstore  Retailers', 'E-Shopping & Mail-Order', 'Vehicle & Accessories',
                                     'Clothing', 'Misc. store retailers'])
topfive_2017_df
624/82:
# top five 2017 plot
topfive_2017_df.plot(kind="bar", color='r', title="Top Five Product Categories, 2017", figsize=(10,10))
plt.xlabel = "Product Category"
plt.ylabel = "Total Sales (Millions)"
plt.savefig("Top Five Product Categories, 2017.png")
plt.tight_layout()
624/83:
topfive_ecom2018 = [453584, 451635, 35348, 10981, 4837]
topfive_ecom2017 = [400279, 398415, 32106, 9374, 4014]
index = ["Vehicle", "Furniture", "Electronics", "Building material/Garden equip.",
         "Food & beverage", "Health & Personal", "Gasoline stations", "Clothing", 
         "Sporting goods, hobby, musical instrument, and book stores", "General merchandise stores", 
         "Misc. store retailers", "Nonstore retailers", "E-shopping and mail-order houses"]
totalrev_plot_df = pd.DataFrame({'Total Sales 2018': totalsales2018,
                                 'Total Ecommerce Sales 2018': totalecom2018,
                                 'Total Sales 2017': totalsales2017,
                                 'Total Ecommerce Sales 2017': totalecom2017
                                }, index=index)
624/84:
# Plot
plt.figure(figsize=(10,8))
totalrev_plot_df.plot(kind="bar", stacked=True)
plt.title("Retail Totals - 2018 & 2017")
plt.xlabel("Industry")
plt.ylabel("Sales in Millions ($)")

labels = []

plt.show()
plt.savefig("Retail Totals, 2018 & 2017.png")
621/1:
# Dependencies
import pandas as pd
import numpy as np
import os
import webbrowser
import io
import matplotlib
import matplotlib.pyplot as plt
import seaborn as sns
621/2:
# Save file path to variable
retail_history_csv = "Datasets/Estimated Annual U.S. Retail Trade Sales Total and E-commerce1- 1998-2018.csv"
US_totalrev_2018_2017_csv = "Datasets/US_Retail_Trade_Sales_2018_2017.csv"
621/3:
# Read with pandas
retail_history_df = pd.read_csv(retail_history_csv)
retail_history_df.head()
621/4:
# Rename Columns
retail_history_df = retail_history_df.rename(columns={"Unnamed: 1": "Kind_of_Business",
                                 "2018": "2018 Total",
                                 "Unnamed: 3": "2018 Ecommerce Total",
                                 "2017r": "2017 Total",
                                 "Unnamed: 5": "2017 Ecommerce Total",
                                 "2016r": "2016 Total",
                                 "Unnamed: 7": "2016 Ecommerce Total",
                                 "2015r": "2015 Total",
                                 "Unnamed: 9": "2015 Ecommerce Total",
                                 "2014r": "2014 Total",
                                 "Unnamed: 11": "2014 Ecommerce Total",
                                 "1998": "1998 Total",
                                 "Unnamed: 43": "1998 Ecommerce Total"
                                 })
621/5:
# Drop columns we do not need
retail_history_df = retail_history_df.drop(columns=['NAICS Code',
                                                    '2013r',
                                                    'Unnamed: 13',
                                                    '2012r',
                                                    'Unnamed: 15',
                                                    '2011r',
                                                    'Unnamed: 17',
                                                    '2010',
                                                     'Unnamed: 19',
                                                     '2009',
                                                     'Unnamed: 21',
                                                     '2008',
                                                     'Unnamed: 23',
                                                     '2007',
                                                     'Unnamed: 25',
                                                     '2006',
                                                     'Unnamed: 27',
                                                     '2005',
                                                     'Unnamed: 29',
                                                     '2004',
                                                     'Unnamed: 31',
                                                     '2003',
                                                     'Unnamed: 33',
                                                     '2002',
                                                     'Unnamed: 35',
                                                     '2001',
                                                     'Unnamed: 37',
                                                     '2000',
                                                     'Unnamed: 39',
                                                     '1999',
                                                     'Unnamed: 41',
                                                    '2004', 
                                                    'Unnamed: 31',
                                                    '2003', 
                                                    'Unnamed: 33',
                                                    '2002',
                                                    'Unnamed: 35',
                                                    '2001',
                                                    'Unnamed: 37',
                                                    '2000',
                                                    'Unnamed: 39',
                                                    '1999',
                                                    'Unnamed: 41',
                                                   
                                                   ])
621/6:
retail_history_df
# D - Denotes an estimate withheld to avoid disclosing data of individual companies; data are included in higher-level totals.
# S - Estimate does not meet publication standards because of high sampling variability (coefficient of variation is greater than 30%), poor response quality (total quantity response rate is less than 50%), or other concerns about the estimate's quality. Unpublished estimates derived from this table by subtraction are subject to these same limitations and should not be attributed to the U.S. Census Bureau. For a description of publication standards and the total quantity response rate, see https://www.census.gov/about/policies/quality/standards/standardf1.html.
621/7: retail_history_df
621/8:
# Drop rows that do not have ecommerce data
retail_history_df = retail_history_df.drop([0, 8])
621/9: retail_history_df = retail_history_df.reset_index(drop=True)
621/10: retail_history_df = retail_history_df.set_index('Kind_of_Business')
621/11:
# Replaced S & D, final DataFrame
retail_history_df = retail_history_df.replace({'S': '0',
                                               'D': '0',
                                              })
retail_history_df
621/12:
# Replaced S & D, final DataFrame
retail_history_df = retail_history_df.replace({'S': '0',
                                               'D': '0',
                                              })
621/13:
# remove commas from dataframe
retail_history_df.replace(',','', regex=True, inplace=True)
621/14:
# reset index 
retail_history_df = retail_history_df.reset_index()
621/15:
# Used the melt function to switch columns and rows of dataframe
retail_historymelted_df = retail_history_df.melt(id_vars=['Kind_of_Business'])
621/16:
# Used the melt function to switch columns and rows of dataframe
retail_historymelted_df = retail_history_df.melt(id_vars=['Kind_of_Business'])
retail_historymelted_df
621/17:
# Created two new columns by locating total and ecommerce total values from variable columns 
retail_historymelted_df.loc[retail_historymelted_df['variable'].str.contains('Total', case=False), 'Type']= 'Retail Total'
retail_historymelted_df.loc[retail_historymelted_df['variable'].str.contains('Ecommerce Total', case=False),'Type']= 'Ecommerce'
621/18:
# Looking for only numbers - reg.ex = regular expression.
retail_historymelted_df['Year'] = retail_historymelted_df['variable'].str.extract('(\d+)', expand=False)
retail_historymelted_df['Year'] = retail_historymelted_df['Year'].astype(int)
retail_historymelted_df['value'] = retail_historymelted_df['value'].astype(float)
621/19:
#Sns line plot
plt.figure(figsize=(20,20))
retail_history_plot = sns.lineplot(data=retail_historymelted_df, x='Year', y='value', hue='Kind_of_Business')
plt.show()
621/20:
# Looking for only numbers - reg.ex = regular expression.
retail_historymelted_df['Year'] = retail_historymelted_df['variable'].str.extract('(\d+)', expand=False)
retail_historymelted_df['Year'] = retail_historymelted_df['Year'].astype(int)
retail_historymelted_df['value'] = retail_historymelted_df['value'].astype(float)
621/21:
#Sns line plot
plt.figure(figsize=(20,20))
retail_history_plot = sns.barplot(data=retail_historymelted_df, x='Year', y='value', hue='Kind_of_Business')
plt.show()
621/22:
topfive_2018_df = pd.DataFrame({ '2018 Ecommerce Sales': [453584, 451635, 35348, 10981, 4837]},
                               index=['Nonstore  Retailers', 'E-Shopping & Mail-Order', 'Vehicle & Accessories',
                                     'Clothing', 'Food & Beverage'])
topfive_2018_df
621/23: retail_historymelted_df
621/24: retail_historymelted_df.nlargest(5,'value')
621/25: retail_historymelted_df.nlargest(5,'variable')
621/26: retail_historymelted_df.nlargest(5,'value')
621/27: retail_historymelted_df
621/28: retail_history_2018 = retail_historymelted_df.groupby('Year'==2018)
621/29:
# 2018 Data
retail_historymelted_df[retail_historymelted_df.year == 2018]
621/30:
# 2018 Data
retail_historymelted_df[retail_historymelted_df.Year == 2018]
621/31:
# 2018 Data
retail_historymelted_df[retail_historymelted_df.Year == 2018 & retail_historymelted_df.Type = Retail]
621/32:
# 2018 Data
retail_historymelted_df[retail_historymelted_df.Year == 2018 & retail_historymelted_df.Type == 'Retail']
621/33:
# 2018 Data
retail_historymelted_df[retail_historymelted_df.Year == 2018]
621/34:
# Created two new columns by locating total and ecommerce total values from variable columns 
# retail_historymelted_df.loc[retail_historymelted_df['variable'].str.contains('Total', case=False), 'Type']= 'Retail Total'
retail_historymelted_df.loc[retail_historymelted_df['variable'].str.contains('Ecommerce Total', case=False),'Type']= 'Ecommerce'
621/35:
# Looking for only numbers - reg.ex = regular expression.
retail_historymelted_df['Year'] = retail_historymelted_df['variable'].str.extract('(\d+)', expand=False)
retail_historymelted_df['Year'] = retail_historymelted_df['Year'].astype(int)
retail_historymelted_df['value'] = retail_historymelted_df['value'].astype(float)
621/36: retail_historymelted_df
621/37:
# Created two new columns by locating total and ecommerce total values from variable columns 
retail_historymelted_df.loc[retail_historymelted_df['variable'].str.contains('Total', case=False), 'Type']= 'Retail Total'
retail_historymelted_df.loc[retail_historymelted_df['variable'].str.contains('Ecommerce Total', case=False),'Type']= 'Ecommerce'
621/38:
# Looking for only numbers - reg.ex = regular expression.
retail_historymelted_df['Year'] = retail_historymelted_df['variable'].str.extract('(\d+)', expand=False)
retail_historymelted_df['Year'] = retail_historymelted_df['Year'].astype(int)
retail_historymelted_df['value'] = retail_historymelted_df['value'].astype(float)
621/39:
# Created two new columns by locating total and ecommerce total values from variable columns 
retail_historymelted_df.loc[retail_historymelted_df['variable'].str.contains('Ecommerce Total', case=False),'Type']= 'Ecommerce'
621/40:
# Looking for only numbers - reg.ex = regular expression.
retail_historymelted_df['Year'] = retail_historymelted_df['variable'].str.extract('(\d+)', expand=False)
retail_historymelted_df['Year'] = retail_historymelted_df['Year'].astype(int)
retail_historymelted_df['value'] = retail_historymelted_df['value'].astype(float)
621/41: retail_historymelted_df
621/42:
# Created two new columns by locating total and ecommerce total values from variable columns 
retail_historymelted_df.loc[retail_historymelted_df['variable'].str.contains('Total', case=False), 'Type']= 'Retail Total'
retail_historymelted_df.loc[retail_historymelted_df['variable'].str.contains('Ecommerce Total', case=False),'Type']= 'Ecommerce'
621/43:
# 2018 Data
data_2018 = retail_historymelted_df[retail_historymelted_df.Year == 2018]
data_2018
621/44:
# 2018 Data
data_2018 = retail_historymelted_df[(retail_historymelted_df.Year == 2018) & (retail_historymelted_df.Type == 'Ecommerce')]
data_2018
621/45:
# 2018 Data
data_2018 = retail_historymelted_df[(retail_historymelted_df.Year == 2018) & (retail_historymelted_df.Type == 'Ecommerce')]
data_2018.nlargest(5, 'value')
621/46:
# 2018 Data
data_2018 = retail_historymelted_df[(retail_historymelted_df.Year == 2018) & (retail_historymelted_df.Type == 'Ecommerce')]
data_2018.nlargest(6, 'value')
621/47:
# 2018 Data
data_2018 = retail_historymelted_df[(retail_historymelted_df.Year == 2018) & (retail_historymelted_df.Type == 'Ecommerce')]
data_2018 = data_2018.nlargest(6, 'value')
621/48:
# 2018 Data
data_2018 = retail_historymelted_df[(retail_historymelted_df.Year == 2018) & (retail_historymelted_df.Type == 'Ecommerce')]
data_2018 = data_2018.nlargest(6, 'value')
data_2018
621/49:
# 2018 Data
data_2018 = retail_historymelted_df[(retail_historymelted_df.Year == 2018) & (retail_historymelted_df.Type == 'Ecommerce')]
data_2018 = data_2018.nlargest(6, 'value')
data_2018

# 2017 Data

data_2017 = retail_historymelted_df[(retail_historymelted_df.Year == 2017) & (retail_historymelted_df.Type == 'Ecommerce')]
data_2017 = data_2017.nlargest(6, 'value')
data_2017
621/50:
# 2018 Data
data_2018 = retail_historymelted_df[(retail_historymelted_df.Year == 2018) & (retail_historymelted_df.Type == 'Ecommerce')]
data_2018 = data_2018.nlargest(6, 'value')

# 2017 Data
data_2017 = retail_historymelted_df[(retail_historymelted_df.Year == 2017) & (retail_historymelted_df.Type == 'Ecommerce')]
data_2017 = data_2017.nlargest(6, 'value')

# 2016 Data
data_2016 = retail_historymelted_df[(retail_historymelted_df.Year == 2016) & (retail_historymelted_df.Type == 'Ecommerce')]
data_2016 = data_2016.nlargest(6, 'value')

# 2015 Data
data_2015 = retail_historymelted_df[(retail_historymelted_df.Year == 2015) & (retail_historymelted_df.Type == 'Ecommerce')]
data_2015 = data_2015.nlargest(6, 'value')

# 2014 Data
data_2014 = retail_historymelted_df[(retail_historymelted_df.Year == 2014) & (retail_historymelted_df.Type == 'Ecommerce')]
data_2014 = data_2014.nlargest(6, 'value')

# 1998 Data
data_1998 = retail_historymelted_df[(retail_historymelted_df.Year == 1998) & (retail_historymelted_df.Type == 'Ecommerce')]
data_1998 = data_1998.nlargest(6, 'value')
621/51:
# Combine Dataframes
top_five_df = (data_2018) + (data_2017) + (data_2016) + (data_2015) + (data_2014) + (data_1998)
top_five_df
621/52:
# 2018 Data
data_2018 = retail_historymelted_df[(retail_historymelted_df.Year == 2018) & (retail_historymelted_df.Type == 'Ecommerce')]
data_2018 = data_2018.nlargest(6, 'value')

# 2017 Data
data_2017 = retail_historymelted_df[(retail_historymelted_df.Year == 2017) & (retail_historymelted_df.Type == 'Ecommerce')]
data_2017 = data_2017.nlargest(6, 'value')

# 2016 Data
data_2016 = retail_historymelted_df[(retail_historymelted_df.Year == 2016) & (retail_historymelted_df.Type == 'Ecommerce')]
data_2016 = data_2016.nlargest(6, 'value')

# 2015 Data
data_2015 = retail_historymelted_df[(retail_historymelted_df.Year == 2015) & (retail_historymelted_df.Type == 'Ecommerce')]
data_2015 = data_2015.nlargest(6, 'value')

# 2014 Data
data_2014 = retail_historymelted_df[(retail_historymelted_df.Year == 2014) & (retail_historymelted_df.Type == 'Ecommerce')]
data_2014 = data_2014.nlargest(6, 'value')

# 1998 Data
data_1998 = retail_historymelted_df[(retail_historymelted_df.Year == 1998) & (retail_historymelted_df.Type == 'Ecommerce')]
data_1998 = data_1998.nlargest(6, 'value')

data_1998
621/53: data_1998.plot(kind='bar')
621/54:
plt.figure(figsize=(20,3))
plt.bar(x_axis, data_2018["value"], color='r', alpha=0.5, align="edge")
plt.xticks(tick_locations, data_2018["Kind_of_Business"], rotation="vertical")
621/55:
# Set x axis and tick locations
x_axis = np.arange(len(data_2018))
tick_locations = [value+0.4 for value in x_axis]
621/56:
plt.figure(figsize=(20,3))
plt.bar(x_axis, data_2018["value"], color='r', alpha=0.5, align="edge")
plt.xticks(tick_locations, data_2018["Kind_of_Business"], rotation="vertical")
621/57:
plt.figure(figsize=(20,3))
plt.bar(x_axis, data_2018["value"], color='r', alpha=0.5, align="edge")
plt.xticks(tick_locations, data_2018["Kind_of_Business"], rotation="vertical")

# Set a Title and labels
plt.title("Top Five Ecommerce Product Categories - 1998")
plt.xlabel("State")
plt.ylabel("Average Amount of Rainfall in Inches")
621/58:
#Sns line plot
plt.figure(figsize=(20,20))
retail_history_plot = sns.barplot(data=retail_historymelted_df, x='Year', y='value', hue='Kind_of_Business')
plt.show()
621/59:
plt.figure(figsize=(20,3))
plt.bar(x_axis, data_2018["value"], color='r', alpha=0.5, align="edge")
plt.xticks(tick_locations, data_2018["Kind_of_Business"], rotation="vertical")

# Set a Title and labels
plt.title("Top Five Ecommerce Product Categories - 1998")
plt.xlabel("Product Category")
plt.ylabel("Total Sales in Millions ($)")
621/60:
# Set x axis and tick locations
x_axis = np.arange(len(data_2018))
tick_locations = [value+0.4 for value in x_axis]


plt.figure(figsize=(20,3))
plt.bar(x_axis, data_2018["value"], color='r', alpha=0.5, align="edge")
plt.xticks(tick_locations, data_2018["Kind_of_Business"], rotation="vertical")

# Set a Title and labels
plt.title("Top Five Ecommerce Product Categories - 1998")
plt.xlabel("Product Category")
plt.ylabel("Total Sales in Millions ($)")
621/61:
# Set x axis and tick locations
x_axis = np.arange(len(data_2018))
tick_locations = [value+0.4 for value in x_axis]


plt.figure(figsize=(20,3))
plt.bar(x_axis, data_2018["value"], color='r', alpha=0.5, align="edge")
plt.xticks(tick_locations, data_2018["Kind_of_Business"], rotation="vertical")

# Set a Title and labels
plt.title("Top Five Ecommerce Product Categories - 2018")
plt.xlabel("Product Category")
plt.ylabel("Total Sales in Millions ($)")
621/62:
# Set x axis and tick locations
x_axis = np.arange(len(data_2017))
tick_locations = [value+0.4 for value in x_axis]


plt.figure(figsize=(20,3))
plt.bar(x_axis, data_2017["value"], color='r', alpha=0.5, align="edge")
plt.xticks(tick_locations, data_2017["Kind_of_Business"], rotation="vertical")

# Set a Title and labels
plt.title("Top Five Ecommerce Product Categories - 2017")
plt.xlabel("Product Category")
plt.ylabel("Total Sales in Millions ($)")
621/63:
# Set x axis and tick locations
x_axis = np.arange(len(data_2017))
tick_locations = [value+0.4 for value in x_axis]


plt.figure(figsize=(20,3))
plt.bar(x_axis, data_2017["value"], color='b', alpha=0.5, align="edge")
plt.xticks(tick_locations, data_2017["Kind_of_Business"], rotation="vertical")

# Set a Title and labels
plt.title("Top Five Ecommerce Product Categories - 2017")
plt.xlabel("Product Category")
plt.ylabel("Total Sales in Millions ($)")
621/64:
# Set x axis and tick locations
x_axis = np.arange(len(data_2016))
tick_locations = [value+0.4 for value in x_axis]


plt.figure(figsize=(20,3))
plt.bar(x_axis, data_2016["value"], color='r', alpha=1, align="edge")
plt.xticks(tick_locations, data_2016["Kind_of_Business"], rotation="vertical")

# Set a Title and labels
plt.title("Top Five Ecommerce Product Categories - 2016")
plt.xlabel("Product Category")
plt.ylabel("Total Sales in Millions ($)")
621/65:
# Set x axis and tick locations
x_axis = np.arange(len(data_2015))
tick_locations = [value+0.4 for value in x_axis]


plt.figure(figsize=(20,3))
plt.bar(x_axis, data_2015["value"], color='r', alpha=0.5, align="edge")
plt.xticks(tick_locations, data_2015["Kind_of_Business"], rotation="vertical")

# Set a Title and labels
plt.title("Top Five Ecommerce Product Categories - 2015")
plt.xlabel("Product Category")
plt.ylabel("Total Sales in Millions ($)")
621/66:
# Set x axis and tick locations
x_axis = np.arange(len(data_2015))
tick_locations = [value+0.4 for value in x_axis]


plt.figure(figsize=(20,3))
plt.bar(x_axis, data_2015["value"], color='y', alpha=0.5, align="edge")
plt.xticks(tick_locations, data_2015["Kind_of_Business"], rotation="vertical")

# Set a Title and labels
plt.title("Top Five Ecommerce Product Categories - 2015")
plt.xlabel("Product Category")
plt.ylabel("Total Sales in Millions ($)")
621/67:
# Set x axis and tick locations
x_axis = np.arange(len(data_2014))
tick_locations = [value+0.4 for value in x_axis]


plt.figure(figsize=(20,3))
plt.bar(x_axis, data_2014["value"], color='o', alpha=0.5, align="edge")
plt.xticks(tick_locations, data_2014["Kind_of_Business"], rotation="vertical")

# Set a Title and labels
plt.title("Top Five Ecommerce Product Categories - 2014")
plt.xlabel("Product Category")
plt.ylabel("Total Sales in Millions ($)")
621/68:
# Set x axis and tick locations
x_axis = np.arange(len(data_2014))
tick_locations = [value+0.4 for value in x_axis]


plt.figure(figsize=(20,3))
plt.bar(x_axis, data_2014["value"], color='orange', alpha=0.5, align="edge")
plt.xticks(tick_locations, data_2014["Kind_of_Business"], rotation="vertical")

# Set a Title and labels
plt.title("Top Five Ecommerce Product Categories - 2014")
plt.xlabel("Product Category")
plt.ylabel("Total Sales in Millions ($)")
621/69:
# Set x axis and tick locations
x_axis = np.arange(len(data_1998))
tick_locations = [value+0.4 for value in x_axis]


plt.figure(figsize=(20,3))
plt.bar(x_axis, data_1998["value"], color='purple', alpha=1, align="edge")
plt.xticks(tick_locations, data_1998["Kind_of_Business"], rotation="vertical")

# Set a Title and labels
plt.title("Top Five Ecommerce Product Categories - 1998")
plt.xlabel("Product Category")
plt.ylabel("Total Sales in Millions ($)")
621/70:
# Set x axis and tick locations
x_axis = np.arange(len(data_2015))
tick_locations = [value+0.4 for value in x_axis]


plt.figure(figsize=(20,3))
plt.bar(x_axis, data_2015["value"], color='y', alpha=1, align="edge")
plt.xticks(tick_locations, data_2015["Kind_of_Business"], rotation="vertical")

# Set a Title and labels
plt.title("Top Five Ecommerce Product Categories - 2015")
plt.xlabel("Product Category")
plt.ylabel("Total Sales in Millions ($)")
621/71:
# Set x axis and tick locations
x_axis = np.arange(len(data_2017))
tick_locations = [value+0.4 for value in x_axis]


plt.figure(figsize=(20,3))
plt.bar(x_axis, data_2017["value"], color='b', alpha=1, align="edge")
plt.xticks(tick_locations, data_2017["Kind_of_Business"], rotation="vertical")

# Set a Title and labels
plt.title("Top Five Ecommerce Product Categories - 2017")
plt.xlabel("Product Category")
plt.ylabel("Total Sales in Millions ($)")
621/72:
#Sns line plot
plt.figure(figsize=(20,20))
retail_history_plot = sns.lineplot(data=retail_historymelted_df, x='Year', y='value', hue='Kind_of_Business')
plt.show()
621/73:
#Sns line plot
plt.figure(figsize=(20,20))
retail_history_plot = sns.scatterplot(data=retail_historymelted_df, x='Year', y='value', hue='Kind_of_Business')
plt.show()
621/74:
#Sns line plot
plt.figure(figsize=(20,20))
retail_history_plot = sns.histplot(data=retail_historymelted_df, x='Year', y='value', hue='Kind_of_Business')
plt.show()
621/75:
# Set x axis and tick locations
x_axis = np.arange(len(data_1998))
tick_locations = [value+0.4 for value in x_axis]


plt.figure(figsize=(20,3))
plt.bar(x_axis, data_1998["value"], color='purple', alpha=1, align="edge")
plt.xticks(tick_locations, data_1998["Kind_of_Business"], rotation="vertical")

# Set a Title and labels
plt.title("Top Five Ecommerce Product Categories - 1998")
plt.xlabel("Product Category")
plt.ylabel("Total Sales in Millions ($)")
plt.savefigure('Top Five Ecommerce Product Categories - 1998.png')
621/76:
# Set x axis and tick locations
x_axis = np.arange(len(data_1998))
tick_locations = [value+0.4 for value in x_axis]


plt.figure(figsize=(20,3))
plt.bar(x_axis, data_1998["value"], color='purple', alpha=1, align="edge")
plt.xticks(tick_locations, data_1998["Kind_of_Business"], rotation="vertical")

# Set a Title and labels
plt.title("Top Five Ecommerce Product Categories - 1998")
plt.xlabel("Product Category")
plt.ylabel("Total Sales in Millions ($)")
plt.savefig('Top Five Ecommerce Product Categories - 1998.png')
621/77:
# Set x axis and tick locations
x_axis = np.arange(len(data_2018))
tick_locations = [value+0.4 for value in x_axis]


plt.figure(figsize=(20,3))
plt.bar(x_axis, data_2018["value"], color='r', alpha=0.5, align="edge")
plt.xticks(tick_locations, data_2018["Kind_of_Business"], rotation="vertical")

# Set a Title and labels
plt.title("Top Five Ecommerce Product Categories - 2018")
plt.xlabel("Product Category")
plt.ylabel("Total Sales in Millions ($)")
plt.savefig('Top Five Ecommerce Product Categories - 2018.png')
621/78:
# Set x axis and tick locations
x_axis = np.arange(len(data_2017))
tick_locations = [value+0.4 for value in x_axis]


plt.figure(figsize=(20,3))
plt.bar(x_axis, data_2017["value"], color='b', alpha=1, align="edge")
plt.xticks(tick_locations, data_2017["Kind_of_Business"], rotation="vertical")

# Set a Title and labels
plt.title("Top Five Ecommerce Product Categories - 2017")
plt.xlabel("Product Category")
plt.ylabel("Total Sales in Millions ($)")
plt.savefig('Top Five Ecommerce Product Categories - 2017.png')
621/79:
# Set x axis and tick locations
x_axis = np.arange(len(data_2016))
tick_locations = [value+0.4 for value in x_axis]


plt.figure(figsize=(20,3))
plt.bar(x_axis, data_2016["value"], color='r', alpha=1, align="edge")
plt.xticks(tick_locations, data_2016["Kind_of_Business"], rotation="vertical")

# Set a Title and labels
plt.title("Top Five Ecommerce Product Categories - 2016")
plt.xlabel("Product Category")
plt.ylabel("Total Sales in Millions ($)")
plt.savefig('Top Five Ecommerce Product Categories - 2016.png')
621/80:
# Set x axis and tick locations
x_axis = np.arange(len(data_2015))
tick_locations = [value+0.4 for value in x_axis]


plt.figure(figsize=(20,3))
plt.bar(x_axis, data_2015["value"], color='y', alpha=1, align="edge")
plt.xticks(tick_locations, data_2015["Kind_of_Business"], rotation="vertical")

# Set a Title and labels
plt.title("Top Five Ecommerce Product Categories - 2015")
plt.xlabel("Product Category")
plt.ylabel("Total Sales in Millions ($)")
plt.savefig('Top Five Ecommerce Product Categories - 2015.png')
621/81:
# Set x axis and tick locations
x_axis = np.arange(len(data_2014))
tick_locations = [value+0.4 for value in x_axis]


plt.figure(figsize=(20,3))
plt.bar(x_axis, data_2014["value"], color='orange', alpha=0.5, align="edge")
plt.xticks(tick_locations, data_2014["Kind_of_Business"], rotation="vertical")

# Set a Title and labels
plt.title("Top Five Ecommerce Product Categories - 2014")
plt.xlabel("Product Category")
plt.ylabel("Total Sales in Millions ($)")
plt.savefig('Top Five Ecommerce Product Categories - 2014.png')
621/82:
# Set x axis and tick locations
x_axis = np.arange(len(data_1998))
tick_locations = [value+0.4 for value in x_axis]


plt.figure(figsize=(20,3))
plt.bar(x_axis, data_1998["value"], color='purple', alpha=1, align="edge")
plt.xticks(tick_locations, data_1998["Kind_of_Business"], rotation="vertical")

# Set a Title and labels
plt.title("Top Five Ecommerce Product Categories - 1998")
plt.xlabel("Product Category")
plt.ylabel("Total Sales in Millions ($)")
plt.savefig('Top Five Ecommerce Product Categories - 1998.png')
621/83: PA_Data_csv = "Datasets/PA Retail Data.csv"
621/84:
PA_Data_csv = "Datasets/PA Retail Data.csv"
IN_Data_csv = "Datasets/Indiana Retail Trade Data.csv"
AZ_retail_csv = "Datasets/AZ Retail Data.csv"
CA_retail_csv = "Datasets/CA Retail Data.csv"
FL_retail_csv = "Datasets/Florida Retail Data.csv"
621/85:
# Read with pandas
PA_Data_df = pd.read_csv(PA_Data_csv)
PA_Data_df.head()
621/86:
# Delete extraneous column
PA_Data_df = PA_Data_df.drop(columns=['Geo Footnote', '2017 NAICS Footnote', 'id', '2017 NAICS code', 
                            'Annual payroll ($1,000)', 'First-quarter payroll ($1,000)', 
                            'Number of employees', 'Range indicating percent of total annual payroll imputed', 
                            'Range indicating percent of total employees imputed', 
                           ])


PA_Data_df
621/87:
# Read with pandas
PA_Data_df = pd.read_csv(PA_Data_csv)
PA_Data_df.head()
621/88:
# Delete extraneous column
PA_Data_df = PA_Data_df.drop(columns=['Geo Footnote', '2017 NAICS Footnote', 'id', '2017 NAICS code', 
                            'Annual payroll ($1,000)', 'First-quarter payroll ($1,000)', 
                            'Number of employees', 'Range indicating percent of total annual payroll imputed', 
                            'Range indicating percent of total employees imputed', 
                           ])


PA_Data_df
621/89: PA_Data_df = PA_Data_df.replace({'D': '0'})
621/90:
PA_Data_df.loc[PA_Data_df['Meaning of NAICS code']=='Electronic shopping and mail-order houses', 'eCommerce?']= 1
PA_Data_df.loc[PA_Data_df['Meaning of NAICS code']!='Electronic shopping and mail-order houses', 'eCommerce?']= 0
621/91: PA_Data_df['Sales, value of shipments, or revenue ($1,000)'] = PA_Data_df['Sales, value of shipments, or revenue ($1,000)'].astype(float)
621/92:
ecommerce_totals = PA_Data_df.groupby('eCommerce?')['Sales, value of shipments, or revenue ($1,000)'].sum()
ecommerce_totals
621/93:
#Create Pie Chart
labels = "eCommerce", "Retail"
value = [ecommerce_totals[1.0], ecommerce_totals[0.0]]
colors = ["purple", "blue"]
explode = (0.3,0)

# Create a pie chart based upon the above data
plt.pie(value, labels=labels, colors=colors, explode=explode,
        autopct="%1.1f%%", shadow=True, startangle=180)
plt.title("eCommerce vs Retail 2017, Pennsylvania")

# Create axes which are equal so we have a perfect circle
plt.axis("equal")
plt.show()
#plt.savefig("../Images/PyPies.png")
621/94:
# Read with pandas
IN_Data_df = pd.read_csv(IN_Data_csv)
IN_Data_df.head()
621/95:
# Read with pandas
IN_Data_df = pd.read_csv(IN_Data_csv)
IN_Data_df.head()
621/96:
# Delete extraneous column
IN_Data_df = IN_Data_df.drop(columns=['Geo Footnote', '2017 NAICS Footnote', 'id', '2017 NAICS code', 
                            'Annual payroll ($1,000)', 'First-quarter payroll ($1,000)', 
                            'Number of employees', 'Range indicating percent of total annual payroll imputed', 
                            'Range indicating percent of total employees imputed', 
                           ])
621/97: IN_Data_df = IN_Data_df.replace({'D': '0'})
621/98:
IN_Data_df.loc[IN_Data_df['Meaning of NAICS code']=='Electronic shopping and mail-order houses', 'eCommerce?']= 1
IN_Data_df.loc[IN_Data_df['Meaning of NAICS code']!='Electronic shopping and mail-order houses', 'eCommerce?']= 0
621/99: IN_Data_df['Sales, value of shipments, or revenue ($1,000)'] = IN_Data_df['Sales, value of shipments, or revenue ($1,000)'].astype(float)
621/100: ecommerce_totals = IN_Data_df.groupby('eCommerce?')['Sales, value of shipments, or revenue ($1,000)'].sum()
621/101:
PA_ecommerce_totals = PA_Data_df.groupby('eCommerce?')['Sales, value of shipments, or revenue ($1,000)'].sum()
PA_ecommerce_totals
621/102:
IN_ecommerce_totals = IN_Data_df.groupby('eCommerce?')['Sales, value of shipments, or revenue ($1,000)'].sum()
IN_ecommerce_totals
621/103:
#Create Pie Chart
labels = "eCommerce", "Retail"
value = [PA_ecommerce_totals[1.0], ecommerce_totals[0.0]]
colors = ["purple", "blue"]
explode = (0.3,0)

# Create a pie chart based upon the above data
plt.pie(value, labels=labels, colors=colors, explode=explode,
        autopct="%1.1f%%", shadow=True, startangle=180)
plt.title("eCommerce vs Retail 2017, Pennsylvania")

# Create axes which are equal so we have a perfect circle
plt.axis("equal")
plt.show()
#plt.savefig("../Images/PyPies.png")
621/104:
#Create Pie Chart
labels = "eCommerce", "Retail"
value = [IN_ecommerce_totals[1.0], ecommerce_totals[0.0]]
colors = ["purple", "red"]
explode = (0.3,0)

# Create a pie chart based upon the above data
plt.pie(value, labels=labels, colors=colors, explode=explode,
        autopct="%1.1f%%", shadow=True, startangle=180)
plt.title("eCommerce vs Retail 2017, Pennsylvania")

# Create axes which are equal so we have a perfect circle
plt.axis("equal")
plt.show()
#plt.savefig("../Images/PyPies.png")
621/105:
#Create Pie Chart
labels = "eCommerce", "Retail"
value = [IN_ecommerce_totals[1.0], ecommerce_totals[0.0]]
colors = ["purple", "red"]
explode = (0.3,0)

# Create a pie chart based upon the above data
plt.pie(value, labels=labels, colors=colors, explode=explode,
        autopct="%1.1f%%", shadow=True, startangle=180)
plt.title("eCommerce vs Retail 2017, Indiana")

# Create axes which are equal so we have a perfect circle
plt.axis("equal")
plt.show()
#plt.savefig("../Images/PyPies.png")
621/106:
# creating a data frame 
AZ_df = pd.read_csv(AZ_retail_csv) 
AZ_df.head()
621/107: AZ_df = AZ_df.replace({'D': '0'})
621/108:
# Delete extraneous column
AZ_df = AZ_df.drop(columns=['Geo Footnote', '2017 NAICS Footnote', 'id', '2017 NAICS code', 
                            'Annual payroll ($1,000)', 'First-quarter payroll ($1,000)', 
                            'Number of employees', 'Range indicating percent of total annual payroll imputed', 
                            'Range indicating percent of total employees imputed', 
                           ])
621/109:
AZ_df.loc[AZ_df['Meaning of NAICS code']=='Electronic shopping and mail-order houses', 'eCommerce?']= 1
AZ_df.loc[AZ_df['Meaning of NAICS code']!='Electronic shopping and mail-order houses', 'eCommerce?']= 0
621/110:

AZ_df['Sales, value of shipments, or revenue ($1,000)'] = AZ_df['Sales, value of shipments, or revenue ($1,000)'].astype(float)
621/111:

my_results_AZ = AZ_df.groupby('eCommerce?')['Sales, value of shipments, or revenue ($1,000)'].sum()
my_results_AZ
621/112:
#Create Pie Chart
labels = "eCommerce", "Retail"
value = [IN_ecommerce_totals[1.0], IN_ecommerce_totals[0.0]]
colors = ["purple", "red"]
explode = (0.3,0)

# Create a pie chart based upon the above data
plt.pie(value, labels=labels, colors=colors, explode=explode,
        autopct="%1.1f%%", shadow=True, startangle=180)
plt.title("eCommerce vs Retail 2017, Indiana")

# Create axes which are equal so we have a perfect circle
plt.axis("equal")
plt.show()
#plt.savefig("../Images/PyPies.png")
621/113:
#Create Pie Chart
labels = "eCommerce", "Retail"
value = [PA_ecommerce_totals[1.0], PA_ecommerce_totals[0.0]]
colors = ["purple", "blue"]
explode = (0.3,0)

# Create a pie chart based upon the above data
plt.pie(value, labels=labels, colors=colors, explode=explode,
        autopct="%1.1f%%", shadow=True, startangle=180)
plt.title("eCommerce vs Retail 2017, Pennsylvania")

# Create axes which are equal so we have a perfect circle
plt.axis("equal")
plt.show()
#plt.savefig("../Images/PyPies.png")
621/114:
#Create Pie Chart
labels = "eCommerce", "Retail"
value = [my_results_AZ[1.0], my_results_AZ[0.0]]
colors = ["purple", "red"]
explode = (0.3,0)

# Create a pie chart based upon the above data
plt.pie(value, labels=labels, colors=colors, explode=explode,
        autopct="%1.1f%%", shadow=True, startangle=180)
plt.title("eCommerce vs Retail 2017, Arizona)

# Create axes which are equal so we have a perfect circle
plt.axis("equal")
plt.show()
#plt.savefig("../Images/PyPies.png")
621/115:
#Create Pie Chart
labels = "eCommerce", "Retail"
value = [my_results_AZ[1.0], my_results_AZ[0.0]]
colors = ["purple", "red"]
explode = (0.3,0)

# Create a pie chart based upon the above data
plt.pie(value, labels=labels, colors=colors, explode=explode,
        autopct="%1.1f%%", shadow=True, startangle=180)
plt.title("eCommerce vs Retail 2017, Arizona")

# Create axes which are equal so we have a perfect circle
plt.axis("equal")
plt.show()
#plt.savefig("../Images/PyPies.png")
621/116:
#Create Pie Chart
labels = "eCommerce", "Retail"
value = [my_results_AZ[1.0], my_results_AZ[0.0]]
colors = ["purple", "orange"]
explode = (0.3,0)

# Create a pie chart based upon the above data
plt.pie(value, labels=labels, colors=colors, explode=explode,
        autopct="%1.1f%%", shadow=True, startangle=180)
plt.title("eCommerce vs Retail 2017, Arizona")

# Create axes which are equal so we have a perfect circle
plt.axis("equal")
plt.show()
#plt.savefig("../Images/PyPies.png")
621/117:
# creating a data frame 
CA_df = pd.read_csv(CA_retail_csv) 
CA_df.head()
621/118:
# Delete extraneous column
CA_df = CA_df.drop(columns=['Geo Footnote', '2017 NAICS Footnote', 'id', '2017 NAICS code', 
                            'Annual payroll ($1,000)', 'First-quarter payroll ($1,000)', 
                            'Number of employees', 'Range indicating percent of total annual payroll imputed', 
                            'Range indicating percent of total employees imputed', 
                           ])
621/119:
CA_df.loc[CA_df['Meaning of NAICS code']=='Electronic shopping and mail-order houses', 'eCommerce?']= 1
CA_df.loc[CA_df['Meaning of NAICS code']!='Electronic shopping and mail-order houses', 'eCommerce?']= 0
621/120: CA_df.groupby('eCommerce?')['Sales, value of shipments, or revenue ($1,000)'].sum()
621/121: CA_totals = CA_df.groupby('eCommerce?')['Sales, value of shipments, or revenue ($1,000)'].sum()
621/122:
CA_totals = CA_df.groupby('eCommerce?')['Sales, value of shipments, or revenue ($1,000)'].sum()
CA_totals
621/123:
labels = "eCommerce", "Retail"
value = [CA_totals[1.0], CA_totals[0.0]]
colors = ["purple", "orange"]
explode = (0.3,0)

# Create a pie chart based upon the above data
plt.pie(value, labels=labels, colors=colors, explode=explode,
        autopct="%1.1f%%", shadow=True, startangle=180)
plt.title("eCommerce vs Retail 2017, California")

# Create axes which are equal so we have a perfect circle
plt.axis("equal")
plt.show()
#plt.savefig("../Images/PyPies.png")
621/124:
labels = "eCommerce", "Retail"
value = [CA_totals[1.0], CA_totals[0.0]]
colors = ["purple", "pink"]
explode = (0.3,0)

# Create a pie chart based upon the above data
plt.pie(value, labels=labels, colors=colors, explode=explode,
        autopct="%1.1f%%", shadow=True, startangle=180)
plt.title("eCommerce vs Retail 2017, California")

# Create axes which are equal so we have a perfect circle
plt.axis("equal")
plt.show()
#plt.savefig("../Images/PyPies.png")
621/125:
# creating a data frame 
FL_df = pd.read_csv(FL_retail_csv)
621/126:
# Delete extraneous column
FL_df = FL_df.drop(columns=['Geo Footnote', '2017 NAICS Footnote', 'id', '2017 NAICS code', 
                            'Annual payroll ($1,000)', 'First-quarter payroll ($1,000)', 
                            'Number of employees', 'Range indicating percent of total annual payroll imputed', 
                            'Range indicating percent of total employees imputed', 
                           ])
621/127: FL_df = FL_df.replace({'D': '0'})
621/128:

FL_df.loc[FL_df['Meaning of NAICS code']=='Electronic shopping and mail-order houses', 'eCommerce?']= 1
FL_df.loc[FL_df['Meaning of NAICS code']!='Electronic shopping and mail-order houses', 'eCommerce?']= 0
621/129:

FL_df['Sales, value of shipments, or revenue ($1,000)'] = FL_df['Sales, value of shipments, or revenue ($1,000)'].astype(int)
621/130:
my_resultsFL = FL_df.groupby('eCommerce?')['Sales, value of shipments, or revenue ($1,000)'].sum()
my_resultsFL
621/131:
labels = "eCommerce", "Retail"
value = [my_resultsFL[1.0], my_resultsFL[0.0]]
colors = ["purple", "yellow"]
explode = (0.3,0)

# Create a pie chart based upon the above data
plt.pie(value, labels=labels, colors=colors, explode=explode,
        autopct="%1.1f%%", shadow=True, startangle=180)
plt.title("eCommerce vs Retail 2017, Florida")

# Create axes which are equal so we have a perfect circle
plt.axis("equal")
plt.show()
#plt.savefig("../Images/PyPies.png")
621/132:
#Create Pie Chart
labels = "eCommerce", "Retail"
value = [PA_ecommerce_totals[1.0], PA_ecommerce_totals[0.0]]
colors = ["purple", "blue"]
explode = (0.3,0)

# Create a pie chart based upon the above data
plt.pie(value, labels=labels, colors=colors, explode=explode,
        autopct="%1.1f%%", shadow=True, startangle=180)
plt.title("eCommerce vs Retail 2017, Pennsylvania")

# Create axes which are equal so we have a perfect circle
plt.axis("equal")
plt.show()
plt.savefig("../Charts/PNPies.png")
621/133:
#Create Pie Chart
labels = "eCommerce", "Retail"
value = [PA_ecommerce_totals[1.0], PA_ecommerce_totals[0.0]]
colors = ["purple", "blue"]
explode = (0.3,0)

# Create a pie chart based upon the above data
plt.pie(value, labels=labels, colors=colors, explode=explode,
        autopct="%1.1f%%", shadow=True, startangle=180)
plt.title("eCommerce vs Retail 2017, Pennsylvania")

# Create axes which are equal so we have a perfect circle
plt.axis("equal")
plt.show()
plt.savefig("PAPies.png")
621/134:
#Create Pie Chart
labels = "eCommerce", "Retail"
value = [IN_ecommerce_totals[1.0], IN_ecommerce_totals[0.0]]
colors = ["purple", "red"]
explode = (0.3,0)

# Create a pie chart based upon the above data
plt.pie(value, labels=labels, colors=colors, explode=explode,
        autopct="%1.1f%%", shadow=True, startangle=180)
plt.title("eCommerce vs Retail 2017, Indiana")

# Create axes which are equal so we have a perfect circle
plt.axis("equal")
plt.show()
plt.savefig("INPies.png")
621/135:
#Create Pie Chart
labels = "eCommerce", "Retail"
value = [my_results_AZ[1.0], my_results_AZ[0.0]]
colors = ["purple", "orange"]
explode = (0.3,0)

# Create a pie chart based upon the above data
plt.pie(value, labels=labels, colors=colors, explode=explode,
        autopct="%1.1f%%", shadow=True, startangle=180)
plt.title("eCommerce vs Retail 2017, Arizona")

# Create axes which are equal so we have a perfect circle
plt.axis("equal")
plt.show()
plt.savefig("AZPies.png")
621/136:
labels = "eCommerce", "Retail"
value = [CA_totals[1.0], CA_totals[0.0]]
colors = ["purple", "pink"]
explode = (0.3,0)

# Create a pie chart based upon the above data
plt.pie(value, labels=labels, colors=colors, explode=explode,
        autopct="%1.1f%%", shadow=True, startangle=180)
plt.title("eCommerce vs Retail 2017, California")

# Create axes which are equal so we have a perfect circle
plt.axis("equal")
plt.show()
plt.savefig("CAPies.png")
621/137:
labels = "eCommerce", "Retail"
value = [my_resultsFL[1.0], my_resultsFL[0.0]]
colors = ["purple", "yellow"]
explode = (0.3,0)

# Create a pie chart based upon the above data
plt.pie(value, labels=labels, colors=colors, explode=explode,
        autopct="%1.1f%%", shadow=True, startangle=180)
plt.title("eCommerce vs Retail 2017, Florida")

# Create axes which are equal so we have a perfect circle
plt.axis("equal")
plt.show()
plt.savefig("FLPies.png")
621/138:
PA_Data_df.loc[PA_Data_df['Meaning of NAICS code']=='Electronic shopping and mail-order houses', 'eCommerce?']= 1
PA_Data_df.loc[PA_Data_df['Meaning of NAICS code']!='Electronic shopping and mail-order houses', 'eCommerce?']= 0
621/139: PA_Data_df['Number of establishments'] = PA_Data_df['Number of establishments'].astype(float)
621/140:
establishments_total = PA_Data_df.groupby('eCommerce?')['Number of establishments'].sum()
establishments_total
620/16:
PAestablishments_total = PA_Data_df.groupby('eCommerce?')['Number of establishments'].sum()
PAestablishments_total
620/17:
# Create Bar Chart
labels = ["eCommerce", "Retail"]
PAestablishments = [PAestablishments_total[1.0], PAestablishments_total[0.0]]
x_axis = np.arange(len(bars_in_cities))
620/18:
# Create Bar Chart
labels = ["eCommerce", "Retail"]
PAestablishments = [PAestablishments_total[1.0], PAestablishments_total[0.0]]
x_axis = np.arange(len(PAestablishments))
620/19: plt.bar(x_axis, PAestablishments, color=colors, align="center")
620/20:
plt.bar(x_axis, PAestablishments, color=colors, align="center")
# Create the ticks for our bar chart's x axis
tick_locations = [value for value in x_axis]
plt.xticks(tick_locations, labels)
620/21:
plt.bar(x_axis, PAestablishments, color=colors, align="center")
# Create the ticks for our bar chart's x axis
tick_locations = [value for value in x_axis]
plt.xticks(tick_locations, labels)
# Set the limits of the x axis
plt.xlim(-0.75, len(x_axis)-0.25)
620/22:
plt.bar(x_axis, PAestablishments, color=colors, align="center")
# Create the ticks for our bar chart's x axis
tick_locations = [value for value in x_axis]
plt.xticks(tick_locations, labels)

# Set the limits of the x & y axis
plt.xlim(-0.75, len(x_axis)-0.25)

plt.ylim(0, max(PAestablishments)+0.4)
620/23:
plt.bar(x_axis, PAestablishments, color=colors, align="center")
# Create the ticks for our bar chart's x axis
tick_locations = [value for value in x_axis]
plt.xticks(tick_locations, labels)

# Set the limits of the x & y axis
plt.xlim(-0.75, len(x_axis)-0.25)

plt.ylim(0, max(PAestablishments)+0.5)
620/24:
plt.bar(x_axis, PAestablishments, color=colors, align="center")
# Create the ticks for our bar chart's x axis
tick_locations = [value for value in x_axis]
plt.xticks(tick_locations, labels)

# Set the limits of the x & y axis
plt.xlim(-0.75, len(x_axis)-0.25)

plt.ylim(0, max(PAestablishments)+2)
620/25:
plt.bar(x_axis, PAestablishments, color=colors, align="center")
# Create the ticks for our bar chart's x axis
tick_locations = [value for value in x_axis]
plt.xticks(tick_locations, labels)

# Set the limits of the x & y axis
plt.xlim(-0.75, len(x_axis)-0.25)
620/26:
plt.bar(x_axis, PAestablishments, color=colors, align="center")
# Create the ticks for our bar chart's x axis
tick_locations = [value for value in x_axis]
plt.xticks(tick_locations, labels)

# Set the limits of the x & y axis
plt.xlim(-0.75, len(x_axis)-0.25)

plt.title("Number of Establishment, eCommerce vs. Retail - Pennsylvania")
plt.ylabel("Number of Establishments")
620/27:
# Create Bar Chart
labels = ["eCommerce", "Retail"]
PAestablishments = [PAestablishments_total[1.0], PAestablishments_total[0.0]]
x_axis = np.arange(len(PAestablishments))
colors = ["purple", "blue"]
620/28:
plt.bar(x_axis, PAestablishments, color=colors, align="center")
# Create the ticks for our bar chart's x axis
tick_locations = [value for value in x_axis]
plt.xticks(tick_locations, labels)

# Set the limits of the x & y axis
plt.xlim(-0.75, len(x_axis)-0.25)

plt.title("Number of Establishment, eCommerce vs. Retail - Pennsylvania")
plt.ylabel("Number of Establishments")
621/141:
PA_Data_df.loc[PA_Data_df['Meaning of NAICS code']=='Electronic shopping and mail-order houses', 'eCommerce?']= 1
PA_Data_df.loc[PA_Data_df['Meaning of NAICS code']!='Electronic shopping and mail-order houses', 'eCommerce?']= 0
621/142: PA_Data_df['Number of establishments'] = PA_Data_df['Number of establishments'].astype(float)
621/143:
PAestablishments_total = PA_Data_df.groupby('eCommerce?')['Number of establishments'].sum()
PAestablishments_total
621/144:
# Create Bar Chart
labels = ["eCommerce", "Retail"]
PAestablishments = [PAestablishments_total[1.0], PAestablishments_total[0.0]]
x_axis = np.arange(len(PAestablishments))
colors = ["purple", "blue"]

plt.bar(x_axis, PAestablishments, color=colors, align="center")
# Create the ticks for our bar chart's x axis
tick_locations = [value for value in x_axis]
plt.xticks(tick_locations, labels)

# Set the limits of the x & y axis
plt.xlim(-0.75, len(x_axis)-0.25)

plt.title("Number of Establishment, eCommerce vs. Retail - Pennsylvania")
plt.ylabel("Number of Establishments")
621/145:
# Create Bar Chart
labels = ["eCommerce", "Retail"]
PAestablishments = [PAestablishments_total[1.0], PAestablishments_total[0.0]]
x_axis = np.arange(len(PAestablishments))
colors = ["purple", "blue"]

plt.bar(x_axis, PAestablishments, color=colors, align="center")
# Create the ticks for our bar chart's x axis
tick_locations = [value for value in x_axis]
plt.xticks(tick_locations, labels)

# Set the limits of the x & y axis
plt.xlim(-0.75, len(x_axis)-0.25)

plt.title("Number of Establishment, eCommerce vs. Retail - Pennsylvania")
plt.ylabel("Number of Establishments")
plt.savefig("PABar.png")
621/146:
IN_Data_df.loc[IN_Data_df['Meaning of NAICS code']=='Electronic shopping and mail-order houses', 'eCommerce?']= 1
IN_Data_df.loc[IN_Data_df['Meaning of NAICS code']!='Electronic shopping and mail-order houses', 'eCommerce?']= 0
621/147: IN_Data_df['Number of establishments'] = IN_Data_df['Number of establishments'].astype(float)
621/148:
INestablishments_total = IN_Data_df.groupby('eCommerce?')['Number of establishments'].sum()
INestablishments_total
621/149:
# Create Bar Chart
labels = ["eCommerce", "Retail"]
INestablishments = [INestablishments_total[1.0], INestablishments_total[0.0]]
x_axis = np.arange(len(PAestablishments))
colors = ["purple", "red"]

plt.bar(x_axis, INestablishments, color=colors, align="center")
# Create the ticks for our bar chart's x axis
tick_locations = [value for value in x_axis]
plt.xticks(tick_locations, labels)

# Set the limits of the x & y axis
plt.xlim(-0.75, len(x_axis)-0.25)

plt.title("Number of Establishment, eCommerce vs. Retail - Indiana")
plt.ylabel("Number of Establishments")
plt.savefig("INBar.png")
621/150:
AZ_df.loc[AZ_df['Meaning of NAICS code']=='Electronic shopping and mail-order houses', 'eCommerce?']= 1
AZ_df.loc[AZ_df['Meaning of NAICS code']!='Electronic shopping and mail-order houses', 'eCommerce?']= 0
621/151: AZ_df['Number of establishments'] = AZ_df['Number of establishments'].astype(float)
621/152: AZ_df.groupby('eCommerce?')['Number of establishments'].sum()
621/153:
AZestablishments = AZ_df.groupby('eCommerce?')['Number of establishments'].sum()
AZestablishments
621/154:
AZestablishments

# Create Bar Chart
labels = ["eCommerce", "Retail"]
AZestablishments_plot = [AZestablishments[1.0], AZestablishments[0.0]]
x_axis = np.arange(len(AZestablishments))
colors = ["purple", "orange"]

plt.bar(x_axis, AZestablishments_plot, color=colors, align="center")
# Create the ticks for our bar chart's x axis
tick_locations = [value for value in x_axis]
plt.xticks(tick_locations, labels)

# Set the limits of the x & y axis
plt.xlim(-0.75, len(x_axis)-0.25)

plt.title("Number of Establishment, eCommerce vs. Retail - Arizona")
plt.ylabel("Number of Establishments")
plt.savefig("AZBar.png")
621/155:
CA_df.loc[CA_df['Meaning of NAICS code']=='Electronic shopping and mail-order houses', 'eCommerce?']= 1
CA_df.loc[CA_df['Meaning of NAICS code']!='Electronic shopping and mail-order houses', 'eCommerce?']= 0
621/156: CA_df.groupby('eCommerce?')['Number of establishments'].sum()
621/157:
CAestablishments = CA_df.groupby('eCommerce?')['Number of establishments'].sum()
CAestablishments
621/158:
labels = "eCommerce", "Retail"
value = [CA_totals[1.0], CA_totals[0.0]]
colors = ["purple", "pink"]
explode = (0.3,0)

# Create a pie chart based upon the above data
plt.pie(value, labels=labels, colors=colors, explode=explode,
        autopct="%1.1f%%", shadow=True, startangle=180)
plt.title("eCommerce vs Retail 2017, California")

# Create axes which are equal so we have a perfect circle
plt.axis("equal")
plt.show()
plt.savefig("CAPies.png")
621/159:
CAestablishments

# Create Bar Chart
labels = ["eCommerce", "Retail"]
CAestablishments_plot = [CAestablishments[1.0], CAestablishments[0.0]]
x_axis = np.arange(len(CAestablishments))
colors = ["purple", "pink"]

plt.bar(x_axis, CAestablishments_plot, color=colors, align="center")
# Create the ticks for our bar chart's x axis
tick_locations = [value for value in x_axis]
plt.xticks(tick_locations, labels)

# Set the limits of the x & y axis
plt.xlim(-0.75, len(x_axis)-0.25)

plt.title("Number of Establishment, eCommerce vs. Retail - California")
plt.ylabel("Number of Establishments")
plt.savefig("CABar.png")
621/160:
# Create Bar Chart
labels = ["eCommerce", "Retail"]
PAestablishments = [PAestablishments_total[1.0], PAestablishments_total[0.0]]
x_axis = np.arange(len(PAestablishments))
colors = ["purple", "blue"]

plt.bar(x_axis, PAestablishments, color=colors, align="center")
# Create the ticks for our bar chart's x axis
tick_locations = [value for value in x_axis]
plt.xticks(tick_locations, labels)

# Set the limits of the x & y axis
plt.xlim(-0.75, len(x_axis)-0.25)

plt.title("Number of Establishments, eCommerce vs. Retail - Pennsylvania")
plt.ylabel("Number of Establishments")
plt.savefig("PABar.png")
621/161:
# Create Bar Chart
labels = ["eCommerce", "Retail"]
INestablishments = [INestablishments_total[1.0], INestablishments_total[0.0]]
x_axis = np.arange(len(PAestablishments))
colors = ["purple", "red"]

plt.bar(x_axis, INestablishments, color=colors, align="center")
# Create the ticks for our bar chart's x axis
tick_locations = [value for value in x_axis]
plt.xticks(tick_locations, labels)

# Set the limits of the x & y axis
plt.xlim(-0.75, len(x_axis)-0.25)

plt.title("Number of Establishments, eCommerce vs. Retail - Indiana")
plt.ylabel("Number of Establishments")
plt.savefig("INBar.png")
621/162:
AZestablishments

# Create Bar Chart
labels = ["eCommerce", "Retail"]
AZestablishments_plot = [AZestablishments[1.0], AZestablishments[0.0]]
x_axis = np.arange(len(AZestablishments))
colors = ["purple", "orange"]

plt.bar(x_axis, AZestablishments_plot, color=colors, align="center")
# Create the ticks for our bar chart's x axis
tick_locations = [value for value in x_axis]
plt.xticks(tick_locations, labels)

# Set the limits of the x & y axis
plt.xlim(-0.75, len(x_axis)-0.25)

plt.title("Number of Establishments, eCommerce vs. Retail - Arizona")
plt.ylabel("Number of Establishments")
plt.savefig("AZBar.png")
621/163:
FL_df.loc[FL_df['Meaning of NAICS code']=='Electronic shopping and mail-order houses', 'eCommerce?']= 1
FL_df.loc[FL_df['Meaning of NAICS code']!='Electronic shopping and mail-order houses', 'eCommerce?']= 0
621/164:
FLestablishments = FL_df.groupby('eCommerce?')['Number of establishments'].sum()
FLestablishments
621/165: FL_df['Number of establishments'] = FL_df['Number of establishments'].astype(float)
621/166:
FLestablishments = FL_df.groupby('eCommerce?')['Number of establishments'].sum()
FLestablishments
621/167:
FLestablishments
# Create Bar Chart
labels = ["eCommerce", "Retail"]
FLestablishments_plot = [FLestablishments[1.0], FLestablishments[0.0]]
x_axis = np.arange(len(FLestablishments))
colors = ["purple", "yellow"]

plt.bar(x_axis, FLestablishments_plot, color=colors, align="center")
# Create the ticks for our bar chart's x axis
tick_locations = [value for value in x_axis]
plt.xticks(tick_locations, labels)

# Set the limits of the x & y axis
plt.xlim(-0.75, len(x_axis)-0.25)

plt.title("Number of Establishment, eCommerce vs. Retail - Florida")
plt.ylabel("Number of Establishments")
plt.savefig("FLBar.png")
621/168:
CAestablishments

# Create Bar Chart
labels = ["eCommerce", "Retail"]
CAestablishments_plot = [CAestablishments[1.0], CAestablishments[0.0]]
x_axis = np.arange(len(CAestablishments))
colors = ["purple", "pink"]

plt.bar(x_axis, CAestablishments_plot, color=colors, align="center")
# Create the ticks for our bar chart's x axis
tick_locations = [value for value in x_axis]
plt.xticks(tick_locations, labels)

# Set the limits of the x & y axis
plt.xlim(-0.75, len(x_axis)-0.25)

plt.title("Number of Establishments, eCommerce vs. Retail - California")
plt.ylabel("Number of Establishments")
plt.savefig("CABar.png")
621/169:
FLestablishments
# Create Bar Chart
labels = ["eCommerce", "Retail"]
FLestablishments_plot = [FLestablishments[1.0], FLestablishments[0.0]]
x_axis = np.arange(len(FLestablishments))
colors = ["purple", "yellow"]

plt.bar(x_axis, FLestablishments_plot, color=colors, align="center")
# Create the ticks for our bar chart's x axis
tick_locations = [value for value in x_axis]
plt.xticks(tick_locations, labels)

# Set the limits of the x & y axis
plt.xlim(-0.75, len(x_axis)-0.25)

plt.title("Number of Establishments, eCommerce vs. Retail - Florida")
plt.ylabel("Number of Establishments")
plt.savefig("FLBar.png")
621/170:
states_combined_df = pd.Dataframe("Florida" = FLestablishments,
                                 "California" = CAestablishments,
                                 "Indiana" = INestablishments_total,
                                 "Arizona" = AZestablishments,
                                 "Pennsylvania" = PAestablishments_total)
states_combined_df
631/1:
CA_df.loc[CA_df['Meaning of NAICS code']=='Electronic shopping and mail-order houses', 'eCommerce?']= 1
CA_df.loc[CA_df['Meaning of NAICS code']!='Electronic shopping and mail-order houses', 'eCommerce?']= 0
CA_df
631/2:
CA_df.loc[CA_df['Meaning of NAICS code']=='Electronic shopping and mail-order houses', 'eCommerce?']= 1
CA_df.loc[CA_df['Meaning of NAICS code']!='Electronic shopping and mail-order houses', 'eCommerce?']= 0
631/3:
# creating a data frame 
CA_df = pd.read_csv(CA_retail_csv) 
CA_df.head()
631/4:
# Delete extraneous column
CA_df = CA_df.drop(columns=['Geo Footnote', '2017 NAICS Footnote', 'id', '2017 NAICS code', 
                            'Annual payroll ($1,000)', 'First-quarter payroll ($1,000)', 
                            'Number of employees', 'Range indicating percent of total annual payroll imputed', 
                            'Range indicating percent of total employees imputed', 
                           ])
631/5:
CA_df.loc[CA_df['Meaning of NAICS code']=='Electronic shopping and mail-order houses', 'eCommerce?']= 1
CA_df.loc[CA_df['Meaning of NAICS code']!='Electronic shopping and mail-order houses', 'eCommerce?']= 0
632/1:
# Dependencies
import pandas as pd
import numpy as np
import os
import webbrowser
import io
import matplotlib
import matplotlib.pyplot as plt
import seaborn as sns
632/2:
# Save file path to variable
retail_history_csv = "Datasets/Estimated Annual U.S. Retail Trade Sales Total and E-commerce1- 1998-2018.csv"
US_totalrev_2018_2017_csv = "Datasets/US_Retail_Trade_Sales_2018_2017.csv"
632/3:
# Read with pandas
retail_history_df = pd.read_csv(retail_history_csv)
632/4:
# Rename Columns
retail_history_df = retail_history_df.rename(columns={"Unnamed: 1": "Kind_of_Business",
                                 "2018": "2018 Total",
                                 "Unnamed: 3": "2018 Ecommerce Total",
                                 "2017r": "2017 Total",
                                 "Unnamed: 5": "2017 Ecommerce Total",
                                 "2016r": "2016 Total",
                                 "Unnamed: 7": "2016 Ecommerce Total",
                                 "2015r": "2015 Total",
                                 "Unnamed: 9": "2015 Ecommerce Total",
                                 "2014r": "2014 Total",
                                 "Unnamed: 11": "2014 Ecommerce Total",
                                 "1998": "1998 Total",
                                 "Unnamed: 43": "1998 Ecommerce Total"
                                 })
632/5:
# Drop columns we do not need
retail_history_df = retail_history_df.drop(columns=['NAICS Code',
                                                    '2013r',
                                                    'Unnamed: 13',
                                                    '2012r',
                                                    'Unnamed: 15',
                                                    '2011r',
                                                    'Unnamed: 17',
                                                    '2010',
                                                     'Unnamed: 19',
                                                     '2009',
                                                     'Unnamed: 21',
                                                     '2008',
                                                     'Unnamed: 23',
                                                     '2007',
                                                     'Unnamed: 25',
                                                     '2006',
                                                     'Unnamed: 27',
                                                     '2005',
                                                     'Unnamed: 29',
                                                     '2004',
                                                     'Unnamed: 31',
                                                     '2003',
                                                     'Unnamed: 33',
                                                     '2002',
                                                     'Unnamed: 35',
                                                     '2001',
                                                     'Unnamed: 37',
                                                     '2000',
                                                     'Unnamed: 39',
                                                     '1999',
                                                     'Unnamed: 41',
                                                    '2004', 
                                                    'Unnamed: 31',
                                                    '2003', 
                                                    'Unnamed: 33',
                                                    '2002',
                                                    'Unnamed: 35',
                                                    '2001',
                                                    'Unnamed: 37',
                                                    '2000',
                                                    'Unnamed: 39',
                                                    '1999',
                                                    'Unnamed: 41',
                                                   
                                                   ])
632/6: retail_history_df
632/7:
# Drop rows that do not have ecommerce data
retail_history_df = retail_history_df.drop([0, 8])
632/8: retail_history_df = retail_history_df.reset_index(drop=True)
632/9: retail_history_df = retail_history_df.set_index('Kind_of_Business')
632/10:
# Replaced S & D, final DataFrame
retail_history_df = retail_history_df.replace({'S': '0',
                                               'D': '0',
                                              })
632/11:
# remove commas from dataframe
retail_history_df.replace(',','', regex=True, inplace=True)
632/12:
# reset index 
retail_history_df = retail_history_df.reset_index()
632/13:
# Used the melt function to switch columns and rows of dataframe
retail_historymelted_df = retail_history_df.melt(id_vars=['Kind_of_Business'])
retail_historymelted_df
632/14:
# Created two new columns by locating total and ecommerce total values from variable columns 
retail_historymelted_df.loc[retail_historymelted_df['variable'].str.contains('Total', case=False), 'Type']= 'Retail Total'
retail_historymelted_df.loc[retail_historymelted_df['variable'].str.contains('Ecommerce Total', case=False),'Type']= 'Ecommerce'
632/15:
# Looking for only numbers - reg.ex = regular expression.
retail_historymelted_df['Year'] = retail_historymelted_df['variable'].str.extract('(\d+)', expand=False)
retail_historymelted_df['Year'] = retail_historymelted_df['Year'].astype(int)
retail_historymelted_df['value'] = retail_historymelted_df['value'].astype(float)
632/16: retail_historymelted_df
632/17:
#sns plot - 
retail_historymelted_df
632/18:
PA_Data_csv = "Datasets/PA Retail Data.csv"
IN_Data_csv = "Datasets/Indiana Retail Trade Data.csv"
AZ_retail_csv = "Datasets/AZ Retail Data.csv"
CA_retail_csv = "Datasets/CA Retail Data.csv"
FL_retail_csv = "Datasets/Florida Retail Data.csv"
632/19:
# Read with pandas
PA_Data_df = pd.read_csv(PA_Data_csv)
PA_Data_df.head()
632/20:
# Delete extraneous column
PA_Data_df = PA_Data_df.drop(columns=['Geo Footnote', '2017 NAICS Footnote', 'id', '2017 NAICS code', 
                            'Annual payroll ($1,000)', 'First-quarter payroll ($1,000)', 
                            'Number of employees', 'Range indicating percent of total annual payroll imputed', 
                            'Range indicating percent of total employees imputed', 
                           ])


PA_Data_df
632/21: PA_Data_df = PA_Data_df.replace({'D': '0'})
632/22:
PA_Data_df.loc[PA_Data_df['Meaning of NAICS code']=='Electronic shopping and mail-order houses', 'eCommerce?']= 1
PA_Data_df.loc[PA_Data_df['Meaning of NAICS code']!='Electronic shopping and mail-order houses', 'eCommerce?']= 0
632/23: PA_Data_df['Sales, value of shipments, or revenue ($1,000)'] = PA_Data_df['Sales, value of shipments, or revenue ($1,000)'].astype(float)
632/24:
PA_ecommerce_totals = PA_Data_df.groupby('eCommerce?')['Sales, value of shipments, or revenue ($1,000)'].sum()
PA_ecommerce_totals
632/25:
PA_Data_df.loc[PA_Data_df['Meaning of NAICS code']=='Electronic shopping and mail-order houses', 'eCommerce?']= 1
PA_Data_df.loc[PA_Data_df['Meaning of NAICS code']!='Electronic shopping and mail-order houses', 'eCommerce?']= 0
632/26: PA_Data_df['Number of establishments'] = PA_Data_df['Number of establishments'].astype(float)
632/27:
establishments_total = PA_Data_df.groupby('eCommerce?')['Number of establishments'].sum()
establishments_total
632/28:
PA_Data_df.loc[PA_Data_df['Meaning of NAICS code']=='Electronic shopping and mail-order houses', 'eCommerce?']= 1
PA_Data_df.loc[PA_Data_df['Meaning of NAICS code']!='Electronic shopping and mail-order houses', 'eCommerce?']= 0
632/29: PA_Data_df['Number of establishments'] = PA_Data_df['Number of establishments'].astype(float)
632/30:
PAestablishments_total = PA_Data_df.groupby('eCommerce?')['Number of establishments'].sum()
PAestablishments_total
632/31:
# Read with pandas
IN_Data_df = pd.read_csv(IN_Data_csv)
IN_Data_df.head()
632/32:
# Delete extraneous column
IN_Data_df = IN_Data_df.drop(columns=['Geo Footnote', '2017 NAICS Footnote', 'id', '2017 NAICS code', 
                            'Annual payroll ($1,000)', 'First-quarter payroll ($1,000)', 
                            'Number of employees', 'Range indicating percent of total annual payroll imputed', 
                            'Range indicating percent of total employees imputed', 
                           ])
632/33: IN_Data_df = IN_Data_df.replace({'D': '0'})
632/34:
IN_Data_df.loc[IN_Data_df['Meaning of NAICS code']=='Electronic shopping and mail-order houses', 'eCommerce?']= 1
IN_Data_df.loc[IN_Data_df['Meaning of NAICS code']!='Electronic shopping and mail-order houses', 'eCommerce?']= 0
632/35: IN_Data_df['Sales, value of shipments, or revenue ($1,000)'] = IN_Data_df['Sales, value of shipments, or revenue ($1,000)'].astype(float)
632/36:
IN_ecommerce_totals = IN_Data_df.groupby('eCommerce?')['Sales, value of shipments, or revenue ($1,000)'].sum()
IN_ecommerce_totals
632/37:
IN_Data_df.loc[IN_Data_df['Meaning of NAICS code']=='Electronic shopping and mail-order houses', 'eCommerce?']= 1
IN_Data_df.loc[IN_Data_df['Meaning of NAICS code']!='Electronic shopping and mail-order houses', 'eCommerce?']= 0
632/38: IN_Data_df['Number of establishments'] = IN_Data_df['Number of establishments'].astype(float)
632/39:
INestablishments_total = IN_Data_df.groupby('eCommerce?')['Number of establishments'].sum()
INestablishments_total
632/40:
# creating a data frame 
AZ_df = pd.read_csv(AZ_retail_csv) 
AZ_df.head()
632/41: AZ_df = AZ_df.replace({'D': '0'})
632/42:
# Delete extraneous column
AZ_df = AZ_df.drop(columns=['Geo Footnote', '2017 NAICS Footnote', 'id', '2017 NAICS code', 
                            'Annual payroll ($1,000)', 'First-quarter payroll ($1,000)', 
                            'Number of employees', 'Range indicating percent of total annual payroll imputed', 
                            'Range indicating percent of total employees imputed', 
                           ])
632/43:
AZ_df.loc[AZ_df['Meaning of NAICS code']=='Electronic shopping and mail-order houses', 'eCommerce?']= 1
AZ_df.loc[AZ_df['Meaning of NAICS code']!='Electronic shopping and mail-order houses', 'eCommerce?']= 0
632/44:

AZ_df['Sales, value of shipments, or revenue ($1,000)'] = AZ_df['Sales, value of shipments, or revenue ($1,000)'].astype(float)
632/45:

my_results_AZ = AZ_df.groupby('eCommerce?')['Sales, value of shipments, or revenue ($1,000)'].sum()
my_results_AZ
632/46:
AZ_df.loc[AZ_df['Meaning of NAICS code']=='Electronic shopping and mail-order houses', 'eCommerce?']= 1
AZ_df.loc[AZ_df['Meaning of NAICS code']!='Electronic shopping and mail-order houses', 'eCommerce?']= 0
632/47: AZ_df['Number of establishments'] = AZ_df['Number of establishments'].astype(float)
632/48:
AZestablishments = AZ_df.groupby('eCommerce?')['Number of establishments'].sum()
AZestablishments
632/49:
# creating a data frame 
CA_df = pd.read_csv(CA_retail_csv) 
CA_df.head()
632/50:
# Delete extraneous column
CA_df = CA_df.drop(columns=['Geo Footnote', '2017 NAICS Footnote', 'id', '2017 NAICS code', 
                            'Annual payroll ($1,000)', 'First-quarter payroll ($1,000)', 
                            'Number of employees', 'Range indicating percent of total annual payroll imputed', 
                            'Range indicating percent of total employees imputed', 
                           ])
632/51:
CA_df.loc[CA_df['Meaning of NAICS code']=='Electronic shopping and mail-order houses', 'eCommerce?']= 1
CA_df.loc[CA_df['Meaning of NAICS code']!='Electronic shopping and mail-order houses', 'eCommerce?']= 0
632/52:
CA_totals = CA_df.groupby('eCommerce?')['Sales, value of shipments, or revenue ($1,000)'].sum()
CA_totals
632/53:
CA_df.loc[CA_df['Meaning of NAICS code']=='Electronic shopping and mail-order houses', 'eCommerce?']= 1
CA_df.loc[CA_df['Meaning of NAICS code']!='Electronic shopping and mail-order houses', 'eCommerce?']= 0
632/54:
CAestablishments = CA_df.groupby('eCommerce?')['Number of establishments'].sum()
CAestablishments
632/55:
# creating a data frame 
FL_df = pd.read_csv(FL_retail_csv)
632/56:
# Delete extraneous column
FL_df = FL_df.drop(columns=['Geo Footnote', '2017 NAICS Footnote', 'id', '2017 NAICS code', 
                            'Annual payroll ($1,000)', 'First-quarter payroll ($1,000)', 
                            'Number of employees', 'Range indicating percent of total annual payroll imputed', 
                            'Range indicating percent of total employees imputed', 
                           ])
632/57: FL_df = FL_df.replace({'D': '0'})
632/58:

FL_df.loc[FL_df['Meaning of NAICS code']=='Electronic shopping and mail-order houses', 'eCommerce?']= 1
FL_df.loc[FL_df['Meaning of NAICS code']!='Electronic shopping and mail-order houses', 'eCommerce?']= 0
632/59:

FL_df['Sales, value of shipments, or revenue ($1,000)'] = FL_df['Sales, value of shipments, or revenue ($1,000)'].astype(int)
632/60:
my_resultsFL = FL_df.groupby('eCommerce?')['Sales, value of shipments, or revenue ($1,000)'].sum()
my_resultsFL
632/61:
FL_df.loc[FL_df['Meaning of NAICS code']=='Electronic shopping and mail-order houses', 'eCommerce?']= 1
FL_df.loc[FL_df['Meaning of NAICS code']!='Electronic shopping and mail-order houses', 'eCommerce?']= 0
632/62: FL_df['Number of establishments'] = FL_df['Number of establishments'].astype(float)
632/63:
FLestablishments = FL_df.groupby('eCommerce?')['Number of establishments'].sum()
FLestablishments
632/64:
# 2018 Data
data_2018 = retail_historymelted_df[(retail_historymelted_df.Year == 2018) & (retail_historymelted_df.Type == 'Ecommerce')]
data_2018 = data_2018.nlargest(6, 'value')

# 2017 Data
data_2017 = retail_historymelted_df[(retail_historymelted_df.Year == 2017) & (retail_historymelted_df.Type == 'Ecommerce')]
data_2017 = data_2017.nlargest(6, 'value')

# 2016 Data
data_2016 = retail_historymelted_df[(retail_historymelted_df.Year == 2016) & (retail_historymelted_df.Type == 'Ecommerce')]
data_2016 = data_2016.nlargest(6, 'value')

# 2015 Data
data_2015 = retail_historymelted_df[(retail_historymelted_df.Year == 2015) & (retail_historymelted_df.Type == 'Ecommerce')]
data_2015 = data_2015.nlargest(6, 'value')

# 2014 Data
data_2014 = retail_historymelted_df[(retail_historymelted_df.Year == 2014) & (retail_historymelted_df.Type == 'Ecommerce')]
data_2014 = data_2014.nlargest(6, 'value')

# 1998 Data
data_1998 = retail_historymelted_df[(retail_historymelted_df.Year == 1998) & (retail_historymelted_df.Type == 'Ecommerce')]
data_1998 = data_1998.nlargest(6, 'value')

data_1998
632/65:
# Set x axis and tick locations
x_axis = np.arange(len(data_2018))
tick_locations = [value+0.4 for value in x_axis]


plt.figure(figsize=(20,3))
plt.bar(x_axis, data_2018["value"], color='r', alpha=0.5, align="edge")
plt.xticks(tick_locations, data_2018["Kind_of_Business"], rotation="vertical")

# Set a Title and labels
plt.title("Top Five Ecommerce Product Categories - 2018")
plt.xlabel("Product Category")
plt.ylabel("Total Sales in Millions ($)")
plt.savefig('Top Five Ecommerce Product Categories - 2018.png')
632/66:
# Set x axis and tick locations
x_axis = np.arange(len(data_2017))
tick_locations = [value+0.4 for value in x_axis]


plt.figure(figsize=(20,3))
plt.bar(x_axis, data_2017["value"], color='b', alpha=1, align="edge")
plt.xticks(tick_locations, data_2017["Kind_of_Business"], rotation="vertical")

# Set a Title and labels
plt.title("Top Five Ecommerce Product Categories - 2017")
plt.xlabel("Product Category")
plt.ylabel("Total Sales in Millions ($)")
plt.savefig('Top Five Ecommerce Product Categories - 2017.png')
632/67:
# Set x axis and tick locations
x_axis = np.arange(len(data_2016))
tick_locations = [value+0.4 for value in x_axis]


plt.figure(figsize=(20,3))
plt.bar(x_axis, data_2016["value"], color='r', alpha=1, align="edge")
plt.xticks(tick_locations, data_2016["Kind_of_Business"], rotation="vertical")

# Set a Title and labels
plt.title("Top Five Ecommerce Product Categories - 2016")
plt.xlabel("Product Category")
plt.ylabel("Total Sales in Millions ($)")
plt.savefig('Top Five Ecommerce Product Categories - 2016.png')
632/68:
# Set x axis and tick locations
x_axis = np.arange(len(data_2015))
tick_locations = [value+0.4 for value in x_axis]


plt.figure(figsize=(20,3))
plt.bar(x_axis, data_2015["value"], color='y', alpha=1, align="edge")
plt.xticks(tick_locations, data_2015["Kind_of_Business"], rotation="vertical")

# Set a Title and labels
plt.title("Top Five Ecommerce Product Categories - 2015")
plt.xlabel("Product Category")
plt.ylabel("Total Sales in Millions ($)")
plt.savefig('Top Five Ecommerce Product Categories - 2015.png')
632/69:
# Set x axis and tick locations
x_axis = np.arange(len(data_2014))
tick_locations = [value+0.4 for value in x_axis]


plt.figure(figsize=(20,3))
plt.bar(x_axis, data_2014["value"], color='orange', alpha=0.5, align="edge")
plt.xticks(tick_locations, data_2014["Kind_of_Business"], rotation="vertical")

# Set a Title and labels
plt.title("Top Five Ecommerce Product Categories - 2014")
plt.xlabel("Product Category")
plt.ylabel("Total Sales in Millions ($)")
plt.savefig('Top Five Ecommerce Product Categories - 2014.png')
632/70:
# Set x axis and tick locations
x_axis = np.arange(len(data_1998))
tick_locations = [value+0.4 for value in x_axis]


plt.figure(figsize=(20,3))
plt.bar(x_axis, data_1998["value"], color='purple', alpha=1, align="edge")
plt.xticks(tick_locations, data_1998["Kind_of_Business"], rotation="vertical")

# Set a Title and labels
plt.title("Top Five Ecommerce Product Categories - 1998")
plt.xlabel("Product Category")
plt.ylabel("Total Sales in Millions ($)")
plt.savefig('Top Five Ecommerce Product Categories - 1998.png')
632/71:
#Create Pie Chart
labels = "eCommerce", "Retail"
value = [PA_ecommerce_totals[1.0], PA_ecommerce_totals[0.0]]
colors = ["purple", "blue"]
explode = (0.3,0)

# Create a pie chart based upon the above data
plt.pie(value, labels=labels, colors=colors, explode=explode,
        autopct="%1.1f%%", shadow=True, startangle=180)
plt.title("eCommerce vs Retail 2017, Pennsylvania")

# Create axes which are equal so we have a perfect circle
plt.axis("equal")
plt.show()
plt.savefig("PAPies.png")
632/72:
# Create Bar Chart
labels = ["eCommerce", "Retail"]
PAestablishments = [PAestablishments_total[1.0], PAestablishments_total[0.0]]
x_axis = np.arange(len(PAestablishments))
colors = ["purple", "blue"]

plt.bar(x_axis, PAestablishments, color=colors, align="center")
# Create the ticks for our bar chart's x axis
tick_locations = [value for value in x_axis]
plt.xticks(tick_locations, labels)

# Set the limits of the x & y axis
plt.xlim(-0.75, len(x_axis)-0.25)

plt.title("Number of Establishments, eCommerce vs. Retail - Pennsylvania")
plt.ylabel("Number of Establishments")
plt.savefig("PABar.png")
632/73:
#Create Pie Chart
labels = "eCommerce", "Retail"
value = [IN_ecommerce_totals[1.0], IN_ecommerce_totals[0.0]]
colors = ["purple", "red"]
explode = (0.3,0)

# Create a pie chart based upon the above data
plt.pie(value, labels=labels, colors=colors, explode=explode,
        autopct="%1.1f%%", shadow=True, startangle=180)
plt.title("eCommerce vs Retail 2017, Indiana")

# Create axes which are equal so we have a perfect circle
plt.axis("equal")
plt.show()
plt.savefig("INPies.png")
632/74:
# Create Bar Chart
labels = ["eCommerce", "Retail"]
INestablishments = [INestablishments_total[1.0], INestablishments_total[0.0]]
x_axis = np.arange(len(PAestablishments))
colors = ["purple", "red"]

plt.bar(x_axis, INestablishments, color=colors, align="center")
# Create the ticks for our bar chart's x axis
tick_locations = [value for value in x_axis]
plt.xticks(tick_locations, labels)

# Set the limits of the x & y axis
plt.xlim(-0.75, len(x_axis)-0.25)

plt.title("Number of Establishments, eCommerce vs. Retail - Indiana")
plt.ylabel("Number of Establishments")
plt.savefig("INBar.png")
632/75:
#Create Pie Chart
labels = "eCommerce", "Retail"
value = [my_results_AZ[1.0], my_results_AZ[0.0]]
colors = ["purple", "orange"]
explode = (0.3,0)

# Create a pie chart based upon the above data
plt.pie(value, labels=labels, colors=colors, explode=explode,
        autopct="%1.1f%%", shadow=True, startangle=180)
plt.title("eCommerce vs Retail 2017, Arizona")

# Create axes which are equal so we have a perfect circle
plt.axis("equal")
plt.show()
plt.savefig("AZPies.png")
632/76:
AZestablishments

# Create Bar Chart
labels = ["eCommerce", "Retail"]
AZestablishments_plot = [AZestablishments[1.0], AZestablishments[0.0]]
x_axis = np.arange(len(AZestablishments))
colors = ["purple", "orange"]

plt.bar(x_axis, AZestablishments_plot, color=colors, align="center")
# Create the ticks for our bar chart's x axis
tick_locations = [value for value in x_axis]
plt.xticks(tick_locations, labels)

# Set the limits of the x & y axis
plt.xlim(-0.75, len(x_axis)-0.25)

plt.title("Number of Establishments, eCommerce vs. Retail - Arizona")
plt.ylabel("Number of Establishments")
plt.savefig("AZBar.png")
632/77:
labels = "eCommerce", "Retail"
value = [CA_totals[1.0], CA_totals[0.0]]
colors = ["purple", "pink"]
explode = (0.3,0)

# Create a pie chart based upon the above data
plt.pie(value, labels=labels, colors=colors, explode=explode,
        autopct="%1.1f%%", shadow=True, startangle=180)
plt.title("eCommerce vs Retail 2017, California")

# Create axes which are equal so we have a perfect circle
plt.axis("equal")
plt.show()
plt.savefig("CAPies.png")
632/78:
CAestablishments

# Create Bar Chart
labels = ["eCommerce", "Retail"]
CAestablishments_plot = [CAestablishments[1.0], CAestablishments[0.0]]
x_axis = np.arange(len(CAestablishments))
colors = ["purple", "pink"]

plt.bar(x_axis, CAestablishments_plot, color=colors, align="center")
# Create the ticks for our bar chart's x axis
tick_locations = [value for value in x_axis]
plt.xticks(tick_locations, labels)

# Set the limits of the x & y axis
plt.xlim(-0.75, len(x_axis)-0.25)

plt.title("Number of Establishments, eCommerce vs. Retail - California")
plt.ylabel("Number of Establishments")
plt.savefig("CABar.png")
632/79:
labels = "eCommerce", "Retail"
value = [my_resultsFL[1.0], my_resultsFL[0.0]]
colors = ["purple", "yellow"]
explode = (0.3,0)

# Create a pie chart based upon the above data
plt.pie(value, labels=labels, colors=colors, explode=explode,
        autopct="%1.1f%%", shadow=True, startangle=180)
plt.title("eCommerce vs Retail 2017, Florida")

# Create axes which are equal so we have a perfect circle
plt.axis("equal")
plt.show()
plt.savefig("FLPies.png")
632/80:
FLestablishments
# Create Bar Chart
labels = ["eCommerce", "Retail"]
FLestablishments_plot = [FLestablishments[1.0], FLestablishments[0.0]]
x_axis = np.arange(len(FLestablishments))
colors = ["purple", "yellow"]

plt.bar(x_axis, FLestablishments_plot, color=colors, align="center")
# Create the ticks for our bar chart's x axis
tick_locations = [value for value in x_axis]
plt.xticks(tick_locations, labels)

# Set the limits of the x & y axis
plt.xlim(-0.75, len(x_axis)-0.25)

plt.title("Number of Establishments, eCommerce vs. Retail - Florida")
plt.ylabel("Number of Establishments")
plt.savefig("FLBar.png")
632/81:
CA_df.loc[CA_df['Meaning of NAICS code']=='Electronic shopping and mail-order houses', 'eCommerce?']= 1
CA_df.loc[CA_df['Meaning of NAICS code']!='Electronic shopping and mail-order houses', 'eCommerce?']= 0
CA_df
632/82:
all_states = pd.concat([FL_df, CA_df, AZ_df, IN_Data_df, PA_Data_df])
all_states
632/83:
all_states = all_states.groupby('eCommerce?')['Number of establishments'].sum()
all_states
632/84:
# Create Bar Chart
labels = ["eCommerce", "Retail"]
all_states = [all_states[1.0], all_states[0.0]]
x_axis = np.arange(len(all_states))
colors = ["purple", "blue"]

plt.bar(x_axis, all_states, color=colors, align="center")
# Create the ticks for our bar chart's x axis
tick_locations = [value for value in x_axis]
plt.xticks(tick_locations, labels)

# Set the limits of the x & y axis
plt.xlim(-0.75, len(x_axis)-0.25)

plt.title("Number of Establishments, eCommerce vs. Retail - Pennsylvania")
plt.ylabel("Number of Establishments")
plt.savefig("PABar.png")
632/85:
all_states_ecom = all_states.loc[all_states['Meaning of NAICS code']=='Electronic shopping and mail-order houses']
all_states_ecom
632/86:
all_states = pd.concat([FL_df, CA_df, AZ_df, IN_Data_df, PA_Data_df])
all_states
632/87:
all_states_establishments = all_states.groupby('eCommerce?')['Number of establishments'].sum()
all_states_establishments
632/88:
all_states_ecom = all_states.loc[all_states['Meaning of NAICS code']=='Electronic shopping and mail-order houses']
all_states_ecom
632/89:
all_states_ecom = all_states.groupby('Geographic Area Name')['Number of establishments'].sum()
all_states_ecom
632/90:
all_states_ecom = all_states.groupby('Geographic Area Name')['Number of establishments'].sum()
all_states_ecom.plot(kind='bar')
632/91:
all_states_ecom = all_states.groupby('Geographic Area Name')['Number of establishments'].sum()
all_states_ecom.plot(kind='bar', color='green')
632/92:
all_states_ecom = all_states.groupby('Geographic Area Name')['Number of establishments'].sum()
all_states_ecom.plot(kind='bar', color='green')
plt.title('Ecommerce Establishments by State')
632/93: all_states_ecom
632/94:
all_states_ecom = all_states.groupby('Geographic Area Name')['Number of establishments'].sum()
all_states_ecom.plot(kind='bar', color='green', sort='best')
plt.title('Ecommerce Establishments by State')
632/95:
all_states_ecom = all_states.groupby('Geographic Area Name')['Number of establishments'].sum()
all_states_ecom.plot(kind='bar', color='green')
plt.title('Ecommerce Establishments by State')
632/96: all_states_ecom
632/97:
all_states_ecom = all_states.groupby('Geographic Area Name')['Number of establishments'].sum()
all_states_ecom.plot(kind='bar', color='green')
plt.title('Ecommerce Establishments by State')
632/98:
# Set x axis and tick locations
x_axis = np.arange(len(data_1998))
tick_locations = [value+0.4 for value in x_axis]


plt.figure(figsize=(20,10))
plt.bar(x_axis, data_1998["value"], color='purple', alpha=1, align="edge")
plt.xticks(tick_locations, data_1998["Kind_of_Business"], rotation="vertical")

# Set a Title and labels
plt.title("Top Five Ecommerce Product Categories - 1998")
plt.xlabel("Product Category")
plt.ylabel("Total Sales in Millions ($)")
plt.savefig('Top Five Ecommerce Product Categories - 1998.png')
632/99:
# Set x axis and tick locations
x_axis = np.arange(len(data_2014))
tick_locations = [value+0.4 for value in x_axis]


plt.figure(figsize=(20,10))
plt.bar(x_axis, data_2014["value"], color='orange', alpha=0.5, align="edge")
plt.xticks(tick_locations, data_2014["Kind_of_Business"], rotation="vertical")

# Set a Title and labels
plt.title("Top Five Ecommerce Product Categories - 2014")
plt.xlabel("Product Category")
plt.ylabel("Total Sales in Millions ($)")
plt.savefig('Top Five Ecommerce Product Categories - 2014.png')
632/100:
# Set x axis and tick locations
x_axis = np.arange(len(data_2015))
tick_locations = [value+0.4 for value in x_axis]


plt.figure(figsize=(20,10))
plt.bar(x_axis, data_2015["value"], color='y', alpha=1, align="edge")
plt.xticks(tick_locations, data_2015["Kind_of_Business"], rotation="vertical")

# Set a Title and labels
plt.title("Top Five Ecommerce Product Categories - 2015")
plt.xlabel("Product Category")
plt.ylabel("Total Sales in Millions ($)")
plt.savefig('Top Five Ecommerce Product Categories - 2015.png')
632/101:
# Set x axis and tick locations
x_axis = np.arange(len(data_2016))
tick_locations = [value+0.4 for value in x_axis]


plt.figure(figsize=(20,10))
plt.bar(x_axis, data_2016["value"], color='r', alpha=1, align="edge")
plt.xticks(tick_locations, data_2016["Kind_of_Business"], rotation="vertical")

# Set a Title and labels
plt.title("Top Five Ecommerce Product Categories - 2016")
plt.xlabel("Product Category")
plt.ylabel("Total Sales in Millions ($)")
plt.savefig('Top Five Ecommerce Product Categories - 2016.png')
632/102:
# Set x axis and tick locations
x_axis = np.arange(len(data_2015))
tick_locations = [value+0.4 for value in x_axis]


plt.figure(figsize=(20,10))
plt.bar(x_axis, data_2015["value"], color='y', alpha=1, align="edge")
plt.xticks(tick_locations, data_2015["Kind_of_Business"], rotation="vertical")
plt.annotate(format(p.get_height(), '.1f'), 
                   (p.get_x() + p.get_width() / 2., p.get_height()), 
                   ha = 'center', va = 'center', 
                   xytext = (0, 9), 
                   textcoords = 'offset points')

# Set a Title and labels
plt.title("Top Five Ecommerce Product Categories - 2015")
plt.xlabel("Product Category")
plt.ylabel("Total Sales in Millions ($)")
plt.savefig('Top Five Ecommerce Product Categories - 2015.png')
632/103:
# Set x axis and tick locations
x_axis = np.arange(len(data_2015))
tick_locations = [value+0.4 for value in x_axis]


plt.figure(figsize=(20,10))
plt.bar(x_axis, data_2015["value"], color='y', alpha=1, align="edge")
plt.xticks(tick_locations, data_2015["Kind_of_Business"], rotation="vertical")

# Set a Title and labels
plt.title("Top Five Ecommerce Product Categories - 2015")
plt.xlabel("Product Category")
plt.ylabel("Total Sales in Millions ($)")
plt.savefig('Top Five Ecommerce Product Categories - 2015.png')
632/104:
all_states_ecom = all_states.groupby('Geographic Area Name')['Number of establishments'].sum()
figure = all_states_ecom.plot(kind='bar', color='green')
plt.title('Ecommerce Establishments by State')
plt.fig

for p in figure.patches:
    figure.annotate(format(p.get_height(), '.1f'), 
                   (p.get_x() + p.get_width() / 2., p.get_height()), 
                   ha = 'center', va = 'center', 
                   xytext = (0, 9), 
                   textcoords = 'offset points')
632/105:
all_states_ecom = all_states.groupby('Geographic Area Name')['Number of establishments'].sum()
figure = all_states_ecom.plot(kind='bar', color='green')
plt.title('Ecommerce Establishments by State')

for p in figure.patches:
    figure.annotate(format(p.get_height(), '.1f'), 
                   (p.get_x() + p.get_width() / 2., p.get_height()), 
                   ha = 'center', va = 'center', 
                   xytext = (0, 9), 
                   textcoords = 'offset points')
632/106:
all_states_ecom = all_states.groupby('Geographic Area Name')['Number of establishments'].sum()
figure = all_states_ecom.plot(kind='bar', color='green', size=(20,10))

for p in figure.patches:
    figure.annotate(format(p.get_height(), '.1f'), 
                   (p.get_x() + p.get_width() / 2., p.get_height()), 
                   ha = 'center', va = 'center', 
                   xytext = (0, 9), 
                   textcoords = 'offset points')
632/107:
all_states_ecom = all_states.groupby('Geographic Area Name')['Number of establishments'].sum()
figure = all_states_ecom.plot(kind='bar', color='green')
plt.figure(figsize=(20,10))

for p in figure.patches:
    figure.annotate(format(p.get_height(), '.1f'), 
                   (p.get_x() + p.get_width() / 2., p.get_height()), 
                   ha = 'center', va = 'center', 
                   xytext = (0, 9), 
                   textcoords = 'offset points')
632/108:
plt.figure(figsize=(20,10))
all_states_ecom = all_states.groupby('Geographic Area Name')['Number of establishments'].sum()
figure = all_states_ecom.plot(kind='bar', color='green')


for p in figure.patches:
    figure.annotate(format(p.get_height(), '.1f'), 
                   (p.get_x() + p.get_width() / 2., p.get_height()), 
                   ha = 'center', va = 'center', 
                   xytext = (0, 9), 
                   textcoords = 'offset points')
632/109:
plt.figure(figsize=(20,10))
all_states_ecom = all_states.groupby('Geographic Area Name')['Number of establishments'].sum()
figure = all_states_ecom.plot(kind='bar', color='green', title='Number of Ecommerce Establisment by State')


for p in figure.patches:
    figure.annotate(format(p.get_height(), '.1f'), 
                   (p.get_x() + p.get_width() / 2., p.get_height()), 
                   ha = 'center', va = 'center', 
                   xytext = (0, 9), 
                   textcoords = 'offset points')
632/110: all_states_ecom = pd.DataFrame(all_states_ecom)
632/111: all_states_ecom
632/112:
plt.figure(figsize=(20,10))
all_states_ecom = all_states.groupby('Geographic Area Name')['Number of establishments'].sum()
figure = all_states_ecom.plot(kind='bar', color='green', title='Number of Ecommerce Establisment by State')


for p in figure.patches:
    figure.annotate(format(p.get_height(), '.1f'), 
                   (p.get_x() + p.get_width() / 2., p.get_height()), 
                   ha = 'center', va = 'center', 
                   xytext = (0, 9), 
                   textcoords = 'offset points')
632/113:
plt.figure(figsize=(20,10))
all_states_ecom = all_states_ecom.groupby('Geographic Area Name')['Number of establishments'].sum()
figure = all_states_ecom.plot(kind='bar', color='green', title='Number of Ecommerce Establisment by State')


for p in figure.patches:
    figure.annotate(format(p.get_height(), '.1f'), 
                   (p.get_x() + p.get_width() / 2., p.get_height()), 
                   ha = 'center', va = 'center', 
                   xytext = (0, 9), 
                   textcoords = 'offset points')
632/114:
plt.figure(figsize=(20,10))
all_states_ecom = all_states.groupby('Geographic Area Name')['Number of establishments'].sum()
figure = all_states_ecom.plot(kind='bar', color='green', title='Number of Ecommerce Establisment by State')


for p in figure.patches:
    figure.annotate(format(p.get_height(), '.1f'), 
                   (p.get_x() + p.get_width() / 2., p.get_height()), 
                   ha = 'center', va = 'center', 
                   xytext = (0, 9), 
                   textcoords = 'offset points')
632/115:
plt.figure(figsize=(20,10))
all_states_ecom = all_states.groupby('Geographic Area Name')['Number of establishments'].sum()
all_states_ecom = pd.DataFrame(all_states_ecom)
figure = all_states_ecom.plot(kind='bar', color='green', title='Number of Ecommerce Establisment by State')


for p in figure.patches:
    figure.annotate(format(p.get_height(), '.1f'), 
                   (p.get_x() + p.get_width() / 2., p.get_height()), 
                   ha = 'center', va = 'center', 
                   xytext = (0, 9), 
                   textcoords = 'offset points')
632/116:
plt.figure(figsize=(20,10))
all_states_ecom = all_states.groupby('Geographic Area Name')['Number of establishments'].sum()
all_states_ecom = pd.DataFrame(all_states_ecom)
figure = all_states_ecom.plot(kind='bar', color='green', title='Number of Ecommerce Establisment by State')


for p in figure.patches:
    figure.annotate(format(p.get_height(), '.1f'), 
                   (p.get_x() + p.get_width() / 2., p.get_height()), 
                   ha = 'center', va = 'center', 
                   xytext = (0, 9), 
                   textcoords = 'offset points')
632/117:
plt.figure(figsize=(20,10))
all_states_ecom = all_states.groupby('Geographic Area Name')['Number of establishments'].sum()
all_states_ecom = pd.DataFrame(all_states_ecom)
all_states_ecom.reset_index(inplace=True)
figure = all_states_ecom.plot(kind='bar', x='Geographic Area Name', y='Number of establishments', color='green', title='Number of Ecommerce Establisment by State')


for p in figure.patches:
    figure.annotate(format(p.get_height(), '.1f'), 
                   (p.get_x() + p.get_width() / 2., p.get_height()), 
                   ha = 'center', va = 'center', 
                   xytext = (0, 9), 
                   textcoords = 'offset points')
632/118:
plt.figure(figsize=(20,10))
all_states_ecom = all_states.groupby('Geographic Area Name')['Number of establishments'].sum()
all_states_ecom = pd.DataFrame(all_states_ecom)
figure = all_states_ecom.plot(kind='bar', x='Geographic Area Name', y='Number of establishments', color='green', title='Number of Ecommerce Establisment by State')


for p in figure.patches:
    figure.annotate(format(p.get_height(), '.1f'), 
                   (p.get_x() + p.get_width() / 2., p.get_height()), 
                   ha = 'center', va = 'center', 
                   xytext = (0, 9), 
                   textcoords = 'offset points')
632/119:
plt.figure(figsize=(20,10))
all_states_ecom = all_states.groupby('Geographic Area Name')['Number of establishments'].sum()
all_states_ecom = pd.DataFrame(all_states_ecom)
figure = all_states_ecom.plot(kind='bar', color='green', title='Number of Ecommerce Establisment by State')


for p in figure.patches:
    figure.annotate(format(p.get_height(), '.1f'), 
                   (p.get_x() + p.get_width() / 2., p.get_height()), 
                   ha = 'center', va = 'center', 
                   xytext = (0, 9), 
                   textcoords = 'offset points')
632/120:
plt.figure(figsize=(20,10))
all_states_ecom = all_states.groupby('Geographic Area Name')['Number of establishments'].sum()
figure = all_states_ecom.plot(kind='bar', color='green', title='Number of Ecommerce Establisment by State')


for p in figure.patches:
    figure.annotate(format(p.get_height(), '.1f'), 
                   (p.get_x() + p.get_width() / 2., p.get_height()), 
                   ha = 'center', va = 'center', 
                   xytext = (0, 9), 
                   textcoords = 'offset points')
632/121:
FLestablishments
# Create Bar Chart
labels = ["eCommerce", "Retail"]
FLestablishments_plot = [FLestablishments[1.0], FLestablishments[0.0]]
x_axis = np.arange(len(FLestablishments))
colors = ["purple", "yellow"]

plt.bar(x_axis, FLestablishments_plot, color=colors, align="center")
# Create the ticks for our bar chart's x axis
tick_locations = [value for value in x_axis]
plt.xticks(tick_locations, labels)

for p in FLestablishments_plot.patches:
    FLestablishments_plot.annotate(format(p.get_height(), '.1f'), 
                   (p.get_x() + p.get_width() / 2., p.get_height()), 
                   ha = 'center', va = 'center', 
                   xytext = (0, 9), 
                   textcoords = 'offset points')

# Set the limits of the x & y axis
plt.xlim(-0.75, len(x_axis)-0.25)

plt.title("Number of Establishments, eCommerce vs. Retail - Florida")
plt.ylabel("Number of Establishments")
plt.savefig("FLBar.png")
632/122:
FLestablishments
# Create Bar Chart
labels = ["eCommerce", "Retail"]
FLestablishments_plot = [FLestablishments[1.0], FLestablishments[0.0]]
x_axis = np.arange(len(FLestablishments))
colors = ["purple", "yellow"]

plt.bar(x_axis, FLestablishments_plot, color=colors, align="center")
# Create the ticks for our bar chart's x axis
tick_locations = [value for value in x_axis]
plt.xticks(tick_locations, labels)

# Set the limits of the x & y axis
plt.xlim(-0.75, len(x_axis)-0.25)

plt.title("Number of Establishments, eCommerce vs. Retail - Florida")
plt.ylabel("Number of Establishments")
plt.savefig("FLBar.png")
   1:
import pandas as pd
import json
import requests
import matplotlib.pyplot as plt
import numpy as np
import config
import os
from time import sleep
from config import weather_api_key
from citipy import citipy

url = "http://api.openweathermap.org/"
units = "metric"
query_url = f"{url}appid={weather_api_key}&units={units}"
   2:
from scipy import integrate

cities = []
latitude_longitude = []
#Latitude & Longitude ranges
coords_lat_range = (-90, 90)
coords_lng_range = (-180, 180)
        
coords_lat = np.random.uniform(low=-90, high=90, size=1500)
coords_lng = np.random.uniform(low=-180, high=180, size=1500)
latitude_longitude = zip(coords_lat, coords_lng)

for latitude_longitude in latitude_longitude:
#     time.sleep(5)
    city = citipy.nearest_city(latitude_longitude[0],latitude_longitude[1]).city_name
    
    if city not in cities:
        cities.append(city)
        
        
len(cities)

# To slice the list in for loop do for value in x[0:3]
   3:
# set up lists to hold response info
city_data = []
city_counter = 0

#Loop through the list of cities and perform a request for data on each

for city in cities:
    response = requests.get(query_url + city).json()
    city_counter = city_counter + 1
    A={'lat': response['coord']['lat'],
          'long': response['coord']['lon'],
          'temp': response['main']['temp'],
           'cloudiness': response['weather']['description'],
           'humidity': response['main']['humidity'],
           #'wind_speed': response[]
          'city': city}
    city_data.append(A)

    print(f'{city_counter} {city}')
   4: cities
   5:
# set up lists to hold response info
city_data = []
city_counter = 0

#Loop through the list of cities and perform a request for data on each

for city in cities:
    response = requests.get(query_url + 'jamestown').json()
    city_counter = city_counter + 1
    A={'lat': response['coord']['lat'],
          'long': response['coord']['lon'],
          'temp': response['main']['temp'],
           'cloudiness': response['weather']['description'],
           'humidity': response['main']['humidity'],
           #'wind_speed': response[]
          'city': city}
    city_data.append(A)

    print(f'{city_counter} {city}')
   6:
import pandas as pd
import json
import requests
import matplotlib.pyplot as plt
import numpy as np
import config
import os
from time import sleep
from config import weather_api_key
from citipy import citipy
from scipy.stats import linregress

url = "http://api.openweathermap.org/data/2.5/weather"
units = "metric"
query_url = f"{url}appid={weather_api_key}&units={units}"
   7:
from scipy import integrate

cities = []
latitude_longitude = []
#Latitude & Longitude ranges
coords_lat_range = (-90, 90)
coords_lng_range = (-180, 180)
        
coords_lat = np.random.uniform(low=-90, high=90, size=1500)
coords_lng = np.random.uniform(low=-180, high=180, size=1500)
latitude_longitude = zip(coords_lat, coords_lng)

for latitude_longitude in latitude_longitude:
#     time.sleep(5)
    city = citipy.nearest_city(latitude_longitude[0],latitude_longitude[1]).city_name
    
    if city not in cities:
        cities.append(city)
        
        
len(cities)

# To slice the list in for loop do for value in x[0:3]
   8:
# set up lists to hold response info
city_data = []
city_counter = 0

#Loop through the list of cities and perform a request for data on each

for city in cities:
    response = requests.get(query_url + 'jamestown').json()
    city_counter = city_counter + 1
    A={'lat': response['coord']['lat'],
          'long': response['coord']['lon'],
          'temp': response['main']['temp'],
           'cloudiness': response['weather']['description'],
           'humidity': response['main']['humidity'],
           #'wind_speed': response[]
          'city': city}
    city_data.append(A)

    print(f'{city_counter} {city}')
   9: %history
  10: %history -g -f WeatherPy-Final
  11: %history -g -f WeatherPy-Final
